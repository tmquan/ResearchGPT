{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import time \n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_KEY = os.getenv(\"PINECONE_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmquan/opt/anaconda3/envs/rsgpt/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "import langchain\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.txt',\n",
       " 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.txt',\n",
       " 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .txt',\n",
       " 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.txt',\n",
       " 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.txt',\n",
       " 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.txt',\n",
       " 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.txt',\n",
       " 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.txt',\n",
       " 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.txt',\n",
       " 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.txt',\n",
       " 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.txt',\n",
       " 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.txt',\n",
       " 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.txt',\n",
       " 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.txt',\n",
       " 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.txt',\n",
       " 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.txt',\n",
       " 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.txt',\n",
       " 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.txt',\n",
       " 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.txt',\n",
       " 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect the file\n",
    "datadir = \"data\"\n",
    "filenames = sorted(glob.glob(os.path.join(datadir, r\"*.txt\")))\n",
    "display(filenames)\n",
    "display(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2022 44th Annual International Conference of the IEEE Engineering in Medicine \\\\& Biology Society (EMBC) Scottish Event Campus, Glasgow, UK, July 11-15, 2022 \\n\\nMedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray \\n\\nAbstract{\\\\textemdash} Computed tomography (CT) is an effective medical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multiplanar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qualitative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets are available at https: //github.com/abrilcf/mednerf \\n\\nClinical relevance{\\\\textemdash} Our model is able to infer the anatomical 3D structure from a few or a single-view X-ray, showing future potential for reduced ionising radiation exposure during the imaging process. \\n\\nI. INTRODUCTION \\n\\n3D medical imaging often involves joining multiple 2D slices from CT or Magnetic Resonance Imaging (MRI), and part of their workflow consists of specifying values for the position of the patient, the imaging source, and the detector. The quality and accuracy of a CT 3D representation require hundreds of X-ray projections with a thin slice thickness [1]. Moreover, this process exposes patients to more ionising radiation than typical X-rays and requires the patient to remain immobile for up to more than 1 hour, depending on the type of test [2]. Continuous 3D representations would give radiologists optics of every point in the internal anatomy captured. While such representations are useful, there are practical challenges in CT due to the increased radiation exposure, angle-dependent structures, and time consumption [3]. \\n\\nThe Neural Radiance Fields (NeRF) [12] model is a recent reformulation for estimating a 3D volumetric representation from images. Such representations encode the radiance field and density of the scene in the parameters of a neural network. The neural network learns to synthesize new views via volume rendering from point samples along cast rays. However, these representations are often captured in controlled settings [13]. First, the scene is taken by a set of fixed cameras within a short time frame. Second, all content in the scene is static and real images often need masking. These constraints prohibit the direct application of NeRF to the medical domain, where the imaging system greatly differs from conventional cameras, and the images are captured over a long time frame hampering the patient{\\\\textquoteright}s stillness. Moreover, the overlapping of anatomical structures in medical images hinders the definition of edges which cannot be easily solved with masking. These aspects explain why the NeRF approach especially shows successes for {\\\\textquotedblleft}natural images{\\\\textquotedblright}. \\n\\nTo address these challenges, we propose MedNeRF, a model that adapts Generative Radiance Fields (GRAF) [14] in the medical domain to render CT projections given a few or even a single-view X-ray. Our approach not only synthesizes realistic images, but also capture the data manifold and provides a continuous representation of how the attenuation and volumetric depth of anatomical structures vary with the viewpoint without 3D supervision. This is achieved via a new discriminator architecture that provides a stronger and more comprehensive signal to GRAF when dealing with CT scans. \\n\\nWe render CT projections of our two datasets of digitally reconstructed radiographs (DRR) from chest and knee. We qualitative and quantitative demonstrate high-fidelity renderings and compare our approach to other recent radiance field-based methods. Furthermore, we render CT projections of a medical instance given a single-view X-ray and show the effectiveness of our model to cover surface and internal structures. \\n\\nII. METHODS \\n\\nTo train our models, we generate DRRs instead of collecting paired X-rays and corresponding CT reconstructions, which would expose patients to more radiation. Furthermore, DRR generation removes patient data and enables control in capture ranges and resolutions. We generated DRRs by using 20 CT chest scans from [15], [16] and five CT knee scans from [17], [18]. These scans cover a diverse group of patients at different contrast types showing both normal and abnormal anatomy. The radiation source and imaging panel are assumed to rotate around the vertical-axis, generating a DRR of 128 {\\\\texttimes} 128 resolution at every five degrees, resulting in 72 DRRs for each object. During training we use the whole set of 72 DRRs (a fifth of all views within a full 360-degree vertical rotation) per patient and let the model render the rest. Our work did not involve experimental procedures on human subjects or animals and thus did not require Institutional Review Board approval. \\n\\nGRAF [14] is a model that builds from NeRF and defines it within an Generative Adversarial Network (GAN). It consists of a generator G\\\\ensuremath{\\\\theta} that predicts an image patch P pred and a discriminator D\\\\ensuremath{\\\\phi} that compares the predicted patch to a patch P real extracted from a real image. GRAF has shown an effective capacity to disentangle 3D shape and viewpoint of objects from 2D images alone, in contrast to the original NeRF [12] and similar approaches such as [19]. Therefore, we aim to translate GRAF{\\\\textquoteright}s methods to our task, and in subsection II-C we describe our new discriminator architecture, which allows us to disentangle 3D properties from DRRs. \\n\\nWe consider the experimental setting to obtain the radiation attenuation response instead of the color used in natural images. To obtain the attenuation response at a pixel location for an arbitrary projection K with pose \\\\ensuremath{\\\\xi}, first, we consider a pattern \\\\ensuremath{\\\\nu} = (u, s) to sample R X-ray beams within a K {\\\\texttimes} K image-patch P . Then, we sample N 3D points x i r along the X-ray beam r originating from the pixel location and ordered between the near and far planes of the projection (Fig. 1a). \\n\\nThe object representation is encoded in a multi-layer perceptron (MLP) that takes as input a 3D position x = (x, y, z) and a viewing direction d = (\\\\ensuremath{\\\\theta}, \\\\ensuremath{\\\\phi}), and produces \\n\\nFig. 1. An overview of GRAF{\\\\textquoteright}s generator. \\n\\nas output a density scalar \\\\ensuremath{\\\\sigma} and a pixel value c. To learn high-frequency features, the input is mapped into a 2Ldimensional representation (Fig. 1b): \\n\\n(1) \\n\\nwhere p represents the 3D position or viewing direction, for j = 0, ..., m \\\\ensuremath{-} 1. \\n\\nFor modeling the shape and appearance of anatomical structures, let zs \\\\ensuremath{\\\\sim} ps and za \\\\ensuremath{\\\\sim} pa be the latent codes sampled from a standard Gaussian distribution, respectively (Fig. 1c). To obtain the density prediction \\\\ensuremath{\\\\sigma}, the shape encoding q is transformed to volume density through a density head \\\\ensuremath{\\\\sigma}\\\\ensuremath{\\\\theta}. Then, the network g\\\\ensuremath{\\\\theta}({\\\\textperiodcentered}) operates on a shape encoding q = (\\\\ensuremath{\\\\gamma}(x), zs) that is later concatenated with the positional encoding of d and appearance code za (Fig. 1c): \\n\\n(2) \\n\\n(3) \\n\\n(4) \\n\\nThe final pixel response cr is computed by the compositing operation (Fig. 1c): \\n\\n(5) \\n\\nwhere \\\\ensuremath{\\\\alpha}ri = 1 \\\\ensuremath{-} exp (\\\\ensuremath{-}\\\\ensuremath{\\\\sigma}ri \\\\ensuremath{\\\\delta}ri ) is the alpha compositing value of sampled point i and \\\\ensuremath{\\\\delta}ri =\\\\ensuremath{\\\\parallel} xri+1 \\\\ensuremath{-} xri \\\\ensuremath{\\\\parallel}2 is the distance between the adjacent sampled points. \\n\\nIn this way, both the density and pixel values are computed at each sampled point along the beam r with network g\\\\ensuremath{\\\\theta}. Finally, combining the results of all R beams, the generator G\\\\ensuremath{\\\\theta} predicts an image patch P pred, as illustrated in Fig. 1d. \\n\\nWe investigate how we can adapt GRAF to the medical domain and apply it to render a volumetric representation from DRRs. Leveraging a large dataset, GRAF{\\\\textquoteright}s discriminator D\\\\ensuremath{\\\\phi} is able to continuously provide useful signals to train the generator G\\\\ensuremath{\\\\theta}. However, medical datasets like those considered in our problem are generally small, which causes two sequential issues: \\n\\nBrittle adversarial training: With a limited training dataset, the generator or discriminator may fall into ill-posed settings such as mode collapse, which would lead to generating a limited number of instances and consequently, a suboptimal data distribution estimation. While some works have applied data augmentation techniques to leverage more data in the medical domain, some transformations could mislead the generator to learn the infrequent or even non-existent augmented data distribution [20]. We find that naively applying classic data augmentation works less favorably than our adopted framework. \\n\\nTo assess global structure in decoded patches from D\\\\ensuremath{\\\\phi}, we use the Learned Perceptual Image Patch Similarity (LPIPS) metric [22]. We compute the weighted pairwise image distance between two VGG16 feature spaces, where the pretrained weights are fit to better match human perceptual judgments. The additional discriminator loss is therefore: \\n\\nwhere \\\\ensuremath{\\\\phi}i({\\\\textperiodcentered}) denotes the ith layer output of a pretrained VGG16 network, and w, h, and d stand for the width, height and depth of a feature space, respectively. Let G be the processing on the intermediate feature-maps f from D\\\\ensuremath{\\\\phi}, and T the processing on real image patches. When coupled with this additional reconstruction loss, the network learns representations that transfer across tasks. \\n\\nWe improve learning of G\\\\ensuremath{\\\\theta} and D\\\\ensuremath{\\\\phi} by adopting the Data Augmentation Optimized for GAN (DAG) framework [20] in which a data augmentation transformation Tk (Fig. 2b) is applied using multiple discriminator heads \\\\{Dk\\\\}. To further reduce memory usage, we share all layers of D\\\\ensuremath{\\\\phi} except the last layers corresponding to each head (Fig. \\n\\nFig. 2. An overview of our discriminator with self-supervised learning and DAG. \\n\\n2c). Because applying differentiable and invertible data augmentation transformations Tk has the Jenssen-Shannon (JS) preserving property [20]: \\n\\n(7) \\n\\nwhere p Td k is the transformed training data distribution and the transformed distribution captured by G\\\\ensuremath{\\\\theta}. By using a total of four transformations combining flipping and rotation, we encourage optimization to the original data distribution, which also brings the most performance boost. These choices allow our model to benefit from not only JS(pd \\\\ensuremath{\\\\parallel} pg) but also thereby improving the learning of G\\\\ensuremath{\\\\theta} and generalization of D\\\\ensuremath{\\\\phi}. Furthermore, using multiple discriminators with weight-sharing provides learning regularization of D\\\\ensuremath{\\\\phi}. \\n\\nReplacing GRAF{\\\\textquoteright}s logistic objective with a hinge loss, we then define our overall loss as below: \\n\\n(8) \\n\\nL(\\\\ensuremath{\\\\theta}, \\\\ensuremath{\\\\phi}k) = \\n\\nwhere f(u) = max(0, 1+u). We optimize this loss with n = 4, where k = 0 corresponds to the identity transformation and \\\\ensuremath{\\\\lambda} = 0.2 (as in [20]). \\n\\nAfter training a model, we reconstruct the complete X-ray projections within a full vertical rotation of a medical instance given a single view X-ray. We follow the relaxed reconstruction formulation in [23], which fits the generator to a single image. Then, we allow the parameters of the generator G\\\\ensuremath{\\\\theta} to be slightly fine-tuned along with the shape and appearance latent vectors zs and za. The distortion and perception tradeoff is well known in GAN methods [24] and therefore we modify our generation objective by adding the distortion Mean Square Error (MSE) loss, which incentivises \\n\\nFig. 3. Knee renderings from continuous viewpoint rotations showing tissue and bone. Given a single-view X-ray from a CT, we can generate the complete set of CT-projections within a full vertical rotation by slightly fine-tuning a pretrained model along with the shape and appearance latent codes. \\n\\nTABLE I. Quantitative results based on PSNR and SSIM of rendered X-ray projections with single-view X-ray input. \\n\\na balance between blurriness and accuracy: \\n\\nwhere NLLL corresponds to the negative log-likelihood loss and the tuned hyperparameters lr = 0.0005, \\\\ensuremath{\\\\beta}1 = 0, \\\\ensuremath{\\\\beta}2 = 0.999, \\\\ensuremath{\\\\lambda}1 = 0.3, \\\\ensuremath{\\\\lambda}2 = 0.1 and \\\\ensuremath{\\\\lambda}3 = 0.3. \\n\\nOnce the model locates an optimal combination of zs and za, we replicate them and use them to render the rest of the X-ray projections by continuously controlling the angle viewpoint. \\n\\nIII. RESULTS \\n\\nHere we provide an evaluation of MedNeRF on our datasets. We compare our model{\\\\textquoteright}s results to the ground truth, two baselines, perform an ablation study, and show qualitative and quantitative evaluations. We train all models for 100,000 iterations with a batch size of 8. Projection parameters (u, v) are chosen to evenly sample points on the surface of a sphere, specifically a slight horizontal elevation of 70-85 degrees and umin = 0, umax = 1 for a full 360-degree vertical rotation. However, we only provide a fifth of the views (72-views each at five degrees) during training and let the model render the rest. \\n\\nWe evaluate our model{\\\\textquoteright}s representation for 3D-aware DRR synthesis given a single-view X-ray as input. We find that despite the implicit linear network{\\\\textquoteright}s limited capacity, our model can disentangle 3D anatomy identity and attenuation response of different medical instances, which are retrieved through the described reconstruction reformulation in II-C.3. \\n\\nOur model can also facilitate distinguishing bone from tissue via a contrast transformation, as it renders a brighter pixel value for denser structures (e.g. bone) (Fig. 3). \\n\\nTable I summarises our results based on the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), which measure the quality of reconstructed signals and human subjective similarity, respectively. We find that our generative loss can achieve a reasonable perception-distortion curve in renderings and show consistency with the location and volumetric depth of anatomical structures at continuous viewpoints compared to the ground truth. \\n\\nWe evaluate our model on the task of 2D rendering and compare it to pixelNeRF [19], and GRAF [14] baseline, wherein the original architecture is used. Our model can more accurately estimate volumetric depth compared to \\n\\nTABLE II. FID and KID analysis comparing other methods. \\n\\nTABLE III. FID and KID analysis of ablations of our model. \\n\\nGRAF and pixelNeRF (Fig. 4). For each category, we find an unseen target instance with a similar view direction and shape. Volumetric depth estimation is given by bright colors (far) and dark colors (near). Lacking a perceptual loss, GRAF is not incentivized to produce high-frequency textures. In contrast, we find our model renders a more detailed internal structure with varied attenuation. GRAF produces a consistent attenuation response, but seems to be unable to distinguish the anatomical shape from the background. Our self-supervised discriminator enables the generator to disentangle shape and background by rendering a brighter color for the background and a darker color for the shape, while GRAF renders a bright or dark color for both. \\n\\nWe find pixelNeRF produces blurred attenuation renderings for all datasets, and volumetric maps tend to exhibit strong color shifts (Fig. 4). We believe these artifacts are due to the see-through nature of the dataset, compared to solid-like natural objects on which NeRFs are trained. This data characteristic impairs not only volumetric maps but also fine anatomical structures. In contrast, our model is better able to render both volumetric depth and attenuation response. We also find pixelNeRF is sensitive to slight changes in projection parameters, hampering optimization for the knee category. Our model produces a consistent 3D geometry and does not rely on explicit projection matrices. \\n\\nTable II compares image quality based on Frechet Inception Distance (FID) and Kernel Inception Distance (KID) metrics, in which lower values mean better. Optimizing pixelNeRF on our datasets leads to particularly poor results that are unable to compete with the GRAF baseline and our model. In contrast, our model outperforms the baselines on FID and KID metrics for all datasets. \\n\\nIV. CONCLUSION \\n\\nWe have presented a novel Deep Learning architecture based on Neural Radiance Fields for learning a continuous representation of CT scans. We learn a medical category encoding of the attenuation response of a set of 2D DRRs in the weights of a generator. Furthermore, we have found that a stronger and more comprehensive signal from our discriminator allows generative radiance fields to model 3Daware CT-projections. Experimental evaluation demonstrates significant qualitative and quantitative reconstructions and improvements over other Neural Radiance Field approaches. Whilst the proposed model may not replace CT entirely, the functionality of generating 3D-aware CT-projections from X-rays has great potential for clinical use in osseous trauma, skeletal evaluation in dysplasia and for orthopaedic pre-surgical planning. This could cut down on the radiation dose given to patients, with significant economic implications such as bringing down the cost of investigations. \\n\\nACKNOWLEDGMENT \\n\\nThis work is partially supported by the Mexican Council of Science and Technology (CONACyT). \\n\\nREFERENCES \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.txt'}),\n",
       " Document(page_content='Plenoxels: Radiance Fields without Neural Networks \\n\\nAlex Yu \\n\\nBenjamin Recht \\n\\nMatthew Tancik \\n\\nAngjoo Kanazawa \\n\\nQinhong Chen \\n\\nUC Berkeley \\n\\nWe introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels. \\n\\n1. Introduction \\n\\nA recent body of research has capitalized on implicit, coordinate-based neural networks as the 3D representation to optimize 3D volumes from calibrated 2D image supervision. In particular, Neural Radiance Fields (NeRF) [28] demonstrated photorealistic novel viewpoint synthesis, capturing scene geometry as well as view-dependent effects. This impressive quality, however, requires extensive computation time for both training and rendering, with training lasting more than a day and rendering requiring 30 seconds per frame, on a single GPU. Multiple subsequent papers [9, 10, 21, 37, 38, 59] reduced this computational cost for rendering, but single GPU training still requires multiple hours, a bottleneck that limits the practical application of photorealistic volumetric reconstruction. \\n\\nIn this paper, we show that we can train a radiance field from scratch, without neural networks, while maintaining NeRF quality and reducing optimization time by two orders of magnitude. We provide a custom CUDA [31] implementation that capitalizes on the model simplicity to achieve substantial speedups. Our typical optimization time on a single Titan RTX GPU is 11 minutes on bounded scenes (compared to roughly 1 day for NeRF, more than a 100 speedup) and 27 minutes on unbounded scenes (compared \\n\\nFigure 1. Plenoxel: Plenoptic Volume Elements for fast optimization of radiance fields. We show that direct optimization of a fully explicit 3D model can match the rendering quality of modern neural based approaches such as NeRF while optimizing over two orders of magnitude faster. \\n\\nto roughly 4 days for NeRF++ [60], again more than a 100 speedup). Although our implementation is not optimized for fast rendering, we can render novel viewpoints at interactive rates (15 fps). If faster rendering is desired, our optimized Plenoxel model can be converted into a PlenOctree [59]. \\n\\nSpecifically, we propose an explicit volumetric representation, based on a view-dependent sparse voxel grid without any neural networks. Our model can render photorealistic novel viewpoints and be optimized end-to-end from calibrated 2D photographs, using the differentiable rendering loss on training views along with a total variation regularizer. \\n\\nWe call our model Plenoxel for plenoptic volume elements, as it consists of a sparse voxel grid in which each voxel stores density and spherical harmonic coefficients, which model view dependence [1]. By interpolating these coefficients, Plenoxels achieve a continuous model of the plenoptic function [1]: the light at every position and in every direction inside a volume. To achieve high resolution on a single GPU, we prune empty voxels and follow a coarse to fine optimization strategy. Although our core model is a bounded voxel grid, we show that unbounded scenes can be modeled by using normalized device coordinates (for forward-facing scenes) or by surrounding our grid with multisphere images to encode the background (for 360 scenes). \\n\\nOur method reveals that photorealistic volumetric reconstruction can be approached using standard tools from inverse problems: a data representation, a forward model, a regularization function, and an optimizer. Our method shows that each of these components can be simple and state of the art results can still be achieved. Our experiments suggest the key element of Neural Radiance Fields is not the neural network but the differentiable volumetric renderer. \\n\\n2. Related Work \\n\\nClassical Volume Reconstruction. We begin with a brief overview of classical methods for volume reconstruction, focusing on those which find application in our work. The most common classical methods for volume rendering are voxel grids [7,14,19,22,43{\\\\textendash}45,53,54] and multi-plane images (MPIs) [27,34,48,49,58,62]. Voxel grids are capable of representing arbitrary topologies but can be memory limited at high resolution. One approach for reducing the memory requirement for voxel grids is to encode hierarchical structure, for instance using octrees [12,40,52,55] (see [17] for a survey); we use an even simpler sparse array structure. Using these grid-based representations combined with some form of interpolation [54] produces a continuous representation that can be arbitrarily resized using standard signal processing methods (see [32] for reference). We combine this classical sampling and interpolation paradigm with the forward volume rendering formula introduced by Max [24] (based on work from Kajiya and Von Herzen [13] and used in NeRF) to directly optimize a 3D grid from indirect 2D observations. We further extend these classical approaches by modeling view dependence, which we accomplish by optimizing spherical harmonic coefficients for each color channel at each voxel. Spherical harmonics are a standard basis for functions over the sphere, and have been used previously to represent view dependence [4,36,47,58,59]. \\n\\nOf these methods, Neural Volumes [22] is the most similar to ours in that it uses a voxel grid with interpolation, but optimizes this grid through a convolutional neural network and applies a learned warping function to improve the effective resolution (of a 1283 grid). We show that the voxel grid can be optimized directly and high resolution can be achieved by pruning and coarse to fine optimization, without any neural networks or warping functions. \\n\\nAccelerating NeRF. In light of the substantial computational requirements of NeRF for both training and rendering, many recent papers have proposed methods to improve efficiency, particularly for rendering. Among these methods are some that achieve speedup by subdividing the 3D volume into regions that can be processed more efficiently [21,37]. Other speedup approaches have focused on a range of computational and pre-or post-processing methods to remove bottlenecks in the original NeRF formulation. JAXNeRF [8], a JAX [5] reimplementation of NeRF, offers a speedup for both training and rendering via parallelization across many GPUs or TPUs. AutoInt [20] restructures the coordinate-based MLP to compute ray integrals exactly, for more than 10 faster rendering with a small loss in quality. Learned Initializations [51] employs meta-learning on many scenes to start from a better MLP initialization, for both \\\\ensuremath{>} 10 faster training and better priors when per-scene data is limited. Other methods [15,29,35] achieve speedup by predicting a surface or sampling near the surface, reducing the number of samples necessary for rendering each ray. \\n\\nAnother approach is to pretrain a NeRF (or similar model) and then extract it into a different data structure that can support fast inference [9,10,38,59]. In particular, PlenOc-trees [59] extracts a NeRF variant into a sparse voxel grid in which each voxel represents view-dependent color using spherical harmonic coefficients. Because the extracted PlenOctree can be further optimized, this method can speed up training by roughly 3, and because it uses an efficient GPU octree implementation without any MLP evaluations, \\n\\nFigure 2. Overview of our sparse Plenoxel model. Given a set of images of an object or a scene, we optimize a (a) sparse voxel ({\\\\textquotedblleft}Plenoxel{\\\\textquotedblright}) grid with density and spherical harmonic coefficients at each voxel. To render a ray, we (b) compute the color and opacity of each sample point via trilinear interpolation of the neighboring voxel coefficients. We integrate the color and opacity of these samples using (c) differentiable volume rendering, following the recent success of NeRF [28]. The voxel coefficients can then be (d) optimized using the standard MSE reconstruction loss relative to the training images, along with a total variation regularizer. \\n\\nit achieves \\\\ensuremath{>} 3000 rendering speedup. Our method extends PlenOctrees to perform end-to-end optimization of a sparse voxel representation with spherical harmonics, offering much faster training (two orders of magnitude speedup compared to NeRF). Our Plenoxel model is a generalization of PlenOctrees to support sparse plenoptic voxel grids of arbitrary resolution (not necessary powers of two) with the ability to perform trilinear interpolation, which is easier to implement with this sparse voxel structure. \\n\\n3. Method \\n\\nOur model is a sparse voxel grid in which each occupied voxel corner stores a scalar density  and a vector of spherical harmonic (SH) coefficients for each color channel. From here on we refer to this representation as Plenoxels. The density and color at an arbitrary position and viewing direction are determined by trilinearly interpolating the values stored at the neighboring voxels and evaluating the spherical harmonics at the appropriate viewing direction. Given a set of calibrated images, we optimize our model directly using the rendering loss on training rays. Our model is illustrated in Fig. 2 and described in detail below. \\n\\nWe use the same differentiable model for volume rendering as in NeRF, where the color of a ray is approximated by integrating over samples taken along the ray: \\n\\n(1) \\n\\n(2) \\n\\nTi represents how much light is transmitted through ray r to sample i, (1  exp(i  i)) denotes how much light is contributed by sample i, i denotes the density of sample i, and ci denotes the color of sample i, with distance i to the next sample. Although this formula is not exact (it assumes single-scattering [13] and constant values between samples [24]), it is differentiable and enables updating the 3D model based on the error of each training ray. \\n\\nSimilar to PlenOctrees [59], we use a sparse voxel grid for our geometry model. However, for simplicity and ease of implementing trilinear interpolation, we do not use an octree for our data structure. Instead, we store a dense 3D index array with pointers into a separate data array containing values for occupied voxels only. Like PlenOctrees, each occupied voxel stores a scalar density  and a vector of spherical harmonic coefficients for each color channel. Spherical harmonics form an orthogonal basis for functions defined over the sphere, with low degree harmonics encoding smooth (more Lambertian) changes in color and higher degree harmonics encoding higher-frequency (more specular) effects. The color of a sample ci is simply the sum of these harmonic basis functions for each color channel, weighted by the corresponding optimized coefficients and evaluated at the appropriate viewing direction. We use spherical harmonics of degree 2, which requires 9 coefficients per color channel for a total of 27 harmonic coefficients per voxel. We use degree 2 harmonics because PlenOctrees found that higher order harmonics confer only minimal benefit. \\n\\nPlenoxel grid uses trilinear interpolation to define a continuous plenoptic function throughout the volume. This is in contrast to PlenOctrees, which assumes that the density and spherical harmonic coefficients remain constant inside \\n\\neach voxel. This difference turns out to be an important factor in successfully optimizing the volume, as we discuss below. All coefficients (for density and spherical harmonics) are optimized directly, without any special initialization or pretraining with a neural network. \\n\\nThe density and color at each sample point along each ray are computed by trilinear interpolation of density and harmonic coefficients stored at the nearest 8 voxels. We find that trilinear interpolation significantly outperforms a simpler nearest neighbor interpolation; see Tab. 1. The benefits of interpolation are twofold: interpolation increases the effective resolution by representing sub-voxel variations in color and density, and interpolation produces a continuous function approximation that is critical for successful optimization. Both of these effects are evident in Tab. 1: doubling the resolution of a nearest-neighbor-interpolating Plenoxel closes much of the gap between nearest neighbor and trilinear interpolation at a fixed resolution, yet some gap remains due to the difficulty of optimizing a discontinuous model. Indeed, we find that trilinear interpolation is more stable with respect to variations in learning rate compared to nearest neighbor interpolation (we tuned the learning rates separately for each interpolation method in Tab. 1, to provide close to the best number possible for each setup). \\n\\nTable 1. Ablation over interpolation method. Results are averaged over the 8 NeRF synthetic scenes. We find that trilinear interpolation provides dual benefits of improving effective resolution and improving optimization, such that trilinear interpolation at resolution 1283 outperforms nearest neighbor interpolation at 2563 . \\n\\nWe optimize voxel densities and spherical harmonic coefficients with respect to the mean squared error (MSE) over rendered pixel colors, with total variation (TV) regularization [41]. Specifically, our base loss function is: \\n\\n(3) \\n\\nWhere the MSE reconstruction loss Lrecon and the total variation regularizer LTV are: \\n\\nwith 2x(v,d) shorthand for the squared difference between the dth value in voxel v := (i,j,k) and the dth value in voxel (i+1,j,k) normalized by the resolution, and analogously for 2y(v,d) and 2z(v,d), where D is the total number of density and spherical harmonic (SH) coefficients stored at each voxel. In practice we use different weights for SH coefficients and  values. These weights are fixed for each scene type (bounded, forward-facing, and 360). \\n\\nFor faster iteration, we use a stochastic sample of the rays R to evaluate the MSE term and a stochastic sample of the voxels V to evaluate the TV term in each optimization step. We use the same learning rate schedule as JAXNeRF and Mip-NeRF [3,8], but tune the initial learning rate separately for density and harmonic coefficients. The learning rate is fixed for all scenes in all datasets in the main experiments. \\n\\nDirectly optimizing voxel coefficients is a challenging problem for several reasons: there are many values to optimize (the problem is high-dimensional), the optimization objective is nonconvex due to the rendering formula, and the objective is poorly conditioned. Poor conditioning is typically best resolved by using a second order optimization algorithm (e.g. as recommended in [30]), but this is practically challenging to implement for a high-dimensional optimization problem because the Hessian is too large to easily compute and invert in each step. Instead, we use RM-SProp [11] to ease the ill-conditioning problem without the full computational complexity of a second-order method. \\n\\nWith minor modifications, Plenoxels extend to real, unbounded scenes, both forward-facing and 360 . For forward-\\n\\nFigure 3. Ablation over TV regularization. Clear artifacts are visible in the forward-facing scenes without TV on both  and SH coefficients, although PSNR does not always reflect this. \\n\\nfacing scenes, we use normalized device coordinates, as defined in the original NeRF paper [28]. \\n\\nBackground model. For 360 scenes, we augment our sparse voxel grid foreground representation with a multi-sphere image (MSI) background model, which also uses learned voxel colors and densities with trilinear interpolation within and between spheres. Note that this is effectively the same as our foreground model, except the voxels are warped into spheres using the simple equirectangular projection (voxels index over sphere angles  and ). We place 64 spheres linearly in inverse radius from 1 to 1 (we pre-scale the inner scene to be approximately contained in the unit sphere). To conserve memory, we store only rgb channels for the colors (only zero-order SH) and store all layers sparsely by using density thresholding as in our main model. This is similar to the background model in NeRF++ [60]. \\n\\nWe illustrate the importance of TV regularization in Fig. 3. In addition to TV regularization, which encourages smoothness and is used on all scenes, for certain types of scenes we also use additional regularizers. \\n\\nOn the real, forward-facing and 360 scenes, we use a sparsity prior based on the Cauchy loss from SNeRG [10]: \\n\\n(4) \\n\\nwhere (ri(tk)) denotes the density of sample k along training ray i. In each minibatch of optimization on forward-facing scenes, we evaluate this loss term at each sample on each active ray. This is also similar to the sparsity loss used in PlenOctrees [59] and encourages voxels to be empty, which helps to save memory. \\n\\nOn the real, 360 scenes, we also use a beta distribution regularizer on the accumulated foreground transmittance of each ray in each minibatch. This loss term, following Neural Volumes [22], promotes a clear foreground-background decomposition by encouraging the foreground to be either \\n\\nFigure 4. Gradient sparsity. The gradient becomes very sparse spatially within the first 12800 batches (one epoch for the synthetic scenes), with as few as 1\\\\% of the voxels updating per batch in the synthetic case. This enables efficient training via sparse parameter updates. The solid lines show the mean and the shaded regions show the full range of values among all scenes of each type. \\n\\nfully opaque or empty. This beta loss is: \\n\\n(5) \\n\\nwhere r are the training rays and TFG(r) is the accumulated foreground transmittance (between 0 and 1) of ray r. \\n\\nSince sparse voxel volume rendering is not well-supported in modern autodiff libraries, we created a custom PyTorch CUDA [31] extension library to achieve fast differentiable volume rendering. We also provide a slower, higher-level JAX [5] implementation. The speed of our implementation is possible in large part because the gradient of our Plenoxel model becomes very sparse very quickly, as shown in Fig. 4. Within the first 1-2 minutes of optimization, fewer than 10\\\\% of the voxels have nonzero gradients. \\n\\n4. Results \\n\\nWe present results on synthetic, bounded scenes; real, unbounded, forward-facing scenes; and real, unbounded, 360 scenes. We include time trial comparisons with prior work, showing dramatic speedup in training compared to all prior methods (alongside real-time rendering). Quantitative comparisons are presented in Tab. 2, and visual comparisons are shown in Fig. 1, Fig. 6, Fig. 7, and Fig. 8. Our method achieves quality results after even the first epoch of optimization, less than 1.5 minutes, as shown in Fig. 5. \\n\\nWe also present the results from various ablation studies of our method. In the main text we present average results (PSNR, SSIM [56], and VGG LPIPS [61]) over all scenes of each type; full quantitative and visual results on each \\n\\nTable 2. Results. Top: average over the 8 synthetic scenes from NeRF; Middle: the 8 real, forward-facing scenes from NeRF; Bottom: the 4 real, 360 scenes from Tanks and Temples [16]. 4 of the synthetic scenes train in under 10 minutes. *LLFF requires pretrain-ing a network to predict MPIs for each view, and then can render novel scenes without further training; this pretraining is amortized across all scenes so we do not include it in the table. \\n\\nscene, and full experimental details (hyperparameters, etc.) are included in the supplement. \\n\\nOur synthetic experiments use the 8 scenes from NeRF: chair, drums, ficus, hotdog, lego, materials, mic, and ship. Each scene includes 100 ground truth training views with 800  800 resolution, from known camera positions distributed randomly in the upper hemisphere facing the object. Each scene is evaluated on 200 test views, also with resolution 800  800 and known inward-facing camera positions in the upper hemisphere. We provide quantitative comparisons in Tab. 2 and visual comparisons in Fig. 6. \\n\\nWe compare our method to Neural Volumes (NV) [22], a prior grid-based method with a 3D convolutional network, and JAXNeRF [8, 28]. For Neural Volumes we use values reported in [28]; for JAXNeRF we report results from our own rerunning, fixing its centered pixel bug [3]. Our method achieves comparable quality compared to the best baseline, while training in an average of 11 minutes per scene on a single GPU and supporting interactive rendering. \\n\\nWe extend our method to unbounded, forward-facing scenes by using normalized device coordinates (NDC), as \\n\\nFigure 5. 1 minute, 20 seconds. Results on the synthetic scenes after 1 epoch of optimization, an average of 1 minute and 20 seconds. \\n\\nFigure 6. Synthetic, bounded scenes. Example results on the lego and ship synthetic scenes from NeRF [28]. \\n\\nderived in NeRF [28]. Our method is otherwise identical to the version we use on bounded, synthetic scenes, except that we use TV regularization (with a stronger weight) throughout the optimization. This change is likely necessary because of the reduced number of training views for these scenes, as described in Sec. 4.4. \\n\\nOur forward-facing experiments use the same 8 scenes as in NeRF, 5 of which are originally from LLFF [27]. Each scene consists of 20 to 60 forward-facing images captured images reserved as a test set. \\n\\nWe compare our method to Local Light Field Fusion (LLFF) [27], a prior method that uses a 3D convolutional network to predict a grid for each input view, and JAXNeRF. We provide quantitative comparisons in Tab. 2 and visual comparisons in Fig. 7. \\n\\nWe extend our method to real, unbounded 360 scenes by surrounding our sparse voxel grid with an multi-sphere image (MSI, based on multi-plane images introduced by [62]) background model, in which each background sphere is also a simple voxel grid with trilinear interpolation (both within each sphere and between adjacent layers). \\n\\nOur 360 experiments use 4 scenes from the Tanks and Temples dataset [16]: M60, playground, train, and truck. For \\n\\nGround Truth \\n\\nJAXNeRF [8,28] \\n\\nPlenoxels \\n\\nFigure 7. Real, forward-facing scenes. Example results on the fern and orchid forward-facing scenes from NeRF. \\n\\neach scene, we use the same train/test split as [39]. \\n\\nWe compare our method to NeRF++ [60], which augments NeRF with a background model to represent unbounded scenes. We present quantitative comparisons in Tab. 2 and visual comparisons in Fig. 8. \\n\\nIn this section, we perform extensive ablation studies of our method to understand which features are core to its success, with such a simple model. In Tab. 1, we show that continuous (in our case, trilinear) interpolation is responsible for dramatic improvement in fidelity compared to nearest neighbor interpolation (i.e. constant within each voxel) [59]. \\n\\nIn Tab. 3, we consider how our method handles a dramatic reduction in training data, from 100 views to 25 views, on the 8 synthetic scenes. We compare our method to NeRF and find that, despite its lack of complex neural priors, by increasing TV regularization our method can outperform NeRF even in this limited data regime. This ablation also sheds light on why our model performs better with higher TV regularization on the real forward-facing scenes compared to the synthetic scenes: the real scenes have many fewer training images, and the stronger regularizer helps our optimization extend smoothly to sparsely-supervised regions. \\n\\nWe also ablate over the resolution of our Plenoxel grid in Tab. 4 and the rendering formula in Tab. 5. The rendering formula from Max [24] yields a substantial improvement compared to that of Neural Volumes [22], perhaps because it is more physically accurate (as discussed further in the supplement). The supplement also includes ablations over the learning rate schedule and optimizer demonstrating Plenoxel optimization to be robust to these hyperparameters. \\n\\n5. Discussion \\n\\nWe present a method for photorealistic scene modeling and novel viewpoint rendering that produces results with comparable fidelity to the state-of-the-art, while taking orders of magnitude less time to train. Our method is also \\n\\nTable 3. Ablation over the number of views. By increasing our TV regularization, we exceed NeRF fidelity even when the number of training views is only a quarter of the full dataset. Results are averaged over the 8 synthetic scenes from NeRF. \\n\\nTable 4. Ablation over the Plenoxel grid resolution. Results are averaged over the 8 synthetic scenes from NeRF. \\n\\nTable 5. Comparison of different rendering formulas. We compare the rendering formula from Max [24] (used in NeRF and our main method) to the one used in Neural Volumes [22], which uses absolute instead of relative transmittance. Results are averaged over the 8 synthetic scenes from NeRF. \\n\\nstrikingly straightforward, shedding light on the core elements that are necessary for solving 3D inverse problems: a differentiable forward model, a continuous representation (in our case, via trilinear interpolation), and appropriate regularization. We acknowledge that the ingredients for this method have been available for a long time, however nonlinear optimization with tens of millions of variables has only recently become accessible to the computer vision practitioner. \\n\\nLimitations and Future Work. As with any underdeter-mined inverse problem, our method is susceptible to artifacts. Our method exhibits different artifacts than neural methods, as shown in Fig. 9, but both methods achieve similar quality in terms of standard metrics (as presented in Sec. 4). Future work may be able to adjust or mitigate these remaining artifacts by studying different regularization priors and/or more physically accurate differentiable rendering functions. \\n\\nAlthough we report all of our results for each dataset with a fixed set of hyperparameters, there is no optimal a priori \\n\\nFigure 8. Real, 360 scenes. Example results on the playground and truck 360 scenes from Tanks and Temples [16]. \\n\\nFigure 9. Artifacts. JAXNeRF and Plenoxel exhibit slightly different artifacts, as shown here in the specularities in the synthetic drums scene. Note that some artifacts are unavoidable for any underdetermined inverse problem, but the specific artifacts vary depending on the priors induced by the model and regularizer. \\n\\nstructure (such as an octree) may provide additional speedup compared to our sparse array implementation, provided that differentiable interpolation is preserved. \\n\\nSince our method is two orders of magnitude faster than NeRF, we believe that it may enable downstream applications currently bottlenecked by the performance of NeRF{\\\\textendash}for example, multi-bounce lighting and 3D generative models across large databases of scenes. By combining our method with additional components such as camera optimization and large-scale voxel hashing, it may enable a practical pipeline for end-to-end photorealistic 3D reconstruction. \\n\\nsetting of the TV weight TV . Better results may be obtained by tuning this parameter on a scene-by-scene basis, which is possible due to our fast training time. This is expected because the scale, smoothness, and number of training views varies between scenes. \\n\\nOur method should extend naturally to support multi-scale rendering with proper anti-aliasing through voxel cone-tracing, similar to the modifications in Mip-NeRF [3]. Another easy addition is tone-mapping to account for white balance and exposure changes [42], which we expect would help especially in the real 360 scenes. A hierarchical data \\n\\nAcknowledgements \\n\\nWe note that Utkarsh Singhal and Sara Fridovich-Keil previously tried a related idea with point clouds. Additionally, we would like to thank Ren Ng for helpful suggestions and Hang Gao for reviewing the paper draft. The project is generously supported in part by the CONIX Research Center, sponsored by DARPA; Google research faculty award to Angjoo Kanazawa; Benjamin Recht{\\\\textquoteright}s ONR awards N00014-20-1-2497 and N00014-18-1-2833, NSF CPS award 1931853, and the DARPA Assured Autonomy program (FA8750-18-C-0101). SFK and MT are supported by the NSF GRFP. \\n\\nReferences \\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.txt'}),\n",
       " Document(page_content=\"X-CTRSNet: 3D cervical vertebra CT reconstruction and segmentation directly from 2D X-ray images \\n\\nabstract \\n\\nOrthogonal 2D cervical vertebra (C-vertebra) X-ray images have the advantages of high imaging efficiency, low radiation risk, easy operation and low cost for rapid primary clinical diagnoses. Especially in emergency departments, this technique is known to be significantly useful in triage for trauma patients. However, the technique can only provide overlapping anatomic information from limited projection views and is unable to visually exhibit full-view anatomy and precise stereo structures without further CT examination. To promote {\\\\textquoteleft}{\\\\textquoteleft}once is enough'' for visualizing 3D anatomy \\\\& structures and reducing repetitive radiation as much as possible, we proposed X-CTRSNet for 2D X-ray images. This is the first powerful work that simultaneously and accurately enables 3D C-vertebra CT reconstruction and segmentation directly from orthogonally anteroposterior-and lateral-view 2D X-ray images. X-CTRSNet combines the reciprocally coupled SpaDRNet for reconstruction \\\\& MulSISNet for segmentation, and a RSC Learning for tasks consistency. The experiment shows that X-CTRSNet successfully reconstructs and segments the 3D C-vertebra CT from the 2D X-ray images with a PSNR of 24.58 dB, an SSIM of 0.749, and an average Dice of 80.44\\\\%. All these findings reveal the great potential of X-CTRSNet in clinical imaging and diagnosis to facilitate emergency triage by enabling precise 3D reconstruction and segmentation on 2D X-ray images. \\n\\n{\\\\textcopyright} 2021 Elsevier B.V. All rights reserved. \\n\\nAccurate 3D cervical vertebra (C-vertebra) CT reconstruction and segmentation directly from orthogonal 2D C-vertebra X-ray images is clinically significant to distinctly enable a detailed 3D imaging diagnosis basis for clinicians, and effectively reduce repetitive radiation for patients, especially in assessing \\n\\nFig. 1. 3D C-vertebra computed tomography (CT) scan can provide full-view anatomy and enable precise stereo structure, compared to the 2D X-ray images. \\n\\nand the no-overlapping distinct anatomic structure from different views [(\\\\ensuremath{<}\\\\ensuremath{>})11{\\\\textendash}(\\\\ensuremath{<}\\\\ensuremath{>})13] such as the axial, coronal and sagittal planes in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1 (b1), (b2) \\\\& (b3). However, additional pricey CT scans may cause repetitive radiation and unnecessary medical resources occupation of over-treatment, and itself also has a high-dose radiation risk due to multi-slice dense projection [(\\\\ensuremath{<}\\\\ensuremath{>})4]. Besides, for the rapid triage of trauma patients in emergency department, it also needs too much time in the race against time, and may overwhelm medical resources in a short time [(\\\\ensuremath{<}\\\\ensuremath{>})14]. (3) For efficient image interpretation, surgical planning and objective assessment, 3D C-vertebra segmentation directly enables the precise stereo biological structures, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1(c). It effectively reflects the morphological shapes, relative locations, and physiological curves for the C-vertebras that has highly flexible anatomy vulnerable to injuries and degeneration. Therefore, it is of great clinical contribution to improve clinical diagnosis efficiency and speed up emergency triage, that with only rapid 2D X-ray image inputs and achieving 3D C-vertebra anatomy and structures as far as possible. It does not aim to replace CT examination completely, but can provide more 3D diagnostic basis on primary 2D X-ray imaging without additional time costs. \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2, we proposed the first powerful work, X-CTRSNet, which simultaneously and accurately enables 3D C-vertebra CT reconstruction and segmentation directly from widely accessible AP and LA view 2D X-ray images. X-CTRSNet is composed of three elements, including Spatial Decomposition-Reconstruction Net (SpaDRNet), Multi-scale Space Interoperability Segmentation Net (MulSISNet), and Reconstruction{\\\\textendash} segmentation Consistency (RSC) Learning. The effects of these three specially-designed elements can be summarized as follows: (1) SpaDRNet (Sect. 2.1) is used to achieve the 3D C-vertebra CT reconstruction directly from AP \\\\& LA projected 2D planes, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(b). It consists of progressive 2D-to-3D conversion to multi-scale decompose the compressed space to the corresponding stereo location from the overlapped domain, hierarchical 3D fusion to interpretively sort out the 3D spatial information in consistency, and multi-view vgg loss to expressively guide the reconstructed scene content learning. And (2) MulSISNet (Sect. 2.2) further enables 3D C-vertebra semantic segmentation on reconstructed CT, and promotes shape constraints to SpaDRNet, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(c). It comprehensively extracts rich stereo structure covering from the small-scale details to the large-scale distribution with the information densely interoperated among multi-scale 3D feature. Interactively, (3) RSC Learning (Sect. 2.3) enhances reconstruction{\\\\textendash}segmentation consistent with the ground truth (GT) for the multi-task learning, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(d). It promotes segmenting on the CT ground truth (GT), and optimize the divergence of the reconstructed CT{\\\\textquoteright}s \\n\\nsegmentation with it. The contributions of this work are summarized as following: \\n\\nFig. 2. Workflow diagram of X-CTRSNet. \\n\\nFig. 3. X-CTRSNet is achieved via reciprocally coupled SpaDRNet of reconstruction \\\\& MulSISNet of Segmentation, and a RSC Learning of tasks consistency, to simultaneously enable 3D C-vertebra CT reconstruction and segmentation directly from the 2D X-ray images. \\n\\nunstable reconstruction; and deeply feeds back the divergence between two segmentations for the reconstructed biological CT anatomy. \\n\\n2. Methodology \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(a), the proposed X-CTRSNet is conducted on the AP and LA views 2D X-ray images to directly make the 3D C-vertebra CT reconstruction and segmentation. So that it achieves the full-view anatomy and precise stereo structure, making up the shortage in 2D imaging. It is built by three collaborate elements: (1) SpaDRNet (R, Section (\\\\ensuremath{<}\\\\ensuremath{>})2.1) combines progressive 2Dto-3D multi-paths, hierarchical 3D fusion and multi-view vgg loss to decompose the overlapped 2D X-ray images into reconstructing the detailed 3D CT. (2) MulSISNet (S, Section (\\\\ensuremath{<}\\\\ensuremath{>})2.2) extracts the \\n\\nrobust multi-scale stereo features for the reciprocal 3D semantic segmentation on reconstructed CT and shape constraint feedback. (3) RSC Learning (LRSC , Section (\\\\ensuremath{<}\\\\ensuremath{>})2.3) promotes segmenting on the CT GT and optimizes the divergence between two segmentations to drive the reconstruction{\\\\textendash}segmentation consistency. Given the AP \\\\& LA views 2D X-ray images xAP \\\\&xLA, the GT of 3D CT yCT and segmentation ySeg , the target of X-CTRSNet is formulated as: \\n\\n(1) \\n\\nOur SpaDRNet in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(b) innovatively uses progressive 2Dto-3D conversion, hierarchical 3D fusion, and multi-view vgg loss, to decompose the latent dimension space in the overlapped 2D projection, and spatially correspondingly reconstruct into 3D CT images, directly from the AP and LA views X-ray images. \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(b), the progressive 2D-to-3D conversion exploits the 2D collateral multi-paths and the view alignment, to extract the 3D spatial information existing in 2D projection. \\n\\nIn detail, the 3D features with projected information extracted from AP and LA views are firstly permuted to be assigned to each other and 3D CT orientation. For LA-view 3D feature FeatLA3D(XLA, YLA, ZLA), the first dimension XLA represents the length dimension of LA-view X-ray image, and corresponds to the width dimension YCT of 3D CT data. Such real-word geometric correspondence of all dimensions between 3D features of X-ray images and CT data can be described as: \\n\\nXLA \\\\ensuremath{\\\\leftrightarrow} YCT \\n\\nZLA \\\\ensuremath{\\\\leftrightarrow} XCT XAP \\\\ensuremath{\\\\leftrightarrow} XCT \\n\\n(5) \\n\\nwhere XLA, YLA, ZLA are dimensions in LA-view 3D feature FeatLA3D (XLA, YLA, ZLA), XAP , YAP , ZAP mean dimensions in AP-view 3D feature Feat3D YCT , ZCT denote dimensionsAP (XAP , YAP , ZAP ), and XCT , in CT feature FeatCT3D(XCT , YCT , ZCT ). Therefore, to promote view alignment, the permutations of FeatLA3D and FeatAP 3D are formulated as: \\n\\n(6) \\n\\nwhere P is the permutation operation, according to the order [{\\\\textperiodcentered}]. Then, the permuted 3D features Feat '3LAD and Feat '3APD are concatenated along channel as: \\n\\n(7) \\n\\nAnd a following multi-scale 3D fusion is conducted to further merge the multi-view decomposed stereo information and integrate multi-scale structure. Given the permuted 3D features depicted as \\n\\nwhere SeqConv3D({\\\\textperiodcentered}) represents sequentially conducting the 3D 3 {\\\\texttimes} 3 {\\\\texttimes} 3 convolution with Leaky ReLU activation function. D({\\\\textperiodcentered}) is the down-sampling operation composed of consecutive strided 3 {\\\\texttimes} 3 {\\\\texttimes} 3 convolutions with stride 2, U({\\\\textperiodcentered}) represents the simple nearest neighbor sampling following a 1 {\\\\texttimes} 1 {\\\\texttimes} 1 convolution. \\n\\nwhere is UConv({\\\\textperiodcentered}) is the up-sampling operation by using the 3 {\\\\texttimes} 3 {\\\\texttimes} 3 transpose convolution with stride 2. \\n\\nHierarchical 3D fusion thus explicitly interprets the consistent multi-scale structure in 3D C-vertebra CT, and reconstructs the stereo anatomy according to the strong spatial relation inter adjacent scale. \\n\\nFor the assembled expressiveness of the voxels in 3D reconstructed CT, multi-view vgg loss is creatively developed on all planes along the axial, the coronal and the sagittal views. It guides content consistency among the voxels in the 3D scene to express the anatomy. It is directly transferred from the widely accepted pre-trained VGGNet. With the pre-trained VGGNet [(\\\\ensuremath{<}\\\\ensuremath{>})29], the loss extracts the high-level feature representations of expressing [(\\\\ensuremath{<}\\\\ensuremath{>})30] for the scene interpretation, and further constraints in multi-views for the stereo context. Given the reconstructed 3D CT y{\\\\textasciicircum}CT and its GT yCT , multi-view vgg loss is defined as: \\n\\nwhere and are planes exported from along the axial, the coronal and the sagittal views. L, W and H are the length, width and height of y{\\\\textasciicircum}CT . \\n\\nFurthermore, the loss function of SpaDRNet consists of lossMV \\\\ensuremath{-}vgg for anatomy expression and lossMAE for voxel details, as follows: \\n\\n(11) \\n\\nOur MulSISNet in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(c) robustly utilizes multi-scale 3D extraction and interoperation to interactively make the 3D C-vertebra semantic segmentation from the reconstructed CT, and transfers the shape constraint to SpaDRNet. \\n\\nand \\n\\n(12) \\n\\n(13) \\n\\nFinally, the multi-scale stereo features are transmitted into the decoder to summarize the semantics for segmentation, formu-laically described as: \\n\\n2.2.2. Dice loss \\n\\nReciprocally, on the basis of the reconstructed CT, there is a strong relation inter the tasks, so that MulSISNet further transfers the segmentation learning to make shape constraint on the reconstruction in SpaDRNet. The segmentation learning loss is calculated with dice as Eq. ((\\\\ensuremath{<}\\\\ensuremath{>})15), where and are voxel segmentation and its GT at (m, n, p). \\n\\n(15) \\n\\nOur RSC Learning in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3(d) creatively introduces CT GT-to-segmentation learning, to drive the reconstruction{\\\\textendash}segmentation consistency of the interactive tasks. \\n\\nFig. 4. X-CTRSNet anatomically enables a 3D C-vertebra physiological structure illustration. \\n\\n(2) RSC Learning on reconstruction: It enhances the reconstruction as penalizing the reconstruction training with the divergence between the two segmentations of reconstructed CT and CT GT, so that deeply reinforces the reconstructed biological CT anatomical texture. By penalizing the inter-segmentations divergence, it can converge the reconstructed CT to the CT with the real CT consistent segmentation. Through beneficial interaction, RSC Learning enables X-CTRSNet the consistently and precisely coupled tasks reconstruction{\\\\textendash}segmentation. \\n\\nGiven the segmentations y{\\\\textasciicircum}Seg , y{\\\\textasciicircum}GTCT \\\\ensuremath{-}Seg of the reconstructed CT and CT GT, the loss function is defined as: \\n\\n(16) \\n\\n3. Experiments and analysis \\n\\nClinical data from 69 patients were used for the evaluation. The segmentation GT of C-vertebras (ordered as C1,C2,C3,C4,C5, C6 and C7) is labeled by two radiologists with cross-check. Specifically, Instance Normalization and Group Normalization are used in SpaDRNet of reconstruction and MulSISNet of segmentation, respectively. The network is implemented using Tensorflow with the Adam optimizer. The initial learning rate is set as 10\\\\ensuremath{-}3 . Ten-folder cross validation is adopted in the performance evaluation and comparison. The dataset is divided into 10 groups. In the first nine groups, there were 7 patients in each group. And the last group contains 6 patients. In each validation, nine groups are used to train the network, and the last group is used for test. The procedure was repeated 10 times, until all the subjects have been processed. \\n\\nStructural similarity index (SSIM) [(\\\\ensuremath{<}\\\\ensuremath{>})32] and peak signal to noise ratio (PSNR) are employed to evaluate the reconstruction performance, as well as Dice coefficient (Dice) [(\\\\ensuremath{<}\\\\ensuremath{>})33] is used for segmentation assessment. SSIM is defined as \\n\\n(17) \\n\\nwhere {\\\\textmu} means average, \\\\ensuremath{\\\\sigma} is standard deviation, cov({\\\\textperiodcentered}) denotes covariance, c represents variables to stabilize the division with weak denominator. \\n\\nPSNR is calculated as \\n\\n(18) \\n\\n(19) \\n\\nAs the last row in (\\\\ensuremath{<}\\\\ensuremath{>})Table (\\\\ensuremath{<}\\\\ensuremath{>})1 shows, the proposed X-CTRSNet successfully achieves high-performance 3D C-vertebra CT reconstruction and segmentation directly from the 2D X-ray images. It gains a high SSIM of 0.749 and a high PSNR of 24.58 dB for the reconstructed CT, as well as an average Dice up to 80.44\\\\% for the seven segmented C-vertebras (C1,C2,C3,C4,C5,C6 and C7). So that it anatomically enables a 3D cervical vertebra physiological structure illustration, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})4, with {\\\\textquoteleft}{\\\\textquoteleft}once is enough'' fast 2D imaging to speed up the diagnostic procedure and reduce the repetitive radiation. \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table (\\\\ensuremath{<}\\\\ensuremath{>})1, the innovative components designed for X-CTRSNet, including SpaDRNet, lossMV \\\\ensuremath{-}vgg , MulSISNet and RSC Learning, enable robust improvements. By using SpaDRNet, the anatomical structure from the overlapped 2D X-ray images are effectively decomposed layer-by-layer as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})5, thanks to its progressive 2D{\\\\textendash}3D conversion with spatial correspondence. By using lossMV \\\\ensuremath{-}vgg , the performance of reconstruction gains 2.11\\\\% improvement in SSIM and 0.31 dB improvement in PSNR. It is beneficial from the guidance of lossMV \\\\ensuremath{-}vgg for content consistency among voxels in 3D scenes to express biological anatomy. By using MulSISNet, an accurate segmentation is further enabled for 3D morphology extraction, and meanwhile enhance 3D CT reconstruction. It is contributed by the multi-scale 3D extraction and interoperation in MulSISNet, as well as utilizing the reciprocal relation inter tasks. By using RSC Learning, the best performance in both reconstruction and segmentation is further achieved, as RSC Learning promotes the reconstruction{\\\\textendash}segmentation consistency of the interactive tasks with CT GT-to-segmentation learning. \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Figs. (\\\\ensuremath{<}\\\\ensuremath{>})2 \\\\& (\\\\ensuremath{<}\\\\ensuremath{>})3, X-CTRSNet achieves superior accuracy in both tasks compared to the state-of-the-art methods: (1) SIT [(\\\\ensuremath{<}\\\\ensuremath{>})22], PSR [(\\\\ensuremath{<}\\\\ensuremath{>})23] and X2CT-GAN [(\\\\ensuremath{<}\\\\ensuremath{>})24] for reconstruction, as well as (2) 3D UNet [(\\\\ensuremath{<}\\\\ensuremath{>})31], DSN [(\\\\ensuremath{<}\\\\ensuremath{>})34], DenseBiasNet [(\\\\ensuremath{<}\\\\ensuremath{>})35], CS2-Net [(\\\\ensuremath{<}\\\\ensuremath{>})36] and ConResNet [(\\\\ensuremath{<}\\\\ensuremath{>})37] for after-reconstruction 3D segmentation. \\n\\nIn the reconstruction comparison ((\\\\ensuremath{<}\\\\ensuremath{>})Table (\\\\ensuremath{<}\\\\ensuremath{>})2), X-CTRSNet improved the SSIM by 13.78\\\\% on average for accurate anatomical structures, and increased the PSNR by 2.27 dB for clearly readable imaging. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})6, it visually enables clear and robust full-view biologic structures without overlapping that have legible distribution, shape and explicitly readable anatomical texture, and further promotes the precise 3D segmentation of the C-vertebras morphology on the reconstructed stereo CT data. So that X-CTRSNet distinctly provides the detailed 3D imaging diagnosis basis from the 2D X-ray imaging characterized by low radiation risk. But the compared ones behave poorly, which causes rough shape of C-vertebras groups, and fails on each detailed C-vertebra and the interlock relation among C-vertebras. \\n\\nIn the segmentation comparison, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table (\\\\ensuremath{<}\\\\ensuremath{>})3, our proposed method still effectively improves the average Dice with 3.25\\\\%, and comprehensive promotes the more precise segmentation for all C-vertebras C1 to C7. Visually in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})7, it is robust to segment the multi-scale structure components distributed among C-vertebras. As the multi-scale structure components (circled by dotted line in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. (\\\\ensuremath{<}\\\\ensuremath{>})7) including foramen transversarium, spinous process, and vertebral body which cause difficulties to the compared methods. Our X-CTRSNet still precisely segments them for the morphology extraction, thanks to the multi-scale path and interoperation in its sub module MulSISNet. \\n\\nFurthermore, besides the above accuracy comparison in both tasks, the comparison of model complexities is also made. As \\n\\nFig. 5. SpaDRNet effectively decomposes the anatomical structure from the overlapped 2D X-ray images, by progressive 2D{\\\\textendash}3D conversion with spatial correspondence. \\n\\nFig. 6. X-CTRSNet achieves the reconstruction of the detailed CT anatomy of legible distribution, shape and explicitly readable anatomical texture, and thus further promotes the precise 3D structure segmentation. \\n\\nTable 1 \\n\\nX-CTRSNet successfully achieves the accurate reconstruction and segmentation, contributed to its innovative components. \\n\\nTable 2 \\n\\nX-CTRSNet gains superior accuracy in reconstruction compared to the state-of-the-art method, with the acceptable model complexity. \\n\\nFig. 7. X-CTRSNet shows superiority to precisely segment C-vertebra from the interactively reconstructed 3D CT. For the cervical vertebra that has multi-scale structure components, it still makes robust segmentation. As the foramen transversarium, spinous process, and vertebral body of the multi-scale structure components (circled by dotted line) cause difficulties to the compared ones, X-CTRSNet precisely segment them for the morphology extraction. \\n\\nfor one patient on a laptop with one Nvidia RTX 3080 GPU and an Intel i9 CPU. As can be seen, our method just takes less than a second for processing, so that remarkably saves time in clinical 3D CT imaging and analysis, as well as reduces unwanted repetitive radiation of excessive examination, especially for the triage of emergency department. (2) Besides, in clinical applications, the accuracy of the method is a more important priority [(\\\\ensuremath{<}\\\\ensuremath{>})38]. The results of the accuracy comparison show that the performance of our method is significantly better than those of other known methods, gaining 13.78\\\\% improvements in SSIM for accurate anatomical structure, and increasing PSNR by 2.27dB \\n\\nfor clearly readable imaging, as well as improving the average Dice with 3.25\\\\% for precise segmentation. Especially compared with SIT and DSN which have the lowest model complexities for the reconstruction and the 3D segmentation, respectively, our method achieves 18.7\\\\% higher SSIM, 2.58dB higher PSNR and 3.10\\\\% higher Dice to achieve the best model accuracy. Combining both the accuracy and complexities of our method, our method has great potential to effectively and quickly make 3D CT reconstruction and segmentation directly from 2D X-ray images in clinical. \\n\\n4. Conclusion \\n\\nIn this paper, we propose X-CTRSNet, the first powerful work to simultaneously and accurately enable 3D C-vertebra CT reconstruction and segmentation directly from 2D X-ray images. The method is innovatively achieved by the following components: (1) SpaDRNet for the overlapped anatomy decomposition and reconstructing into the pathological information detailed 3D CT; (2) MulSISNet for the multi-scale stereo structure extraction and the further segmentation on the reconstructed CT, where the shape constrains are interpretively fed back; and (3) RSC Learning for the reconstruction{\\\\textendash}segmentation consistency in the interactive multi-tasks. Extensive experiments on reconstruction and segmentation reveal {\\\\textquoteleft}{\\\\textquoteleft}once is enough'' with X-CTRSNet to improve the diagnosis efficiency of 2D X-ray imaging and avoid the repetitive radiation of overtreatment in clinical. \\n\\nCRediT authorship contribution statement \\n\\nRongjun Ge: Conceptualization, Methodology, Software, Validation, Writing {\\\\textendash} original draft, Writing {\\\\textendash} review \\\\& editing. Yuting He: Formal analysis, Investigation. Cong Xia: Resources, Data curation. Chenchu Xu: Methodology. Weiya Sun: Validation. Guanyu Yang: Formal analysis. Junru Li: Validation. Zhihua Wang: Validation. Hailing Yu: Validation. Daoqiang Zhang: Supervision, Project administration. Yang Chen: Project administration, Funding acquisition. Limin Luo: Supervision. Shuo Li: Supervision, Conceptualization. Yinsu Zhu: Resources, Data curation, Writing {\\\\textendash} review \\\\& editing. \\n\\nDeclaration of competing interest \\n\\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. \\n\\nAcknowledgments \\n\\nThis study was funded by the Fundamental Research Funds for the Central University, China (No. NS2021067); the National Natural Science Foundation, China (No. 62101249, 61871117, 62171123 and 81871444); the China Postdoctoral Science Foundation (No. 2021TQ0149); the Natural Science Foundation of Jiangsu Province (No. BK20210291); the State{\\\\textquoteright}s Key Project of Research and Development Plan (No. 2017YFC0109202, 2018YFA0704102). \\n\\nReferences \\n\", metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .txt'}),\n",
       " Document(page_content='Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN \\n\\n2nd Mengxi Zhang \\n\\n3rd Ran Wei \\n\\n4th Bo Liu \\n\\n5th Xiangzhi Bai \\n\\n6th Fugen Zhou \\n\\nAbstract{\\\\textemdash}Computed tomography can provide a 3D view of the patient{\\\\textquoteright}s internal anatomy. However, traditional CT reconstruction methods require hundreds of X-ray projections through a full rotational scan of the body, which cannot be performed on a typical X-ray machine. In order to deal with the impact of organ movement caused by respiration in radiotherapy on the accuracy of radiotherapy, we propose to reconstruct CT from a single X-ray projection view using the conditional variational autoencoder. Conditional variational autoencoder encodes the features of a 2D X-ray projection. The decoder decodes the hidden variables encoded by the encoder and increase data dimension from 2D (X-rays) to 3D (CT) to generates a corresponding 3D CT. In addition, we use the discriminator to distinguish the generated 3D CT from the real 3D CT to make the generated 3D CT more realistic. We demonstrate the feasibility of the approach with 3D CT of two patients with lung cancer. \\n\\nIndex Terms{\\\\textemdash}CT reconstruction, X-ray projection, VAE, GAN \\n\\nI. INTRODUCTION \\n\\nThe precise knowledge of tumor position is important for intra-operative image-guidance of various treatment. For example, in radiotherapy of lung cancer , the respiratory motion causes the change of the position of the tumor and surrounding tissues, which leads to the uncertainty of the radiation dose. It is very necessary to obtain the actual 3D anatomical information during the treatment to guide the treatment and analyse the actual dose distribution. \\n\\nX-ray projections enable us to observe the human body in real time and non-invasively. However, the anatomy is projected onto a plane in X-ray projections. The human tissues overlay each other and reduces the visibility. Computed tomography (CT) imaging can generate 3D volume with high spatial resolution of the internal anatomy of the human body. However, standard CT reconstruction algorithms need a set of X-ray projections rotationally obtained around the patient, which are unavailable during the treatment. \\n\\nFollowing these observations, we propose a method to reconstruct 3D CT from a single X-ray projection image. To summarize, we make the following contributions: \\n\\nFig. 1. The architecture of generator. \\n\\nII. NETWORK ARCHITECTURE \\n\\nOur network, used in reconstruction of volumetric computed tomography images from a single projection view, is similar to other GAN architectures, which involves a generator and a discriminator. \\n\\nTo perform reconstruction of volumetric computed tomography images from a single projection view, the design of our structure follows the conditional variational autoencoder framework. \\n\\nEncoder In the encoder part, the information of 2D X-ray image is encoded. The encoder has 2 branch, one of which encodes the 3D CT samples and the other encodes the a 2D X-ray image as conditional variable. The outputs of each branch are concatenated and mapped to two separate fully-connected layers to generate {\\\\textmu}(Y, X) and \\\\ensuremath{\\\\Sigma}(Y, X), which will be combined with \\\\ensuremath{\\\\Sigma} to create z. Dense connectivity [9] has a compelling advantage in the feature extraction process. To optimally utilize information from 2D X-ray images, we embed dense modules to generator{\\\\textquoteright}s encoding path. Each dense module consists of a down-sampling block, a densely connected convolution block and a compressing block. The cascaded dense modules encode different level information of the input image and pass it to the decoder along different shortcut paths. \\n\\nwhere DKL[a||b] = Ez\\\\ensuremath{\\\\sim}Q[log(a) \\\\ensuremath{-} log(b))] represents the Kullback-Leibler(KL) divergence, Q(z|Y, X) = N(z|{\\\\textmu}(Y, X), \\\\ensuremath{\\\\Sigma}(Y, X)) where {\\\\textmu} and \\\\ensuremath{\\\\Sigma} are arbitrary, deterministic functions learned from the data. \\n\\nSince P(z|X) \\\\ensuremath{\\\\sim} N(0, 1), this choice of Q(z|Y, X) allows us to compute DKL[Q(z|Y, X)||P(z|X)] as the KL-divergence between two Gaussian distribution, which has a closed-form solution. As Q is assumed as a high-capacity function which can approximate P(z|Y, X), DKL[Q(z|X, Y )||P(z|Y, X)] will tend to 0. Therefore, P(Y |X) can be directly optimized through optimizing the right hand side of (1) via stochastic gradient descent. During the training time, we use the reparameterization trick to make the sampling of z differentiable with respect to {\\\\textmu} and \\\\ensuremath{\\\\Sigma}, and define zi = {\\\\textmu}(Y i , Xi) + \\\\ensuremath{\\\\eta}\\\\ensuremath{\\\\Sigma}(Y i , Xi), where \\\\ensuremath{\\\\eta} \\\\ensuremath{\\\\sim} N(0, I).Based on equation (1), the reconstruction network can be implemented. The function Q takes the form of the encoder, encoding Y and X into a d-dimensional latent space z, via {\\\\textmu} and \\\\ensuremath{\\\\Sigma}. \\n\\nFig. 2. The loss of network. \\n\\nDecoder In the decoder part, the conditional dependency on X is explicitly modeled by the concatenation of z with the vector representation of X. Then, the fully connected layer{\\\\textquoteright}s output is reshaped to 3D. Through the model-training process, the transformation module learns the underlying relationship between feature representations across dimension and reshape 2D features to 3D features, making it possible to generate a volumetric CT image from a 2D projection. However, as most of the 2D spatial information gets lost during such conversion, we use skip connection between the encoder and the decoder. It enforces the channel number of the encoder being equal to the one on the corresponding decoder side by a basic 2D convolution block, expand the 2D feature map to a pseudo3D one by duplicating the 2D information along the third axis and use a basic 3D convolution block to encode the pseudo-3D feature map. The abundant low-level information across two parts of the network imposes strong correlations on the shape and appearance between input and output. At test time, the decoder operates as a generative reconstruction network given the 2D X-ray image X, generating 3D CT volume by sampling z \\\\ensuremath{\\\\sim} N(0, I). In particular, we generate the highest-confidence prediction with z = 0. \\n\\nPatchGANs [10] have been used frequently in recent works due to the good generalization property. We adopt a similar architecture in our discriminator network from 3D Patch Discriminator [10]. It consists of 3D convolution layer, instance normalization layer and rectified linear unit. The proposed discriminator architecture improves the discriminative capacity inherited from the PatchGAN framework and can distinguish real or fake 3D volumes. \\n\\nIII. LOSS FUNCTIONS \\n\\nIn this section, we introduce loss functions that are used to constrain the proposed network. \\n\\nThe intention of GAN is to learn deep generative models while avoiding approximating intractable probabilistic computations that arise in other strategies such as maximum likelihood estimation. In the learning procedure, a discriminator D and a generator G would compete with each other to learn a generator distribution pG(x) that matches the real data distribution pdata(x). An ideal generator could generate samples that are indistinguishable from the real samples by the discriminator. More formally, the minmax game is summarized by the following expression: \\n\\n(2) \\n\\nwhere z is sampled from a noise distribution. \\n\\nTo learn a non-linear mapping from X-rays to CT, the generated CT volume should be consistent with the semantic information provided by the input X-rays. Therefore, LSGAN [11] is more suitable for our task. The conditional LSGAN loss is defined as: \\n\\n(3) \\n\\nIn conditional variational autoencoder, Kullback-Leibler divergence constrains the distribution of hidden variables. Since the conditional distribution is defined as a multidimensional Gaussian distribution N(0, I), the KL loss is defined as [7]: \\n\\n(5) \\n\\nThe conditional adversarial loss can not guarantee that the output generated by generator has the structural consistency with the input. Moreover, CT scans require higher precision of internal structures in 3D. Consequently, to enforce the reconstructed CT to be voxel-wise close to the ground truth, the reconstruction loss is defined as MSE [1]: \\n\\n(6) \\n\\nTo improve the training efficiency, simple shape priors could be utilized as auxiliary regularizations. Therefore, 2D projections of the predicted volume are enforced to match the ones from corresponding ground-truth on axial, coronal and sagittal planes. As this auxiliary loss focuses only on the general shape consistency, it can use orthogonal projections instead of perspective projections to simplify the process. The proposed projection loss is defined as [6]: \\n\\n(7) \\n\\nwhere the Pax, Pco and Psa represent the projection in the axial, coronal and sagittal plane, respectively. The L1 distance is used to enforce sharper image boundaries. \\n\\nGiven the definitions of the adversarial loss, KL loss, reconstruction loss and projection loss, the final loss function is formulated as: \\n\\n(8) \\n\\nwhere \\\\ensuremath{\\\\lambda}1, \\\\ensuremath{\\\\lambda}2 and \\\\ensuremath{\\\\lambda}3 control the relative importance of different loss terms. In reconstruction of 3D CT from X-ray projection, the adversarial loss is important to encourage local realism of the synthesized CT, but global shape consistency should be prioritized during the optimization process. \\n\\nIV. EXPERIMENTS \\n\\nIn this section, we introduce an augmented dataset built on a ten-phase lung 4D CT scan of two patients. We evaluate the proposed model with several widely used metrics, e.g., peak signaltonoise ratio (PSNR) and structural similarity (SSIM). To demonstrate the effectiveness of our method, we compare our method with other methods including PatRecon and X2CT. Fair comparisons and comprehensive analysis are given to demonstrate the improvement of our proposed method. \\n\\nAs there is no large dataset with paired 2D X-ray projection images and corresponding 3D CT, we use real 3D CT to synthesize corresponding X-ray projections through digitally reconstructed radiographs technology [12]. \\n\\nTo be specific, two ten-phase lung 4D CT scans of two patients for radiation therapy treatment planning are selected. The tumor diameter of patient 1 is about 40mm, and the tumor diameter of patient 2 is about 10mm, which represents large tumors and small tumors respectively. For each patients, a respiratory motion model based on principal component analysis (PCA) is constructed, which can described anatomical deformation induced by breathing using a linear combination of principal components and corresponding coefficients. Then, 1080 3D-CTs in different phases are generated by sampling the PCA coefficients. Next, to avoid the difficulty of reconstruction of 3D CT from 2D X-ray projection caused by the heterogeneous of imaging protocols, we first resample the CT scans to 1{\\\\texttimes}1{\\\\texttimes}1 mm3 resolution. Then, a cubic area is cropped from each CT scan. For each 3D CT, we synthesize corresponding X-rays from four different projection angles (ie, 0{\\\\textdegree}, 30{\\\\textdegree}, 60{\\\\textdegree}, and 90{\\\\textdegree}). Among all the paired data of CT-DRR, 60\\\\% are selected for training, 20\\\\% for verification, and the rest 20\\\\% for testing. Pytorch [13] is used to build a neural network, and Adam [14] is used as the network optimizer. \\n\\nMAE Mean Absolute Error(MAE) is L1-norm error between the reconstructed image and the real image, which is commonly used to estimate the difference between the prediction and groundtruth images. \\n\\nPSNR Peak Signal-to-Noise Ratio(PSNR) is often used to measure the quality of reconstructed image. Conventionally, CT value is recorded with 12 bits, representing a range of [0, 4095] (the actual Hounsfield unit equals the CT value minus 1024), which makes PSNR an ideal criterion for image quality evaluation. \\n\\nSSIM Structural SIMilarity(SSIM) is a metric of the overall similarity of two images, including brightness, contrast and structure. SSIM can match human{\\\\textquoteright}s subjective evaluation better. \\n\\nThe visible results of CT reconstruction are shown in 3. From the visual quality evaluation, it is obvious that the proposed method can reconstruct a complete 3D CT from a \\n\\nFig. 3. CT scans reconstructed from DRRs from different projection angles. \\n\\nsingle X-ray projection image from various angles, accurately reconstruct the tiny anatomical structure and obtain a clear image boundary. It can be seen from the figure that the 0{\\\\textdegree} X-ray projection can reconstruct the 3D CT better. The possible reason is that body thickness along the sagittal axis is smaller than body thickness along other axes and the visibility of the 0{\\\\textdegree} X-ray projection is better. \\n\\nQuantitative results are summarized in Table 1. Compared with the existing X2CT [6] and PatRecon [1], the proposed method has lower mean absolute error, higher peak signal-to-noise ratio and structure similarity, which represent higher reconstruction accuracy. Moreover, by tuning the weights of the voxel-level MSE loss and semantic-level adversarial loss in cost function, we can make a reasonable trade-off between the visual image quality and qualitative results. \\n\\nV. CONCLUSIONS \\n\\nIn this paper, we explored the possibility of reconstructing a 3D CT scan from a single 2D X-rays at different projection angles in an end to end network. In order to solve this challenging task, we use a variational autoencoder to learn the features of the input 2D X-ray projection image, and construct a decoder for 3D CT reconstruction. Moreover, a GAN structure is adopted to make the generated 3D CT more realistic. Experiments have proved that this method can obtain accurate 3D CT using a single X-ray projection image. The proposed method can be used for tumor motion control and dynamic dose assessment of radiotherapy, which has high application value. \\n\\nACKNOWLEDGMENT \\n\\nThis work was supported by the National Key R\\\\&D Program of China under Grant No. 2018YFA0704100 and 2018YFA0704101, the National Natural Science Foundation of China under Grant Nos. 61601012. \\n\\nREFERENCES \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.txt'}),\n",
       " Document(page_content='BARF : Bundle-Adjusting Neural Radiance Fields \\n\\n1. Introduction \\n\\nHumans have strong capabilities of reasoning about 3D geometry through our vision from the slightest ego-motion. When watching movies, we can immediately infer the 3D spatial structures of objects and scenes inside the videos. This is because we have an inherent ability of associating spatial correspondences of the same scene across continuous observations, without having to make sense of the relative camera or ego-motion. Through pure visual perception, not only can we recover a mental 3D representation of what we are looking at, but meanwhile we can also recognize where we are looking at the scene from. \\n\\nSimultaneously solving for the 3D scene representation from RGB images (i.e. reconstruction) and localizing the given camera frames (i.e. registration) is a long-standing chicken-and-egg problem in computer vision {\\\\textemdash} recovering \\n\\nNeRF \\n\\nBARF (ours) \\n\\nFigure 1: Training NeRF requires accurate camera poses for all images. We present BARF for learning 3D scene representations from imperfect (or even unknown) camera poses by jointly optimizing for registration and reconstruction. \\n\\nthe 3D structure requires observations with known camera poses, while localizing the cameras requires reliable correspondences from the reconstruction. Classical methods such as structure from motion (Sf M) [17, 44] or SLAM [13, 32] approach this problem through local registration followed by global geometric bundle adjustment (BA) on both the structure and cameras. Sf M and SLAM systems, however, are sensitive to the quality of local registration and easily fall into suboptimal solutions. In addition, the sparse nature of output 3D point clouds (often noisy) limits downstream vision tasks that requires dense geometric reasoning. \\n\\nClosely related to 3D reconstruction from imagery is the problem of view synthesis. Though not primarily purposed for recovering explicit 3D structures, recent advances on photorealistic view synthesis have opted to recover an intermediate dense 3D-aware representation (e.g. depth [15, 61], multi-plane images [71, 51, 55], or volume density [27, 31]), followed by neural rendering techniques [14, 29, 47, 54] to \\n\\nsynthesize the target images. In particular, Neural Radiance Fields (NeRF) [31] have demonstrated its remarkable ability for high-fidelity view synthesis. NeRF encodes 3D scenes with a neural network mapping 3D point locations to color and volume density. This allows the scenes to be represented with compact memory footprint without limiting the resolution of synthesized images. The optimization process of the network is constrained to obey the principles of classical volume rendering [23], making the learned representation interpretable as a continuous 3D volume density function. \\n\\nDespite its notable ability for photorealistic view synthesis and 3D scene representation, a hard prerequisite of NeRF (as well as other view synthesis methods) is accurate camera poses of the given images, which is typically obtained through auxiliary off-the-shelf algorithms. One straightforward way to circumvent this limitation is to additionally optimize the pose parameters with the NeRF model via back-propagation. As discussed later in the paper, however, na{\\\\\"\\\\i}ve pose optimization with NeRF is sensitive to initialization. It may lead to suboptimal solutions of the 3D scene representation, degrading the quality of view synthesis. \\n\\nIn this paper, we address the problem of training NeRF representations from imperfect camera poses {\\\\textemdash} the joint problem of reconstructing the 3D scene and registering the camera poses (Fig. 1). We draw inspiration from the success of classical image alignment methods and establish a theoretical connection, showing that coarse-to-fine registration is also critical to NeRF. Specifically, we show that positional encoding [57] of input 3D points plays a crucial role {\\\\textemdash} as much as it enables fitting to high-frequency functions [53], positional encoding is also more susceptible to suboptimal registration results. To this end, we present Bundle-Adjusting NeRF (BARF), a simple yet effective strategy for coarse-to-fine registration on coordinate-based scene representations. BARF can be regarded as a type of photometric BA [8, 2, 26] using view synthesis as the proxy objective. Unlike traditional BA, however, BARF can learn scene representations from scratch (i.e. from randomly initialized network weights), lifting the reliance of local registration subprocedures and allowing for more generic applications. \\n\\nIn summary, we present the following contributions: \\n\\n2. Related Work \\n\\nStructure from motion (SfM) and SLAM. Given a set of input images, Sf M [37, 38, 48, 49, 1, 62] and SLAM [33, 13, 32, 64] systems aim to recover the 3D structure and the sensor poses simultaneously. These can be classified into (a) indirect methods that rely on keypoint detection and matching [6, 32] and (b) direct methods that exploit photometric consistency [2, 12]. Modern pipelines following the indirect route have achieved tremendous success [44]; however, they often suffer at textureless regions and repetitive patterns, where distinctive keypoints cannot be reliably detected. Researchers have thus sought to use neural networks to learn discriminative features directly from data [10, 35, 11]. \\n\\nDirect methods, on the other hand, do not rely on such distinctive keypoints {\\\\textemdash} every pixel can contribute to maximizing photometric consistency, leading to improved robustness in sparsely textured environments [59]. They can also be naturally integrated into deep learning frameworks through image reconstruction losses [70, 58, 66]. Our method BARF lies under the broad umbrella of direct methods, as BARF learns 3D scene representations from RGB images while also localizing the respective cameras. However, unlike classical Sf M and SLAM that represent 3D structures with explicit geometry (e.g. point clouds), BARF encodes the scenes as coordinate-based representations with neural networks. \\n\\nView synthesis. Given a set of posed images, view synthesis attempts to simulate how a scene would look like from novel viewpoints [5, 24, 52, 19]. The task has been closely tied to 3D reconstruction since its introduction [7, 72, 18]. Researchers have investigated blending pixel colors based on depth maps [4] or leveraging proxy geometry to warp and composite the synthesized image [22]. However, since the problem is inherently ill-posed, there are still multiple restrictions and assumptions on the synthesized viewpoints. \\n\\nState-of-the-art methods have capitalized on neural networks to learn both the scene geometry and statistical priors from data. Various representations have been explored in this direction, e.g. depth [15, 61, 42, 43], layered depth [56, 46], multi-plane images [71, 51, 55], volume density [27, 31], and mesh sheets [20]. Unfortunately, these view synthesis methods still require the camera poses to be known a priori, largely limiting their applications in practice. In contrast, our method BARF is able to effectively learn 3D representations that encodes the underlying scene geometry from imperfect or even unknown camera poses. \\n\\nNeural Radiance Fields (NeRF). Recently, Mildenhall et al. [31] proposed NeRF to synthesize novel views of static, complex scenes from a set of posed input images. The key idea is to model the continuous radiance field of a scene with a multi-layer perceptron (MLP), followed by differentiable volume rendering to synthesize the images and backpropa-gate the photometric errors. NeRF has drawn wide attention \\n\\nacross the vision community [68, 34, 40, 36, 65] due to its simplicity and extraordinary performance. It has also been extended on many fronts, e.g. reflectance modeling for pho-torealistic relighting [3, 50] and dynamic scene modeling that integrates the motion of the world [25, 63, 39]. Recent works have also sought to exploit a large corpus of data to pretrain the MLP, enabling the ability to infer the radiance field from a single image [16, 67, 41, 45]. \\n\\nWhile impressive results have been achieved by the above NeRF-based models, they have a common drawback {\\\\textemdash} the requirement of posed images. Our proposed BARF allows us to circumvent such requirement. We show that with a simple coarse-to-fine bundle adjustment technique, we can recover from imperfect camera poses (including unknown poses of video sequences) and learn the NeRF representation simultaneously. Concurrent to our work, NeRF--[60] introduced an empirical, two-stage pipeline to estimate unknown camera poses. Our method BARF, in contrast, is motivated by mathematical insights and can recover the camera poses within a single course of optimization, allowing for direct utilities for various NeRF applications and extensions. \\n\\n3. Approach \\n\\nWe unfold this paper by motivating with the simpler 2D case of classical image alignment as an example. Then we discuss how the same concept is also applicable to the 3D case, giving inspiration to our proposed BARF. \\n\\nLet x \\\\ensuremath{\\\\in} R2 be the 2D pixel coordinates and I : R2 {\\\\textrightarrow} R3 be the imaging function. Image alignment aims to find the relative geometric transformation which minimizes the photometric error between two images I1 and I2. The problem can be formulated with a synthesis-based objective: \\n\\n(1) \\n\\nwhere W : R2 {\\\\textrightarrow} R2 is the warp function parametrized by p \\\\ensuremath{\\\\in} RP (with P as the dimensionality). As this is a nonlinear problem, gradient-based optimization is the method of choice: given the current warp state p, warp updates \\\\ensuremath{\\\\Delta}p are iteratively solved for and updated to the solution via p {\\\\textleftarrow} p + \\\\ensuremath{\\\\Delta}p. Here, \\\\ensuremath{\\\\Delta}p can be written in a generic form of \\n\\nwhere is termed the steepest descent image, and A is a generic transformation which depends on the choice of the optimization algorithm. The seminal Lucas-Kanade ternatively, one could also choose first-order optimizers such \\n\\nFigure 2: Predicting alignment from signal differences. Consider two 1D signals where f1(x) = f2(x + c) differs by an offset c. When solving for alignment, smoother signals can predict more coherent displacements than complex signals, which easily results in suboptimal alignment. \\n\\nas (stochastic) gradient descent which can be more naturally incorporated into modern deep learning frameworks, where A would correspond to a scalar learning rate. \\n\\nThe steepest descent image J can be expanded as \\n\\n(3) \\n\\nAt the heart of gradient-based registration are the image gradients modeling a local per-pixel linear relationship between appearance and spatial displacements, which is classically estimated via finite differencing. The overall warp update \\\\ensuremath{\\\\Delta}p can be more effectively estimated from pixel value differences if the per-pixel predictions are coherent (Fig. 2), i.e. the image signals are smooth. However, as natural images are typically complex signals, gradient-based registration on raw images is susceptible to suboptimal solutions if poorly initialized. Therefore, coarse-to-fine strategies have been practiced by blurring the images at earlier stages of registration, effectively widening the basin of attraction and smoothening the alignment landscape. \\n\\nImages as neural networks. An alternative formulation of the problem is to learn a coordinate-based image representation with a neural network while also solving for the warp p. Writing the network as f : R2 {\\\\textrightarrow} R3 and denoting \\\\ensuremath{\\\\Theta} as its parameters, one can instead choose to optimize the objective \\n\\nor alternatively, one may choose to solve for warp parameters \\n\\np1 and p2 respectively for both images I1 and I2 through \\n\\nwhere M = 2 is the number of images. Albeit similar to (1), the image gradients become the analytical Jacobian of the network instead of numerical estimation. By manipulating the network f, this also enables more principled control of the signal smoothness for alignment without having to rely on heuristic blurring on images, making these forms generalizable to 3D scene representations (Sec. 3.2). \\n\\nWe discuss the 3D case of recovering the 3D scene representation from Neural Radiance Fields (NeRF) [31] jointly with the camera poses. To signify the analogy to Sec. 3.1, we deliberately overload the notations x as 3D points, W as camera pose transformations, and f as the network in NeRF. \\n\\nNeRF encodes a 3D scene as a continuous 3D representation using an MLP f : R3 {\\\\textrightarrow} R4 to predict the RGB color c \\\\ensuremath{\\\\in} R3 and volume density \\\\ensuremath{\\\\sigma} \\\\ensuremath{\\\\in} R for each input 3D point x \\\\ensuremath{\\\\in} R3 . This can be summarized as y = [c; \\\\ensuremath{\\\\sigma}] = f(x; \\\\ensuremath{\\\\Theta}), where \\\\ensuremath{\\\\Theta} is the network parameters1 . NeRF assumes an emission-only model, i.e. the rendered color of a pixel is dependent only on the emitted radiance of 3D points along the viewing ray, without considering external lighting factors. \\n\\nWe first formulate the rendering operation of NeRF in the camera view space. Given pixel coordinates u \\\\ensuremath{\\\\in} R2 and denoting its homogeneous coordinates as u{\\\\textasciimacron} = [u; 1] \\\\ensuremath{\\\\in} R3 , we can express a 3D point xi along the viewing ray at depth zi as xi = ziu{\\\\textasciimacron}. The RGB color I{\\\\textasciicircum} at pixel location u is extracted by volume rendering via \\n\\n(6) \\n\\nwhere and znear and zfar are bounds on the depth range of interest. We refer our readers to Levoy [23] and Mildenhall et al. [31] for a more detailed treatment on volume rendering. In practice, the above integral formulations are approximated numerically via quadrature on discrete N points at depth \\\\{z1,...,zN \\\\} sampled along the ray. This involves N evaluations of the network f, whose output \\\\{y1,...,yN \\\\} are further composited through volume rendering. We can summarize the ray compositing function as and rewrite as Note that g is differentiable but deterministic, i.e. there are no learnable parameters associated. \\n\\nUnder a 6-DoF camera pose parametrized by p \\\\ensuremath{\\\\in} R6 , a 3D point x in the camera view space can be transformed to \\n\\nthe 3D world coordinates through a 3D rigid transformation W : R3 {\\\\textrightarrow} R3 . Therefore, the synthesized RGB value at pixel u becomes a function of the camera pose p as \\n\\nGiven M images \\\\{Ii\\\\}M i=1, our goal is to optimize NeRF and the camera poses \\\\{pi\\\\}iM=1 over the synthesis-based objective \\n\\nwhere I{\\\\textasciicircum} also depends on the network parameters \\\\ensuremath{\\\\Theta}. \\n\\nOne may notice the analogy between the synthesis-based objectives of 2D image alignment (5) and NeRF (8). Similarly, we can also derive the {\\\\textquotedblleft}steepest descent image{\\\\textquotedblright} as \\n\\nwhich is formed via backpropagation in practice. The lin-earization (9) is also analogous to the 2D case of (3), where the Jacobian of the network \\\\ensuremath{\\\\partial}\\\\ensuremath{\\\\partial}xy = \\\\ensuremath{\\\\partial}f\\\\ensuremath{\\\\partial}(xx) linearly relates the change of color c and volume density \\\\ensuremath{\\\\sigma} with 3D spatial displacements. To solve for effective camera pose updates \\\\ensuremath{\\\\Delta}p through backpropagation, it is also desirable to control the smoothness of f for predicting coherent geometric displacements from the sampled 3D points \\\\{x1,...,xN \\\\}. \\n\\nThe key of enabling NeRF to synthesize views with high fidelity is positional encoding [57], a deterministic mapping of input 3D coordinates x to higher dimensions of different sinusoidal frequency bases2 . We denote \\\\ensuremath{\\\\gamma} : R3 {\\\\textrightarrow} R3+6L as the positional encoding with L frequency bases, defined as \\n\\nwhere the k-th frequency encoding \\\\ensuremath{\\\\gamma}k(x) is \\n\\n(11) \\n\\nwith the sinusoidal functions operating coordinate-wise. The special case of L = 0 makes \\\\ensuremath{\\\\gamma} an identity mapping function. The network f is thus a composition of f(x) = f {\\\\textopenbullet} \\\\ensuremath{\\\\gamma}(x), where f is the subsequent learnable MLP. Positional encoding allows coordinate-based neural networks, which are typically bandwidth limited, to represent signals of higher frequency with faster convergence behaviors [53]. \\n\\nThe Jacobian of the k-th positional encoding \\\\ensuremath{\\\\gamma}k is \\n\\n(12) \\n\\nwhich amplifies the gradient signals from the MLP f by 2k\\\\ensuremath{\\\\pi} with its direction changing at the same frequency. This makes it difficult to predict effective updates \\\\ensuremath{\\\\Delta}p, since gradient signals from the sampled 3D points are incoherent (in terms of both direction and magnitude) and can easily cancel out each other. Therefore, na{\\\\\"\\\\i}vely applying positional encoding can become a double-edged sword to NeRF for the task of joint registration and reconstruction. \\n\\nWe describe our proposed BARF, a simple yet effective strategy for coarse-to-fine registration for NeRF. The key idea is to apply a smooth mask on the encoding at different frequency bands (from low to high) over the course of optimization, which acts like a dynamic low-pass filter. Inspired by recent work of learning coarse-to-fine deformation flow fields [36], we weigh the k-th frequency component of \\\\ensuremath{\\\\gamma} as \\n\\n(13) \\n\\nand \\\\ensuremath{\\\\alpha} \\\\ensuremath{\\\\in} [0,L] is a controllable parameter proportional to the optimization progress. The Jacobian of \\\\ensuremath{\\\\gamma}k thus becomes \\n\\nWhen wk(\\\\ensuremath{\\\\alpha}) = 0, the contribution to the gradient from the k-th (and higher) frequency component is nullified. \\n\\nStarting from the raw 3D input x (\\\\ensuremath{\\\\alpha} = 0), we gradually activate the encodings of higher frequency bands until full positional encoding is enabled (\\\\ensuremath{\\\\alpha} = L), equivalent to the original NeRF model. This allows BARF to discover the correct registration with an initially smooth signal and later shift focus to learning a high-fidelity scene representation. \\n\\n4. Experiments \\n\\nWe validate the effectiveness of our proposed BARF with a simple experiment of 2D planar image alignment, and show how the same coarse-to-fine registration strategy can be generalized to NeRF [31] for learning 3D scene representations. \\n\\nExperimental settings. We investigate how positional encoding impacts this problem by comparing networks with na{\\\\\"\\\\i}ve (full) positional encoding and without any encoding. We use a simple ReLU MLP for f with four 256-dimensional hidden units, and we use the Adam optimizer [21] to optimize both the network weights and the warp parameters for 5000 iterations with a learning rate of 0.001. For BARF, we linearly adjust \\\\ensuremath{\\\\alpha} for the first 2000 iterations and activate all frequency bands (L = 8) for the remaining iterations. \\n\\nResults. We visualize the registration results in Fig. 4. Alignment with full positional encoding results in suboptimal registration with ghostly artifacts in the recovered image representation. On the other hand, alignment without positional encoding achieves decent registration results, but cannot recover the image with sufficient fidelity. BARF discovers the precise geometric warps with the image representation optimized with high fidelity, quantitatively reflected in Table 1. The image alignment experiment demonstrates the general advantage of BARF for coordinate-based representations. \\n\\nWe investigate the problem of learning 3D scene representations with Neural Radiance Fields (NeRF) [31] from imperfect camera poses. We experiment with the 8 synthetic object-centric scenes provided by Mildenhall et al. [31], which consists of M = 100 rendered images with ground-truth camera poses for each scene for training. \\n\\nExperimental settings. We parametrize the camera poses p with the se(3) Lie algebra and assume known intrinsics. For each scene, we synthetically perturb the camera poses with additive noise \\\\ensuremath{\\\\delta}p \\\\ensuremath{\\\\sim} N (0,0.15I), which corresponds to a standard deviation of 14.9{\\\\textdegree} in rotation and 0.26 in translational magnitude (Fig. 5(a)). We optimize the objective in (8) jointly for the scene representation and the camera poses. We evaluate BARF mainly against the original NeRF model with na{\\\\\"\\\\i}ve (full) positional encoding; for completeness, we also compare with the same model without positional encoding. \\n\\nImplementation details. We follow the architectural settings from the original NeRF [31] with some modifications. We train a single MLP with 128 hidden units in each layer and without additional hierarchical sampling for simplicity. We resize the images to 400 {\\\\texttimes} 400 pixels and randomly sample 1024 pixel rays at each optimization step. We choose N = 128 sample for numerical integration along each ray, and we use the softplus activation on the volume density output \\\\ensuremath{\\\\sigma} for improved stability. We use the Adam optimizer and train all models for 200K iterations, with a learning rate of 5{\\\\texttimes}10\\\\ensuremath{-}4 exponentially decaying to 1{\\\\texttimes}10\\\\ensuremath{-}4 for the network f and 1{\\\\texttimes}10\\\\ensuremath{-}3 decaying to 1{\\\\texttimes}10\\\\ensuremath{-}5 for the poses p. For \\n\\n(a) image patches given for optimization \\n\\n(c) ground-truth warps \\n\\nFigure 3: Given image patches color-coded in (a), we aim to recover the alignment and the neural representation of the entire image, with the patches initialized to center crops shown in (b) and the corresponding ground-truth warps shown in (c). \\n\\nTable 1: Quantitative results of planar image alignment. BARF optimizes for more accurate alignment and patch reconstruction compared to the baselines. \\n\\n(a) na{\\\\\"\\\\i}ve pos. enc. \\n\\n(b) w/o pos. enc. \\n\\n(c) BARF \\n\\nFigure 4: Qualitative results of the planar image alignment experiment. We visualize the optimized warps (top row), the patch reconstructions in corresponding colors (middle row), and recovered image representation from f (bottom row). BARF is able to recover accurate alignment and high-fidelity image reconstruction, while baselines result in suboptimal alignment with na{\\\\\"\\\\i}ve positional encoding and blurry reconstruction without any encoding. Best viewed in color. \\n\\n(a) initial camera poses \\n\\n(b) full positional encoding \\n\\nperturbed/optimized camera poses \\n\\ntranslational error \\n\\n(c) BARF (ours) \\n\\nFigure 5: Visual comparison of the initial and optimized camera poses (Procrustes aligned) for the chair scene. BARF successfully realigns all the camera frames while NeRF na{\\\\\"\\\\i}ve positional encoding gets stuck at suboptimal solutions. \\n\\nBARF, we linearly adjust \\\\ensuremath{\\\\alpha} from iteration 20K to 100K and activate all frequency bands (up to L = 10) subsequently. \\n\\nEvaluation criteria. We measure the performance in two aspects: pose error for registration and view synthesis quality for the scene representation. Since both the scene and camera poses are variable up to a 3D similarity transformation, we evaluate the quality of registration by pre-aligning the optimized poses to the ground truth with Procrustes analysis on the camera locations. For evaluating view synthesis, we run an additional step of test-time photometric optimization on the trained models [26, 65] to factor out the pose error that may contaminate the view synthesis quality. We report the average rotation and translation errors for pose and PSNR, SSIM and LPIPS [69] for view synthesis. \\n\\nResults. We visualize the results in Fig. 6 and report the quantitative results in Table 2. BARF takes the best of both worlds of recovering the neural scene representation with the camera pose successfully registered, while na{\\\\\"\\\\i}ve NeRF with full positional encoding finds suboptimal solutions. Fig. 5 shows that BARF can achieve near-perfect registration for the synthetic scenes. Although the NeRF model without positional encoding can also successfully recover alignment, the learned scene representations (and thus the synthesized images) lack the reconstruction fidelity. As a reference, we also compare the view synthesis quality against standard NeRF models trained under ground-truth poses, showing that BARF can achieve comparable view synthesis quality in all metrics, albeit initialized from imperfect camera poses. \\n\\nground truth \\n\\nfull pos. enc. \\n\\nw/o pos. enc. \\n\\nBARF (ours) \\n\\nFigure 6: Qualitative results of NeRF on synthetic scenes. We visualize the image synthesis (top) and the expected depth through ray compositing (bottom). BARF achieves comparable synthesis quality to the reference NeRF (trained under perfect camera poses), while full positional encoding results in suboptimal registration, leading to synthesis artifacts. \\n\\nreference NeRF \\n\\nTable 2: Quantitative results of NeRF on synthetic scenes. BARF successfully optimizes for camera registration (with less than 0.2{\\\\textdegree} rotation error) while still consistently achieving high-quality view synthesis that is comparable to the reference NeRF models (trained under perfect camera poses). Translation errors are scaled by 100. \\n\\nWe investigate the challenging problem of learning neural 3D representations with NeRF on real-world scenes, where the camera poses are unknown. We consider the LLFF dataset [30], which consists of 8 forward-facing scenes with RGB images sequentially captured by hand-held cameras. \\n\\nExperimental settings. We parametrize the camera poses p with se(3) following Sec. 4.2 but initialize all cameras with the identity transformation, i.e. pi = 0 \\\\ensuremath{\\\\forall}i. We assume known camera intrinsics (provided by the dataset). We compare against the original NeRF model with na{\\\\\"\\\\i}ve positional encoding, and we use the same evaluation criteria described in Sec. 4.2. However, we note that the camera poses provided in LLFF are also estimations from Sf M packages [44]; therefore, the pose evaluation is at most an indication of how well BARF agrees with classical geometric pose estimation. \\n\\nImplementation details. We follow the same architectural settings from the original NeRF [31] and resize the images to 480{\\\\texttimes}640 pixels. We train all models for 200K iterations and randomly sample 2048 pixel rays at each optimization step, with a learning rate of 1{\\\\texttimes}10\\\\ensuremath{-}3 for the network f decaying to 1{\\\\texttimes}10\\\\ensuremath{-}4 , and 3{\\\\texttimes}10\\\\ensuremath{-}3 for the pose p decaying to 1{\\\\texttimes}10\\\\ensuremath{-}5 . We linearly adjust \\\\ensuremath{\\\\alpha} for BARF from iteration 20K to 100K and activate all bands (up to L = 10) subsequently. \\n\\nResults. The quantitative results (Table 3) show that the recovered camera poses from BARF highly agrees with those estimated from off-the-shelf Sf M methods (visualized in Fig. 8), demonstrating the ability of BARF to localize from scratch. Furthermore, BARF can successfully recover the 3D scene representation with high fidelity (Fig. 7). In contrast, NeRF with na{\\\\\"\\\\i}ve positional encoding diverge to incorrect camera poses, which in turn results in poor view synthesis. This highlights the effectiveness of BARF utilizing a coarse-to-fine strategy for joint registration and reconstruction. \\n\\nFigure 7: Qualitative results of NeRF on real-world scenes from unknown camera poses. Compared to a reference NeRF model trained with camera poses provided from Sf M [44], BARF can effectively optimize for the poses jointly with the scene representation. NeRF models with full positional encoding diverge to incorrect localization and hence poor synthesis quality. \\n\\nTable 3: Quantitative results of NeRF on the LLFF forward-facing scenes from unknown camera poses. BARF can optimize for accurate camera poses (with an average \\\\ensuremath{<} 0.6{\\\\textdegree} rotation error) and high-fidelity scene representations, enabling novel view synthesis whose quality is comparable to reference NeRF model trained under Sf M poses. Translation errors are scaled by 100. \\n\\n(a) full pos. enc. \\n\\n(b) BARF (ours) \\n\\nFigure 8: Visualization of optimized camera poses from the fern scene (Procrustes aligned). Results from BARF highly agrees with Sf M, whereas the baseline poses are suboptimal. \\n\\n5. Conclusion \\n\\nWe present Bundle-Adjusting Neural Radiance Fields (BARF), a simple yet effective strategy for training NeRF from imperfect camera poses. By establishing a theoretical connection to classical image alignment, we demonstrate that coarse-to-fine registration is necessary for joint registration and reconstruction with coordinate-based scene representations. Our experiments show that BARF can effectively learn the 3D scene representations from scratch and resolve large camera pose misalignment at the same time. \\n\\nDespite the intriguing results at the current stage, BARF has similar limitations to the original NeRF formulation [31] (e.g. slow optimization and rendering, rigidity assumption, sensitivity to dense 3D sampling), as well as reliance on heuristic coarse-to-fine scheduling strategies. Nevertheless, since BARF keeps a close formulation to NeRF, many of the latest advances on improving NeRF are potentially transferable to BARF as well. We believe BARF opens up exciting avenues for rethinking visual localization for Sf M/SLAM systems and self-supervised dense 3D reconstruction frameworks using view synthesis as a proxy objective. \\n\\nReferences \\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.txt'}),\n",
       " Document(page_content='Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications \\n\\nInstitute of Information and Communication Technologies, Electronics and Applied Mathematics (ICTEAM), UCLouvain, Place de l{\\\\textquoteright}Universit{\\\\textasciiacute}e 1, 1348 Louvain-la-Neuve, Belgium   \\n\\nARTICLE INFO  \\n\\nKeywords: 3D-CT reconstruction \\n\\nABSTRACT  \\n\\nBackground and purpose: Radiotherapy is commonly chosen to treat thoracic and abdominal cancers. However, irradiating mobile tumors accurately is extremely complex due to the organs{\\\\textquoteright} breathing-related movements. Different methods have been studied and developed to treat mobile tumors properly. The combination of X-ray projection acquisition and implanted markers is used to locate the tumor in two dimensions (2D) but does not provide three-dimensional (3D) information. The aim of this work is to reconstruct a high-quality 3D computed tomography (3D-CT) image based on a single X-ray projection to locate the tumor in 3D without the need for implanted markers. \\n\\nMaterials and Methods: Nine patients treated for a lung or liver cancer in radiotherapy were studied. For each patient, a data augmentation tool was used to create 500 new 3D-CT images from the planning four-dimensional computed tomography (4D-CT). For each 3D-CT, the corresponding digitally reconstructed radiograph was generated, and the 500 2D images were input into a convolutional neural network that then learned to reconstruct the 3D-CT. The dice score coefficient, normalized root mean squared error and difference between the ground-truth and the predicted 3D-CT images were computed and used as metrics. \\n\\nResults: Metrics{\\\\textquoteright} averages across all patients were 85.5\\\\% and 96.2\\\\% for the gross target volume, 0.04 and 0.45 Hounsfield unit (HU), respectively. \\n\\nConclusions: The proposed method allows reconstruction of a 3D-CT image from a single digitally reconstructed radiograph that could be used in real-time for better tumor localization and improved treatment of mobile tumors without the need for implanted markers.   \\n\\n1. Introduction \\n\\nRadiotherapy is one of the most widely used treatments in oncology and is prescribed for more than half of all cancer patients, either alone or in combination with surgery and chemotherapy (\\\\ensuremath{<}\\\\ensuremath{>})[1]. In radiotherapy, ionizing radiation is used to kill cancer cells. A trade-off must be made between delivering the prescribed dose to the target and not delivering large doses to healthy tissues, which could lead to undesirable effects and induce secondary cancer (\\\\ensuremath{<}\\\\ensuremath{>})[2]. Applying radiotherapy to lung and liver cancers is even more challenging as the treatment must consider the respiratory motion. This requires specific strategies in the radiotherapy workflow to ensure adequate target coverage through successive treatment fractions. These strategies are generally classified in two categories. \\n\\nThe first category consists in acquiring a four-dimensional computed tomography (4D-CT) scan prior to the treatment and defining security margins. Safety margins ensure target coverage regardless of the breathing phase, but this method irradiates more the surrounding healthy organs (\\\\ensuremath{<}\\\\ensuremath{>})[3]. The breathing motion in the treatment room may also differ significantly from the motion captured in the 4D-CT from time to time (\\\\ensuremath{<}\\\\ensuremath{>})[4]. \\n\\nThe second category encompasses breathing-synchronized methods that aim to minimize the contribution of the tumor{\\\\textquoteright}s motion in the computation of the safety margins by monitoring the tumor{\\\\textquoteright}s position or reducing/regularizing its motion amplitude during breathing. These methods gather abdominal compression (\\\\ensuremath{<}\\\\ensuremath{>})[5], audio coaching (\\\\ensuremath{<}\\\\ensuremath{>})[6], mechanically assisted ventilation (\\\\ensuremath{<}\\\\ensuremath{>})[7] and respiratory gating (\\\\ensuremath{<}\\\\ensuremath{>})[8]. Tumor \\n\\nmonitoring in these techniques is based on external surrogates of the internal motion to avoid the use of invasive procedures (the placement of markers pinpoints the tumor position with greater accuracy but involves surgery before the treatment (\\\\ensuremath{<}\\\\ensuremath{>})[9]). This approach requires a stable correlation between the internal tumor motion and its external surrogate, which is usually not the case when changes occur in the patient{\\\\textquoteright}s breathing movement. \\n\\nImage-guided radiation therapy (IGRT) incorporates imaging techniques during each treatment session. By adding detailed images, it ensures that the radiation is narrowly focused on the target. A broad range of IGRT is now available (\\\\ensuremath{<}\\\\ensuremath{>})[10]. X-ray projections are commonly acquired to estimate the tumor{\\\\textquoteright}s position, but their use often requires implanted markers to identify the tumor volume correctly and make it visible on the X-ray projection (\\\\ensuremath{<}\\\\ensuremath{>})[11]. Another disadvantage of this method is that it does not provide 3D information. \\n\\nAll these methods result in a small reduction in the safety margins, while adapting the treatment in 3D and in real-time will lead to a big reduction in the motion margins thanks to precise tracking of the 3D anatomical structures. To achieve this, the real-time positions of the target and surrounding organs must be known throughout treatment delivery. Most of the radiotherapy treatment rooms are equipped with 2D fluoroscopy to validate the patient positioning before treatment, we propose to rely on this equipment to estimate the related 3D information. \\n\\nMany studies that reconstruct a 3D volume from a 2D X-ray projection have already been performed. Different fields of application in the biomedical sector have been explored: Henzler et al. investigated how to reconstruct 3D volumes from 2D cranial x-rays by applying deep learning (\\\\ensuremath{<}\\\\ensuremath{>})[12], while Liang et al. developed a new model architecture to reconstruct a tooth in 3D from a single panoramic radiograph (\\\\ensuremath{<}\\\\ensuremath{>})[13]. Montaya et al. in (\\\\ensuremath{<}\\\\ensuremath{>})[14], as well as Ying et al. in (\\\\ensuremath{<}\\\\ensuremath{>})[15], demonstrated that it was possible to reconstruct a 3D-CT image from biplanar X-ray projections using a neural network, and Shen et al. used a neural network to reconstruct a 3D image from a single projection view (\\\\ensuremath{<}\\\\ensuremath{>})[16]. \\n\\n2. Materials and methods \\n\\nThe data used in this work come from nine patients who were treated for lung or liver cancer at Cliniques universitaires Saint-Luc in Brussels between 2010 and 2015. This retrospective study was approved by the Hospital Research Ethics Committee (B403201628906). (\\\\ensuremath{<}\\\\ensuremath{>})Table 1 shows patients information (tumor size and location, and its motion in the different sets). A planning 4D-CT composed of 10 breathing phases evenly spread over the respiratory cycle was acquired for each patient prior to treatment delivery. The dimensions of each 3D-CT image were 512 {\\\\texttimes} 512 {\\\\texttimes} 173, and the voxel size was 1 mm2 in plane with a slice thickness of 2 mm. The Mid-Position (MidP)-CT image, defined as the local mean position in the respiratory cycle, was computed using the average of all velocity fields obtained by non-rigid registration between the 4D-CT phases (\\\\ensuremath{<}\\\\ensuremath{>})[18]. On the MidP-CT image, the gross target volume (GTV) and surrounding organs at risk were delineated manually by an experienced radiation oncologist. \\n\\nAs training a neural network requires a lot of data, it was necessary to generate new 3D-CT images. To do so, we consider a polar coordinate system (r, n) related to a breathing cycle, whose origin is the MidP-CT image and where n are the periodic phases. In this system, we know the deformation fields associated to the 10 breathing phases of the 4DCT which are F(1, N), with N \\\\ensuremath{\\\\in} \\\\{0, 0.1{\\\\textellipsis}, 0.9\\\\}. Then, to generate the breathing phase n at a normalized distance r of the MidP-CT, we compute the deformation field F(r, n) using a linear interpolation between the two closest discrete breathing phases plus a scaling: \\n\\nFig. 1. Overview of the proposed method{\\\\textquoteright}s workflow.  \\n\\nPatient characteristics. MR4D{\\\\L} CT, MRTrainSet and MRTestSet stand for the motion range in 3D of the GTV{\\\\textquoteright}s centroid in the 4D-CT, training set and test set, respectively. The motion range is defined as the Euclidean distance between the two most distant positions.  \\n\\n(1)  \\n\\nwhere N\\\\ensuremath{\\\\leqslant}n\\\\ensuremath{\\\\leqslant}N + 0.1. Using this method, based on a previous work of our team (\\\\ensuremath{<}\\\\ensuremath{>})[19] and developed in (\\\\ensuremath{<}\\\\ensuremath{>})[20], we can generate slightly different 3D-CT images, spread around the ten original phases of the 4D-CT, for every patient. The training set was composed of 500 images where n was a uniform random draw between 0 and 1, and r a random sample from a normal distribution N (1, 0.25) truncated between 0.4 and 1.1. A digitally reconstructed radiograph (DRR) was generated from each of these images using the Beer{\\\\textendash}Lambert absorption-only model (implemented in the TomoPy Python library (\\\\ensuremath{<}\\\\ensuremath{>})[21]) and a projection angle of 0{\\\\textopenbullet} along the anterior-posterior axis. The projection geometry was a 1440 {\\\\texttimes} 1440 image with a pixel size of 0.296 {\\\\texttimes} 0.296 mm2. The source-to-origin and source-to-detector distances were 1000 mm and 1500 mm. Each patient{\\\\textquoteright}s training dataset was made up of 500 pairs containing the created 3D-CT image and the associated DRR. An independent test set composed of 100 3D-CT/DRR pairs was also created for each patient. For each image of the test set, the masks of the GTV, lungs and heart were also generated by deforming the MidP-CT image{\\\\textquoteright}s 3D binary masks. The difference between the test and training sets comes from the normalized distance r used to generate the 3D-CT image. In the case of the training set, r was a random sample from a normal distribution N (1, 0.25) truncated between 0.4 and 1.1, while r was a random sample from a normal distribution N (1, 0.5) truncated between 0.8 and 1.5 for the test set. This means that deeper breathing situations were present in the test set than in the training set. All breathing phases were used in both cases. \\n\\nIn order to evaluate the performance of the proposed method, 100 3D-CT images independent of the training set were created for each patient. These 3D-CT images are called the ground truth (GT) 3D-CT images in the rest of the paper. 100 DRRs were generated from these images to form the test set. The trained network was used on these ra-diographs to predict the corresponding 3D-CT images, called the predicted (P) 3D-CT images. The predicted 3D-CT images were compared with the ground truth 3D-CT images to evaluate the performance of the model using several metrics. \\n\\nDice similarity coefficient (DSC) is a common overlap-based metric used to measure the performance of a segmentation algorithm, and is defined by: \\n\\n(2)  \\n\\nwhere A and B are the sets containing the matrix indices of both binary masks A and B. In this work, the DSC was computed between a 3D binary mask in the ground-truth 3D-CT image and the corresponding mask in the predicted 3D-CT image to evaluate the quality of the predicted 3DCT image in terms of anatomical structure positions. The 3D binary masks of a predicted 3D-CT image were obtained by computing the Morphons non-rigid registration (\\\\ensuremath{<}\\\\ensuremath{>})[23], then applying the resulting deformation fields to deform the masks on the predicted image. This was done between this predicted image and either the ground-truth 3D-CT image (GT-based), or the MidP-CT image (MidP-based). Using the ground-truth 3D-CT image for this part serves as a post-training quality evaluation, to evaluate if a state-of-the-art registration algorithm sees a difference between the ground-truth and the predicted images. Using the MidP-CT image simulates how it could be used to evaluate the quality of the predicted images after each treatment fraction as the ground-truth 3D-CT images are not available during a treatment. For both versions, the DSC was computed for the same 50 images of the 100 items constituting the test set, for each organ and each patient. In either case, this metric was an evaluation tool and not part of the real-time process as the computation time of the Morphons is about 150 s. As a complement to this analysis, the Euclidean distance was computed \\n\\n(further details in Appendix A. (\\\\ensuremath{<}\\\\ensuremath{>})Supplementary data). Normalized root mean squared error (NRMSE) was computed between two images A and B, and is defined by: \\n\\n(3)  \\n\\nwhere Xa is the voxel a in the image X. Amax and Amin stand for the maximum and minimum in image A, the ground-truth 3D-CT image. The NRMSE was computed between the latter and the corresponding predicted 3D-CT image. This was repeated for all images in the test set. \\n\\nDifference was computed between a ground-truth 3D-CT image and the corresponding predicted 3D-CT image, and the mean and median of the difference were studied, as well as quantifying the percentage of the absolute value of the difference below a certain threshold to evaluate the proportion of the image that was correctly reconstructed. \\n\\n3. Results \\n\\nThe results of the DSC analysis for both GT-based and MidP-based versions are summarized in (\\\\ensuremath{<}\\\\ensuremath{>})Table 2. For the GT-based version, the mean, the median and the 95th percentile of the DSC vary respectively from 93.2\\\\% to 99.8\\\\%, from 93.2\\\\% to 99.9\\\\%, and from 95.1\\\\% to 99.9\\\\% for the GTV; from 96.3\\\\% to 99.8\\\\%, from 96.5\\\\% to 99.9\\\\%, and from 96.8\\\\% to 99.9\\\\% for both lungs; from 93.5\\\\% to 99.8\\\\%, from 94.3\\\\% to 99.8\\\\%, and from 95.1\\\\% to 99.9\\\\% for the heart. While, for the MidP-based version, the mean, the median and the 95th percentile of this metric vary respectively from 76.7\\\\% to 90.6\\\\%, from 77.6\\\\% to 90.8\\\\%, and from 82.7\\\\% to 93.4\\\\% for the GTV; from 90.9\\\\% to 97.3\\\\%, from 93.4\\\\% to 97.1\\\\%, and from 96.1\\\\% to 98.3\\\\% for both lungs; from 78.1\\\\% to 90.1\\\\%, from 79.2\\\\% to 89.9\\\\%, and from 81.5\\\\% to 91.7\\\\% for the heart. \\n\\nThe DSC results of the MidP-based version are lower than those of GT- based, but still over 75\\\\%. As the same 50 images were used for both, the difference might be due to the approximations in the deformations and re-binarization of the masks, that probably have a higher impact with deformations over multiple voxels, but this was not quantified. \\n\\nThe results of the NRMSE analysis are displayed in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 2. The mean of this metric is lower for Patients 5, 2, 6 and 1 who have smaller motions in the test set (from 0.032 to 0.039) than the mean obtained for Patients 7, 8, 3 and 9 (from 0.047 to 0.051) who have larger motions. This is also observed for the median and the 95th percentile, which range respectively from 0.032 to 0.038, and from 0.039 to 0.045 for the first batch of patients, while they are respectively between 0.045 and 0.052, and between 0.051 and 0.059 for the second group of patients. This analysis also shows that the breathing phases have no impact on the reconstruction process as there are uniformly distributed along the NRMSE values range. \\n\\nThe results of the difference analysis are summarized in (\\\\ensuremath{<}\\\\ensuremath{>})Table 3. The mean of the difference between a ground-truth 3D-CT image and the corresponding predicted 3D-CT image ranges from {\\\\L} 1.32 Hounsfield unit (HU) to 2.24 HU, with an average over all patients of 0.45 HU. The median of this metric is between {\\\\L} 0.26 HU and 1.93 HU, with an average over all patients of 0.24 HU. Depending on the patient, 25.1\\\\% to 39.8\\\\% of the image volume has an absolute value of the difference lower than 5 HU, 69.9\\\\% to 81.9\\\\% below 25 HU, and 88.6\\\\% to 94.6\\\\% less than 50 HU. In summary, the difference between the ground-truth and the predicted images is very small, with about 91\\\\% of the image volume having an absolute value of the difference smaller than 50 HU, which represents 1.25\\\\% of the range of possible values, since the scale of a 3D- \\n\\nFig. 2. Results of the NRMSE analysis. The NRMSE was computed between the ground-truth 3D-CT image and the corresponding predicted 3D-CT image for each test set data. The color of a dot represents the breathing phase at which the ground-truth 3D-CT image was created. Patients are sorted by increasing motion range in the test set. \\n\\nResults of the difference analysis. V\\\\ensuremath{<}5HU, V\\\\ensuremath{<}25HU and V\\\\ensuremath{<}50HU stand for the percentage of the 3D-CT image{\\\\textquoteright}s volume having an absolute value of the difference below 5 HU, 25 HU and 50 HU.  \\n\\nCT image typically runs from {\\\\L} 1000 HU for air to 3000 HU for dense bone (\\\\ensuremath{<}\\\\ensuremath{>})[24]. \\n\\nA representative example (whose results are: DSCGT (GTV) = 98.5\\\\% , DSCMidP (GTV) = 88.6\\\\%, NRMSE = 0.053, mean of the difference = {\\\\L} 1.73 HU and V\\\\ensuremath{<}25HU = 80.3\\\\%) of the results obtained using the proposed method can be seen in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3. For a human eye, the predicted 3DCT image looks pretty close in terms of anatomical structures. The zoom shows that a red pixel (difference \\\\ensuremath{\\\\approx} 200 HU) is commonly adjacent to a blue pixel (difference \\\\ensuremath{\\\\approx}  200 HU) or surrounded by two turquoise pixel (difference \\\\ensuremath{\\\\approx}  100 HU). This phenomenon is usually observed at tissue borders. Looking at the histogram, one sees that there are few voxels with a significant difference and over 30\\\\% of the voxels have a difference between  5 HU and 5 HU. \\n\\n4. Discussion \\n\\nIn this paper, it has been showed that the proposed CNN-based methodology (which requires a patient-specific training) allows to reconstruct a high-quality 3D-CT image from a single digitally reconstructed radiograph. \\n\\nTable 2 \\n\\nResults of the DSC analysis for both GT-based and MidP-based versions. DSCGT and DSCMidP stand for the mean of the DSC over the 50 images taken from the test set for the GT-based version and MidP-based version, respectively. Patient 5{\\\\textquoteright}s lungs and heart were not delineated.  \\n\\nFig. 3. Visualization of three slices of the ground-truth 3D-CT image of one patient compared with the corresponding slices of the predicted 3D-CT image, as well as the results of the difference analysis and a zoom of the boxed area. On the right of the color bar is the histogram of the difference concatenated for all patients and the 100 images of the nine test sets. \\n\\nThe dice values computed between the masks of the predicted 3D-CT image and the corresponding ground-truth 3D-CT are all greater than 75\\\\%, which is reliable. If we compare our results of the MidP-based version ((\\\\ensuremath{<}\\\\ensuremath{>})Table 2) for lungs and heart (94.6\\\\% and 83.9\\\\%) to previous works (\\\\ensuremath{<}\\\\ensuremath{>})[25{\\\\textendash}27], whose goal was to segment organs at risk in lung cancer utilizing deep learning algorithms, (best in (\\\\ensuremath{<}\\\\ensuremath{>})[27]: 97.5\\\\% and 92.5\\\\%), lungs have similar results to the literature and the heart has a higher difference. However, our results should be taken in hindsight, given that the masks in the predicted image are defined as the manually segmented masks on the MidP-CT image deformed using the deformation fields obtained by the Morphons registration between both images. \\n\\nThe mean of the difference between the ground truth image and the predicted image is small for each patient, with an average value of 0.45 HU over all patients. Comparing these results ((\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3) with those obtained by (\\\\ensuremath{<}\\\\ensuremath{>})[16] when they use only 1 view, the quality of our reconstructed image is similar to their own. Their method also performs less at tissue borders. However, there is no scale or numerical value in their difference analysis, so it is not clear that the difference values are similar. \\n\\nOne limitation of this study is that the CNN was trained using training sets composed of 3D-CT images created from deformations of a planning 4D-CT acquired prior to the treatment and paired DRRs generated using the Beer{\\\\textendash}Lambert absorption-only model. This method supposes that inter-fraction variations such as tumor shrinking, tumor baseline shift and stomach and bladder fillings are not included in the training set. A next step of this work is to evaluate whether the network must be retrained for each fraction or whether these variations are negligible in the reconstruction process. Another possibility to counteract this limitation is to improve the data augmentation tool and incorporate inter-fraction changes in the training set. \\n\\nIn conclusion, this study presents a method that allows reconstruction of a 3D-CT image from a single DRR. This method relies on a data augmentation algorithm and on a patient-specific training of a CNN. However, the study still needs to integrate inter-fractions changes and adjust the image resolution to confirm the potential clinical use of the method. \\n\\nDeclaration of Competing Interest \\n\\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. \\n\\nAcknowledgments \\n\\nEstelle Lo\\\\\"yen is a Televie grantee of the Fonds de la Recherche Sci-entifique - F.N.R.S. Damien Dasnoy-Sumell is supported by the Walloon Region, SPWEER Win2Wal program project 2010149. \\n\\nAppendix A. Supplementary data \\n\\nSupplementary data associated with this article can be found, in the online version, at (\\\\ensuremath{<}https://doi.org/10.1016/j.phro.2023.100444\\\\ensuremath{>})https://doi.org/10.1016/j.phro.2023.100444. \\n\\nReferences \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.txt'}),\n",
       " Document(page_content='arXiv:2003.08934v2  [cs.CV]  3 Aug 2020\\n\\nKeywords: scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning \\n\\n1 Introduction \\n\\nIn this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images. \\n\\nWe represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction (\\\\ensuremath{\\\\theta}, \\\\ensuremath{\\\\varphi}) at each point (x, y, z) in space, and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through (x, y, z). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate (x, y, z, \\\\ensuremath{\\\\theta}, \\\\ensuremath{\\\\varphi}) to a single volume density and view-dependent RGB color. To render this neural radiance field (NeRF) \\n\\nFig. 1: We present a method that optimizes a continuous 5D neural radiance field representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation. \\n\\nfrom a particular viewpoint we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally differentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content. Figure (\\\\ensuremath{<}\\\\ensuremath{>})2 visualizes this overall pipeline. \\n\\nWe find that the basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently high-resolution representation and is inefficient in the required number of samples per camera ray. We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation. \\n\\nOur approach inherits the benefits of volumetric representations: both can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images. Crucially, our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions. In summary, our technical contributions are: \\n\\nWe demonstrate that our resulting neural radiance field method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, including works that fit neural 3D representations to scenes as well as works that train deep convolutional networks to predict sampled volumetric representations. As far as we know, this paper presents the first continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings. \\n\\n2 Related Work \\n\\nA promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape, such as the signed distance [(\\\\ensuremath{<}\\\\ensuremath{>})6] at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same fidelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes. \\n\\nA similar approach of using MLPs to map from low-dimensional coordinates to colors has also been used for representing other graphics functions such as images [(\\\\ensuremath{<}\\\\ensuremath{>})44], textured materials [(\\\\ensuremath{<}\\\\ensuremath{>})12,(\\\\ensuremath{<}\\\\ensuremath{>})31,(\\\\ensuremath{<}\\\\ensuremath{>})36,(\\\\ensuremath{<}\\\\ensuremath{>})37], and indirect illumination values [(\\\\ensuremath{<}\\\\ensuremath{>})38]. \\n\\nNeural 3D shape representations Recent work has investigated the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map xyz coordinates to signed distance functions [(\\\\ensuremath{<}\\\\ensuremath{>})15,(\\\\ensuremath{<}\\\\ensuremath{>})32] or occupancy fields [(\\\\ensuremath{<}\\\\ensuremath{>})11,(\\\\ensuremath{<}\\\\ensuremath{>})27]. However, these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet [(\\\\ensuremath{<}\\\\ensuremath{>})3]. Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating differentiable rendering functions that allow neural implicit shape representations to be optimized using only 2D images. Niemeyer et al. [(\\\\ensuremath{<}\\\\ensuremath{>})29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann et al. [(\\\\ensuremath{<}\\\\ensuremath{>})42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a differentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located. \\n\\nView synthesis and image-based rendering Given a dense sampling of views, photorealistic novel views can be reconstructed by simple light field sample interpolation techniques [(\\\\ensuremath{<}\\\\ensuremath{>})21,(\\\\ensuremath{<}\\\\ensuremath{>})5,(\\\\ensuremath{<}\\\\ensuremath{>})7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made significant progress by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based representations of scenes with either diffuse [(\\\\ensuremath{<}\\\\ensuremath{>})48] or view-dependent [(\\\\ensuremath{<}\\\\ensuremath{>})2,(\\\\ensuremath{<}\\\\ensuremath{>})8,(\\\\ensuremath{<}\\\\ensuremath{>})49] appearance. Differentiable rasterizers [(\\\\ensuremath{<}\\\\ensuremath{>})4,(\\\\ensuremath{<}\\\\ensuremath{>})10,(\\\\ensuremath{<}\\\\ensuremath{>})23,(\\\\ensuremath{<}\\\\ensuremath{>})25] or pathtracers [(\\\\ensuremath{<}\\\\ensuremath{>})22,(\\\\ensuremath{<}\\\\ensuremath{>})30] can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization [(\\\\ensuremath{<}\\\\ensuremath{>})22], which is typically unavailable for unconstrained real-world scenes. \\n\\nAnother class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids [(\\\\ensuremath{<}\\\\ensuremath{>})19,(\\\\ensuremath{<}\\\\ensuremath{>})40,(\\\\ensuremath{<}\\\\ensuremath{>})45]. More recently, several methods [(\\\\ensuremath{<}\\\\ensuremath{>})9,(\\\\ensuremath{<}\\\\ensuremath{>})13,(\\\\ensuremath{<}\\\\ensuremath{>})17,(\\\\ensuremath{<}\\\\ensuremath{>})28,(\\\\ensuremath{<}\\\\ensuremath{>})33,(\\\\ensuremath{<}\\\\ensuremath{>})43,(\\\\ensuremath{<}\\\\ensuremath{>})46,(\\\\ensuremath{<}\\\\ensuremath{>})52] have used large datasets of multiple scenes to train deep networks that predict a sampled volumetric representation from a set of input images, and then use either alpha-compositing [(\\\\ensuremath{<}\\\\ensuremath{>})34] or learned compositing along rays to render novel views at test time. Other works have optimized a combination of convolutional networks (CNNs) and sampled voxel grids for each specific scene, such that the CNN can compensate for discretization artifacts from low resolution voxel grids [(\\\\ensuremath{<}\\\\ensuremath{>})41] or allow the predicted voxel grids to vary based on input time or animation controls [(\\\\ensuremath{<}\\\\ensuremath{>})24]. While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling {\\\\textemdash} rendering higher resolution images requires a finer sampling of 3D space. We circumvent this problem by instead encoding a continuous volume within the parameters of a deep fully-connected neural network, which not only produces significantly higher quality renderings than prior volumetric approaches, but also requires just a fraction of the storage cost of those sampled volumetric representations. \\n\\n3 Neural Radiance Field Scene Representation \\n\\nWe represent a continuous scene as a 5D vector-valued function whose input is a 3D location x = (x, y, z) and 2D viewing direction (\\\\ensuremath{\\\\theta}, \\\\ensuremath{\\\\varphi}), and whose output is an emitted color c = (r, g, b) and volume density \\\\ensuremath{\\\\sigma}. In practice, we express \\n\\nFig. 2: An overview of our neural radiance field scene representation and differ-entiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable, so we can optimize our scene representation by minimizing the residual between synthesized and ground truth observed images (d). \\n\\ndirection as a 3D Cartesian unit vector d. We approximate this continuous 5D scene representation with an MLP network F\\\\ensuremath{\\\\Theta} : (x, d) {\\\\textrightarrow} (c, \\\\ensuremath{\\\\sigma}) and optimize its weights \\\\ensuremath{\\\\Theta} to map from each input 5D coordinate to its corresponding volume density and directional emitted color. \\n\\nWe encourage the representation to be multiview consistent by restricting the network to predict the volume density \\\\ensuremath{\\\\sigma} as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction. To accomplish this, the MLP F\\\\ensuremath{\\\\Theta} first processes the input 3D coordinate x with 8 fully-connected layers (using ReLU activations and 256 channels per layer), and outputs \\\\ensuremath{\\\\sigma} and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray{\\\\textquoteright}s viewing direction and passed to one additional fully-connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color. \\n\\nSee Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3 for an example of how our method uses the input viewing direction to represent non-Lambertian effects. As shown in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})4, a model trained without view dependence (only x as input) has difficulty representing specularities. \\n\\n4 Volume Rendering with Radiance Fields \\n\\nOur 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering [(\\\\ensuremath{<}\\\\ensuremath{>})16]. The volume density \\\\ensuremath{\\\\sigma}(x) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location x. The expected color C(r) of camera ray r(t) = o + td with near and far bounds tn and tf is: \\n\\nFig. 3: A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position x and viewing direction d. Here, we visualize example directional color distributions for two spatial locations in our neural representation of the Ship scene. In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing specular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions. \\n\\nThe function T (t) denotes the accumulated transmittance along the ray from tn to t, i.e., the probability that the ray travels from tn to t without hitting any other particle. Rendering a view from our continuous neural radiance field requires estimating this integral C(r) for a camera ray traced through each pixel of the desired virtual camera. \\n\\nWe numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation{\\\\textquoteright}s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition [tn, tf ] into N evenly-spaced bins and then draw one sample uniformly at random from within each bin: \\n\\n(2) \\n\\nAlthough we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate C(r) with the quadrature rule discussed in the volume rendering review by Max [(\\\\ensuremath{<}\\\\ensuremath{>})26]: \\n\\n(3) \\n\\nwhere \\\\ensuremath{\\\\delta}i = ti+1 \\\\ensuremath{-} ti is the distance between adjacent samples. This function for calculating C{\\\\textasciicircum}(r) from the set of (ci, \\\\ensuremath{\\\\sigma}i) values is trivially differentiable and reduces to traditional alpha compositing with alpha values \\\\ensuremath{\\\\alpha}i = 1 \\\\ensuremath{-} exp(\\\\ensuremath{-}\\\\ensuremath{\\\\sigma}i\\\\ensuremath{\\\\delta}i). \\n\\nGround Truth \\n\\nComplete Model \\n\\nNo View Dependence \\n\\nNo Positional Encoding \\n\\nFig. 4: Here we visualize how our full model benefits from representing view-dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the positional encoding drastically decreases the model{\\\\textquoteright}s ability to represent high frequency geometry and texture, resulting in an oversmoothed appearance. \\n\\n5 Optimizing a Neural Radiance Field \\n\\nIn the previous section we have described the core components necessary for modeling a scene as a neural radiance field and rendering novel views from this representation. However, we observe that these components are not sufficient for achieving state-of-the-art quality, as demonstrated in Section (\\\\ensuremath{<}\\\\ensuremath{>})6.4). We introduce two improvements to enable representing high-resolution complex scenes. The first is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions, and the second is a hierarchical sampling procedure that allows us to efficiently sample this high-frequency representation. \\n\\nDespite the fact that neural networks are universal function approximators [(\\\\ensuremath{<}\\\\ensuremath{>})14], we found that having the network F\\\\ensuremath{\\\\Theta} directly operate on xyz\\\\ensuremath{\\\\theta}\\\\ensuremath{\\\\varphi} input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman et al. [(\\\\ensuremath{<}\\\\ensuremath{>})35], which shows that deep networks are biased towards learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation. \\n\\nWe leverage these findings in the context of neural scene representations, and show that reformulating F\\\\ensuremath{\\\\Theta} as a composition of two functions F\\\\ensuremath{\\\\Theta} = F\\\\ensuremath{\\\\Theta} {\\\\textopenbullet} \\\\ensuremath{\\\\gamma}, one learned and one not, significantly improves performance (see Fig. (\\\\ensuremath{<}\\\\ensuremath{>})4 and Table (\\\\ensuremath{<}\\\\ensuremath{>})2). Here \\\\ensuremath{\\\\gamma} is a mapping from R into a higher dimensional space R2L , and F\\\\ensuremath{\\\\Theta} is still simply a regular MLP. Formally, the encoding function we use is: \\n\\n(4) \\n\\nThis function \\\\ensuremath{\\\\gamma}({\\\\textperiodcentered}) is applied separately to each of the three coordinate values in x (which are normalized to lie in [\\\\ensuremath{-}1, 1]) and to the three components of the \\n\\nCartesian viewing direction unit vector d (which by construction lie in [\\\\ensuremath{-}1, 1]). In our experiments, we set L = 10 for \\\\ensuremath{\\\\gamma}(x) and L = 4 for \\\\ensuremath{\\\\gamma}(d). \\n\\nA similar mapping is used in the popular Transformer architecture [(\\\\ensuremath{<}\\\\ensuremath{>})47], where it is referred to as a positional encoding. However, Transformers use it for a different goal of providing the discrete positions of tokens in a sequence as input to an architecture that does not contain any notion of order. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable our MLP to more easily approximate a higher frequency function. Concurrent work on a related problem of modeling 3D protein structure from projections [(\\\\ensuremath{<}\\\\ensuremath{>})51] also utilizes a similar input coordinate mapping. \\n\\nOur rendering strategy of densely evaluating the neural radiance field network at N query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. We draw inspiration from early work in volume rendering [(\\\\ensuremath{<}\\\\ensuremath{>})20] and propose a hierarchical representation that increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering. \\n\\nInstead of just using a single network to represent the scene, we simultaneously optimize two networks: one {\\\\textquotedblleft}coarse{\\\\textquotedblright} and one {\\\\textquotedblleft}fine{\\\\textquotedblright}. We first sample a set of Nc locations using stratified sampling, and evaluate the {\\\\textquotedblleft}coarse{\\\\textquotedblright} network at these locations as described in Eqns. (\\\\ensuremath{<}\\\\ensuremath{>})2 and (\\\\ensuremath{<}\\\\ensuremath{>})3. Given the output of this {\\\\textquotedblleft}coarse{\\\\textquotedblright} network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume. To do this, we first rewrite the alpha composited color from the coarse network C{\\\\textasciicircum} c(r) in Eqn. (\\\\ensuremath{<}\\\\ensuremath{>})3 as a weighted sum of all sampled colors ci along the ray: \\n\\n(5) \\n\\nNormalizing these weights as produces a piecewise-constant PDF along the ray. We sample a second set of Nf locations from this distribution using inverse transform sampling, evaluate our {\\\\textquotedblleft}fine{\\\\textquotedblright} network at the union of the first and second set of samples, and compute the final rendered color of the ray C{\\\\textasciicircum}f (r) using Eqn. (\\\\ensuremath{<}\\\\ensuremath{>})3 but using all Nc +Nf samples. This procedure allocates more samples to regions we expect to contain visible content. This addresses a similar goal as importance sampling, but we use the sampled values as a nonuniform discretization of the whole integration domain rather than treating each sample as an independent probabilistic estimate of the entire integral. \\n\\n(6) \\n\\nwhere R is the set of rays in each batch, and C(r), C{\\\\textasciicircum} c(r), and C{\\\\textasciicircum} f (r) are the ground truth, coarse volume predicted, and fine volume predicted RGB colors for ray r respectively. Note that even though the final rendering comes from C{\\\\textasciicircum} f (r), we also minimize the loss of C{\\\\textasciicircum} c(r) so that the weight distribution from the coarse network can be used to allocate samples in the fine network. \\n\\nIn our experiments, we use a batch size of 4096 rays, each sampled at Nc = 64 coordinates in the coarse volume and Nf = 128 additional coordinates in the fine volume. We use the Adam optimizer [(\\\\ensuremath{<}\\\\ensuremath{>})18] with a learning rate that begins at 5 {\\\\texttimes} 10\\\\ensuremath{-}4 and decays exponentially to 5 {\\\\texttimes} 10\\\\ensuremath{-}5 over the course of optimization (other Adam hyperparameters are left at default values of \\\\ensuremath{\\\\beta}1 = 0.9, \\\\ensuremath{\\\\beta}2 = 0.999, and  = 10\\\\ensuremath{-}7). The optimization for a single scene typically take around 100{\\\\textendash} 300k iterations to converge on a single NVIDIA V100 GPU (about 1{\\\\textendash}2 days). \\n\\n6 Results \\n\\nWe quantitatively (Tables (\\\\ensuremath{<}\\\\ensuremath{>})1) and qualitatively (Figs. (\\\\ensuremath{<}\\\\ensuremath{>})8 and (\\\\ensuremath{<}\\\\ensuremath{>})6) show that our method outperforms prior work, and provide extensive ablation studies to validate our design choices (Table (\\\\ensuremath{<}\\\\ensuremath{>})2). We urge the reader to view our supplementary video to better appreciate our method{\\\\textquoteright}s significant improvement over baseline methods when rendering smooth paths of novel views. \\n\\nSynthetic renderings of objects We first show experimental results on two datasets of synthetic renderings of objects (Table (\\\\ensuremath{<}\\\\ensuremath{>})1, {\\\\textquotedblleft}Diffuse Synthetic 360{\\\\textopenbullet}{\\\\textquotedblright} and {\\\\textquotedblleft}Realistic Synthetic 360{\\\\textopenbullet}{\\\\textquotedblright}). The DeepVoxels [(\\\\ensuremath{<}\\\\ensuremath{>})41] dataset contains four Lamber-tian objects with simple geometry. Each object is rendered at 512 {\\\\texttimes} 512 pixels from viewpoints sampled on the upper hemisphere (479 as input and 1000 for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at 800 {\\\\texttimes} 800 pixels. \\n\\nTable 1: Our method quantitatively outperforms prior work on datasets of both synthetic and real images. We report PSNR/SSIM (higher is better) and LPIPS [(\\\\ensuremath{<}\\\\ensuremath{>})50] (lower is better). The DeepVoxels [(\\\\ensuremath{<}\\\\ensuremath{>})41] dataset consists of 4 diffuse objects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian materials. The real dataset consists of handheld forward-facing captures of 8 real-world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Though LLFF achieves slightly better LPIPS, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines. \\n\\nReal images of complex scenes We show results on complex real-world scenes captured with roughly forward-facing images (Table (\\\\ensuremath{<}\\\\ensuremath{>})1, {\\\\textquotedblleft}Real Forward-Facing{\\\\textquotedblright}). This dataset consists of 8 scenes captured with a handheld cellphone (5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62 images, and hold out 1/8 of these for the test set. All images are 1008{\\\\texttimes}756 pixels. \\n\\nTo evaluate our model we compare against current top-performing techniques for view synthesis, detailed below. All methods use the same set of input views to train a separate network for each scene except Local Light Field Fusion [(\\\\ensuremath{<}\\\\ensuremath{>})28], which trains a single 3D convolutional network on a large dataset, then uses the same trained network to process input images of new scenes at test time. \\n\\nNeural Volumes (NV) [(\\\\ensuremath{<}\\\\ensuremath{>})24] synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D convolutional network to predict a discretized RGB\\\\ensuremath{\\\\alpha} voxel grid with 1283 samples as well as a 3D warp grid with 323 samples. The algorithm renders novel views by marching camera rays through the warped voxel grid. \\n\\nScene Representation Networks (SRN) [(\\\\ensuremath{<}\\\\ensuremath{>})42] represent a continuous scene as an opaque surface, implicitly defined by a MLP that maps each (x, y, z) coordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the final step is decoded into a single color for that point on the surface. Note that SRN is a better-performing followup to DeepVoxels [(\\\\ensuremath{<}\\\\ensuremath{>})41] by the same authors, which is why we do not include comparisons to DeepVoxels. \\n\\nFig. 5: Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically-based renderer. Our method is able to recover fine details in both geometry and appearance, such as Ship{\\\\textquoteright}s rigging, Lego{\\\\textquoteright}s gear and treads, Microphone{\\\\textquoteright}s shiny stand and mesh grille, and Material{\\\\textquoteright}s non-Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone stand and Material{\\\\textquoteright}s object edges and ghosting artifacts in Ship{\\\\textquoteright}s mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone{\\\\textquoteright}s grille or Lego{\\\\textquoteright}s gears, and it completely fails to recover the geometry of Ship{\\\\textquoteright}s rigging. \\n\\nFig. 6: Comparisons on test-set views of real world scenes. LLFF is specifically designed for this use case (forward-facing captures of real scenes). Our method is able to represent fine geometry more consistently across rendered views than LLFF, as shown in Fern{\\\\textquoteright}s leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any fine detail. \\n\\nLocal Light Field Fusion (LLFF) [(\\\\ensuremath{<}\\\\ensuremath{>})28] LLFF is designed for producing pho-torealistic novel views for well-sampled forward facing scenes. It uses a trained 3D convolutional network to directly predict a discretized frustum-sampled RGB\\\\ensuremath{\\\\alpha} grid (multiplane image or MPI [(\\\\ensuremath{<}\\\\ensuremath{>})52]) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint. \\n\\nWe thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set. \\n\\nThe SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying explicit 1283 voxel grid prevents it from scaling to represent fine details at high resolutions. LLFF specifically provides a {\\\\textquotedblleft}sampling guideline{\\\\textquotedblright} to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate correct geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between views. Additionally, LLFF blends between different scene representations for rendering different views, resulting in perceptually-distracting inconsistency as is apparent in our supplementary video. \\n\\nThe biggest practical tradeoffs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under 10 minutes. However, LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one {\\\\textquotedblleft}Realistic Synthetic{\\\\textquotedblright} scene). Our method requires only 5 MB for the network weights (a relative compression of 3000{\\\\texttimes} compared to LLFF), which is even less memory than the input images alone for a single scene from any of our datasets. \\n\\nWe validate our algorithm{\\\\textquoteright}s design choices and parameters with an extensive ablation study in Table (\\\\ensuremath{<}\\\\ensuremath{>})2. We present results on our {\\\\textquotedblleft}Realistic Synthetic 360{\\\\textopenbullet}{\\\\textquotedblright} scenes. Row 9 shows our complete model as a point of reference. Row 1 shows a minimalist version of our model without positional encoding (PE), view-dependence (VD), or hierarchical sampling (H). In rows 2{\\\\textendash}4 we remove these three components one at a time from the full model, observing that positional encoding (row 2) and view-dependence (row 3) provide the largest quantitative benefit followed by hierarchical sampling (row 4). Rows 5{\\\\textendash}6 show how our performance decreases as the number of input images is reduced. Note that our method{\\\\textquoteright}s performance using only 25 input images still exceeds NV, SRN, and LLFF across all metrics when they are provided with 100 images (see supplementary material). In rows 7{\\\\textendash}8 we validate our choice of the maximum frequency \\n\\nTable 2: An ablation study of our model. Metrics are averaged over the 8 scenes from our realistic synthetic dataset. See Sec. (\\\\ensuremath{<}\\\\ensuremath{>})6.4 for detailed descriptions. \\n\\nL used in our positional encoding for x (the maximum frequency used for d is scaled proportionally). Only using 5 frequencies reduces performance, but increasing the number of frequencies from 10 to 15 does not improve performance. We believe the benefit of increasing L is limited once 2L exceeds the maximum frequency present in the sampled input images (roughly 1024 in our data). \\n\\n7 Conclusion \\n\\nOur work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that representing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations. \\n\\nAlthough we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and render neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes. \\n\\nReferences \\n\\nA Additional Implementation Details \\n\\nNetwork Architecture Fig. (\\\\ensuremath{<}\\\\ensuremath{>})7 details our simple fully-connected architecture. \\n\\nVolume Bounds Our method renders views by querying the neural radiance field representation at continuous 5D coordinates along camera rays. For experiments with synthetic images, we scale the scene so that it lies within a cube of side length 2 centered at the origin, and only query the representation within this bounding volume. Our dataset of real images contains content that can exist anywhere between the closest point and infinity, so we use normalized device coordinates to map the depth range of these points into [\\\\ensuremath{-}1, 1]. This shifts all the ray origins to the near plane of the scene, maps the perspective rays of the camera to parallel rays in the transformed volume, and uses disparity (inverse depth) instead of metric depth, so all coordinates are now bounded. \\n\\nTraining Details For real scene data, we regularize our network by adding random Gaussian noise with zero mean and unit variance to the output \\\\ensuremath{\\\\sigma} values (before passing them through the ReLU) during optimization, finding that this slightly improves visual performance for rendering novel views. We implement our model in Tensorflow [(\\\\ensuremath{<}\\\\ensuremath{>})1]. \\n\\nRendering Details To render new views at test time, we sample 64 points per ray through the coarse network and 64 + 128 = 192 points per ray through the fine network, for a total of 256 network queries per ray. Our realistic synthetic \\n\\nFig. 7: A visualization of our fully-connected network architecture. Input vectors are shown in green, intermediate hidden layers are shown in blue, output vectors are shown in red, and the number inside each block signifies the vector{\\\\textquoteright}s dimension. All layers are standard fully-connected layers, black arrows indicate layers with ReLU activations, orange arrows indicate layers with no activation, dashed black arrows indicate layers with sigmoid activation, and {\\\\textquotedblleft}+{\\\\textquotedblright} denotes vector concatenation. The positional encoding of the input location (\\\\ensuremath{\\\\gamma}(x)) is passed through 8 fully-connected ReLU layers, each with 256 channels. We follow the DeepSDF [(\\\\ensuremath{<}\\\\ensuremath{>})32] architecture and include a skip connection that concatenates this input to the fifth layer{\\\\textquoteright}s activation. An additional layer outputs the volume density \\\\ensuremath{\\\\sigma} (which is rectified using a ReLU to ensure that the output volume density is nonnegative) and a 256-dimensional feature vector. This feature vector is concatenated with the positional encoding of the input viewing direction (\\\\ensuremath{\\\\gamma}(d)), and is processed by an additional fully-connected ReLU layer with 128 channels. A final layer (with a sigmoid activation) outputs the emitted RGB radiance at position x, as viewed by a ray with direction d. \\n\\ndataset requires 640k rays per image, and our real scenes require 762k rays per image, resulting in between 150 and 200 million network queries per rendered image. On an NVIDIA V100, this takes approximately 30 seconds per frame. \\n\\nB Additional Baseline Method Details \\n\\nNeural Volumes (NV) [(\\\\ensuremath{<}\\\\ensuremath{>})24] We use the NV code open-sourced by the authors at (\\\\ensuremath{<}https://github.com/facebookresearch/neuralvolumes\\\\ensuremath{>})https://github.com/facebookresearch/neuralvolumes and follow their procedure for training on a single scene without time dependence. \\n\\nScene Representation Networks (SRN) [(\\\\ensuremath{<}\\\\ensuremath{>})42] We use the SRN code open-sourced by the authors at (\\\\ensuremath{<}https://github.com/vsitzmann/scene-representation-networks\\\\ensuremath{>})https://github.com/vsitzmann/scene-representation-networks and follow their procedure for training on a single scene. \\n\\nLocal Light Field Fusion (LLFF) [(\\\\ensuremath{<}\\\\ensuremath{>})28] We use the pretrained LLFF model open-sourced by the authors at (\\\\ensuremath{<}https://github.com/Fyusion/LLFF\\\\ensuremath{>})https://github.com/Fyusion/LLFF. \\n\\nQuantitative Comparisons The SRN implementation published by the authors requires a significant amount of GPU memory, and is limited to an image resolution of 512 {\\\\texttimes} 512 pixels even when parallelized across 4 NVIDIA V100 GPUs. We compute quantitative metrics for SRN at 512 {\\\\texttimes} 512 pixels for our synthetic datasets and 504 {\\\\texttimes} 376 pixels for the real datasets, in comparison to 800 {\\\\texttimes} 800 and 1008 {\\\\texttimes} 752 respectively for the other methods that can be run at higher resolutions. \\n\\nC NDC ray space derivation \\n\\nWe reconstruct real scenes with {\\\\textquotedblleft}forward facing{\\\\textquotedblright} captures in the normalized device coordinate (NDC) space that is commonly used as part of the triangle rasterization pipeline. This space is convenient because it preserves parallel lines while converting the z axis (camera axis) to be linear in disparity. \\n\\nHere we derive the transformation which is applied to rays to map them from camera space to NDC space. The standard 3D perspective projection matrix for homogeneous coordinates is: \\n\\n(7) \\n\\nwhere n, f are the near and far clipping planes and r and t are the right and top bounds of the scene at the near clipping plane. (Note that this is in the convention where the camera is looking in the \\\\ensuremath{-}z direction.) To project a homogeneous point (x, y, z, 1) , we left-multiply by M and then divide by the fourth coordinate: \\n\\n(8) \\n\\n(9) \\n\\nThe projected point is now in normalized device coordinate (NDC) space, where the original viewing frustum has been mapped to the cube [\\\\ensuremath{-}1, 1]3 . \\n\\nOur goal is to take a ray o + td and calculate a ray origin o and direction d in NDC space such that for every t, there exists a new t for which \\\\ensuremath{\\\\pi}(o + td) = o + td (where \\\\ensuremath{\\\\pi} is projection using the above matrix). In other words, the projection of the original ray and the NDC space ray trace out the same points (but not necessarily at the same rate). \\n\\nLet us rewrite the projected point from Eqn. (\\\\ensuremath{<}\\\\ensuremath{>})9 as (axx/z, ayy/z, az +bz/z) . The components of the new origin o and direction d must satisfy: \\n\\n(10) \\n\\nTo eliminate a degree of freedom, we decide that t = 0 and t = 0 should map to the same point. Substituting t = 0 and t = 0 Eqn. (\\\\ensuremath{<}\\\\ensuremath{>})10 directly gives our NDC space origin o: \\n\\n(11) \\n\\nThis is exactly the projection \\\\ensuremath{\\\\pi}(o) of the original ray{\\\\textquoteright}s origin. By substituting this back into Eqn. (\\\\ensuremath{<}\\\\ensuremath{>})10 for arbitrary t, we can determine the values of t and d: \\n\\n(12) \\n\\n(13) \\n\\n(14) \\n\\nFactoring out a common expression that depends only on t gives us: \\n\\n(15) \\n\\n(16) \\n\\nNote that, as desired, t = 0 when t = 0. Additionally, we see that t {\\\\textrightarrow} 1 as t {\\\\textrightarrow} \\\\ensuremath{\\\\infty}. Going back to the original projection matrix, our constants are: \\n\\n(17) \\n\\n(18) \\n\\n(19) \\n\\n(20) \\n\\nUsing the standard pinhole camera model, we can reparameterize as: \\n\\n(21) \\n\\n(22) \\n\\nwhere W and H are the width and height of the image in pixels and fcam is the focal length of the camera. \\n\\nIn our real forward facing captures, we assume that the far scene bound is infinity (this costs us very little since NDC uses the z dimension to represent inverse depth, i.e., disparity). In this limit the z constants simplify to: \\n\\n(23) \\n\\n(24) \\n\\nCombining everything together: \\n\\n(25) \\n\\n(26) \\n\\nOne final detail in our implementation: we shift o to the ray{\\\\textquoteright}s intersection with the near plane at z = \\\\ensuremath{-}n (before this NDC conversion) by taking on = o + tnd for tn = \\\\ensuremath{-}(n+ oz)/dz. Once we convert to the NDC ray, this allows us to simply sample t linearly from 0 to 1 in order to get a linear sampling in disparity from n to \\\\ensuremath{\\\\infty} in the original space. \\n\\nPedestal \\n\\nCube \\n\\nFig. 8: Comparisons on test-set views for scenes from the DeepVoxels [(\\\\ensuremath{<}\\\\ensuremath{>})41] synthetic dataset. The objects in this dataset have simple geometry and perfectly diffuse reflectance. Because of the large number of input images (479 views) and simplicity of the rendered objects, both our method and LLFF [(\\\\ensuremath{<}\\\\ensuremath{>})28] perform nearly perfectly on this data. LLFF still occasionally presents artifacts when interpolating between its 3D volumes, as in the top inset for each object. SRN [(\\\\ensuremath{<}\\\\ensuremath{>})42] and NV [(\\\\ensuremath{<}\\\\ensuremath{>})24] do not have the representational power to render fine details. \\n\\nD Additional Results \\n\\nPer-scene breakdown Tables (\\\\ensuremath{<}\\\\ensuremath{>})3, (\\\\ensuremath{<}\\\\ensuremath{>})4, (\\\\ensuremath{<}\\\\ensuremath{>})5, and (\\\\ensuremath{<}\\\\ensuremath{>})6 include a breakdown of the quantitative results presented in the main paper into per-scene metrics. The per-scene breakdown is consistent with the aggregate quantitative metrics presented in the paper, where our method quantitatively outperforms all baselines. Although LLFF achieves slightly better LPIPS metrics, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines. \\n\\nPSNR{\\\\textuparrow} \\n\\nLPIPS{\\\\textdownarrow} \\n\\nTable 4: Per-scene quantitative results from our realistic synthetic dataset. The {\\\\textquotedblleft}scenes{\\\\textquotedblright} in this dataset are all objects with more complex gometry and non-Lambertian materials, rendered using Blender{\\\\textquoteright}s Cycles pathtracer. \\n\\nTable 5: Per-scene quantitative results from our real image dataset. The scenes in this dataset are all captured with a forward-facing handheld cellphone. \\n\\nTable 6: Per-scene quantitative results from our ablation study. The scenes used here are the same as in Table (\\\\ensuremath{<}\\\\ensuremath{>})4. \\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.txt'}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding \\n\\nALEXANDER KELLER, NVIDIA, Germany \\n\\nFig. 1. We demonstrate instant training of neural graphics primitives on a single GPU for multiple tasks. In Gigapixel image we represent a gigapixel image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. Neural radiance caching (NRC) [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] employs a neural network that is trained in real-time to cache costly lighting calculations. Lastly, NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. In all tasks, our encoding and its efficient implementation provide clear benefits: rapid training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance. Tokyo gigapixel photograph {\\\\textcopyright}Trevor Dobson (\\\\ensuremath{<}https://creativecommons.org/licenses/by-nc-nd/2.0/\\\\ensuremath{>})(CC BY-NC-ND 2.0), Lego bulldozer 3D model {\\\\textcopyright}H\\\\r{a}vard Dalen (\\\\ensuremath{<}https://creativecommons.org/licenses/by-nc/2.0/\\\\ensuremath{>})(CC BY-NC 2.0) \\n\\nNeural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a \\n\\nAuthors{\\\\textquoteright} addresses: Thomas M\\\\\"uller, NVIDIA, Z\\\\\"urich, Switzerland, tmueller@nvidia. com; Alex Evans, NVIDIA, London, United Kingdom, alexe@nvidia.com; Christoph Schied, NVIDIA, Seattle, USA, cschied@nvidia.com; Alexander Keller, NVIDIA, Berlin, Germany, akeller@nvidia.com. \\n\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. {\\\\textcopyright} 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0730-0301/2022/7-ART102 \\\\$15.00 \\n\\nmultiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920{\\\\texttimes}1080. \\n\\nCCS Concepts: {\\\\textbullet} Computing methodologies {\\\\textrightarrow} Massively parallel algorithms; Vector / streaming algorithms; Neural networks. \\n\\nAdditional Key Words and Phrases: Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. \\n\\nThomas M\\\\\"uller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans. Graph. 41, 4, Article 102 (July 2022), (\\\\ensuremath{<}\\\\ensuremath{>})15 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3528223.3530127\\\\ensuremath{>})https://doi.org/10. (\\\\ensuremath{<}https://doi.org/10.1145/3528223.3530127\\\\ensuremath{>})1145/3528223.3530127 \\n\\n1 INTRODUCTION \\n\\nComputer graphics primitives are fundamentally represented by mathematical functions that parameterize appearance. The quality and performance characteristics of the mathematical representation are crucial for visual fidelity: we desire representations that remain fast and compact while capturing high-frequency, local detail. Functions represented by multi-layer perceptrons (MLPs), used as neural graphics primitives, have been shown to match these criteria (to varying degree), for example as representations of shape [(\\\\ensuremath{<}\\\\ensuremath{>})Martel (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Park et al. (\\\\ensuremath{<}\\\\ensuremath{>})2019] and radiance fields [(\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020, (\\\\ensuremath{<}\\\\ensuremath{>})2021]. \\n\\nThe important commonality of the these approaches is an encoding that maps neural network inputs to a higher-dimensional space, which is key for extracting high approximation quality from compact models. Most successful among these encodings are trainable, task-specific data structures [(\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] that take on a large portion of the learning task. This enables the use of smaller, more efficient MLPs. However, such data structures rely on heuristics and structural modifications (such as pruning, splitting, or merging) that may complicate the training process, limit the method to a specific task, or limit performance on GPUs where control flow and pointer chasing is expensive. \\n\\nWe address these concerns with our multiresolution hash encoding, which is adaptive and efficient, independent of the task. It is configured by just two values{\\\\textemdash}the number of parameters \\\\ensuremath{\\\\mathit{T}} and the desired finest resolution \\\\ensuremath{\\\\mathit{N}}max{\\\\textemdash}yielding state-of-the-art quality on a variety of tasks ((\\\\ensuremath{<}\\\\ensuremath{>})Figure 1) after a few seconds of training. \\n\\nKey to both the task-independent adaptivity and efficiency is a multiresolution hierarchy of hash tables: \\n\\nWe validate our multiresolution hash encoding in four representative tasks (see (\\\\ensuremath{<}\\\\ensuremath{>})Figure 1): \\n\\nThe source code and data pertaining to this paper can be found at (\\\\ensuremath{<}https://nvlabs.github.io/instant-ngp\\\\ensuremath{>})https://nvlabs.github.io/instant-ngp. \\n\\nIn the following, we first review prior neural network encodings ((\\\\ensuremath{<}\\\\ensuremath{>})Section 2), then we describe our encoding ((\\\\ensuremath{<}\\\\ensuremath{>})Section 3) and its implementation ((\\\\ensuremath{<}\\\\ensuremath{>})Section 4), followed lastly by our experiments ((\\\\ensuremath{<}\\\\ensuremath{>})Section 5) and discussion thereof ((\\\\ensuremath{<}\\\\ensuremath{>})Section 6). \\n\\n2 BACKGROUND AND RELATED WORK \\n\\nEarly examples of encoding the inputs of a machine learning model into a higher-dimensional space include the one-hot encoding [(\\\\ensuremath{<}\\\\ensuremath{>})Har-(\\\\ensuremath{<}\\\\ensuremath{>})ris and Harris (\\\\ensuremath{<}\\\\ensuremath{>})2013] and the kernel trick [(\\\\ensuremath{<}\\\\ensuremath{>})Theodoridis (\\\\ensuremath{<}\\\\ensuremath{>})2008] by which complex arrangements of data can be made linearly separable. \\n\\nFor neural networks, input encodings have proven useful in the attention components of recurrent architectures [(\\\\ensuremath{<}\\\\ensuremath{>})Gehring et al. (\\\\ensuremath{<}\\\\ensuremath{>})2017] and, subsequently, transformers [(\\\\ensuremath{<}\\\\ensuremath{>})Vaswani et al. (\\\\ensuremath{<}\\\\ensuremath{>})2017], where they help the neural network to identify the location it is currently processing. Vaswani et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2017] encode scalar positions \\\\ensuremath{\\\\mathit{x}} \\\\ensuremath{\\\\in} R as a multiresolution sequence of \\\\ensuremath{\\\\mathit{L}} \\\\ensuremath{\\\\in} N sine and cosine functions \\n\\nThis has been adopted in computer graphics to encode the spatio-directionally varying light field and volume density in the NeRF algorithm [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020]. The five dimensions of this light field are independently encoded using the above formula; this was later extended to randomly oriented parallel wavefronts [(\\\\ensuremath{<}\\\\ensuremath{>})Tancik (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] and level-of-detail filtering [(\\\\ensuremath{<}\\\\ensuremath{>})Barron et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a]. We will refer to this family of encodings as frequency encodings. Notably, frequency encodings followed by a linear transformation have been used in other computer graphics tasks, such as approximating the visibility function [(\\\\ensuremath{<}\\\\ensuremath{>})Annen et al. (\\\\ensuremath{<}\\\\ensuremath{>})2007; (\\\\ensuremath{<}\\\\ensuremath{>})Jansen and Bavoil (\\\\ensuremath{<}\\\\ensuremath{>})2010]. \\n\\nM\\\\\"uller et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2019; (\\\\ensuremath{<}\\\\ensuremath{>})2020] suggested a continuous variant of the one-hot encoding based on rasterizing a kernel, the one-blob encoding, which can achieve more accurate results than frequency encodings in bounded domains at the cost of being single-scale. \\n\\nParametric encodings. Recently, state-of-the-art results have been achieved by parametric encodings which blur the line between classical data structures and neural approaches. The idea is to arrange additional trainable parameters (beyond weights and biases) in an auxiliary data structure, such as a grid [(\\\\ensuremath{<}\\\\ensuremath{>})Chabra et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Jiang (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Mehta et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Peng et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020a; (\\\\ensuremath{<}\\\\ensuremath{>})Sun (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Tang et al. (\\\\ensuremath{<}\\\\ensuremath{>})2018; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a] or a tree [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021], and to look-up and (optionally) interpolate these parameters depending on the input vector x \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{d}} . This arrangement trades a larger memory footprint for a smaller computational cost: whereas for each gradient propagated backwards through the network, every weight in the fully connected MLP network must be updated, for the trainable input encoding parameters ({\\\\textquotedblleft}feature vectors{\\\\textquotedblright}), only a very small number are affected. For example, with a trilinearly interpolated 3D grid of feature vectors, only 8 such grid points need to be updated for each sample back-propagated to the encoding. In this way, although the total number of parameters is much higher for a parametric encoding than a fixed input encoding, the number of FLOPs and memory accesses required for the update during training is not increased significantly. By reducing the size of the MLP, \\n\\n(a) No encoding \\n\\n411 k + 0 parameters 11:28 (mm:ss) / PSNR 18.56 \\n\\n(b) Frequency [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] \\n\\n438 k + 0 12:45 / PSNR 22.90 \\n\\n(c) Dense grid Single resolution \\n\\n10 k + 16.3 M 1:26 / PSNR 23.62 \\n\\n(e) Hash table (ours) \\\\ensuremath{\\\\mathit{T}} = 214 \\n\\n10 k + 494 k 1:48 / PSNR 22.61 \\n\\n(f) Hash table (ours) \\\\ensuremath{\\\\mathit{T}} = 219 \\n\\n10 k + 12.6 M 1:47 / PSNR 24.58 \\n\\nFig. 2. A demonstration of the reconstruction quality of different encodings and parametric data structures for storing trainable feature embeddings. Each configuration was trained for 11 000 steps using our fast NeRF implementation ((\\\\ensuremath{<}\\\\ensuremath{>})Section 5.4), varying only the input encoding and MLP size. The number of trainable parameters (MLP weights + encoding parameters), training time and reconstruction accuracy (PSNR) are shown below each image. Our encoding (e) with a similar total number of trainable parameters as the frequency encoding configuration (b) trains over 8{\\\\texttimes} faster, due to the sparsity of updates to the parameters and smaller MLP. Increasing the number of parameters (f) further improves reconstruction accuracy without significantly increasing training time. \\n\\nsuch parametric models can typically be trained to convergence much faster without sacrificing approximation quality. \\n\\nAnother parametric approach uses a tree subdivision of the domain R\\\\ensuremath{\\\\mathit{d}} , wherein a large auxiliary coordinate encoder neural network (ACORN) [(\\\\ensuremath{<}\\\\ensuremath{>})Martel et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] is trained to output dense feature grids in the leaf node around x. These dense feature grids, which have on the order of 10 000 entries, are then linearly interpolated, as in (\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2020]. This approach tends to yield a larger degree of adaptivity compared with the previous parametric encodings, albeit at greater computational cost which can only be amortized when sufficiently many inputs x fall into each leaf node. \\n\\nSparse parametric encodings. While existing parametric encodings tend to yield much greater accuracy than their non-parametric predecessors, they also come with downsides in efficiency and versatility. Dense grids of trainable features consume much more memory than the neural network weights. To illustrate the trade-offs and to motivate our method, (\\\\ensuremath{<}\\\\ensuremath{>})Figure 2 shows the effect on reconstruction quality of a neural radiance field for several different encodings. Without any input encoding at all (a), the network is only able to learn a fairly smooth function of position, resulting in a poor approximation of the light field. The frequency encoding (b) allows the same moderately sized network (8 hidden layers, each 256 wide) to represent the scene much more accurately. The middle image (c) pairs a smaller network with a dense grid of 1283 trilinearly interpolated, 16-dimensional feature vectors, for a total of 33.6 million trainable parameters. The large number of trainable parameters can be efficiently updated, as each sample only affects 8 grid points. \\n\\nHowever, the dense grid is wasteful in two ways. First, it allocates as many features to areas of empty space as it does to those areas near the surface. The number of parameters grows as O(\\\\ensuremath{\\\\mathit{N}} 3), while the visible surface of interest has surface area that grows only as O(\\\\ensuremath{\\\\mathit{N}} 2). In this example, the grid has resolution 1283 , but only 53 807 (2.57\\\\%) of its cells touch the visible surface. \\n\\nSecond, natural scenes exhibit smoothness, motivating the use of a multi-resolution decomposition [(\\\\ensuremath{<}\\\\ensuremath{>})Chibane et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Hadadan (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. (\\\\ensuremath{<}\\\\ensuremath{>})Figure 2 (d) shows the result of using an encoding in \\n\\nwhich interpolated features are stored in eight co-located grids with resolutions from 163 to 1733 , each containing 2-dimensional feature vectors. These are concatenated to form a 16-dimensional (same as (c)) input to the network. Despite having less than half the number of parameters as (c), the reconstruction quality is similar. \\n\\nIf the surface of interest is known a priori, a data structure such as an octree [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] or sparse grid [(\\\\ensuremath{<}\\\\ensuremath{>})Chabra et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Chibane et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Hadadan et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Jiang et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Liu (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Peng et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020a] can be used to cull away the unused features in the dense grid. However, in the NeRF setting, surfaces only emerge during training. NSVF [(\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] and several concurrent works [(\\\\ensuremath{<}\\\\ensuremath{>})Sun et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a] adopt a multistage, coarse to fine strategy in which regions of the feature grid are progressively refined and culled away as necessary. While effective, this leads to a more complex training process in which the sparse data structure must be periodically updated. \\n\\nOur method{\\\\textemdash}(\\\\ensuremath{<}\\\\ensuremath{>})Figure 2 (e,f){\\\\textemdash}combines both ideas to reduce waste. We store the trainable feature vectors in a compact spatial hash table, whose size is a hyper-parameter \\\\ensuremath{\\\\mathit{T}} which can be tuned to trade the number of parameters for reconstruction quality. It neither relies on progressive pruning during training nor on a priori knowledge of the geometry of the scene. Analogous to the multi-resolution grid in (d), we use multiple separate hash tables indexed at different resolutions, whose interpolated outputs are concatenated before being passed through the MLP. The reconstruction quality is comparable to the dense grid encoding, despite having 20{\\\\texttimes} fewer parameters. \\n\\nUnlike prior work that used spatial hashing [(\\\\ensuremath{<}\\\\ensuremath{>})Teschner et al. (\\\\ensuremath{<}\\\\ensuremath{>})2003] for 3D reconstruction [(\\\\ensuremath{<}\\\\ensuremath{>})Nieer et al. (\\\\ensuremath{<}\\\\ensuremath{>})2013], we do not explicitly handle collisions of the hash functions by typical means like probing, bucketing, or chaining. Instead, we rely on the neural network to learn to disambiguate hash collisions itself, avoiding control flow divergence, reducing implementation complexity and improving performance. Another performance benefit is the predictable memory layout of the hash tables that is independent of the data that is represented. While good caching behavior is often hard to achieve with tree-like data structures, our hash tables can be fine-tuned for low-level architectural details such as cache size. \\n\\nFig. 3. Illustration of the multiresolution hash encoding in 2D. (1) for a given input coordinate x, we find the surrounding voxels at \\\\ensuremath{\\\\mathit{L}} resolution levels and assign indices to their corners by hashing their integer coordinates. (2) for all resulting corner indices, we look up the corresponding \\\\ensuremath{\\\\mathit{F}} -dimensional feature vectors from the hash tables \\\\ensuremath{\\\\mathit{l}} and (3) linearly interpolate them according to the relative position of x within the respective \\\\ensuremath{\\\\mathit{l}}-th voxel. (4) we concatenate the result of each level, as well as auxiliary inputs  \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{E}} , producing the encoded MLP input \\\\ensuremath{\\\\mathit{y}} \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{L}}\\\\ensuremath{\\\\mathit{F}} +\\\\ensuremath{\\\\mathit{E}} , which (5) is evaluated last. To train the encoding, loss gradients are backpropagated through the MLP (5), the concatenation (4), the linear interpolation (3), and then accumulated in the looked-up feature vectors. \\n\\nTable 1. Hash encoding parameters and their ranges in this paper. Only the hash table size \\\\ensuremath{\\\\mathit{T}} and max. resolution \\\\ensuremath{\\\\mathit{N}}max need to be tuned to the task. \\n\\n3 MULTIRESOLUTION HASH ENCODING \\n\\nGiven a fully connected neural network \\\\ensuremath{\\\\mathit{m}} (y; \\\\ensuremath{\\\\Phi}), we are interested in an encoding of its inputs y = enc(x;  ) that improves the approximation quality and training speed across a wide range of applications without incurring a notable performance overhead. Our neural network not only has trainable weight parameters \\\\ensuremath{\\\\Phi}, but also trainable encoding parameters  . These are arranged into \\\\ensuremath{\\\\mathit{L}} levels, each containing up to \\\\ensuremath{\\\\mathit{T}} feature vectors with dimensionality \\\\ensuremath{\\\\mathit{F}} . Typical values for these hyperparameters are shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table 1. (\\\\ensuremath{<}\\\\ensuremath{>})Figure 3 illustrates the steps performed in our multiresolution hash encoding. Each level (two of which are shown as red and blue in the figure) is independent and conceptually stores feature vectors at the vertices of a grid, the resolution of which is chosen to be a geometric progression between the coarsest and finest resolutions [\\\\ensuremath{\\\\mathit{N}}min, \\\\ensuremath{\\\\mathit{N}}max]: \\n\\n(2) \\n\\n(3) \\n\\n\\\\ensuremath{\\\\mathit{N}}max is chosen to match the finest detail in the training data. Due to the large number of levels \\\\ensuremath{\\\\mathit{L}}, the growth factor is usually small. Our use cases have \\\\ensuremath{\\\\mathit{b}} \\\\ensuremath{\\\\in} [1.26, 2]. \\n\\nConsider a single level \\\\ensuremath{\\\\mathit{l}} . The input coordinate x \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{d}} is scaled by that level{\\\\textquoteright}s grid resolution before rounding down and up \\\\ensuremath{\\\\lfloor}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rfloor} := \\\\ensuremath{\\\\lfloor}x {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{N}}\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rfloor}, \\\\ensuremath{\\\\lceil}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rceil} := \\\\ensuremath{\\\\lceil}x {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{N}}\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rceil}. \\n\\n\\\\ensuremath{\\\\lfloor}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rfloor} and \\\\ensuremath{\\\\lceil}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rceil} span a voxel with 2\\\\ensuremath{\\\\mathit{d}} integer vertices in Z\\\\ensuremath{\\\\mathit{d}} . We map each corner to an entry in the level{\\\\textquoteright}s respective feature vector array, which has fixed size of at most\\\\ensuremath{\\\\mathit{T}} . For coarse levels where a dense grid requires fewer than \\\\ensuremath{\\\\mathit{T}} parameters, i.e. (\\\\ensuremath{\\\\mathit{N}}\\\\ensuremath{\\\\mathit{l}} + 1)\\\\ensuremath{\\\\mathit{d}} \\\\ensuremath{\\\\leq} \\\\ensuremath{\\\\mathit{T}} , this mapping is 1:1. At finer levels, we use a hash function \\\\ensuremath{h} : Z\\\\ensuremath{\\\\mathit{d}} {\\\\textrightarrow} Z\\\\ensuremath{\\\\mathit{T}} to index into the array, effectively treating it as a hash table, although there is no explicit collision handling. We rely instead on the gradient-based optimization to store appropriate sparse detail in the array, and the subsequent neural network \\\\ensuremath{\\\\mathit{m}}(y; \\\\ensuremath{\\\\Phi}) for collision resolution. The number of trainable encoding parameters  is therefore O(\\\\ensuremath{\\\\mathit{T}} ) and bounded by \\\\ensuremath{\\\\mathit{T}} {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{L}} {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{F}} which in our case is always \\\\ensuremath{\\\\mathit{T}} {\\\\textperiodcentered} 16 {\\\\textperiodcentered} 2 ((\\\\ensuremath{<}\\\\ensuremath{>})Table 1). We use a spatial hash function [(\\\\ensuremath{<}\\\\ensuremath{>})Teschner et al. (\\\\ensuremath{<}\\\\ensuremath{>})2003] of the form \\n\\n(4) \\n\\nwhere \\\\ensuremath{\\\\oplus} denotes the bit-wise XOR operation and \\\\ensuremath{\\\\mathit{i}} are unique, large prime numbers. Effectively, this formula XORs the results of a per-dimension linear congruential (pseudo-random) permutation [(\\\\ensuremath{<}\\\\ensuremath{>})Lehmer (\\\\ensuremath{<}\\\\ensuremath{>})1951], decorrelating the effect of the dimensions on the hashed value. Notably, to achieve (pseudo-)independence, only \\\\ensuremath{\\\\mathit{d}} \\\\ensuremath{-} 1 of the \\\\ensuremath{\\\\mathit{d}} dimensions must be permuted, so we choose 1 := 1 for better cache coherence, 2 = 2 654 435 761, and 3 = 805 459 861. \\n\\nLastly, the feature vectors at each corner are \\\\ensuremath{\\\\mathit{d}} -linearly interpolated according to the relative position of x within its hypercube, i.e. the interpolation weight is w\\\\ensuremath{\\\\mathit{l}} := x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{-} \\\\ensuremath{\\\\lfloor}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rfloor}. \\n\\nRecall that this process takes place independently for each of the \\\\ensuremath{\\\\mathit{L}} levels. The interpolated feature vectors of each level, as well as auxiliary inputs  \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{E}} (such as the encoded view direction and textures in neural radiance caching), are concatenated to produce y \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{L}}\\\\ensuremath{\\\\mathit{F}} +\\\\ensuremath{\\\\mathit{E}} , which is the encoded input enc(x;  ) to the MLP \\\\ensuremath{\\\\mathit{m}}(y; \\\\ensuremath{\\\\Phi}). \\n\\nPerformance vs. quality. Choosing the hash table size \\\\ensuremath{\\\\mathit{T}} provides a trade-off between performance, memory and quality. Higher values of \\\\ensuremath{\\\\mathit{T}} result in higher quality and lower performance. The memory \\n\\nFig. 4. The main curves plot test error over training time for varying hash table size \\\\ensuremath{\\\\mathit{T}} which determines the number of trainable encoding parameters. Increasing \\\\ensuremath{\\\\mathit{T}} improves reconstruction, at the cost of higher memory usage and slower training and inference. A performance cliff is visible at \\\\ensuremath{\\\\mathit{T}} \\\\ensuremath{>} 219 where the cache of our RTX 3090 GPU becomes oversubscribed (particularly visible for SDF and NeRF). The plot also shows model convergence over time leading up to the final state. This highlights how high quality results are already obtained after only a few seconds. Jumps in the convergence (most visible towards the end of SDF training) are caused by learning rate decay. For NeRF and Gigapixel image, training finishes after 31 000 steps and for SDF after 11 000 steps. \\n\\nGigapixel image: Tokyo \\n\\nSigned Distance Function: Cow \\n\\nNeural Radiance Field: Lego \\n\\nFig. 5. Test error over training time for fixed values of feature dimensionality \\\\ensuremath{\\\\mathit{F}} as the number of hash table levels \\\\ensuremath{\\\\mathit{L}} is varied. To maintain a roughly equal trainable parameter count, the hash table size \\\\ensuremath{\\\\mathit{T}} is set according to \\\\ensuremath{\\\\mathit{F}} {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{T}} {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{L}} = 224 for SDF and NeRF, whereas gigapixel image uses 228 . Since (\\\\ensuremath{\\\\mathit{F}} = 2, \\\\ensuremath{\\\\mathit{L}} = 16) is near the best-case performance and quality (top-left) for all applications, we use this configuration in all results. \\\\ensuremath{\\\\mathit{F}} = 1 is slow on our RTX 3090 GPU since atomic half-precision accumulation is only efficient for 2D vectors but not for scalars. For NeRF and Gigapixel image, training finishes after 31 000 steps whereas SDF completes at 11 000 steps. \\n\\nfootprint is linear in \\\\ensuremath{\\\\mathit{T}} , whereas quality and performance tend to scale sub-linearly. We analyze the impact of \\\\ensuremath{\\\\mathit{T}} in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 4, where we report test error vs. training time for a wide range of \\\\ensuremath{\\\\mathit{T}} -values for three neural graphics primitives. We recommend practitioners to use \\\\ensuremath{\\\\mathit{T}} to tweak the encoding to their desired performance characteristics. \\n\\nThe hyperparameters \\\\ensuremath{\\\\mathit{L}} (number of levels) and \\\\ensuremath{\\\\mathit{F}} (number of feature dimensions) also trade off quality and performance, which we analyze for an approximately constant number of trainable encoding parameters  in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 5. In this analysis, we found (\\\\ensuremath{\\\\mathit{F}} = 2, \\\\ensuremath{\\\\mathit{L}} = 16) to be a favorable Pareto optimum in all our applications, so we use these values in all other results and recommend them as the default. \\n\\nImplicit hash collision resolution. It may appear counter-intuitive that this encoding is able to reconstruct scenes faithfully in the presence of hash collisions. Key to its success is that the different resolution levels have different strengths that complement each other. The coarser levels, and thus the encoding as a whole, are injective{\\\\textemdash}that is, they suffer from no collisions at all. However, they can only represent a low-resolution version of the scene, since they offer features which are linearly interpolated from a widely spaced grid of points. Conversely, fine levels can capture small features due to their fine grid resolution, but suffer from many collisions{\\\\textemdash}that is, disparate points which hash to the same table entry. Nearby inputs \\n\\nwith equal integer coordinates \\\\ensuremath{\\\\lfloor}x\\\\ensuremath{\\\\mathit{l}} \\\\ensuremath{\\\\rfloor} are not considered a collision; a collision occurs when different integer coordinates hash to the same index. Luckily, such collisions are pseudo-randomly scattered across space, and statistically unlikely to occur simultaneously at every level for a given pair of points. \\n\\nWhen training samples collide in this way, their gradients average. Consider that the importance to the final reconstruction of such samples is rarely equal. For example, a point on a visible surface of a radiance field will contribute strongly to the reconstructed image (having high visibility and high density, both multiplicatively affecting the magnitude of gradients) causing large changes to its table entries, while a point in empty space that happens to refer to the same entry will have a much smaller weight. As a result, the gradients of the more important samples dominate the collision average and the aliased table entry will naturally be optimized in such a way that it reflects the needs of the higher-weighted point. \\n\\nThe multiresolution aspect of the hash encoding covers the full range from a coarse resolution \\\\ensuremath{\\\\mathit{N}}min that is guaranteed to be collision-free to the finest resolution \\\\ensuremath{\\\\mathit{N}}max that the task requires. Thereby, it guarantees that all scales at which meaningful learning could take place are included, regardless of sparsity. Geometric scaling allows covering these scales with only O  log (\\\\ensuremath{\\\\mathit{N}}max/\\\\ensuremath{\\\\mathit{N}}min)  many levels, which allows picking a conservatively large value for \\\\ensuremath{\\\\mathit{N}}max. \\n\\nOnline adaptivity. Note that if the distribution of inputs x changes over time during training, for example if they become concentrated in a small region, then finer grid levels will experience fewer collisions and a more accurate function can be learned. In other words, the multiresolution hash encoding automatically adapts to the training data distribution, inheriting the benefits of tree-based encodings [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] without task-specific data structure maintenance that might cause discrete jumps during training. One of our applications, neural radiance caching in (\\\\ensuremath{<}\\\\ensuremath{>})Section 5.3, continually adapts to animated viewpoints and 3D content, greatly benefitting from this feature. \\n\\n\\\\ensuremath{\\\\mathit{d}}-linear interpolation. Interpolating the queried hash table entries ensures that the encoding enc(x;  ), and by the chain rule its composition with the neural network \\\\ensuremath{\\\\mathit{m}} (enc(x;  ); \\\\ensuremath{\\\\Phi}), are continuous. Without interpolation, grid-aligned discontinuities would be present in the network output, which would result in an undesirable blocky appearance. One may desire higher-order smoothness, for example when approximating partial differential equations. A concrete example from computer graphics are signed distance functions, in which case the gradient \\\\ensuremath{\\\\mathit{m}} (enc(x;  ); \\\\ensuremath{\\\\Phi})/x, i.e. the surface normal, would ideally also be continuous. If higher-order smoothness must be guaranteed, we describe a low-cost approach in (\\\\ensuremath{<}\\\\ensuremath{>})Appendix A, which we however do not employ in any of our results due to a small decrease in reconstruction quality. \\n\\n4 IMPLEMENTATION \\n\\nTo demonstrate the speed of the multiresolution hash encoding, we implemented it in CUDA and integrated it with the fast fully-fused MLPs of the tiny-cuda-nn framework [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller (\\\\ensuremath{<}\\\\ensuremath{>})2021].1 (\\\\ensuremath{<}\\\\ensuremath{>})We release the source code of the multiresolution hash encoding as an update to (\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller [(\\\\ensuremath{<}\\\\ensuremath{>})2021] and the source code pertaining to the neural graphics primitives at (\\\\ensuremath{<}https://github.com/nvlabs/instant-ngp\\\\ensuremath{>})https://github.com/nvlabs/instant-ngp. \\n\\nPerformance considerations. In order to optimize inference and backpropagation performance, we store hash table entries at half precision (2 bytes per entry). We additionally maintain a master copy of the parameters in full precision for stable mixed-precision parameter updates, following (\\\\ensuremath{<}\\\\ensuremath{>})Micikevicius et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2018]. \\n\\nTo optimally use the GPU{\\\\textquoteright}s caches, we evaluate the hash tables level by level: when processing a batch of input positions, we schedule the computation to look up the first level of the multiresolution hash encoding for all inputs, followed by the second level for all inputs, and so on. Thus, only a small number of consecutive hash tables have to reside in caches at any given time, depending on how much parallelism is available on the GPU. Importantly, this structure of computation automatically makes good use of the available caches and parallelism for a wide range of hash table sizes \\\\ensuremath{\\\\mathit{T}} . \\n\\nOn our hardware, the performance of the encoding remains roughly constant as long as the hash table size stays below \\\\ensuremath{\\\\mathit{T}} \\\\ensuremath{\\\\leq} 219 . Beyond this threshold, performance starts to drop significantly; see (\\\\ensuremath{<}\\\\ensuremath{>})Figure 4. This is explained by the 6 MB L2 cache of our NVIDIA RTX 3090 GPU, which becomes too small for individual levels when 2 {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{T}} {\\\\textperiodcentered} \\\\ensuremath{\\\\mathit{F}} \\\\ensuremath{>} 6 {\\\\textperiodcentered} 220 , with 2 being the size of a half-precision entry. \\n\\nThe optimal number of feature dimensions \\\\ensuremath{\\\\mathit{F}} per lookup depends on the GPU architecture. On one hand, a small number favors cache locality in the previously mentioned streaming approach, but on the other hand, a large \\\\ensuremath{\\\\mathit{F}} favors memory coherence by allowing for \\\\ensuremath{\\\\mathit{F}} -wide vector load instructions. \\\\ensuremath{\\\\mathit{F}} = 2 gave us the best cost-quality trade-off (see (\\\\ensuremath{<}\\\\ensuremath{>})Figure 5) and we use it in all experiments. \\n\\nArchitecture. In all tasks, except for NeRF which we will describe later, we use an MLP with two hidden layers that have a width of 64 neurons, rectified linear unit (ReLU) activation functions on their hidden layers, and a linear output layer. The maximum resolution \\\\ensuremath{\\\\mathit{N}}max is set to 2048 {\\\\texttimes} scene size for NeRF and signed distance functions, to half of the gigapixel image width, and 219 in radiance caching (large value to support close-by objects in expansive scenes). \\n\\nInitialization. We initialize neural network weights according to Glorot and Bengio [(\\\\ensuremath{<}\\\\ensuremath{>})2010] to provide a reasonable scaling of activations and their gradients throughout the layers of the neural network. We initialize the hash table entries using the uniform distribution U(\\\\ensuremath{-}10\\\\ensuremath{-}4 , 10\\\\ensuremath{-}4) to provide a small amount of randomness while encouraging initial predictions close to zero. We also tried a variety of different distributions, including zero-initialization, which all resulted in a very slightly worse initial convergence speed. The hash table appears to be robust to the initialization scheme. \\n\\nTraining. We jointly train the neural network weights and the hash table entries by applying Adam [(\\\\ensuremath{<}\\\\ensuremath{>})Kingma and Ba (\\\\ensuremath{<}\\\\ensuremath{>})2014], where we set 1 = 0.9, 2 = 0.99,  = 10\\\\ensuremath{-}15 , The choice of 1 and 2 makes only a small difference, but the small value of  = 10\\\\ensuremath{-}15 can significantly accelerate the convergence of the hash table entries when their gradients are sparse and weak. To prevent divergence after long training periods, we apply a weak L2 regularization (factor 10\\\\ensuremath{-}6) to the neural network weights, but not to the hash table entries. \\n\\nWhen fitting gigapixel images or NeRFs, we use the L 2 loss. For signed distance functions, we use the mean absolute percentage error (MAPE), defined as |prediction \\\\ensuremath{-} target| |target| + 0.01 , and for neural radiance caching we use a luminance-relative L 2 loss [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. \\n\\nWe observed fastest convergence with a learning rate of 10\\\\ensuremath{-}4 for signed distance functions and 10\\\\ensuremath{-}2 otherwise, as well a a batch size of 214 for neural radiance caching and 218 otherwise. \\n\\nLastly, we skip Adam steps for hash table entries whose gradient is exactly 0. This saves \\\\ensuremath{\\\\sim}10\\\\% performance when gradients are sparse, which is a common occurrence with \\\\ensuremath{\\\\mathit{T}} \\\\ensuremath{\\\\gg} BatchSize. Even though this heuristic violates some of the assumptions behind Adam, we observe no degradation in convergence. \\n\\nNon-spatial input dimensions  \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{E}} . The multiresolution hash encoding targets spatial coordinates with relatively low dimensionality. All our experiments operate either in 2D or 3D. However, it is frequently useful to input auxiliary dimensions  \\\\ensuremath{\\\\in} R\\\\ensuremath{\\\\mathit{E}} to the neural network, such as the view direction and material parameters when learning a light field. In such cases, the auxiliary dimensions can be encoded with established techniques whose cost does not scale superlinearly with dimensionality; we use the one-blob encoding [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2019] in neural radiance caching [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] and the spherical harmonics basis in NeRF, similar to concurrent work [(\\\\ensuremath{<}\\\\ensuremath{>})Verbin et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a]. \\n\\nFig. 6. Approximating an RGB image of resolution 20 000 {\\\\texttimes} 23 466 (469 M RGB pixels) with our multiresolution hash encoding. With hash table sizes \\\\ensuremath{\\\\mathit{T}} of 212 , 217 , and 222 the models shown have 117 k, 2.7 M, and 47.5 M trainable parameters respectively. With only 3.4\\\\% of the degrees of freedom of the input, the last model achieves a reconstruction PSNR of 29.8 dB. {\\\\textquotedblleft}Girl With a Pearl Earring{\\\\textquotedblright} renovation {\\\\textcopyright}Koorosh Orooj (\\\\ensuremath{<}http://profoundism.com/free\\\\_licenses.html\\\\ensuremath{>})(CC BY-SA 4.0) \\n\\n5 EXPERIMENTS \\n\\nTo highlight the versatility and high quality of the encoding, we compare it with previous encodings in four distinct computer graphics primitives that benefit from encoding spatial coordinates. \\n\\nLearning the 2D to RGB mapping of image coordinates to colors has become a popular benchmark for testing a model{\\\\textquoteright}s ability to represent high-frequency detail [(\\\\ensuremath{<}\\\\ensuremath{>})Martel et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2019; (\\\\ensuremath{<}\\\\ensuremath{>})Sitzmann et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Tancik et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020]. Recent breakthroughs in adaptive coordinate networks (ACORN) [(\\\\ensuremath{<}\\\\ensuremath{>})Martel et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] have shown impressive results when fitting very large images{\\\\textemdash}up to a billion pixels{\\\\textemdash}with high fidelity at even the smallest scales. We target our multiresolution hash encoding at the same task and converge to high-fidelity images in seconds to minutes ((\\\\ensuremath{<}\\\\ensuremath{>})Figure 4). \\n\\nFor comparison, on the Tokyo panorama from (\\\\ensuremath{<}\\\\ensuremath{>})Figure 1, ACORN achieves a PSNR of 38.59 dB after 36.9 h of training. With a similar number of parameters (\\\\ensuremath{\\\\mathit{T}} = 224), our method achieves the same PSNR after 2.5 minutes of training, peaking at 41.9 dB after 4 min. (\\\\ensuremath{<}\\\\ensuremath{>})Figure 6 showcases the level of detail contained in our model for a variety of hash table sizes \\\\ensuremath{\\\\mathit{T}} on another image. \\n\\nIt is difficult to directly compare the performance of our encoding to ACORN; a factor of \\\\ensuremath{\\\\sim} 10 stems from our use of fully fused CUDA kernels, provided by the tiny-cuda-nn framework [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller (\\\\ensuremath{<}\\\\ensuremath{>})2021]. The input encoding allows for the use of a much smaller MLP than with ACORN, which accounts for much of the remaining 10{\\\\texttimes}{\\\\textendash}100{\\\\texttimes} speedup. That said, we believe that the biggest value-add of the multiresolution hash encoding is its simplicity. ACORN relies on an adaptive subdivision of the scene as part of a learning curriculum, none of which is necessary with our encoding. \\n\\nSigned distance functions (SDFs), in which a 3D shape is represented as the zero level-set of a function of position x, are used in many applications including simulation, path planning, 3D modeling, and video games. DeepSDF [(\\\\ensuremath{<}\\\\ensuremath{>})Park et al. (\\\\ensuremath{<}\\\\ensuremath{>})2019] uses a large MLP to represent one or more SDFs at a time. In contrast, when just a single SDF needs to be fit, a spatially learned encoding, such as ours can be employed and the MLP shrunk significantly. This is the \\n\\napplication we investigate in this section. As baseline, we compare with NGLOD [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021], which achieves state-of-the-art results in both quality and speed by prefixing its small MLP with a lookup from an octree of trainable feature vectors. Lookups along the hierarchy of this octree act similarly to our multiresolution cascade of grids: they are a collision-free analog to our technique, with a fixed growth factor \\\\ensuremath{\\\\mathit{b}} = 2. To allow meaningful comparisons in terms of both performance and quality, we implemented an optimized version of NGLOD in our framework, details of which we describe in (\\\\ensuremath{<}\\\\ensuremath{>})Appendix B. Details pertaining to real-time training of SDFs are described in (\\\\ensuremath{<}\\\\ensuremath{>})Appendix C. \\n\\nIn (\\\\ensuremath{<}\\\\ensuremath{>})Figure 7, we compare NGLOD with our multiresolution hash encoding at roughly equal parameter count. We also show a straightforward application of the frequency encoding [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] to provide a baseline, details of which are found in (\\\\ensuremath{<}\\\\ensuremath{>})Appen-(\\\\ensuremath{<}\\\\ensuremath{>})dix D. By using a data structure tailored to the reference shape, NGLOD achieves the highest visual reconstruction quality. However, even without such a dedicated data structure, our encoding approaches a similar fidelity to NGLOD in terms of the intersection-over-union metric (IoU2(\\\\ensuremath{<}\\\\ensuremath{>})) with similar performance and memory cost. Furthermore, the SDF is defined everywhere within the training volume, as opposed to NGLOD, which is only defined within the octree (i.e. close to the surface). This permits the use of certain SDF rendering techniques such as approximate soft shadows from a small number of off-surface distance samples [(\\\\ensuremath{<}\\\\ensuremath{>})Evans (\\\\ensuremath{<}\\\\ensuremath{>})2006], as shown in the adjacent figure. \\n\\nTo emphasize differences between the compared methods, we visualize the SDF using a shading model. The resulting colors are sensitive to even slight changes in the surface normal, which emphasizes small fluctuations in the prediction more strongly than in other graphics primitives where color is predicted directly. This sensitivity reveals undesired microstructure in our hash encoding on the scale \\n\\nHash (ours) \\n\\nHash (ours) \\n\\nFig. 7. Neural signed distance functions trained for 11 000 steps. The frequency encoding [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] struggles to capture the sharp details on these intricate models. NGLOD [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] achieves the highest visual quality, at the cost of only training the SDF inside the cells of a close-fitting octree. Our hash encoding exhibits similar numeric quality in terms of intersection over union (IoU) and can be evaluated anywhere in the scene. However, it also exhibits visually undesirable surface roughness that we attribute to randomly distributed hash collisions. Bearded Man {\\\\textcopyright}Oliver Laric (\\\\ensuremath{<}https://creativecommons.org/licenses/by-nc-sa/2.0/\\\\ensuremath{>})(CC BY-NC-SA 2.0) \\n\\nFeature buffers \\n\\nFig. 8. Summary of the neural radiance caching application [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. The MLP \\\\ensuremath{\\\\mathit{m}}  enc(\\\\ensuremath{\\\\mathit{x}} ;  ); \\\\ensuremath{\\\\Phi}  is tasked with predicting photorealistic pixel colors from feature buffers independently for each pixel. The feature buffers contain, among other variables, the world-space position x, which we propose to encode with our method. Neural radiance caching is a challenging application, because it is supervised online during real-time rendering. The training data are a sparse set of light paths that are continually spawned from the camera view. As such, the neural network and encoding do not learn a general mapping from features to color, but rather they continually overfit to the current shape and lighting. To support animated content, training has a budget of one millisecond per frame. \\n\\nFig. 9. Neural radiance caching [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] gains much improved quality from the multiresolution hash encoding with only a mild performance penalty: 133 versus 147 frames per second at a resolution of 1920{\\\\texttimes}1080px. To demonstrate the online adaptivity of the multiple hash resolutions vs. the prior triangle wave encoding, we show screenshots from a smooth camera motion that starts with a far-away view of the scene (left) and zooms onto a close-by view of an intricate shadow (right). Throughout the camera motion, which takes just a few seconds, the neural radiance cache continually learns from sparse camera paths, enabling the cache to learn ({\\\\textquotedblleft}overfit{\\\\textquotedblright}) intricate detail at the scale of the content that the camera is momentarily observing. \\n\\nFig. 10. The effect of the MLP size on test error vs. training time (31 000 training steps) on the Lego scene. Other scenes behave almost identically. Each curve represents a different MLP depth, where the color MLP has \\\\ensuremath{\\\\mathit{N}}layers hidden layers and the density MLP has 1 hidden layer; we do not observe an improvement with deeper density MLPs. The curves sweep the number of neurons the hidden layers of the density and color MLPs from 16 to 256. Informed by this analysis, we choose \\\\ensuremath{\\\\mathit{N}}layers = 2 and \\\\ensuremath{\\\\mathit{N}}neurons = 64. \\n\\nof the finest grid resolution, which is absent in NGLOD and does not disappear with longer training times. Since NGLOD is essentially a collision-free analog to our hash encoding, we attribute this artifact to hash collisions. Upon close inspection, similar microstruc-ture can be seen in other neural graphics primitives, although with significantly lower magnitude. \\n\\nIn neural radiance caching [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021], the task of the MLP is to predict photorealistic pixel colors from feature buffers; see (\\\\ensuremath{<}\\\\ensuremath{>})Fig-(\\\\ensuremath{<}\\\\ensuremath{>})ure 8. The MLP is run independently for each pixel (i.e. the model is not convolutional), so the feature buffers can be treated as per-pixel feature vectors that contain the 3D coordinate x as well as additional features. We can therefore directly apply our multiresolution hash encoding to x while treating all additional features as auxiliary encoded dimensions  to be concatenated with the encoded position, using the same encoding as (\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2021]. We integrated our work into M\\\\\"uller et al.{\\\\textquoteright}s implementation of neural radiance caching and therefore refer to their paper for implementation details. \\n\\nFor photorealistic rendering, the neural radiance cache is typically queried only for indirect path contributions, which masks its reconstruction error behind the first reflection. In contrast, we would like to emphasize the neural radiance cache{\\\\textquoteright}s error, and thus the improvement that can be obtained by using our multiresolution hash encoding, so we directly visualize the neural radiance cache at the first path vertex. \\n\\nIn the NeRF setting, a volumetric shape is represented in terms of a spatial (3D) density function and a spatiodirectional (5D) emission \\n\\nFig. 11. Feeding the result of our encoding through a linear transformation (no neural network) versus an MLP when learning a NeRF. The models were trained for 1 min. The MLP allows for resolving specular details and reduces the amount of background noise caused by hash collisions. Due to the small size and efficient implementation of the MLP, it is only 15\\\\% more expensive{\\\\textemdash}well worth the significantly improved quality. \\n\\nfunction, which we represent by a similar neural network architecture as (\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2020]. We train the model in the same ways as Mildenhall et al.: by backpropagating through a differentiable ray marcher driven by 2D RGB images from known camera poses. \\n\\nModel Architecture. Unlike the other three applications, our NeRF model consists of two concatenated MLPs: a density MLP followed by a color MLP [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020]. The density MLP maps the hash encoded position y = enc(x;  ) to 16 output values, the first of which we treat as log-space density. The color MLP adds view-dependent color variation. Its input is the concatenation of \\n\\nIts output is an RGB color triplet, for which we use either a sigmoid activation when the training data has low dynamic-range (sRGB) or an exponential activation when it has high dynamic range (linear HDR). We prefer HDR training data due to the closer resemblance to physical light transport. This brings numerous advantages as has also been noted in concurrent work [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. \\n\\nInformed by the analysis in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 10, our results were generated with a 1-hidden-layer density MLP and a 2-hidden-layer color MLP, both 64 neurons wide. \\n\\nAccelerated ray marching. When marching along rays for both training and rendering, we would like to place samples such that they contribute somewhat uniformly to the image, minimizing wasted computation. Thus, we concentrate samples near surfaces by maintaining an occupancy grid that coarsely marks empty vs. nonempty space. In large scenes, we additionally cascade the occupancy grid and distribute samples exponentially rather than uniformly along the ray. (\\\\ensuremath{<}\\\\ensuremath{>})Appendix E describes these procedures in detail. \\n\\nAt HD resolutions, synthetic and even real-world scenes can be trained in seconds and rendered at 60 FPS, without the need of caching of the MLP outputs [(\\\\ensuremath{<}\\\\ensuremath{>})Garbin et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Wizadwongsa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021b]. This high performance makes it tractable to add effects such as anti-aliasing, motion blur and depth of field by brute-force tracing of multiple rays per pixel, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 12. \\n\\nFig. 12. NeRF reconstruction of a modular synthesizer and large natural 360 scene. The left image took 5 seconds to accumulate 128 samples at 1080p on a single RTX 3090 GPU, allowing for brute force defocus effects. The right image was taken from an interactive session running at 10 frames per second on the same GPU. \\n\\nComparison with direct voxel lookups. (\\\\ensuremath{<}\\\\ensuremath{>})Figure 11 shows an ablation where we replace the entire neural network with a single linear matrix multiplication, in the spirit of (although not identical to) concurrent direct voxel-based NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Sun et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a]. While the linear layer is capable of reproducing view-dependent effects, the quality is significantly compromised as compared to the MLP, which is better able to capture specular effects and to resolve hash collisions across the interpolated multiresolution hash tables (which manifest as high-frequency artifacts). Fortunately, the MLP is only 15\\\\% more expensive than the linear layer, thanks to its small size and efficient implementation. \\n\\nComparison with high-quality offline NeRF. In (\\\\ensuremath{<}\\\\ensuremath{>})Table 2, we compare the peak signal to noise ratio (PSNR) our NeRF implementation with multiresolution hash encoding ({\\\\textquotedblleft}Ours: Hash{\\\\textquotedblright}) with that of NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020], mip-NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Barron et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a], and NSVF [(\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020], which all require on the order of hours to train. In contrast, we list results of our method after training for 1 s to 5 min. Our PSNR is competitive with NeRF and NSVF after \\n\\njust 15 s of training, and competitive with mip-NeRF after 1 min to 5 min of training. \\n\\nOn one hand, our method performs best on scenes with high geometric detail, such as Ficus, Drums, Ship and Lego, achieving the best PSNR of all methods. On the other hand, mip-NeRF and NSVF outperform our method on scenes with complex, view-dependent reflections, such as Materials; we attribute this to the much smaller MLP that we necessarily employ to obtain our speedup of several orders of magnitude over these competing implementations. \\n\\nNext, we analyze the degree to which our speedup originates from our efficient implementation versus from our encoding. To this end, we additionally report PSNR for a nearly identical version of our implementation: we replace the hash encoding by the frequency encoding and enlarge the MLP to approximately match the architecture of (\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2020] ({\\\\textquotedblleft}Ours: Frequency{\\\\textquotedblright}); see (\\\\ensuremath{<}\\\\ensuremath{>})Appendix D for details. This version of our algorithm approaches NeRF{\\\\textquoteright}s quality after training for just \\\\ensuremath{\\\\sim} 5 min, yet is outperformed by our full method after training for a much shorter duration (5 s{\\\\textendash}15 s), amounting to a 20{\\\\textendash}60{\\\\texttimes} improvement caused by the hash encoding and smaller MLP. \\n\\nFor {\\\\textquotedblleft}Ours: Hash{\\\\textquotedblright}, the cost of each training step is roughly constant at \\\\ensuremath{\\\\sim}6 ms per step. This amounts to 50 k steps after 5 min at which point the model is well converged. We decay the learning rate after 20 k steps by a factor of 0.33, which we repeat every further 10 k steps. In contrast, the larger MLP used in {\\\\textquotedblleft}Ours: Frequency{\\\\textquotedblright} requires \\\\ensuremath{\\\\sim}30 ms per training step, meaning that the PSNR listed after 5 min corresponds to about 10 k steps. It could thus keep improving slightly if trained for extended periods of time, as in the offline NeRF variants that are often trained for several 100 k steps. \\n\\nWhile we isolated the performance and convergence impact of our hash encoding and its small MLP, we believe an additional study is required to quantify the impact of advanced ray marching schemes (such as ours, coarse-fine [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020], or DONeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Neff (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]) independently from the encoding and network architecture. We report additional information in (\\\\ensuremath{<}\\\\ensuremath{>})Section E.3 to aid in such an analysis. \\n\\n(a) Offline rendered reference \\n\\n(b) Hash (ours), trained for 10 s Rendered in 32 ms (2 samples per pixel) \\n\\nFig. 13. Preliminary results of training a NeRF cloud model (b) from real-time path tracing data. Within 32 ms, a 1024{\\\\texttimes}1024 image of our model convincingly approximates the offline rendered ground truth (a). Our model exhibits less noise than a GPU path tracer that ran for an equal amount of time (c). The cloud data is {\\\\textcopyright}Walt Disney Animation Studios (\\\\ensuremath{<}http://creativecommons.org/licenses/by-sa/3.0/\\\\ensuremath{>})(CC BY-SA 3.0) \\n\\n6 DISCUSSION AND FUTURE WORK \\n\\nConcatenation vs. reduction. At the end of the encoding, we concatenate rather than reduce (for example, by summing) the \\\\ensuremath{\\\\mathit{F}} -dimensional feature vectors obtained from each resolution. We prefer concatenation for two reasons. First, it allows for independent, fully parallel processing of each resolution. Second, a reduction of the dimensionality of the encoded result y from \\\\ensuremath{\\\\mathit{L}}\\\\ensuremath{\\\\mathit{F}} to \\\\ensuremath{\\\\mathit{F}} may be too small to encode useful information. While \\\\ensuremath{\\\\mathit{F}} could be increased proportionally, it would make the encoding much more expensive. \\n\\nHowever, we recognize that there may be applications in which reduction is favorable, such as when the neural network is significantly more expensive than the encoding, in which case the added computational cost of increasing \\\\ensuremath{\\\\mathit{F}} could be insignificant. We thus argue for concatenation by default and not as a hard-and-fast rule. In our applications, concatenation, coupled with \\\\ensuremath{\\\\mathit{F}} = 2 always yielded by far the best results. \\n\\nChoice of hash function. A good hash function is efficient to compute, leads to coherent look-ups, and uniformly covers the feature vector array regardless of the structure of query points. We chose our hash function for its good mixture of these properties and also experimented with three others: \\n\\nAlternatively to hand-crafted hash functions, it is conceivable to optimize the hash function in future work, turning the method into a dictionary-learning approach. Two possible avenues are (1) developing a continuous formulation of indexing that is amenable to analytic differentiation or (2) applying an evolutionary optimization algorithm that can efficiently explore the discrete function space. \\n\\nMicrostructure due to hash collisions. The salient artifact of our encoding is a small amount of {\\\\textquotedblleft}grainy{\\\\textquotedblright} microstructure, most visible on the learned signed distance functions ((\\\\ensuremath{<}\\\\ensuremath{>})Figure 1 and (\\\\ensuremath{<}\\\\ensuremath{>})Figure 7). The graininess is a result of hash collisions that the MLP is unable to fully compensate for. We believe that the key to achieving state-of-the-art quality on SDFs with our encoding will be to find a way to overcome this microstructure, for example by filtering hash table lookups or by imposing an additional smoothness prior on the loss. \\n\\nGenerative setting. Parametric input encodings, when used in a generative setting, typically arrange their features in a dense grid which can then be populated by a separate generator network, typically a CNN such as StyleGAN [(\\\\ensuremath{<}\\\\ensuremath{>})Chan et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})DeVries et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Peng et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020b]. Our hash encoding adds an additional layer of complexity, as the features are not arranged in a regular pattern through the input domain; that is, the features are not bijective with a regular grid of points. We leave it to future work to determine how best to overcome this difficulty. \\n\\nOther applications. We are interested in applying the multireso-lution hash encoding to other low-dimensional tasks that require accurate, high-frequency fits. The frequency encoding originated from the attention mechanism of transformer networks [(\\\\ensuremath{<}\\\\ensuremath{>})Vaswani (\\\\ensuremath{<}\\\\ensuremath{>})et al. (\\\\ensuremath{<}\\\\ensuremath{>})2017]. We hope that parametric encodings such as ours can lead to a meaningful improvement in general, attention-based tasks. \\n\\nHeterogenous volumetric density fields, such as cloud and smoke stored in a VDB [(\\\\ensuremath{<}\\\\ensuremath{>})Museth (\\\\ensuremath{<}\\\\ensuremath{>})2013, (\\\\ensuremath{<}\\\\ensuremath{>})2021] data structure, often include empty space on the outside, a solid core on the inside, and sparse detail on the volumetric surface. This makes them a good fit for our encoding. In the code released alongside this paper, we have included a preliminary implementation that fits a radiance and density field directly from the noisy output of a volumetric path tracer. The initial results are promising, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 13, and we intend to pursue this direction further in future work. \\n\\n7 CONCLUSION \\n\\nMany graphics problems rely on task specific data structures to exploit the sparsity or smoothness of the problem at hand. Our multi-resolution hash encoding provides a practical learning-based \\n\\nalternative that automatically focuses on relevant detail, independent of the task. Its low overhead allows it to be used even in time-constrained settings like online training and inference. In the context of neural network input encodings, it is a drop-in replacement, for example speeding up NeRF by several orders of magnitude and matching the performance of concurrent non-neural 3D reconstruction techniques. \\n\\nSlow computational processes in any setting, from lightmap baking to the training of neural networks, can lead to frustrating workflows due to long iteration times [(\\\\ensuremath{<}\\\\ensuremath{>})Enderton and Wexler (\\\\ensuremath{<}\\\\ensuremath{>})2011]. We have demonstrated that single-GPU training times measured in seconds are within reach for many graphics applications, allowing neural approaches to be applied where previously they may have been discounted. \\n\\nACKNOWLEDGMENTS \\n\\nWe are grateful to Andrew Tao, Andrew Webb, Anjul Patney, David Luebke, Fabrice Rousselle, Jacob Munkberg, James Lucas, Jonathan Granskog, Jonathan Tremblay, Koki Nagano, Marco Salvi, Nikolaus Binder, and Towaki Takikawa for profound discussions, proofreading, feedback, and early testing. We also thank Arman Toorians and Saurabh Jain for the factory robot dataset in (\\\\ensuremath{<}\\\\ensuremath{>})Figure 12 (right). \\n\\nREFERENCES \\n\\nThomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. 2007. Convolution Shadow Maps. In Rendering Techniques, Jan Kautz and Sumanta Pattanaik (Eds.). The Eurographics Association. (\\\\ensuremath{<}https://doi.org/10.2312/EGWR/EGSR07/051-060\\\\ensuremath{>})https://doi.org/10.2312/EGWR/ (\\\\ensuremath{<}https://doi.org/10.2312/EGWR/EGSR07/051-060\\\\ensuremath{>})EGSR07/051-060 \\n\\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. 2021a. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. arXiv (2021). (\\\\ensuremath{<}https://jonbarron.info/mipnerf/\\\\ensuremath{>})https://jonbarron.info/mipnerf/ \\n\\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hed-man. 2021b. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. arXiv:2111.12077 (Nov. 2021). \\n\\nRohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Love-grove, and Richard Newcombe. 2020. Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction. In Computer Vision {\\\\textendash} ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 608{\\\\textendash}625. \\n\\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative Adversarial Networks. arXiv:2112.07945 (2021). arXiv(\\\\ensuremath{<}https://arxiv.org/abs/2112.07945\\\\ensuremath{>}):2112.07945 [cs.CV] \\n\\nJulian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. 2020. Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. \\n\\nTerrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and Joshua M. Susskind. 2021. Unconstrained Scene Generation with Locally Conditioned Radiance Fields. arXiv (2021). \\n\\nEric Enderton and Daniel Wexler. 2011. The Workflow Scale. In Computer Graphics International Workshop on VFX, Computer Animation, and Stereo Movies. \\n\\nAlex Evans. 2006. Fast Approximations for Global Illumination on Dynamic Scenes. In ACM SIGGRAPH 2006 Courses (Boston, Massachusetts) (SIGGRAPH {\\\\textquoteright}06). Association for Computing Machinery, New York, NY, USA, 153{\\\\textendash}171. (\\\\ensuremath{<}https://doi.org/10.1145/1185657.1185834\\\\ensuremath{>})https://doi.org/10.1145/ (\\\\ensuremath{<}https://doi.org/10.1145/1185657.1185834\\\\ensuremath{>})1185657.1185834 \\n\\nStephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. 2021. FastNeRF: High-Fidelity Neural Rendering at 200FPS. arXiv:2103.10380 (March 2021). \\n\\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th International Conference on Machine Learning -Volume 70 (Sydney, NSW, Australia) (ICML{\\\\textquoteright}17). JMLR.org, 1243{\\\\textemdash}-1252. \\n\\nXavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics (Sardinia, Italy, May 13{\\\\textendash}15). JMLR.org, 249{\\\\textendash}256. \\n\\nSaeed Hadadan, Shuhong Chen, and Matthias Zwicker. 2021. Neural radiosity. ACM Transactions on Graphics 40, 6 (Dec. 2021), 1{\\\\textemdash}-11. (\\\\ensuremath{<}https://doi.org/10.1145/3478513.3480569\\\\ensuremath{>})https://doi.org/10.1145/3478513. \\n\\nDavid Money Harris and Sarah L. Harris. 2013. 3.4.2 -State Encodings. In Digital Design and Computer Architecture (second ed.). Morgan Kaufmann, Boston, 129{\\\\textendash}131. (\\\\ensuremath{<}https://doi.org/10.1016/B978-0-12-394424-5.00002-1\\\\ensuremath{>})https://doi.org/10.1016/B978-0-12-394424-5.00002-1 \\n\\nJon Jansen and Louis Bavoil. 2010. Fourier Opacity Mapping. In Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (Washington, D.C.) (I3D {\\\\textquoteright}10). Association for Computing Machinery, New York, NY, USA, 165{\\\\textemdash}-172. (\\\\ensuremath{<}https://doi.org/10.1145/1730804.1730831\\\\ensuremath{>})https://doi.org/10.1145/1730804.1730831 \\n\\nChiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nieer, and Thomas Funkhouser. 2020. Local Implicit Grid Representations for 3D Scenes. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). \\n\\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. arXiv:1412.6980 (June 2014). \\n\\nDerrick H. Lehmer. 1951. Mathematical Methods in Large-scale Computing Units. In Proceedings of the Second Symposium on Large Scale Digital Computing Machinery. Harvard University Press, Cambridge, United Kingdom, 141{\\\\textendash}146. \\n\\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration without Clean Data. arXiv:1803.04189 (March 2018). \\n\\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural Sparse Voxel Fields. NeurIPS (2020). (\\\\ensuremath{<}https://lingjie0206.github.io/papers/NSVF/\\\\ensuremath{>})https://lingjie0206.github.io/papers/ (\\\\ensuremath{<}https://lingjie0206.github.io/papers/NSVF/\\\\ensuremath{>})NSVF/ \\n\\nJulien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, and Gordon Wetzstein. 2021. ACORN: Adaptive Coordinate Networks for Neural Representation. ACM Trans. Graph. (SIGGRAPH) (2021). \\n\\nIshit Mehta, Micha\\\\\"el Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. 2021. Modulated Periodic Activations for Generalizable Local Functional Representations. In IEEE International Conference on Computer Vision. IEEE. \\n\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training. arXiv:1710.03740 (Oct. 2018). \\n\\nBen Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and Jonathan T. Barron. 2021. NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images. arXiv:2111.13679 (Nov. 2021). \\n\\nBen Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM Trans. Graph. 38, 4, Article 29 (July 2019), 14 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3306346.3322980\\\\ensuremath{>})https://doi.org/10.1145/3306346.3322980 \\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. \\n\\nThomas M\\\\\"uller. 2021. Tiny CUDA Neural Network Framework. https://github.com/nvlabs/tiny-cuda-nn. \\n\\nThomas M\\\\\"uller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Nov\\\\\\'ak. 2019. Neural Importance Sampling. ACM Trans. Graph. 38, 5, Article 145 (Oct. 2019), 19 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3341156\\\\ensuremath{>})https://doi.org/10.1145/3341156 \\n\\nThomas M\\\\\"uller, Fabrice Rousselle, Alexander Keller, and Jan Nov\\\\\\'ak. 2020. Neural Control Variates. ACM Trans. Graph. 39, 6, Article 243 (Nov. 2020), 19 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3414685.3417804\\\\ensuremath{>})https: (\\\\ensuremath{<}https://doi.org/10.1145/3414685.3417804\\\\ensuremath{>})//doi.org/10.1145/3414685.3417804 \\n\\nThomas M\\\\\"uller, Fabrice Rousselle, Jan Nov\\\\\\'ak, and Alexander Keller. 2021. Real-time Neural Radiance Caching for Path Tracing. ACM Trans. Graph. 40, 4, Article 36 (Aug. 2021), 16 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3450626.3459812\\\\ensuremath{>})https://doi.org/10.1145/3450626.3459812 \\n\\nKen Museth. 2013. VDB: High-Resolution Sparse Volumes with Dynamic Topology. ACM Trans. Graph. 32, 3, Article 27 (July 2013), 22 pages. (\\\\ensuremath{<}https://doi.org/10.1145/2487228.2487235\\\\ensuremath{>})https://doi.org/10.1145/ (\\\\ensuremath{<}https://doi.org/10.1145/2487228.2487235\\\\ensuremath{>})2487228.2487235 \\n\\nKen Museth. 2021. NanoVDB: A GPU-Friendly and Portable VDB Data Structure For Real-Time Rendering And Simulation. In ACM SIGGRAPH 2021 Talks (Virtual Event, USA) (SIGGRAPH {\\\\textquoteright}21). Association for Computing Machinery, New York, NY, USA, Article 1, 2 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3450623.3464653\\\\ensuremath{>})https://doi.org/10.1145/3450623.3464653 \\n\\nThomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum 40, 4 (2021). (\\\\ensuremath{<}https://doi.org/10.1111/cgf.14340\\\\ensuremath{>})https://doi.org/10.1111/cgf.14340 \\n\\nMatthias Nieer, Michael Zollh\\\\\"ofer, Shahram Izadi, and Marc Stamminger. 2013. Real-Time 3D Reconstruction at Scale Using Voxel Hashing. ACM Trans. Graph. 32, 6, Article 169 (nov 2013), 11 pages. (\\\\ensuremath{<}https://doi.org/10.1145/2508363.2508374\\\\ensuremath{>})https://doi.org/10.1145/2508363.2508374 \\n\\nFakir S. Nooruddin and Greg Turk. 2003. Simplification and Repair of Polygonal Models Using Volumetric Techniques. IEEE Transactions on Visualization and Computer \\n\\nGraphics 9, 2 (apr 2003), 191{\\\\textendash}{\\\\textendash}205. (\\\\ensuremath{<}https://doi.org/10.1109/TVCG.2003.1196006\\\\ensuremath{>})https://doi.org/10.1109/TVCG.2003.1196006 Melissa E. O{\\\\textquoteright}Neill. 2014. PCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation. Technical Report HMC-CS-2014-0905. Harvey Mudd College, Claremont, CA. \\n\\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape \\n\\nRepresentation. arXiv:1901.05103 (Jan. 2019). \\n\\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. 2020a. Convolutional Occupancy Networks. In European Conference on Computer Vision (ECCV). \\n\\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. 2020b. Convolutional Occupancy Networks. (2020). arXiv(\\\\ensuremath{<}https://arxiv.org/abs/2003.04618\\\\ensuremath{>}):2003.04618 [cs.CV] \\n\\nMatt Pharr, Wenzel Jakob, and Greg Humphreys. 2016. Physically Based Rendering: From Theory to Implementation (3rd ed.) (3rd ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. 1266 pages. \\n\\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation Functions. In Proc. NeurIPS. \\n\\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2021. Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. arXiv:2111.11215 (Nov. 2021). \\n\\nTowaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (2021). \\n\\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. 2020. Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. NeurIPS (2020). (\\\\ensuremath{<}https://bmild.github.io/fourfeat/index.html\\\\ensuremath{>})https://bmild.github.io/fourfeat/index.html \\n\\nDanhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, Shahram Izadi, and Andrea Tagliasacchi. 2018. Real-Time Compression and Streaming of 4D Performances. ACM Trans. Graph. 37, 6, Article 256 (dec 2018), 11 pages. (\\\\ensuremath{<}https://doi.org/10.1145/3272127.3275096\\\\ensuremath{>})https: (\\\\ensuremath{<}https://doi.org/10.1145/3272127.3275096\\\\ensuremath{>})//doi.org/10.1145/3272127.3275096 \\n\\nMatthias Teschner, Bruno Heidelberger, Matthias M\\\\\"uller, Danat Pomeranets, and Markus Gross. 2003. Optimized Spatial Hashing for Collision Detection of Deformable Objects. In Proceedings of VMV{\\\\textquoteright}03, Munich, Germany. 47{\\\\textendash}54. \\n\\nSergios Theodoridis. 2008. Pattern Recognition. Elsevier. \\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 (June 2017). \\n\\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. 2021. Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields. arXiv:2112.03907 (Dec. 2021). \\n\\nSuttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. 2021. NeX: Real-time View Synthesis with Neural Basis Expansion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). \\n\\nAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks. arXiv:2112.05131 (Dec. 2021). \\n\\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021b. PlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV. \\n\\nA SMOOTH INTERPOLATION \\n\\nOne may desire smoother interpolation than the \\\\ensuremath{\\\\mathit{d}} -linear interpolation that our multiresolution hash encoding uses by default. \\n\\nIn this case, the obvious solution would be using a \\\\ensuremath{\\\\mathit{d}} -quadratic or \\\\ensuremath{\\\\mathit{d}}-cubic interpolation, both of which are however very expensive due to requiring the lookup of 3\\\\ensuremath{\\\\mathit{d}} and 4\\\\ensuremath{\\\\mathit{d}} instead of 2\\\\ensuremath{\\\\mathit{d}} vertices, respectively. As a low-cost alternative, we recommend applying the smoothstep function, \\n\\n(5) \\n\\nto the \\\\ensuremath{\\\\mathit{d}}-linear interpolation weights. Crucially, the derivative of the smoothstep, \\n\\nvanishes at 0 and at 1, causing the discontinuity in the derivatives of the encoding to vanish by the chain rule. The encoding thus becomes \\\\ensuremath{\\\\mathit{C}} 1-smooth. \\n\\nHowever, by this trick, we have merely traded discontinuities for zero-points in the individual levels which are not necessarily more desirable. So, we offset each level by half of its voxel size 1/(2\\\\ensuremath{\\\\mathit{N}}\\\\ensuremath{\\\\mathit{l}} ), which prevents the zero derivatives from aligning across all levels. \\n\\nThe encoding is thus able to learn smooth, non-zero derivatives for all spatial locations x. \\n\\nFor higher-order smoothness, higher-order smoothstep functions \\\\ensuremath{\\\\mathit{S}}\\\\ensuremath{\\\\mathit{n}} can be used at small additional cost. In practice, the computational cost of the 1st order smoothstep function \\\\ensuremath{\\\\mathit{S}}1 is hidden by memory bottlenecks, making it essentially free. However, the reconstruction quality tends to decrease as higher-order interpolation is used. This is why we do not use it by default. Future research is needed to explain the loss of quality. \\n\\nB IMPLEMENTATION DETAILS OF NGLOD \\n\\nWe designed our implementation of NGLOD [(\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021] such that it closely resembles that of our hash encoding, only differing in the underlying data structure; i.e. using the vertices of an octree around ground-truth triangle mesh to store collision-free feature vectors, rather than relying on hash tables. This results in a notable difference to the original NGLOD: the looked-up feature vectors are concatenated rather than summed, which in our implementation serendipitously resulted in higher reconstruction quality compared to the summation of an equal number of trainable parameters. \\n\\nThe octree implies a fixed growth factor \\\\ensuremath{\\\\mathit{b}} = 2, which leads to a smaller number of levels than our hash encoding. We obtained the most favorable performance vs. quality trade-off at a roughly equal number of trainable parameters as our method, through the following configuration: \\n\\nThe last point is important for two reasons: first, it matches the coarsest resolution of our hash tables 24 = 16 = \\\\ensuremath{\\\\mathit{N}}min, and second, it prevents a performance bottleneck that would arise when all threads of the GPU atomically accumulate gradients in few, coarse entries. We experimentally verified that this does not lead to reduced quality, compared to looking up the entire hierarchy. \\n\\nC REAL-TIME SDF TRAINING DATA GENERATION \\n\\nIn order to not bottleneck our SDF training, we must be able to generate a large number of ground truth signed distances to high-resolution meshes very quickly (\\\\ensuremath{\\\\sim}millions per second). \\n\\nLastly, for those surface samples that must be perturbed, we add a random 3D vector, each dimension independently drawn from a logistic distribution (similar shape to a Gaussian, but cheaper to compute) with standard deviation \\\\ensuremath{\\\\mathit{r}} /1024, where \\\\ensuremath{\\\\mathit{r}} is the bounding radius of the mesh. \\n\\nOctree sampling for NGLOD. When training our implementation of (\\\\ensuremath{<}\\\\ensuremath{>})Takikawa et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2021], we must be careful to rarely generate training positions outside of octree leaf nodes. To this end, we replace the uniform unit cube sampling routine with one that creates uniform 3D positions in the leaf nodes of the octree by first rejection sampling a uniformly random leaf node from the array of all nodes and then generating a uniform random position within the node{\\\\textquoteright}s voxel. Fortunately, the standard deviation \\\\ensuremath{\\\\mathit{r}} /1024 of our logistic perturbation is small enough to almost never leave the octree, so we do not need to modify the surface sampling routine. \\n\\nFor each sampled 3D position x, we must compute the signed distance to the triangle mesh. To this end, we first construct a triangle bounding volume hierarchy (BVH) with which we perform efficient unsigned distance queries; O  log \\\\ensuremath{\\\\mathit{N}}triangles  on average. \\n\\nNext, we sign these distances by tracing 32 {\\\\textquotedblleft}stab rays{\\\\textquotedblright} [(\\\\ensuremath{<}\\\\ensuremath{>})Nooruddin (\\\\ensuremath{<}\\\\ensuremath{>})and Turk (\\\\ensuremath{<}\\\\ensuremath{>})2003], which we distribute uniformly over the sphere using a Fibonacci lattice that is pseudorandomly and independently offset for every training position. If any of these rays reaches infinity, the corresponding position x is deemed {\\\\textquotedblleft}outside{\\\\textquotedblright} of the object and the distance is marked positive. Otherwise, it is marked negative.3 (\\\\ensuremath{<}\\\\ensuremath{>})\\n\\nFor maximum efficiency, we use NVIDIA ray tracing hardware through the OptiX 7 framework, which is over an order of magnitude faster than using the previously mentioned triangle BVH for ray-shape intersections on our RTX 3090 GPU. \\n\\nD BASELINE MLPS WITH FREQUENCY ENCODING \\n\\nIn our signed distance function (SDF), neural radiance caching (NRC), and neural radiance and density fields (NeRF) experiments, we use an MLP prefixed by a frequency encoding as baseline. The respective architectures are equal to those in the main text, except that the MLPs are larger and that the hash encoding is replaced by sine and cosine waves (SDF and NeRF) or triangle waves (NRC). The following table lists the number of hidden layers, neurons per hidden layer, frequency cascades (each scaled by a factor of 2 as per (\\\\ensuremath{<}\\\\ensuremath{>})Vaswani et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2017]), and adjusted learning rates. \\n\\nFor NeRF, the first listed number corresponds to the density MLP \\n\\nand the second number to the color MLP. For SDFs, we make two additional changes: (1) we optimize against the relative L 2 loss [(\\\\ensuremath{<}\\\\ensuremath{>})Lehti-(\\\\ensuremath{<}\\\\ensuremath{>})nen et al. (\\\\ensuremath{<}\\\\ensuremath{>})2018] instead of the MAPE described in the main text, and (2) we perturb training samples with a standard deviation of \\\\ensuremath{\\\\mathit{r}} /128 as opposed to the value of \\\\ensuremath{\\\\mathit{r}} /1024 from Appendix (\\\\ensuremath{<}\\\\ensuremath{>})C.1. Both changes smooth the loss landscape, resulting in a better reconstruction with the above configuration. \\n\\nNotably, even though the above configurations have fewer parameters and are slower than our configurations with hash encoding, they represent favorable performance vs. quality trade-offs. An equal parameter count comparison would make pure MLPs too expensive due to their scaling with O(\\\\ensuremath{\\\\mathit{n}} 2) as opposed to the sub-linear scaling of trainable encodings. On the other hand, an equal throughput comparison would require prohibitively small MLPs, thus underselling the reconstruction quality that pure MLPs are capable of. \\n\\nWe also experimented with Fourier features [(\\\\ensuremath{<}\\\\ensuremath{>})Tancik et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] but did not obtain better results compared to the axis-aligned frequency encodings mentioned previously. \\n\\nE ACCELERATED NERF RAY MARCHING \\n\\nThe performance of ray marching algorithms such as NeRF strongly depends on the marching scheme. We utilize three techniques with imperceivable error to optimize our implementation: \\n\\nIn all other scenes, based on the intercept theorem4 (\\\\ensuremath{<}\\\\ensuremath{>}), we set the step size proportional to the distance \\\\ensuremath{\\\\mathit{t}} along the ray \\\\ensuremath{\\\\Delta}\\\\ensuremath{\\\\mathit{t}} := \\\\ensuremath{\\\\mathit{t}} /256,\\\\ensuremath{\\\\sqrt{}} \\\\ensuremath{\\\\sqrt{}}  clamped to the interval 3/1024, \\\\ensuremath{\\\\mathit{s}} {\\\\textperiodcentered} 3/1024 , where \\\\ensuremath{\\\\mathit{s}} is size of the largest axis of the scene{\\\\textquoteright}s bounding box. This choice of step size exhibits exponential growth in \\\\ensuremath{\\\\mathit{t}} , which means that the computation cost grows only logarithmically in scene diameter, with no perceivable loss of quality. \\n\\nLastly, we stop ray marching and set the remaining contribution to zero as soon as the transmittance of the ray drops below a threshold; in our case  = 10\\\\ensuremath{-}4 . \\n\\nRelated work. (\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. [(\\\\ensuremath{<}\\\\ensuremath{>})2019] already identified a nonlinear step size as benefitial: they recommend sampling uniformly in the disparity-space of the average camera frame, which is more aggressive than our exponential stepping, requiring on one hand only a constant number of steps, but on the other hand can lead to a loss of fidelity compared to exponential stepping [(\\\\ensuremath{<}\\\\ensuremath{>})Neff et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. \\n\\nTo skip ray marching steps in empty space, we maintain a cascade of \\\\ensuremath{\\\\mathit{K}} multiscale occupancy grids, where \\\\ensuremath{\\\\mathit{K}} = 1 for all synthetic NeRF scenes (single grid) and \\\\ensuremath{\\\\mathit{K}} \\\\ensuremath{\\\\in} [1, 5] for larger real-world scenes (up to 5 grids, depending on scene size). Each grid has a resolution of 1283 , spanning a geometrically growing domain [\\\\ensuremath{-}2\\\\ensuremath{\\\\mathit{k}} \\\\ensuremath{-}1 +0.5, 2\\\\ensuremath{\\\\mathit{k}}\\\\ensuremath{-}1 +0.5]3 that is centered around (0.5, 0.5, 0.5). \\n\\nEach grid cell stores occupancy as a single bit. The cells are laid out in Morton (z-curve) order to facilitate memory-coherent traversal by a digital differential analyzer (DDA). During ray marching, whenever a sample is to be placed according to the step size from the previous section, the sample is skipped if its grid cell{\\\\textquoteright}s bit is low. \\n\\nWhich one of the \\\\ensuremath{\\\\mathit{K}} grids is queried is determined by both the sample position x and the step size \\\\ensuremath{\\\\Delta}\\\\ensuremath{\\\\mathit{t}} : among the grids covering x, the finest one with cell side-length larger than \\\\ensuremath{\\\\Delta}\\\\ensuremath{\\\\mathit{t}} is queried. \\n\\nUpdating the occupancy grids. To continually update the occupancy grids while training, we maintain a second set of grids that have the same layout, except that they store full-precision floating point density values rather than single bits. \\n\\nWe update the grids after every 16 training iterations by performing the following steps. We \\n\\nThe sampling strategy of the \\\\ensuremath{\\\\mathit{M}} candidate cells depends on the training progress since the occupancy grid does not store reliable information in early iterations. During the first 256 training steps, we sample \\\\ensuremath{\\\\mathit{M}} = \\\\ensuremath{\\\\mathit{K}} {\\\\textperiodcentered} 1283 cells uniformly without repetition. For subsequent training steps we set \\\\ensuremath{\\\\mathit{M}} = \\\\ensuremath{\\\\mathit{K}} {\\\\textperiodcentered} 1283/2 which we partition into two sets. The first \\\\ensuremath{\\\\mathit{M}}/2 cells are sampled uniformly among all cells. Rejection sampling is used for the remaining samples to restrict selection to cells that are currently occupied. \\n\\nRelated work. The idea of constraining the MLP evaluation to occupied cells has already been exploited in prior work on trainable, cell-based encodings [(\\\\ensuremath{<}\\\\ensuremath{>})Liu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020; (\\\\ensuremath{<}\\\\ensuremath{>})Sun et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021; (\\\\ensuremath{<}\\\\ensuremath{>})Yu et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021a,(\\\\ensuremath{<}\\\\ensuremath{>})b]. In contrast to these papers, our occupancy grid is independent from the learned encoding, allowing us to represent it more compactly as a bitfield (and thereby at a resolution that is decoupled from that of the encoding) and to utilize it when comparing against other methods that do not have a trained spatial encoding, e.g. {\\\\textquotedblleft}Ours: Frequency{\\\\textquotedblright} in (\\\\ensuremath{<}\\\\ensuremath{>})Table 2. \\n\\nEmpty space can also be skipped by importance sampling the depth distribution, such as by resampling the result of a coarse \\n\\nTable 3. Batch size, number of rays per batch, and number of samples per ray for our full method ({\\\\textquotedblleft}Ours: Hash{\\\\textquotedblright}), our implementation of frequency encoding NeRF ({\\\\textquotedblleft}Ours: Freq.{\\\\textquotedblright}) and mip-NeRF. Since the values corresponding to our method vary by scene, we report minimum and maximum values over the synthetic scenes from (\\\\ensuremath{<}\\\\ensuremath{>})Table 2. \\n\\nprediction [(\\\\ensuremath{<}\\\\ensuremath{>})Mildenhall et al. (\\\\ensuremath{<}\\\\ensuremath{>})2020] or through neural importance sampling [(\\\\ensuremath{<}\\\\ensuremath{>})M\\\\\"uller et al. (\\\\ensuremath{<}\\\\ensuremath{>})2019] as done in DONeRF [(\\\\ensuremath{<}\\\\ensuremath{>})Neff et al. (\\\\ensuremath{<}\\\\ensuremath{>})2021]. \\n\\nThe batch size has a significant effect on the quality and speed of NeRF convergence. We found that training from a larger number of rays, i.e. incorporating more viewpoint variation into the batch, converged to lower error in fewer steps. In our implementation where the number of samples per ray is variable due to occupancy, we therefore include as many rays as possible in batches of fixed size rather than building variable-size batches from a fixed ray count. \\n\\nIn (\\\\ensuremath{<}\\\\ensuremath{>})Table 3, we list ranges of the resulting number of rays per batch and corresponding samples per ray. We use a batch size of 256 Ki, which resulted in the fastest wall-clock convergence in our experiments. This is 4{\\\\texttimes} smaller than the batch size chosen in mip-NeRF, likely due to the larger number of samples each of their rays requires. However, due to the myriad other differences across implementations, a more detailed study must be carried out to draw a definitive conclusion. \\n\\nLastly, we note that the occupancy grid in our frequency-encoding baseline ({\\\\textquotedblleft}Ours: Freq.{\\\\textquotedblright}; (\\\\ensuremath{<}\\\\ensuremath{>})Appendix D) produces even fewer samples than when used alongside our hash encoding. This can be explained by the slightly more detailed reconstruction of the hash encoding: when the extra detail is finer than the occupancy grid resolution, its surrounding empty space can not be effectively culled away and must be traversed by extra steps. \\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.txt'}),\n",
       " Document(page_content='CCX-RAYNET: A CLASS CONDITIONED CONVOLUTIONAL NEURAL NETWORK FOR BIPLANAR X-RAYS TO CT VOLUME \\n\\n2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) | 978-1-6654-1246-9/20/\\\\$31.00 {\\\\textcopyright}2021 IEEE | DOI: 10.1109/ISBI48211.2021.9433870\\n\\nSchool of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada \\n\\nABSTRACT \\n\\nDespite the advancement of the deep neural network, the 3D CT reconstruction from its correspondence 2D X-ray is still a challenging task in computer vision. To tackle this issue here, we proposed a new class-conditioned network, namely CCX-rayNet, which is proficient in recapturing the shapes and textures with prior semantic information in the resulting CT volume. Firstly, we propose a Deep Feature Transform (DFT) module to modulate the 2D feature maps of semantic segmentation spatially by generating the affine transformation parameters. Secondly, by bridging 2D and 3D features (Depth-Aware Connection), we heighten the feature representation of the X-ray image. Particularly, we approximate a 3D attention mask to be employed on the enlarged 3D feature map, where the contextual association is emphasized. Furthermore, in the biplanar view model, we incorporate the Adaptive Feature Fusion (AFF) module to relieve the registration problem that occurs with unrestrained input data by using the similarity matrix. As far as we are aware, this is the first study to utilize prior semantic knowledge in the 3D CT reconstruction. Both qualitative and quantitative analyses manifest that our proposed CCX-rayNet outperforms the baseline method. \\n\\nIndex Terms{\\\\textemdash} CT Reconstruction, GAN, X-ray, Medical Imaging, CNN, LS-GAN, 3D Patch Discriminator \\n\\n1. INTRODUCTION \\n\\nUnlike several previous methods where hundreds of X-rays are required, for CCX-rayNet, we need at most two X-rays to provide a corresponding CT. Here, the contributions of our work are three folds: (1) to the best of our knowledge, we are the first to apply the semantic prior constraint during 3D CT reconstruction to provide the network distinct anatomical structures and textures information about different organs as different organs with different semantics should treat differently. Inspired by [3], we proposed DFT modules to modulate the 2D feature maps of semantic segmentation spatially. This procedure provides the structure and textures of different organs in our system to tackle the shape distortion problem. (2) We proposed Depth Aware Connection (DAC) to lessen the depth information loss and to remove insignificant features when bridging 2D and 3D features. (3) The proposed AFF module used weighted sum instead of the average sum to fuse the multiple views of features. The AFF module vastly relies on the attention mechanism that helps the network amplify the most productive features and restrain the unregistered ones. \\n\\n2. METHOD AND MATERIALS \\n\\nWe have two parallel encoder-decoder networks for frontal and lateral X-rays, and a fusion network (in the middle) includes 3D basic blocks to fuse the information. We follow the same structure as X2CT [2] for the reconstruction and utilize UNet [4] to achieve the frontal prior segmentation information and feed this in the frontal encoder-decoder network. We precisely plug-in the DFT and DAC in encoder-decoder networks and attached AFF in the fusion network. We apply a 3D patch discriminator [5] with CCX-rayNet to produce CCX-rayGAN to improve the visualization of CT volumes. \\n\\nAs reflected in Fig. 1, to conserve the shape information and topology, we fed the segmentation map of the frontal view X-\\n\\nFig. 1. The generator part of biplanar CCX-rayNet contains two encoder-decoder networks. It includes the proposed DFT, DAC, and AFF modules. The input of the system is posterior-anterior and lateral views of X-ray, and the segmentation map. \\n\\nray into the condition network to originate the conditions. To transform the 2D feature, the DFT module uses the mapping function M to provide modulation parameter pair (\\\\ensuremath{\\\\alpha}, \\\\ensuremath{\\\\beta}). \\n\\n(1) \\n\\nWhere c indicates the input conditions. The main feature map will be shifted and scaled after obtaining (\\\\ensuremath{\\\\alpha}, \\\\ensuremath{\\\\beta}). \\n\\n(2) \\n\\nFin has the similar dimension of \\\\ensuremath{\\\\alpha} and \\\\ensuremath{\\\\beta}. With specific semantic prior information, the element-wise addition (+) and element-wise product  use to perform an element-wise transformation in DFT module. As illustrated in Fig. 1, conditions have been set as shared intermediate values and transmitted to every 2D encoder so that the DFT can acquire few parameters. End-to-end training use to optimize M. \\n\\nAt first, the intermediate 3D feature map has been expanded from a 2D feature map by duplicating this 2D one along the third axis. Then, employ the 3D feature map into a basic CNN block impose it with a shape of (B, C, D, H, W). Next, from the 2D feature map, originate an attention matrix with the shape of (B, D, H, W). Lastly, we enhance the attention matrix C times and perform the element-wise multiplication with the 3D feature map. The action sums up as: \\n\\nI, G, and G{\\\\textasciicircum} denote the input 2D feature map, intermediate 3D feature map, and 3D feature map before applying the attention map, respectively. Moreover, expanding operation is signified as E, and the attention matrix is produced from I. \\n\\n(7) \\n\\nHere, we compute a similarity matrix SIM for the distance of two 3D feature maps in the fusing part. I1 and I2 are orthogonal feature maps. The similarity matrix is a typical blueprint of the non-local module and is utilized when corresponding feature voxels are consistent. We apply the dot distance for distance measurement due to computational issues. Besides, two weighting matrices flexibly enhance one of the feature point and compress the other one{\\\\textquoteright}s contribution. \\n\\n(8) \\n\\n(9) \\n\\nThe conv 3D does not share the weights, and the weighted sum is implemented to fuse the features by multiplying the 3D feature maps with the corresponding weighting matrices. \\n\\n(10) \\n\\nFig. 2. Posterior-anterior chest X-rays from the dataset (first row); predicted segmentation map from UNets (second row). \\n\\nFor training, we require an X-ray and CT paired dataset. Ying et al. [2] provide synthetic X-rays from real CT scans by employing digitally reconstructed radiograph (DRR) technology followed by CycleGAN [6]. The 1018 chest CT scans are gathered from the LIDC-IDRI dataset [7]. For training and testing, we use 916 and 102 CT scans, respectively. \\n\\n3. EXPERIMENT \\n\\nTo show the efficacy of our proposed CCX-rayNet in this section, we provide the model{\\\\textquoteright}s qualitative and quantitive analyses. We also display the network settings and ablation study. \\n\\nWe train this network for 100 epochs with Adam optimizer, where the initial learning rate is 2e\\\\ensuremath{-} 3 and decay the learning rate after 50 epochs with 30 percent. For training, we apply instance normalization instead of batch normalization. \\n\\nWe follow two different approaches to train our model: (1) the projection pixel-wise L1 loss and voxel-wise loss are incorporated, signified as CCX-rayNet. (2) the GAN-based training approach is called CCX-rayGAN, where the backpropagation steps of the generator and discriminator imitate the same procedure as in LS-GAN [11]. We resized the frontal and lateral view X-rays, and frontal view segmentation map to 128{\\\\texttimes}128 for training. In the frontal view segmentation map, we acquire two categories, such as the heart and lungs, but the {\\\\textquoteleft}background{\\\\textquoteright} category is utilized to encompass regions that do not include in the categories above. Finally, the output dimension of these models is 128 {\\\\texttimes} 128 {\\\\texttimes} 128. \\n\\nFig. 3. Lateral view (first row) and axial view (second row) from a CT volume yielded by CCX-rayGAN+B and X2CTGAN+B. CCX-rayGAN+B generates precise anatomical reconstructions than X2CT-GAN+B. \\n\\nCT volume reconstruction from X-rays is comparatively a new proposed approach, and our CCX-rayGAN+B can vastly enhance the perceptual quality of the reconstruction result. In \\n\\nFig. 4. 3D CT volumes are reconstructed from CCX-rayGAN and X2CT-GAN. {\\\\textquoteright}+B{\\\\textquoteright} denotes biplanar X-ray inputs. The first row indicates the posterior-anterior (PA) view of the CT, and the second row signifies the bone structure reconstruction. \\n\\nFig. 3 and Fig. 4, we compare the visual quality of our GAN-based biplanar network and baseline X2CT-GAN+B method. \\n\\nThrough the lateral and axial view of Fig.3, we can exhibit that CCX-rayGAN+B generates high-quality reconstruction outcomes with intricate anatomical parts compared to X2CTGAN+B. Specifically, our method yields sharper boundaries and internal textures of the organs compared to the state-of-the-art-method. Next, in Fig. 4, we can inspect that the CCX-rayGAN+B can reconstruct the outline shape of the organs (e.g., lungs) and small complex anatomies such as small vessels in the lungs posterior-anterior (PA) view (first row). Besides, our network can reconstruct the chest (ribs) bone structure and backbone (second row) close to the ground truth. We can produce superior results in both internal anatomies and bone structure than the baseline method X2CT-GAN+B. \\n\\nTable 2. Several combinations of proposed biplanar CCX-rayNet and the best outcome marked in bold \\n\\nachieve 3.4 dB more PSNR value in our biplanar method (CCX-rayNet+B) than the single view one (CCX-rayNet+S). \\n\\nFor the CCX-rayNet+B, we assess the impacts of our proposed modules, and the result displayed in Table 2. Without three modules, the network{\\\\textquoteright}s PSNR value is 27.29 dB; wheres the combination of DFT, DAC, and AFF modules improve PSNR value to 28.18 dB. Additionally, the DFT and DAC modules also enhance the PSNR value than the basic model. \\n\\n4. CONCLUSION \\n\\nThis paper represents a class-conditioned network called CCX-rayNet, to reconstruct 3D CT volume from synthetic chest X-rays. We proposed three modules (DFT, DAC, AFF) to recapturing textures and shapes faithful to semantic classes in the practical-world scenario. In order to make the dataset, we use two binary UNet architectures to generate the segmentation maps. Our proposed biplanar method with prior semantic information restore density information, anatomical structure, and shape in the reconstructed 3D volumes with high visual quality than baseline models. Our ablation study proclaims the competence of our proposed modules. \\n\\n5. COMPLIANCE WITH ETHICAL STANDARDS \\n\\nThis is a numerical simulation study for which no ethical approval was required. \\n\\n6. CONFLICTS OF INTEREST \\n\\nNo funding was received for conducting this study. The authors have no relevant financial or non-financial interests to disclose. \\n\\n7. REFERENCES \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.txt'}),\n",
       " Document(page_content=\"PROCEEDINGS OF SPIE \\n\\nHarnessing the power of deep learning for volumetric CT imaging with single or limited number of projections \\n\\nLiyue Shen, Wei Zhao, Lei Xing \\n\\nLiyue Shen, Wei Zhao, Lei Xing, ''Harnessing the power of deep learning for volumetric CT imaging with single or limited number of projections,'' Proc. SPIE 10948, Medical Imaging 2019: Physics of Medical Imaging, 1094826 (1 March 2019); doi: 10.1117/12.2513032 \\n\\nEvent: SPIE Medical Imaging, 2019, San Diego, California, United States \\n\\nLiyue Shena , Wei Zhaob , and Lei Xinga,b \\n\\nABSTRACT \\n\\nTomographic imaging using a penetrating wave, such as X-ray, light and microwave, is a fundamental approach to generate cross-sectional views of internal anatomy in a living subject or interrogate material composition of an object and plays an important role in modern science. To obtain an image free of aliasing artifacts, a sufficiently dense angular sampling that satisfies the Shannon-Nyquist criterion is required. In the past two decades, image reconstruction strategy with sparse sampling has been investigated extensively using approaches such as compressed-sensing. This type of approach is, however, ad hoc in nature as it encourages certain form of images. Recent advancement in deep learning provides an enabling tool to transform the way that an image is constructed. Along this line, Zhu et al1 presented a data-driven supervised learning framework to relate the sensor and image domain data and applied the method to magnetic resonance imaging (MRI). Here we investigate a deep learning strategy of tomographic X-ray imaging in the limit of a single-view projection data input. For the first time, we introduce the concept of dimension transformation in image feature domain to facilitate volumetric imaging by using a single or multiple 2D projections. The mechanism here is fundamentally different from the traditional approaches in that the image formation is driven by prior knowledge casted in the deep learning model. This work pushes the boundary of tomographic imaging to the single-view limit and opens new opportunities for numerous practical applications, such as image guided interventions and security inspections. It may also revolutionize the hardware design of future tomographic imaging systems. \\n\\nKeywords: image reconstruction, deep learning, convolutional neural network, sparse data sampling \\n\\n1. INTRODUCTION \\n\\nThe ability of tomographic imaging to take a deep and quantitative look into a patient or an object with high spatial resolution holds significant value in scientific explorations and medical practice. Traditionally, a tomographic image is obtained by mathematical inversion of the encoding function of the imaging wave for a given set of measured data from different angular positions. A prerequisite of artifacts-free inversion is the satisfaction of classical Shannon-Nyquist theorem in angular data sampling, which imposes a practically achievable limit in imaging time and object irradiation. To mitigate the problem, image reconstruction strategy with sparse or limited sampling has been investigated extensively using techniques such as compressed-sensing,2{\\\\textendash}5 and maximum a posteriori.6, 7 This type of approach introduces a regularization term to the fidelity function to encourage some ad hoc or presumed characteristics in the resultant image.8{\\\\textendash}17 The resultant sparsity without compromising the image quality is generally limited and does not meet the unmet demand for real-time imaging with substantially reduced subject irradiation. Indeed, while continuous effort has been made in imaging with reduced angular measurements over the years, tomographic imaging with ultra-sparse sampling has yet to be realized. In this work, we push the sparsity to the limit of a single projection and demonstrate what seemingly unlikely scenario of a single-view tomographic imaging is readily achievable by leveraging from the state-of-the-art deep learning technique and seamless integration of prior knowledge in the deep learning-based image reconstruction process. \\n\\nDeep neural network has recently attracted much attention for its unprecedented ability to learn complex relationships and incorporate existing knowledge into the inference model through feature extraction and representation learning.18{\\\\textendash}20 The method has found widespread applications across disciplines, such as computer \\n\\nMedical Imaging 2019: Physics of Medical Imaging, edited by Taly Gilat Schmidt, Guang-Hong Chen, Hilde Bosmans, Proc. of SPIE Vol. 10948, 1094826 {\\\\textperiodcentered} {\\\\textcopyright} 2019 SPIE {\\\\textperiodcentered} CCC code: 1605-7422/19/\\\\$18 {\\\\textperiodcentered} doi: 10.1117/12.2513032 \\n\\nTable 1: Table 1. Quantitative reconstruction results. Different numbers of 2D projections are utilized for prediction. We evaluate on testing dataset and display average values of four metrics including mean absolute error (MAE), root mean squared error (RMSE), structural similarity (SSIM), and peak signal noise ratio (PSNR). \\n\\nusing corresponding mean and variance, which is usually used to make the data distribution closer to normal distribution in statistics. \\n\\nFor the training objective, we define the cost function based on the mean squared error (L2 norm loss) between the predicted results and the ground truth. For comparison purpose, we use the same training strategy and hyper-parameters for all experiments. We implement the network with PyTorch36 deep learning framework. In training process, we use a mean squared error (MSE) loss function to compute voxel-wise mean-squared-error between the ground truth and predicted 3D images as defined below. By using a random initialization for network parameters, the Adam optimizer37 is utilized to minimize loss objective and update network parameters through back-propagation with iterative epochs. Specifically, we use the learning rate of 0.00002 and the mini-batch size of 1 because of the memory limitation. As shown in 4, the training loss objective is minimized iteratively. And at the end of each epoch, the trained model is evaluated on an independent validation data set. This strategy is commonly used to monitor the model performance and avoid overfitting the training samples. In addition, learning rate is scheduled decaying according to the validation loss. Specifically, if the validation loss remains unchanged for 10 epochs, learning rate will be reduced by a factor 2. Finally, the best checkpoint model with the smallest validation loss is selected as final model in the experiments. We train the network using one NVIDIA TITAN V100 graphics processing unit (GPU) for 100 epochs. \\n\\nReconstruction experiments were conducted using the proposed method and 4D-CT data as described above. To evaluate the performance of our network, we deployed the trained model on a separate testing dataset and analyzed reconstruction results using both qualitative and quantitative evaluation metrics. Specially, in order to investigate reconstruction performance with different number of 2D projections, we have conducted experiments with 1, 2, 5, and 10 projections as input for comparison purpose. The multiple view angles are distributed evenly around a 180-degree semicircle. For instance, for 2-views, the two orthogonal directions are 0 degree (AP) and 90 degrees (lateral). Here we stack the 2D projections from different view angles as different channels of the input data and modify the first convolution layer to fit the input data size. \\n\\nWith the same model training procedure and hyper-parameter, we obtain the qualitative reconstructed CT images for 1, 2, 5, and 10 views in Figs. 5-7. In the figures, we display our reconstruction results for one example chosen from testing set. Specially, we demonstrate the 3D CT image from three view points: axial (Fig. 5), coronal (Fig. 6), and sagittal (Fig. 7). For visualization purpose, in each figure, each column shows one slice image selected from the predicted 3D images together with the ground truth. The difference image between the predicted image and ground truth image is also shown. It is seen that the prediction images are very similar to the target images, which shows that our model performs well for 3D CT reconstruction even with only a single projection. \\n\\nFor quantitative evaluation, the metrics of mean absolute error (MAE), root mean squared error (RMSE), structural similarity (SSIM) are calculated to measure the prediction error between estimated images and ground truth images. In addition, we also compute the peak signal noise ratio (PSNR) to show the reconstructed image \\n\\nquality. By computing the average values of various evaluation metrics for all the 100 examples in testing set, the quantitative evaluation of the results are presented in Table 1. Interestingly, we observed that a single 2D projection provides sufficient data to produce a high-quality reconstruction similar to the reconstructions performed with multiple projection images, when comparing the quantitative evaluation metrics. \\n\\nFrom above results, we conclude that the proposed deep learning reconstruction framework is capable of providing reasonable 3D images by using only a single or a few view projections. \\n\\n4. CONCLUSION \\n\\nWe have presented a novel deep learning framework for volumetric imaging with ultra-sparse data sampling. It is both intriguing and important that the proposed strategy is capable of holistically extracting the feature characteristics embedded in a single or a few 2D projection data and transform them into the corresponding 3D image with high fidelity. Although this work is focused on the most commonly used X-ray imaging, the concept and implementation should be easily extendable to other imaging modalities with ultra-sparse sampling. Practically, the single-view imaging may present a revolutionary solution to various practical applications, ranging from image guidance in interventions, cellular imaging, objection inspection, to greatly simplified imaging system design. A few medical applications along these directions are underway. \\n\\nREFERENCES \\n\", metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.txt'}),\n",
       " Document(page_content='https://doi.org/10.1038/s41551-019-0466-4 \\n\\nPatient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning \\n\\nTomographic imaging using penetrating waves generates cross-sectional views of the internal anatomy of a living subject. For artefact-free volumetric imaging, projection views from a large number of angular positions are required. Here we show that a deep-learning model trained to map projection radiographs of a patient to the corresponding 3D anatomy can subsequently generate volumetric tomographic X-ray images of the patient from a single projection view. We demonstrate the feasibility of the approach with upper-abdomen, lung, and head-and-neck computed tomography scans from three patients. Volumetric reconstruction via deep learning could be useful in image-guided interventional procedures such as radiation therapy and needle biopsy, and might help simplify the hardware of tomographic imaging systems. \\n\\nThe ability of computed tomography (CT) to take a deep and quantitative image of a patient or an object with high spatial resolution is highly valuable in scientific research and medical practice. Traditionally, a tomographic image is obtained via the mathematical inversion of the encoding function of the imaging wave for a given set of measured data from different angular posi-tions (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1a,b). A prerequisite for artefact-free inversion is the satisfaction of the classical Shannon{\\\\textendash}Nyquist theorem in angular-data sampling, which imposes a practically achievable limit in imaging time and object irradiation. To mitigate the problem, image reconstruction with sparse sampling has been investigated extensively using techniques such as compressed sensing1(\\\\ensuremath{<}\\\\ensuremath{>}){\\\\textendash} and maximum a posteriori,. These types of approaches introduce a regularization term to the inversion to encourage some ad hoc or presumed char-acteristics in the resultant image{\\\\textendash}. If imaging quality cannot be compromised, the resultant sparsity is generally limited and does not address the unmet demand for real-time imaging with substan-tially reduced subject irradiation (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1c). Indeed, while continuous efforts have been made to reduce the number of angular measure-ments in medical imaging, tomographic imaging with ultra-sparse sampling has not yet been realized. \\n\\nIn this study, we push sparse sampling to the limit of a single projection view and demonstrate single-view tomographic imaging with a patient-specific prior by leveraging deep learning and the seamless integration of prior knowledge in the data-driven image-reconstruction process. The harnessing of prior knowledge by machine-learning techniques in different data domains for improved imaging is an emerging topic of research. Some recent studies{\\\\textendash}have also investigated machine-learning-based image reconstruction. Whereas the data-driven approach represents a potentially general strategy for image reconstruction, here single-view CT imaging is achieved via a patient-specific prior. Practically, it is actually advantageous to work with the patient-specific prior: for many image-guided interventional applications, the approach would enable scenarios most relevant to the specific patient under treatment. \\n\\nDeep neural networks have attracted much attention for their ability to learn complex relationships and to incorporate existing knowledge into the inference model through feature extraction and representation learning{\\\\textendash}. The method has found widespread applications across disciplines, such as computer vision{\\\\textendash}, autonomous driving, natural language processing and biomedicine,{\\\\textendash}. Here we design a hierarchical neural network for X-ray CT imaging with ultra-sparse projection views, and develop a structured training process for deep learning to generate three-dimensional (3D) CT images from two-dimensional (2D) X-ray projections. Our approach introduces a feature-space transformation between a 2D projection and a 3D volumetric CT image within a representation{\\\\textendash}generation (encoder{\\\\textendash}decoder) framework. By using the transformation module, we transfer the representations learned from the 2D projection to a representative tensor for 3D volume reconstruction in the subsequent generation network. Through the model-training process, the transformation module learns the underlying relationship between feature representations across dimensionality, making it possible to generate a volumetric CT image from a 2D projection. It should be emphasized that an X-ray projection is not a purely 2D cross-sectional image, as higher dimensional information is already encoded during the projection process (see schematic in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1a), with the encoding function determined by the physics of interactions between the X-ray and media. Generally, a single projection alone is not sufficient for capturing the anatomical information in the projection direction for the subsequent volumetric image reconstruction. What enables our deep-learning model for patient-specific volumetric image reconstruction is that anatomical relations (including the information in the direction of the projection view) are encoded during the model-training process via the use of augmented datasets containing different 2D{\\\\textendash}3D data pairs of body positions and anatomical distributions. The deep-learning transformation deciphers the hidden information in the projection data and predicts a volumetric image with the help of prior knowledge gained during model training (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1d). \\n\\nFig. 1 | 3d image reconstruction with ultra-sparse projection-view data. a, A geometric view of an X-ray source, a patient and a detector in a CT system. b, X-ray projection views of a patient from three different angles. c, Different image-reconstruction schemes in the context of prior knowledge and projection sampling. d, Volumetric image reconstruction using deep learning with one or multiple 2D projection images. \\n\\nFig. 2 | architecture of the deep-learning network. a, The input of the model is a single projection view or multiple 2D projection views. b, The representation network learns the feature representation of the imaged object from the input. c, The extracted 2D features are reshaped and transferred by the transformation module to a 3D representation, for subsequent reconstruction. d, The generation network uses representation features extracted in the former stages to generate the corresponding volumetric images. e, The output of the model is the correponding volumetric image. Conv, convolution layer; deconv, deconvolution layer (the numbers indicate the specific kernel size used); BN, batch normalization; ReLU, rectified linear unit; +, feature-map addition with residual path; the numbers underneath each layer denote the number of feature maps for each layer. \\n\\nresults \\n\\nFig. 3 | training-loss and validation-loss curves for the abdominal ct and lung ct cases. a,b, Graph of mean-squared error (MSE) loss for training data (in blue) and validation data (in orange) against the number of training iterations for abdominal CT (a) and lung CT (b), during the image reconstruction with a single projection view. Details are shown in the zoom-in figures on the right. \\n\\nembedding features and learns a semantic representation of the actual 3D scene from the input 2D projection(s). The transformation module bridges the representation and generation networks through convolution and deconvolution operations and relates the 2D and 3D feature representations. The role of the generation network is to provide volumetric images with subtle structures on the basis of the learned features from the representation network. In constructing the model, we assume that one or more 2D projections and the corresponding 3D image possess the same semantic representation, as they represent the same object or scene. In other words, the representation in feature space remains invariant in the transformation of a 2D projection into a 3D image. To a large extent, the task of 3D image reconstruction here is to train the encoder (that is, the representation network) and decoder (that is, the generation network) to reliably learn the relationship between the feature space and the image space. Details about the network architecture are included in Methods. \\n\\nWe evaluate the approach by using different disease sites: an upper-abdominal case, a lung case and a head-and-neck case. We use the anterior{\\\\textendash}posterior 2D projection as input (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2). In all experiments, the same network architecture and training strategy are used. The loss curves (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3) indicate that the model is trained to fit the training data well and can also be generalized to work on the data not included in the training datasets. The details of dataset generation and the training process are described in Methods. \\n\\nTo evaluate the feasibility of the approach, we deploy the trained network on an independent testing dataset. Figure (\\\\ensuremath{<}\\\\ensuremath{>})4a shows our reconstruction results with a single anterior{\\\\textendash}posterior-view input for the abdominal CT and lung CT cases, together with the ground-truth CT images and the difference images between the obtained images and the ground truth. The deep-learning-derived images resemble the target images, indicating the potential of the model for volumetric imaging. We also reconstruct volumetric images \\n\\nFig. 4 | examples from the abdominal ct and lung ct cases. a{\\\\textendash}d, Images reconstructed by using 1 (a), 2 (b), 5 (c) and 10 (d) projection views. Predicted and difference images between predicted and ground truth are shown. The corresponding coronal and sagittal views of the images for both experiments are presented in Supplementary Figs. 3{\\\\textendash}6. For the abdominal CT case, 720, 180 and 600 images are used for training, validation and testing, respectively. For the lung CT case, 2,400, 600 and 200 images are used for training, validation and testing, respectively. \\n\\nwith a single lateral view as input for the abdominal case, with similar results (see the Experiments section of the Supplementary Information). Furthermore, we use multiple quantitative evalua-tion metrics to measure the results. Table (\\\\ensuremath{<}\\\\ensuremath{>})1 summarizes the average values of the evaluation metrics. The qualitative and quantitative results demonstrate that our model is capable of achieving 3D image reconstruction even with only a single 2D projection. The results (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})5) also confirm the validity of our approach. \\n\\ndiscussion \\n\\nTo better understand the deep-learning model, we analyse the semantic representations learned from the model. Generally \\n\\nFig. 5 | examples from the head-and-neck ct case. a, 3D CT images of a head-and-neck case used for the training of the deep-learning model. b, Left: testing samples and the corresponding difference images (with respect to the training samples) in the transverse, sagittal and coronal planes. Right: predicted images and the corresponding difference images (with respect to the ground truth) in the transverse, sagittal and coronal planes. For this case, 2,000, 500 and 200 images are used for training, validation and testing, respectively. \\n\\nFig. 6 | analysis of feature maps. a, Visualization of the feature maps, learned from different 2D projections, for two testing samples. The different colours in the figure indicate different intensity values in the feature maps (a lighter colour indicates a higher intensity value). b, t-SNE visualization of the feature representations of 15 testing examples with the input of different 2D views. A total of 15 clusters (4 points of the same colour) are shown. The four points in a cluster represent the learned features from 1-, 2-, 5- and 10-view reconstruction models. Each cluster denotes the embedded representations for each of the 15 randomly chosen testing samples. c, Correlation matrix of representation vectors in 1-view and 2-view reconstructions from 50 randomly chosen testing samples out of 600. \\n\\nfrom the transformation module for two testing samples. For visualization purposes, only 5 randomly chosen channels among the 4,096 feature maps are shown, each with a size of 4 {\\\\texttimes}4 pixels. The feature maps learned from different numbers of 2D projections are displayed separately in different columns. The results show that, when different 2D views are given, the model extracts similar semantic representations of the underlying 3D scene. Furthermore, Fig. (\\\\ensuremath{<}\\\\ensuremath{>})6b shows the visualization of t-distributed stochastic neighbour embedding (t-SNE) for the feature maps of 15 testing samples. The t-SNE technique is commonly used to visualize high-dimensional data by embedding each sample as a point in a 2D space. The four points in a cluster of the same colour represent the learned features from one-, two-, five- and ten-view reconstructions. The figure shows clustering behaviour for feature maps from the same sample, indicating that the model learns a similar representation from different 2D projections. \\n\\nOutlook. We have described a deep-learning approach for volumetric imaging with ultra-sparse data sampling and a patient-specific prior. The data-driven strategy is capable of holistically extracting the feature characteristics embedded in a single projection or in a few 2D projections, and of transforming them into the corresponding 3D image through model learning. The image-feature space transformation plays an essential role in the ultra-sparse image reconstruction. At the training stage, the method incorporates diverse forms of a priori knowledge into the reconstruction. The manifold-mapping function is learned from the training datasets, rather than relying on any ad hoc form of motion trajectory. Although we have used X-ray imaging and patient-specific data, the concept and implementation of the approach could be extended to other imaging modalities or to other data domains with ultra-sparse sampling. Practically, single-view imaging represents a potential solution for many image-guided interventional procedures and may help to simplify the hardware of tomographic imaging systems. \\n\\nmethods \\n\\nProblem formulation. We formulate the problem of 3D image reconstruction from 2D projection(s) into a deep-learning framework. Given a sequence of 2D projections denoted as  where  for alland N is the number of given 2D projections, the goal is to generate a volumetric 3D image Y describing the corresponding 3D physical scene. With the sequence of 2D projections as input, the deep-learning model outputs the predicted 3D volume denoted as  while  is the ground-truth 3D image as the reconstruction target. Note that network prediction Ypred is of the same size as ground-truth image Ytruth, where each entry is a voxel-wise intensity value. Thus, the problem is formulated as finding a mapping function F transforming 2D projections to volumetric images. To tackle this problem, a deep-learning model is trained to find the mapping function F, which uses 2D projections fX1; X2; ; XN g as input and predicts the corresponding 3D image Ypred, as expressed in Equation ((\\\\ensuremath{<}\\\\ensuremath{>})1). \\n\\n{\\\\dh}1{\\\\TH} \\n\\nIn order to use a sequence of 2D projections as model input, we stack all the 2D  as a 3D tensor. In other words, a set of 2D projections  are stacked as a 3D volume where N is the number of 2D projections. In what follows, we introduce the model architecture of the deep neural network in detail. \\n\\nRepresentation network. Superb performance has been achieved by deep residual networks (such as ResNet) in many tasks. A key step in residual learning is the identity mapping that facilitates the training process and avoids gradient vanish in back-propagation, which encourages residual learning of the hierarchical representation at each stage and eases the training of the deep network. Motivated by this feature, we introduce a residual-learning scheme in the representation network (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2), in which the 2D convolution residual block is used to assist the deep model to learn semantic representations from 2D projections. More details about the residual-learning scheme are presented in Supplementary Information, Ablative study and discussion, and the results are summarized in Supplementary Table 2. Specifically, each 2D convolution residual block consists of a pattern of {\\\\textquoteleft}2D convolution layer (with kernel size 4 and stride 2) {\\\\textrightarrow} 2D batch normalization layer {\\\\textrightarrow} ReLU layer {\\\\textrightarrow} 2D convolution layer (with kernel size 3 and stride 1) {\\\\textrightarrow} 2D batch normalization layer {\\\\textrightarrow} ReLU layer{\\\\textquoteright}. The first layer performs 2D convolution operations using a 4 {\\\\texttimes} 4 kernel with sliding stride 2 {\\\\texttimes} 2, which down-samples the spatial size of the feature map by a factor 2. In addition, to keep the sparsity of high-dimensional feature representation, we correspondingly double the channel number of the feature maps by increasing the number of convolutional filters. A distribution normalization layer among the training mini-batch (batch normalization) then follows before feeding the feature maps through the ReLU layer. Next, the second 2D convolution layer and 2D batch normalization layer are done by a kernel size of 3 {\\\\texttimes} 3 and sliding stride 1 {\\\\texttimes} 1, which keeps the spatial shape of the feature maps. Moreover, before applying the second ReLU layer, an extra shortcut path is established to add up the output of the first convolution layer to obtain the final output. By setting up the shortcut path of identity mapping, the second convolution layer is encouraged to learn the residual feature representations. To extract hierarchical semantic features from 2D projections, we constructed the representation network by concatenating five 2D convolution residual blocks with different number of convolutional filters. A detailed discussion of the network depth is available in Supplementary Information, Ablative study and discussion, with some results illustrated in Supplementary Fig. 8. To be concise, we use the notation k {\\\\texttimes} m {\\\\texttimes} n to denote k channels of feature maps in a spatial size of m {\\\\texttimes} n. In the generation network, the size of input images is denoted as N {\\\\texttimes} 128 {\\\\texttimes} 128, where N is the number of 2D projections. The change of feature-map size through the network is N {\\\\texttimes} 128 {\\\\texttimes} 128 {\\\\textrightarrow} 256 {\\\\texttimes} 64 {\\\\texttimes} 64 {\\\\textrightarrow} 512 {\\\\texttimes} 32 {\\\\texttimes} 32 {\\\\textrightarrow} 1024 {\\\\texttimes} 16 {\\\\texttimes} 16 {\\\\textrightarrow} 2048 {\\\\texttimes} 8 {\\\\texttimes} 8 {\\\\textrightarrow} 4096 {\\\\texttimes} 4 {\\\\texttimes} 4, where each {\\\\textquoteleft}{\\\\textrightarrow} {\\\\textquoteright} represents going through a 2D convolution residual block as described above, except that batch normalization and ReLU activation are removed in the first convolution layer. Thus, the output of the representation network is a feature representation  extracted from 2D projections with a size of 4096 {\\\\texttimes} 4 {\\\\texttimes} 4. \\n\\nTransformation module. To bridge the representation and generation networks, a transformation module is deployed after learning the representations. As shown in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2, by taking the convolution operations with a kernel size of 1 {\\\\texttimes} 1 and ReLU activation, the 2D convolution layer learns a transformation across all 2D feature maps. Then, we reshape embedded representations from 4096 {\\\\texttimes} 4 {\\\\texttimes} 4 to 2048 {\\\\texttimes} 2 {\\\\texttimes} 4 {\\\\texttimes} 4. In this way, we transform the feature representation across dimensions for subsequent 3D volume generation. Next, a 3D deconvolution layer with a kernel size of 1 {\\\\texttimes} 1 {\\\\texttimes} 1 and sliding stride of 1 {\\\\texttimes} 1 {\\\\texttimes} 1 learns a transformation among all 3D feature cubes while keeping the feature size unchanged. This transformation module bridges the 2D and 3D feature spaces. Moreover, as described in previous work, we also remove the batch normalization in the transformation module to help the knowledge transfer through this module. \\n\\nMaterials. The approach is evaluated by using three cases of different disease sites. In the first study, a ten-phase upper-abdominal 4D CT scan of a patient for radiation therapy treatment planning is selected. To proceed, the first six phases are used to generate the CT-DRR pairs for model training and validation with the procedure described above. We use the anterior{\\\\textendash}posterior 2D projection as input (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2). With translation, rotation and deformation introduced to the CT volume, we obtain a total of 720 DRRs representing different scenarios of the patient anatomy for model training and 180 DRRs for validation. To ensure that the testing data are not seen in the model-training process, we generate 600 testing DRR samples independently from the remaining 4 phases of the 4D CT. The 4D CT images are acquired with 120 kV, 80 mA on a positron emission tomography{\\\\textendash} CT simulator (Biograph mCT 128, Siemens Medical Solutions) together with a Varian Real-time Position Management system (Varian Medical Systems). The 2D projection data are obtained by projecting each of the 3D CT data points in the geometry of the on-board imager of the TrueBeam system (Varian Medical System). In the second experiment, a lung cancer patient is chosen with two independent treatment-planning 4D CT scans acquired at two different times with the same imaging parameter settings as above. Using the data-augmentation strategy as described above, the first 4D CT is used to generate training (2,400 samples) and validation (600 samples) datasets, whereas the second 4D CT was used to generate a testing dataset (200 samples). For each of the images in the training and testing datasets, the corresponding 2D projections are produced by projecting the 3D CT volume in the geometry of the on-board imager of the TrueBeam system. To build a reliable model, the training and testing datasets might come from the same data distribution, but the datasets are independently sampled. The data acquisition and processing for the head-and-neck case are described in Supplementary Information. \\n\\nImage pre-processing. Data are pre-processed before feeding them into the network. First, we resize all data samples to the same size. For example, all the 2D projection images are resized to 128 {\\\\texttimes} 128. The volumetric images of the abdominal CT and the lung CT are resized to 46 {\\\\texttimes} 128 {\\\\texttimes} 128 and 168 {\\\\texttimes} 128 {\\\\texttimes} 128 respectively, due to their different depths in the z axis. Each data sample is a pair of 2D projected view(s) and the corresponding 3D CT. Similar to other deep-learning-based imaging studies, down-sampling is introduced purely because of the memory limitation and for the purpose of computational efficiency. The formulation and algorithm are scalable to full-size images (512 {\\\\texttimes} 512), because the component layers used in our model are also scalable to images of different sizes. At the current resolution of 128 {\\\\texttimes} 128 (which is the same as that used in the deep-learning-based MRI reconstruction), small motions of less than 3 mm may not be described accurately. However, we should emphasize that this resolution does not represent a fundamental limit of the deep-learning-based approach and can be improved as computational technology advances. In practice, methods such as deep-learning-based super-resolution are being actively pursued, which may be employed to improve the spatial resolution of the approach. Additionally, following the standard protocol of data pre-processing, we conduct scaling normalization for both the 2D projections and the 3D volumetric images, where pixel-wise or voxel-wise intensities are normalized to the interval [0,1]. Moreover, we normalize the statistical distribution of the pixel-wise intensity values in the input 2D projections to be closer to a standard Gaussian distribution N {\\\\dh}0; 1{\\\\TH}. Specifically, we calculate the statistical mean and standard derivation among all the training data. When a new sample is inputted, we subtract the mean value from the input image(s) and divide the image(s) by the standard derivation to get the input 2D image(s). \\n\\nEvaluation. To evaluate the performance of the approach, we deploy the trained model on a testing dataset, and analyse the reconstruction results using both qualitative and quantitative evaluation metrics. We use four different metrics to measure the quality of predicted 3D images: MAE, RMSE, SSIM and PSNR.  We compute the average values across all testing samples, and they are shown in Table (\\\\ensuremath{<}\\\\ensuremath{>})1. MAE/MSE is the L1-norm/L2-norm error between Ypred and Ytruth. As usual, we take the square root of MSE to get RMSE. In practice, MAE and RMSE are commonly used to estimate the difference between the prediction and ground-truth images. SSIM score is calculated with a windowing approach in an image, and is used for measuring the overall similarity between two images. In general, a lower value of MAE and RMSE or a higher SSIM score indicates a better prediction closer to the ground-truth images. PSNR is defined as the ratio between the maximum signal power and the noise power that affects the image quality.  PSNR is widely used to measure the quality of image reconstruction. \\n\\nComparison study. To better benchmark the proposed method against the existing techniques, we conduct a comparative study with the published PCA-based method{\\\\textendash} and elaborate the difference and advantages of our proposed approach. The comparison is done for a special situation of 4D CT reconstruction (abdominal CT), where the anatomical motion may be characterized by principal components. We find that the PCA and deep-learning-based methods produce similar results in an ideal case when there is no inter-scan variation in patient positioning (since the results are very similar, the resultant images are not shown). However, the deep-learning model outperforms the PCA method in more realistic scenarios when the patient position deviates slightly from that of the reference  scan (see Supplementary Information for details). \\n\\nReporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article. \\n\\ndata availability \\n\\nThe authors declare that the main data supporting the results in this study are available within the paper and its Supplementary Information. The raw datasets from Stanford Hospital are protected because of patient privacy yet can be made available upon request provided that approval is obtained after an Institutional Review Board procedure at Stanford. \\n\\ncode availability \\n\\nThe source code of the deep-learning algorithm is available for research uses at (\\\\ensuremath{<}https://github.com/liyues/PatRecon\\\\ensuremath{>})https://github.com/liyues/PatRecon. \\n\\nReceived: 29 November 2018; Accepted: 19 September 2019; Published online: 28 October 2019 \\n\\nreferences \\n\\nacknowledgements \\n\\nThis research is partially supported by the National Institutes of Health (R01CA176553 and R01EB016777). The contents of this article are solely the responsibility of the authors and do not necessarily represent the official NIH views. \\n\\nauthor contributions \\n\\nL.X. proposed the original notion of single-view reconstruction for tomographic imaging and supervised the research, L.S. designed and implemented the algorithm. W.Z. designed the experiments and implemented the data generation process. L.S. and W.Z. carried out experimental work. L.X., L.S. and W.Z. wrote the manuscript. All the authors reviewed the manuscript. \\n\\ncompeting interests \\n\\nThe authors declare no competing interests. \\n\\nadditional information \\n\\nSupplementary information is available for this paper at (\\\\ensuremath{<}https://doi.org/10.1038/s41551-019-0466-4\\\\ensuremath{>})https://doi.org/10.1038/ (\\\\ensuremath{<}https://doi.org/10.1038/s41551-019-0466-4\\\\ensuremath{>})s41551-019-0466-4. \\n\\nCorrespondence and requests for materials should be addressed to L.X. \\n\\nReprints and permissions information is available at (\\\\ensuremath{<}http://www.nature.com/reprints\\\\ensuremath{>})www.nature.com/reprints. \\n\\nPublisher{\\\\textquoteright}s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. \\n\\nReporting Summary \\n\\nCorresponding author(s): Lei Xing \\n\\nLast updated by author(s): Sep 19, 2019 \\n\\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Research policies, see Authors \\\\& Referees and the Editorial Policy Checklist. \\n\\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section. \\n\\nOur web collection on statistics for biologists contains articles on many of the points above. \\n\\nPolicy information about availability of computer code \\n\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code \\\\& software for further information. \\n\\nPolicy information about availability of data \\n\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n\\nThe authors declare that the main data supporting the results in this study are available within the paper and its Supplementary Information. The raw datasets from Stanford Hospital are protected because of patient privacy yet can be made available upon request provided that approval is obtained after an Institutional Review Board procedure at Stanford. \\n\\nField-specific reporting \\n\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection. \\n\\nLife sciences \\n\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf \\n\\nLife sciences study design \\n\\nAll studies must disclose on these points even when the disclosure is negative. \\n\\nReporting for specific materials, systems and methods \\n\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\n\\nPolicy information about clinical studies \\n\\nAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions. \\n\\nnature research  |  reporting summary \\n\\nOctober 2018 \\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.txt'}),\n",
       " Document(page_content=\"HHS Public Access \\n\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01. \\n\\nPublished in final edited form as: \\n\\nMed Image Anal. 2022 April ; 77: 102372. doi:10.1016/j.media.2022.102372. \\n\\nLiyue Shena , Lequan Yub , Wei Zhaob , John Paulya , Lei Xinga,b \\n\\nAbstract \\n\\nX-ray imaging is a widely used approach to view the internal structure of a subject for clinical diagnosis, image-guided interventions and decision-making. The X-ray projections acquired at different view angles provide complementary information of patient{\\\\textquoteright}s anatomy and are required for stereoscopic or volumetric imaging of the subject. In reality, obtaining multiple-view projections inevitably increases radiation dose and complicates clinical workflow. Here we investigate a strategy of obtaining the X-ray projection image at a novel view angle from a given projection image at a specific view angle to alleviate the need for actual projection measurement. Specifically, a Deep Learning-based Geometry-Integrated Projection Synthesis (DL-GIPS) framework is proposed for the generation of novel-view X-ray projections. The proposed deep learning model extracts geometry and texture features from a source-view projection, and then conducts geometry transformation on the geometry features to accommodate the change of view angle. At the final stage, the X-ray projection in the target view is synthesized from the transformed geometry and the shared texture features via an image generator. The feasibility and potential impact of the proposed DL-GIPS model are demonstrated using lung imaging cases. The proposed strategy can be generalized to a general case of multiple projections synthesis from multiple input views and potentially provides a new paradigm for various stereoscopic and volumetric imaging with substantially reduced efforts in data acquisition. \\n\\nKeywords \\n\\nprojection view synthesis; X-ray imaging; geometry-integrated deep learning \\n\\nAuthor statement \\n\\nLiyue Shen: Conceptualization, Methodology, Software, Investigation, Writing - Original Draft, Formal analysis, Visualization Lequan Yu: Writing - Review \\\\& Editing, Software \\n\\nPublisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. \\n\\nDeclaration of interests \\n\\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. \\n\\n1. Main Text \\n\\nMedical imaging such as X-ray imaging, computed tomography (CT), magnetic resonance imaging (MRI), and positron emission tomography (PET) presents a significant approach to view the internal structure of a patient for diagnosis, image-guided interventions and many other clinical decision-making procedures. Every year in the U.S., over hundred millions of medical imaging examinations are performed for patient care. In 2006, about 377 million diagnostic and interventional radiologic examinations were performed in the U.S. (Mettler et al., 2009). Among them, X-ray imaging and X-ray CT are widely used modalities in various applications (Xing et al., 2020; Winder et al., 2021). In X-ray imaging, an incident X-ray beam goes through the patient body, and produces a projection image of the internal anatomic structure of the body on the image plane as illustrated in Fig. 1. For image guidance of interventional procedures, projection images from different view angles are often needed to localize or recognize a structure accurately in 3D. X-ray projection data acquired at many different angles around the patient are also required in tomographic CT imaging. \\n\\nConsidering the practical needs for multi-view X-ray projections at different view angles and the general overhead associated with the data acquisition, it is desirable to develop alternative ways to obtain the multi-view projections with minimal cost such as computational image synthesis. Such a technique can not only reduce the cost to acquire multi-view X-ray projections, but also open new possibilities for some significant relevant challenges such as sparse-view tomographic imaging. For example, in sparse-view tomographic CT image reconstruction, synthesized X-ray projections at novel view angles could potentially help to reconstruct better CT images while reducing the needs of X-ray projection acquisition, thus, reducing the radiation dose during the imaging protocol (Shen et al., 2021). However, there is no previous work specifically discussing this interesting research topic. Toward this goal, in this work, we investigate an effective strategy of synthesizing novel-view X-ray projections by using geometry-integrated deep learning. \\n\\nRecent advances in deep learning have led to impressive progress in many application fields (Xing et al., 2020), including image reconstruction (Zhu et al., 2018; Mardani et al., 2018; Shen et al., 2019) and image recognition (Krizhevsky et al., 2012; He et al., 2016; Shen et al., 2018). Moreover, in computer vision research, deep learning also brings new possibility for view synthesis task for natural images, i.e., to synthesize the novel-view images from the images at the given view angles (Eslami et al., 2018; Sitzmann et al., 2019; Mildenhall et al., 2020). Although some progress has been made in solving the view synthesis problem through volume rendering with deep learning methodology (Lombardi et al., 2019; Wiles et al., 2020), these methods cannot be directly transferred to novel-view X-ray projection synthesis because of the difference in image-forming physics between photographic and tomographic imaging. For photographic imaging, the natural lights hit the surface of objects and reflect back to the camera image plane to form an image of the object shape and the background in the RGB format. In tomographic imaging, the penetrating waves such X-ray passes through the object and project the internal structures onto image plane by ray integration. Therefore, a new formulation for the novel-view X-ray projection synthesis problem is required. \\n\\nUnderstanding the underlying geometry changes in the physical world is important to solve this X-ray projection synthesis problem. In this case, each pixel value in the X-ray projection does not represent the RGB value at a certain point like that in the natural image, but indicates the integration along the ray line in the projection direction. Therefore, to synthesize an X-ray projection at a new view angle, it is necessary to consider the geometric changes in the physical world, which relates the projections at different angles. Specifically, we observe that the underlying subject is rotated when viewed from different angles, thus, the geometry features should also be transformed according to the rotation angle of view point when synthesizing a new-view projection. In reality, since the projections from different views depict the same imaging subject, they should share some common characteristics such as the subject texture. Therefore, in synthesizing a projection, we assume that the projections at different view angles share the common texture features while keeping the view-specific geometry features. In this way, we integrate the geometric relationship from physical world in the deep learning to construct a robust and interpretable projection synthesis model. \\n\\nIn this work, we introduce a Deep Learning-based Geometry-Integrated Projection Synthesis (DL-GIPS) framework for generating novel-view X-ray projections. Specifically, given the X-ray projections at certain angle(s), the model learns to extract the geometric and texture features from input projection(s) simultaneously, which are then transformed and combined to form the X-ray projection image at the target angle(s). The texture feature extracts the appearance characteristics such as the subject texture from the input projection to help to synthesize the target projection, whereas the geometry feature captures the subject geometric structure, such as the spatial distribution, contour, shape, size of bones, organs, soft tissues, and other body parts. To incorporate the geometry information into the view synthesis procedure, the extracted geometry features are transformed according to the source and target view angle changes based on the 3D cone-beam projection geometry, which relates to the subject{\\\\textquoteright}s geometry changes. Such a combination of geometry priors and deep learning also makes the model more robust and reliable. More details will be introduced in the subsequent sections. \\n\\nOverall, the main contributions of this paper are three folds: \\n\\n2 Related Work \\n\\nThe view synthesis problem in computer graphics has been researched for many years (Eslami et al., 2018; Sitzmann et al., 2019; Mildenhall et al., 2020; Lombardi et al., 2019; Wiles et al., 2020). With the development of deep learning, the recent works are focused on solving this problem through volume rendering (Lombardi et al., 2019; Wiles et al., 2020), neural scene representation learning (Eslami et al., 2018; Sitzmann et al., 2019), neural radiance field (Mildenhall et al., 2020). These works employ deep neural networks to learn the representation of the underlying object or scene and try to find the intersection points of the lights and the object outer surface to generate the RGB values of the corresponding pixels onto the image plane. However, because of the physical difference of X-ray imaging scanning, it is difficult, if not possible, to apply these methods to X-ray projection forming. In this work, we propose an X-ray projection synthesis method with integration of X-ray imaging physics. \\n\\nIn recent years, there is a line of research for image-to-image translation (Zhu et al., 2017; Huang et al., 2018; Choi et al., 2018; Lee et al., 2019; Shen et al., 2020; Lyu et al., 2021), where a deep learning model is trained to translate or synthesize images across multiple domains or modalities, such as multi-contrast MRI (Lee et al., 2019; Shen et al., 2020), MRI to CT (Zhang et al., 2018) or facial images with different expressions (Huang et al., 2018; Choi et al., 2018; Shen et al., 2020). Some of the image translation methods generate new-domain images by disentangling the image semantics as content features and style features (Huang et al., 2018; Shen et al., 2020). In these methods, it is assumed that the content features are shared across domains, while style features are domain-specific. For the novel-view projection synthesis problem, it is possible to treat projection images at different view angles as different image domains (Shen et al., 2021). In this way, the problem is transferred to an image-to-image translation problem. However, such assumption completely ignores the specific viewpoint change and the corresponding geometric relationship between projections from different views. \\n\\n3 Method \\n\\nAs shown in Fig. 2, the input to the framework is the source-view projection image, and the output of the model is the synthesized target-view projection image. In order to generate the projection from the input view angle to the target view angle, two aspects of information are required. First, an important knowledge embedded in the input projection is the anatomic geometry structure of the scanned subject including the distribution and location of various organs and body parts. It is important to understand this patient-specific geometry information to generate the projection at a new view angle. Due to the view angle change from the source to the target views, the underlying subject should also be rotated and transformed accordingly, as shown in Fig. 1. Particularly, the geometry transformation should comply with the physical model of the X-ray imaging. Correctly conducting the geometric transformation from the source view to the target view helps the model to capture the change of the projected subject structure. \\n\\nBeyond the geometry information, the texture characteristics are also critical to generate high quality target-view projection, and ensure the consistency of the texture and appearance characteristics between the source-view and target-view projections. We assume that the source-view and target-view images are X-ray projections scanned under the same imaging setting, which should share some common image characteristics such as the subject texture and image appearance. Therefore, the texture features captured from source-view projection are combined with the transformed geometry features to synthesize target-view projection simultaneously. \\n\\nBased on the above assumptions, we propose a DL-GIPS framework for novel-view generation. As illustrated in Fig. 2, the proposed framework consists of three modules: the texture and geometry feature extraction encoders, the projection transformation based on geometry priors and view angles, and the image generator to synthesize projections. Specifically, the two image encoders extract the texture features and the geometry features from the input image, respectively. Then, the geometry features are transformed through the physical model of backward projection and forward projection. Finally, the transformed geometry features at the target view are combined with the shared texture features to synthesize the target-view projection via an image generator. The details of each module are described as follows. \\n\\n(1) \\n\\n(2) \\n\\nTo be specific, the encoder firstly contains a convolutional layer with kernel size 7 and channel dimension of 16. Then, the feature maps are downsampled by two 2D convolutional layer with kernel size 4 and stride 2, where the channel dimension is doubled in each layer. Each convolutional layer is followed by instance normalization (Ulyanov et al., 2016) and ReLU activation function. Next, four residual blocks are built up to further process the learned features. Each residual block contains two convolutional layers with kernel size 3 and the same channel dimension. A skip connection is added up from the output of the first convolutional layer to the second convolutional layer. Thus, the feature maps keep the same spatial size and channel dimension when getting through all the residual blocks. The geometry feature encoder and texture feature encoder have a similar architecture, except that there is an additional convolutional layer at the end of geometry feature encoder to reduce the geometry feature channel dimension to 32 for the subsequent geometry transformation module. \\n\\n(3) \\n\\nwhere we denote the transformed features after forward and backward projection as Fsrcg , F tgt g . P f and Pg represent the forward projection and backward projection operators. Note that during the forward projection, the 3D feature volume not only projects onto the target view, but also projects back to the source view to keep consistency. Therefore, the projection transformation module outputs the geometry features for target view Fgtgt  as well as source view Fsrcg . \\n\\nNote that the backward projection and forward projection are deterministic operations without any learnable variables. All the transformation parameters are set up based on the geometry priors including the physical model from X-ray imaging scanner, and the source and target view angles. The 3D feature refinement network is built up by two 3D residual convolutional blocks. Each residual block contains two 3D convolutional layers with kernel size 3, followed by instance normalization and ReLU activation function. An additive path connects the outputs of the first and the second layer to enforce residual learning. Since the backward and forward projectors are differentiable, the geometry transformations are wrapped up together with network modules for end-to-end optimization. \\n\\n3.2.3 Image generator{\\\\textemdash}Combining the transformed geometry features and extracted texture features, image generator learns to synthesize the projection in the target view from the semantic representations of both sides. The model structure of image generator is the mirror of feature encoder, which is constructed by residual convolutional blocks and upsampling convolutional layers. The geometry features and texture features are concatenated together in the channel dimension. Denoting the generator as G, the model outputs the target-view projection as well as the source-view projection based on the different geometry features: \\n\\n(4) \\n\\n(5) \\n\\nIn our method, we build up a multi-scale image discriminator (Huang et al., 2018; Shen et al., 2020). Specifically, we use the 2D average pooling layer to downsample the image to three different scales by 2 times. Separate classifier networks are constructed for input images at different scales. The classifier contains four 2D convolutional layers with kernel size 4 and stride 2, which further downsample the spatial size of feature maps. The final layer is the 2D convolution with kernel size 1. During training, the outputs from image discriminator are used to compute the adversarial loss based on the method of least squares generative adversarial networks (Mao et al., 2017). \\n\\nIn order to train the whole model reliably, we design a training strategy that contains the image and feature consistency loss, image reconstruction loss, and adversarial loss. \\n\\nwhere the expectation is sampled based on all the training samples. This constraint guarantee that the feature encoding and image generation keep consistency and can recover the input image. \\n\\nFurthermore, in order to guarantee that the geometry features represent the correct semantic information for corresponding views, we add additional constraint on the consistency of geometry features. Suppose the generator output the synthesized projections I' src, I' tgt at source view and target view respectively. The geometry features extracted from the generated projections should have the same representation as the previous transformed geometry features, from which the synthesized projections are derived, as shown in Fig. 2. This also further guarantee the geometry feature encoding and image generation conduct the inverse operations. Thus, the geometry feature consistency loss is denoted as follows: \\n\\n(7) \\n\\nFor convenience of notation, we denote the aforementioned image and feature consistency constraints as a total consistency loss. That is, \\n\\n(8) \\n\\nThis constraint makes sure the backward and forward projection operators conduct the correct geometry transformation as expected. Compared with Eq. (6), the difference between the consistency loss and reconstruction loss on source-view projection is from the different geometry feature. In Eq. (6), the source-view geometry feature is directly extracted from the input source-view projection through feature encoder. In Eq. (9), the source-view geometry feature is outputted after the geometry transformation including the 3D feature refinement network. In other words, we assume the geometry feature of source view should keep consistent in these two positions. The geometry transformation module helps to derive the target-view geometry feature while it should also keep the correctness of the source-view geometry feature. \\n\\n(10) \\n\\nThe feature encoders, image generators, and image discriminators are jointly trained to optimize the total loss as follows: \\n\\n(11) \\n\\nwhere \\\\ensuremath{\\\\lambda}cyc, \\\\ensuremath{\\\\lambda}rec, \\\\ensuremath{\\\\lambda}adv are the hyper-parameters to balance the different parts of the total loss. The whole framework is trained end-to-end to optimize the total loss objective. \\n\\n4. Experiments \\n\\nTo validate the feasibility of the proposed DL-GIPS model for view synthesis of X-ray projection images, we conduct experiments on lung X-ray CT images for novel-view projection synthesis. In the following sections, we will describe the dataset, experimental setting and training details. \\n\\nThe experiments were conducted on a public dataset of The Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) (Armato et al., 2011; Armato et al., 2015; Clark et al., 2013). This dataset contains 1018 thoracic 3D CT images from different patients. We regarded each CT image as an independent data sample for model training. In data pre-processing, all the CT images were resampled with the same resolution of 1 mm in the z-axis, and were resized to the same image size of 128 {\\\\texttimes} 128 in the xy-plane. \\n\\nIn order to obtain the X-ray projections at different angles, we projected the 3D CT image to get the digitally reconstructed radiographs (DRRs) in different view angles. The cone-beam geometry of the projection operation was defined according to the clinical on-board cone-beam CT system for radiation therapy. Each 2D X-ray projection was of the size 180 {\\\\texttimes} 300. Following the image processing of model training, the intensity values of the all the 2D X-ray projection images were normalized to the data range of [0, 1]. In experiments, we randomly selected 80\\\\% of the dataset for training and validation (815 samples) while 20\\\\% of the data were held out for testing (203 samples). \\n\\nIn the experiments of multi-to-multi view synthesis, the model was trained to generate the projections at 30 and 60 degrees from the AP (0 degree) and LT (90 degree) projections. This is a more general case with multiple input sources views and more than one target views. Such multi-to-multi projection generation may further contribute to the application of sparse-view 3D CT image reconstruction for the real-time applications. \\n\\nThe deep neural networks in the framework were implemented with PyTorch (Paszke et al., 2017). The backward projection and forward projection operators were implemented based on the Operator Discretization Library (ODL) in Python (Adler et al., 2017), which were wrapped up as the differentiable PyTorch module. Thus, the whole framework was built up by PyTorch layers and was trained in an end-to-end fashion. Specifically, the whole model was trained by optimizing the total loss in Eq. (11), with the loss weights \\\\ensuremath{\\\\lambda}cyc, \\\\ensuremath{\\\\lambda}adv as 1 and \\\\ensuremath{\\\\lambda}rec as 10. We trained the model using the Adam optimizer (Kingma and Ba, 2015) with the beta parameters as (0.5, 0.999), the learning rate of 0.0001, and batch size 1. The model was trained for 100000 iterations in total. We will release our code publicly upon the paper acceptance. \\n\\n5. Results \\n\\nIn the following section, we demonstrate the qualitative and quantitative results respectively for different experimental settings: one-to-one projection synthesis and multi-to-multi projection synthesis. Also, we compare the results of the proposed method with the previous methods for image translation. Finally, we conduct ablative studies to further investigate the importance of each component in the proposed DL-GIPS model. \\n\\nUNet (Ronneberger et al., 2015): Since there is no previous method particularly designed for X-ray projection synthesis problem, we firstly compare the proposed DL-GIPS model with the baseline UNet model (Ronneberger et al., 2015), which has been widely used for image segmentation and restoration in both natural image and medical image applications. In the UNet structure, the skip connections are built up between the multi-scale features of the encoder and the decoder, but there is not feature disentanglement. To apply the UNet model for X-ray projection synthesis, the given source projections are stacked as multi-channel images for model inputs while the model outputs the stacked multi-view images altogether. The same image reconstruction loss (i.e., L1-norm loss) is applied to measure the difference between the outputs and ground truth images. The same training strategy is used to train the UNet model. \\n\\nThe qualitative results of synthesized projections are demonstrated in Fig. 4 and Fig. 5. Fig. 4 shows the results of synthesized LT projections from AP projections for five testing samples, while Fig. 5 shows the corresponding results of synthesized AP projections from LT projections. Each row shows the results of one testing sample. The columns present the input projection, the output projection from UNet model, the output projection from ReMIC model, the output image from DL-GIPS model, and the ground truth image. The corresponding quantitative results averaged across all the testing data are reported in Table I for both {\\\\textquotedblleft}AP {\\\\textrightarrow} LT{\\\\textquotedblright} and {\\\\textquotedblleft}LT {\\\\textrightarrow} AP{\\\\textquotedblright}. The evaluation metrics include mean absolute error (MAE), normalized root mean squared error (RMSE), structural similarity (SSIM) (Wang et al., 2004) and peak signal noise ratio (PSNR). \\n\\nFrom both qualitative and quantitative results, we can see that the proposed DL-GIPS model obtains more accurate synthesized projections especially about the illustration of the human body and organs in terms of the shape, contour and size. For example, the synthesized projections obtained by DL-GIPS model get more accurate human body contours, as pointed out by the red arrows in Fig. 4. Besides, as shown in the Fig. 5, the projection images synthesized by the DL-GIPS model obtain a better shape estimation of the heart and liver denoted by the red arrows. These advantages result from the proposed geometry transformation and feature disentanglement in the DL-GIPS model. In the ReMIC model, although the learned features extracted from images are also disentangled as the shared content feature and the view-specific style feature (Shen et al., 2020), there is no explicit or implicit geometry priors to guide the feature transformation across different view angles in the feature disentanglement. Therefore, in term of human body structure and internal anatomy, the proposed DL-GIPS model is able to generate more accurate results than the ReMIC model. Moreover, the synthesized images of DL-GIPS model contain more accurate details in the bone and lung region compare with the UNet model, which results from the adversarial loss in the model training. But we also notice that the adversarial loss may also introduce inaccuracy in some cases such as the unclear liver region in the testing sample 4. \\n\\nIn the experiments of multi-to-multi projection synthesis, the model is further developed to simultaneously generate multiple projections from the given multiple source views. To be specific, the model aims to synthesize the projections at the view angle of 30-degree and 60-degree when given the AP and LT projections at 0-degree and 90-degree. In Fig. 6, we show the results of five testing samples in rows respectively. The columns display the input projections, UNet results, ReMIC results, DL-GIPS results and the ground truth projections at two different output angles, respectively. We also compute the quantitative evaluation metrics averaged across all the testing samples and report in Table I as {\\\\textquotedblleft}Multi{\\\\textrightarrow}Multi{\\\\textquotedblright}. \\n\\nFirst of all, these results show that the proposed DL-GIPS method is a general projection synthesis approach that can be generalized to accept multiple source-view projections and predict multiple target-view projections. Furthermore, we see that more input projections can provide more information for the underlying subject and synthesize more accurate novel-view projections compared with the ground truth, especially for the structure of bones, organs, and soft tissues. Thus, this indicates the source-view projections can always be increased adaptively to obtain more precise synthesized projections according to the different requirements in specific practical applications. \\n\\nIn comparison methods, we also observe the increased source-view projections and the increased target-view supervisions also help the UNet and ReMIC models to generate better synthesized images than the results in one-to-one projection synthesis. Due to this, UNet model can provide more reasonable structures of human body and organs in the synthesized projections despite without precise details, which even gets better quantitative results than ReMIC model in Table I. This is also because the ReMIC model synthesizes some inaccurate details due to the adversarial training loss. Similar phenomenon was also found in previous works (Ying et al., 2019) that the adversarial loss brings the trade-off between the qualitative image qualities and the quantitative evaluation scores using the metrics like MAE, SSIM, PSNR. \\n\\nIn the proposed DL-GIPS model, the geometry priors and integrated transformation relief such disadvantages, which not only gets correct anatomic structures with high SSIM score, but also synthesizes precise fine details with high PSNR score, as reported in Table I. Besides, as shown in Fig. 6 the synthesized projections from DL-GIPS model obtain more accurate anatomy structures pointed out by the red arrows. \\n\\nIn order to further analyze the proposed DL-GIPS model, we conduct the ablative study experiments for studying the necessity of different losses introduced in the total loss objective. Firstly, we train the DL-GIPS model under the same experimental settings while removing the consistency loss. In Table II, we report the results of ablative study with averaged quantitative metrics across all the testing samples, which includes MAE, RMSE, SSIM and PSNR. According to the results, the consistency loss can improve the synthesized images especially in one-to-one projection synthesis, as the feature consistency loss makes the semantic information transferred more smoothly. In the experiments of multi-to-multi projection synthesis, more given input information makes the task easier, and thus, the consistency loss does not make an obvious improvement in this case. \\n\\nThen, we conduct ablative study by removing the adversarial loss. As the results shown in Fig. 8, the synthesized projections without adversarial loss are obviously blurry. Thus, adding the adversarial loss helps the model to obtain more fine details and achieve better visualized image quality, which is important to the radiologists in the practical applications. \\n\\nIn addition, since both ReMIC and DL-GIPS models use the same backbone model structures for feature encoder and image generator, with the same training losses, the comparison results between ReMIC and DL-GIPS models as shown in Figs. 4{\\\\textendash}6 and Table I demonstrate the superiority of geometric feature transformation in view synthesis. This indicates that the geometry priors introduced in DL-GIPS model structure help to reconstruct more reliable object structures. \\n\\nBeyond the above, we also conduct the ablation study of the model structure on the learned feature dimensions. In the proposed DL-GIPS model structure, a key module is the geometry feature and texture feature encoder and decoder. Thus, we investigate how the learned feature dimensions would influence the final performance of view synthesis. The ablation experiments are conducted in the AP {\\\\textrightarrow} LT task. The results for variant feature dimensions in the model structure are shown in Table III. For fixed geometry (texture) feature dimension, increasing texture (geometry) features lead to better performance of view synthesis, as this transfers more useful information through feature encoding and decoding for generating novel-view projection images. This also indicates that both geometry features and texture features are necessary representation learning to synthesize novel-view projections. \\n\\n6. Discussion \\n\\nThe proposed DL-GIPS model adopts the geometry priors of X-ray imaging model to transform the geometry features and relate the source and target view angles. Such a geometric transformation is derived from the X-ray imaging system and properly integrated with the deep learning networks in the proposed approach. Beyond the proposed approach, there may be more advanced methods to leverage the geometry priors especially in medical imaging. For example, the other image modalities of the same patients such as Magnetic Resonance Image (MRI) can also provide the prior information of anatomic structure for the same patient, which may further contribute to synthesizing the fine details in the novel-view X-ray projections. \\n\\nComputational efficiency of the current method is still not ideal. The incorporation of geometry transformation increases the time consumption as compared with the standard deep learning model training process. In the experiment setting of one-to-one projection synthesis, the inference time of one data sample for different methods are around: 0.04 s, 0.05 s, 0.56 s for UNet, ReMIC, and DL-GIPS models respectively. How to speed up this process by, for examples, CUDA acceleration, parallel computing and more efficient model design, represents an interesting direction of future research. \\n\\nIn real CT data, there may be some specific issues that need further attention. For instance, data truncation is a common issue for most deep learning-based approaches for CT imaging and has raised a lot of attention in recent research. For example, in the recent work (Huang et al., 2021), the authors proposed a method for truncation correction in CT by extrapolating the projections. This problem is orthogonal to the view synthesis problem studied in our work and can naturally become another useful future research direction following our work. \\n\\n7. Conclusion \\n\\nAcknowledgements \\n\\nThe authors acknowledge the funding supports from Stanford Bio-X Bowes Graduate Student Fellowship and NIH/NCI (R01CA227713, R01CA256890, and R01CA223667). The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study. \\n\\nReferences \\n\\nFig. 1. \\n\\nSketch of X-ray projection imaging procedure. X-ray wave penetrates through the patient{\\\\textquoteright}s body and projects on the detector plane. When the X-ray source is located at different positions, different projections at different view angles are obtained. The projections in the view of anterior-posterior (AP) direction and lateral (LT) directions are shown in the figure. \\n\\nFig. 2. \\n\\nIllustration of the proposed Deep Learning-based Geometry-Integrated Projection Synthesis framework (DL-GIPS). The pipeline contains texture and geometry feature encoders, projection transformation and image generator. The projection transformation contains the geometric operation of backward projection and forward projection based on the X-ray imaging physical model, and 3D feature refinement model. \\n\\nFig. 3. \\n\\nIllustration of geometry transformation with backward projection (blue) and forward projection (purple). The back projector puts the pixel intensities in the source-view image back to the corresponding voxels in the 3D volume according to the cone-beam geometry of the physical model. When the X-ray source rotates to the target view angles, the forward projection operator integrates along the projection line and projects onto the detector plane. \\n\\nFig. 4. \\n\\nResults of synthesized lateral projection from AP projection. Each row shows the results of one testing sample. Regions of interest are zoomed in for more clear comparison in structural details. The columns are input projection, UNet synthesized projection, ReMIC synthesized projection, DL-GIPS synthesized projection, and the ground truth projection, respectively. (Red arrows highlight the compared difference among different images.) \\n\\nFig. 5. \\n\\nResults of synthesized AP projection from lateral projection. Each row shows the results of one testing sample. Regions of interest are zoomed in for more clear comparison in structural details. The columns are input projection, UNet synthesized projection, ReMIC synthesized projection, DL-GIPS synthesized projection, and the ground truth projection, respectively. (Red arrows highlight the compared difference among images.) \\n\\nFig. 6. \\n\\nResults of synthesizing projections at the view angle of 30 degrees and 60 degrees from AP and lateral projections. Each row shows the results of one testing sample. Regions of interest are zoomed in for more clear comparison in structural details. The columns are the input projections, UNet synthesized projections, ReMIC synthesized projections, DL-GIPS synthesized projections, and the ground truth projections respectively. Please note that the model outputs the two target projections at 30 degrees and 60 degrees at one time. (Red arrows highlight the compared difference among images.) \\n\\nFig. 7. \\n\\nFeature map visualization. We demonstrate the different features introduced in Fig. 2. The first and the second rows show the geometry feature extracted from AP view and LT view respectively. The final row shows the texture features extracted from source view as shown in Fig. 2. \\n\\nFig. 8. \\n\\nQualitative results of ablative study for synthesizing LT projection from AP projection. Each row shows results of one testing sample. Columns are synthesized projections for DL-GIPS without adversarial loss, DL-GIPS synthesized projection, and ground truth projection. \\n\\nTABLE I \\n\\nRESULTS OF NOVEL-VIEW PROJECTION SYNTHESIS \\n\\nTABLE II \\n\\nRESULTS OF ABLATIVE STUDY ON LOSS FUNCTION \\n\\nTABLE III \\n\", metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.txt'}),\n",
       " Document(page_content='chengsun@gapp.nthu.edu.tw \\n\\nMin Sun1,3 \\n\\nsunmin@ee.nthu.edu.tw \\n\\nHwann-Tzong Chen1,4 \\n\\nhtchen@cs.nthu.edu.tw \\n\\nWe present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF{\\\\textquoteright}s quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO. \\n\\n1. Introduction \\n\\nAchieving free-viewpoint navigation of 3D objects or scenes from only a set of calibrated images as input is a demanding task. For instance, it enables online product showcase to provide an immersive user experience comparing to static image demonstration. Recently, Neural Radiance Fields (NeRFs) [37] have emerged as powerful representations yielding state-of-the-art quality on this task. \\n\\n23.64 PSNR. 32.72 PSNR. 34.22 PSNR. Ours at 2.33 mins. Ours at 5.07 mins. Ours at 13.72 mins. (a) The synthesized novel view by our method at three training checkpoints. \\n\\n(b) The training curves of different methods on Lego scene. The training time of each method is measured on our machine with a single NVIDIA RTX 2080 Ti GPU. \\n\\nFigure 1. Super-fast convergence by our method. The key to our speedup is to optimize the volume density modeled in a dense voxel grid directly. Note that our method needs neither a conversion step from any trained implicit model (e.g., NeRF) nor a cross-scene pretraining, i.e., our voxel grid representation is directly and efficiently trained from scratch for each scene. \\n\\nDespite its effectiveness in representing scenes, NeRF is known to be hampered by the need of lengthy training time and the inefficiency in rendering new views. This makes NeRF infeasible for many application scenarios. Several follow-up methods [15, 18, 29, 30, 42, 43,66] have shown significant speedup of FPS in testing phase, some of which even achieve real-time rendering. However, only few methods show training times speedup, and the improvements are not comparable to ours [1,10,31] or lead to worse quality [6,59]. On a single GPU machine, several hours of per scene optimization or a day of pretraining is typically required. \\n\\nTo reconstruct a volumetric scene representation from a set of images, NeRF uses multilayer perceptron (MLP) to implicitly learn the mapping from a queried 3D point (with a viewing direction) to its colors and densities. The queried properties along a camera ray can then be accumulated into a pixel color by volume rendering techniques. Our work takes inspiration from the recent success [15, 18, 66] that uses classic voxel grid to explicitly store the scene properties, \\n\\nwhich enables real-time rendering and shows good quality. However, their methods can not train from scratch and need a conversion step from the trained implicit model, which causes a bottleneck to the training time. \\n\\nThe key to our speedup is to use a dense voxel grid to directly model the 3D geometry (volume density). Developing an elaborate strategy for view-dependent colors is not in the main scope of this paper, and we simply use a hybrid representation (feature grid with shallow MLP) for colors. \\n\\nDirectly optimizing the density voxel grid leads to super-fast converges but is prone to suboptimal solutions, where our method allocates {\\\\textquotedblleft}cloud{\\\\textquotedblright} at free space and tries to fit the photometric loss with the cloud instead of searching a geometry with better multi-view consistency. Our solution to this problem is simple and effective. First, we initialize the density voxel grid to yield opacities very close to zero everywhere to avoid the geometry solutions being biased toward the cameras{\\\\textquoteright} near planes. Second, we give a lower learning rate to voxels visible to fewer views, which can avoid redundant voxels that are allocated just for explaining the observations from a small number of views. We show that the proposed solutions can successfully avoid the suboptimal geometry and work well on the five datasets. \\n\\nUsing the voxel grid to model volume density still faces a challenge in scalability. For parsimony, our approach automatically finds a BBox tightly encloses the volume of interest to allocate the voxel grids. Besides, we propose post-activation{\\\\textemdash}applying all the activation functions after trilinearly interpolating the density voxel grid. Previous work either interpolates the voxel grid for the activated opacity or uses nearest-neighbor interpolation, which results in a smooth surface in each grid cell. Conversely, we prove mathematically and empirically that the proposed post-activation can model (beyond) a sharp linear surface within a single grid cell. As a result, we can use fewer voxels to achieve better qualities{\\\\textemdash}our method with 1603 dense voxels already outperforms NeRF in most cases. \\n\\nIn summary, we have two main technical contributions. First, we implement two priors to avoid suboptimal geometry in direct voxel density optimization. Second, we propose the post-activated voxel-grid interpolation, which enables sharp boundary modeling in lower grid resolution. The resulting key merits of this work are highlighted as follows: \\n\\nto 13003 to achieve NeRF-comparable quality. \\n\\n2. Related work \\n\\nRepresentations for novel view synthesis. Images synthesis from novel viewpoints given a set of images capturing the scene is a long-standing task with rich studies. Previous work has presented several scene representations reconstructed from the input images to synthesize the unobserved viewpoints. Lumigraph [4, 16] and light field representation [7, 23, 24, 46] directly synthesize novel views by interpolating the input images but require very dense scene capture. Layered depth images [11, 45, 47, 57] work for sparse input views but rely on depth maps or estimated depth with sacrificed quality. Mesh-based representations [8, 54, 58, 63] can run in real-time but have a hard time with gradient-based optimization without template meshes provided. Recent approaches employ 2D/3D Con-volutional Neural Network (CNNs) to estimate multiplane images (MPIs) [12, 26, 36, 51, 56, 71] for forward-facing captures; estimate voxel grid [17, 32, 48] for inward-facing captures. Our method uses gradient-descent to optimize voxel grids directly and does not rely on neural networks to predict the grid values, and we still outperform the previous works [17, 32, 48] with CNNs by a large margin. \\n\\nNeural radiance fields. Recently, NeRF [37] stands out to be a prevalent method for novel view synthesis with rapid progress, which takes a moderate number of input images with known camera poses. Unlike traditional explicit and discretized volumetric representations (e.g., voxel grids and MPIs), NeRF uses coordinate-based multilayer perceptrons (MLP) as an implicit and continuous volumetric representation. NeRF achieves appealing quality and has good flexibility with many follow-up extensions to various setups, e.g., relighting [2, 3, 50, 70], deformation [13, 38{\\\\textendash}40, 55], self-calibration [19, 27, 28, 35, 61], meta-learning [52], dynamic scene modeling [14, 25, 33, 41, 64], and generative modeling [5, 22, 44]. Nevertheless, NeRF has unfavorable limitations of lengthy training progress and slow rendering speed. In this work, we mainly follow NeRF{\\\\textquoteright}s original setup, while our method can optimize the volume density explicitly encoded in a voxel grid to speed up both training and testing by a large margin with comparable quality. \\n\\nHybrid volumetric representations. To combine NeRF{\\\\textquoteright}s implicit representation and traditional grid representations, the coordinate-based MLP is extended to also conditioning on the local feature in the grid. Recently, hybrid vox-els [18, 30] and MPIs [62] representations have shown success in fast rendering speed and result quality. We use hybrid representation to model view-dependent color as well. \\n\\nFast NeRF rendering. NSVF [30] uses octree in its hybrid representation to avoid redundant MLP queries in free \\n\\nFigure 2. Approach overview. We first review NeRF in Sec. 3. In Sec. 4, we present a novel post-activated density voxel grid to support sharp surface modeling in lower grid resolutions. In Sec. 5, we show our approach to the reconstruction of radiance field with super-fast convergence, where we first find a coarse geometry in Sec. 5.1 and then reconstruct the fine details and view-dependent effects in Sec. 5.2. \\n\\nspace. However, NSVF still needs many training hours due to the deep MLP in its representation. Recent methods further use thousands of tiny MLPs [43] or explicit volumetric representations [15,18,62,66] to achieve real-time rendering. Unfortunately, gradient-based optimization is not directly applicable to their methods due to their topological data structures or the lack of priors. As a result, these methods [15, 18, 43, 62, 66] still need a conversion step from a trained implicit model (e.g., NeRF) to their final representation that supports real-time rendering. Their training time is still burdened by the lengthy implicit model optimization. \\n\\nFast NeRF convergence. Recent works that focus on fewer input views setup also bring faster convergence as a side benefit. These methods rely on generalizable pre-training [6, 59, 67] or external MVS depth information [10, 31], while ours does not. Further, they still require several per-scene fine-tuning hours [10] or fail to achieve NeRF quality in the full input-view setup [6,59,67]. Most recently, NeuRay [31] shows NeRF{\\\\textquoteright}s quality with 40 minutes per-scene training time in the lower-resolution setup. Under the same GPU spec, our method achieves NeRF{\\\\textquoteright}s quality in 15 minutes per scene on the high-resolution setup and does not require depth guidance and cross-scene pre-training. \\n\\n3. Preliminaries \\n\\nTo represent a 3D scene for novel view synthesis, Neural Radiance Fields (NeRFs) [37] employ multilayer perceptron (MLP) networks to map a 3D position x and a viewing direction d to the corresponding density \\\\ensuremath{\\\\sigma} and view-dependent color emission c: \\n\\nTo render the color of a pixel C{\\\\textasciicircum}(r), we cast the ray r from the camera center through the pixel; K points are then sampled on r between the pre-defined near and far planes; the K ordered sampled points are then used to query for their densities and colors \\\\{(\\\\ensuremath{\\\\sigma}i, ci)\\\\}K i=1 (MLPs are queried in NeRF). Finally, the K queried results are accumulated into a single color with the volume rendering quadrature in accordance with the optical model given by Max [34]: \\n\\n(2a) \\n\\n(2b) \\n\\n(2c) \\n\\nwhere \\\\ensuremath{\\\\alpha}i is the probability of termination at the point i; Ti is the accumulated transmittance from the near plane to point i; \\\\ensuremath{\\\\delta}i is the distance to the adjacent sampled point, and cbg is a pre-defined background color. \\n\\nGiven the training images with known poses, NeRF model is trained by minimizing the photometric MSE between the observed pixel color C(r) and the rendered color C{\\\\textasciicircum}(r): \\n\\n(3) \\n\\nwhere R is the set of rays in a sampled mini-batch. \\n\\nFigure 3. A single grid cell with post-activation is capable of modeling sharp linear surfaces. Left: We depict the toy task for a 2D grid cell, where a grid cell is optimized for the linear surface (decision boundary) across it. Right: Each column shows an example task for three different methods. The results show that a single grid cell with post-activation (Eq. (6c)) is adequate to recover faithfully the linear surface. Conversely, pre-activation (Eq. (6a)) and in-activation (Eq. (6b)) fail to accomplish the tasks as they can only fit into smooth results, and thus would require more grid cells to recover the surface detail. See supplementary material for the mathematical proof. \\n\\n4. Post-activated density voxel grid \\n\\nVoxel-grid representation. A voxel-grid representation models the modalities of interest (e.g., density, color, or feature) explicitly in its grid cells. Such an explicit scene representation is efficient to query for any 3D positions via interpolation: \\n\\nwhere x is the queried 3D point, V is the voxel grid, C is the dimension of the modality, and Nx {\\\\textperiodcentered} Ny {\\\\textperiodcentered} Nz is the total number of voxels. Trilinear interpolation is applied if not specified otherwise. \\n\\nDensity voxel grid for volume rendering. Density voxel grid, V (density) , is a special case with C = 1, which stores the density values for volume rendering (Eq. (2)). We use \\\\ensuremath{\\\\sigma}{\\\\textasciidieresis} \\\\ensuremath{\\\\in} R to denote the raw voxel density before applying the density activation (i.e., a mapping of R {\\\\textrightarrow} R\\\\ensuremath{\\\\geq}0). In this work, we use the shifted softplus mentioned in Mip-NeRF [1] as the density activation: \\n\\n(5) \\n\\nwhere the shift b is a hyperparameter. Using softplus instead of ReLU is crucial to optimize voxel density directly, as it is irreparable when a voxel is falsely set to a negative value with ReLU as the density activation. Conversely, softplus allows us to explore density very close to 0. \\n\\nSharp decision boundary via post-activation. The interpolated voxel density is processed by softplus (Eq. (5)) and alpha (Eq. (2b)) functions sequentially for volume rendering. We consider three different orderings{\\\\textemdash}pre-activation, in-activation, and post-activation{\\\\textemdash}of plugging in the tri-linear interpolation and performing the activation, given a \\n\\n(a) Visual comparison of image fitting results under grid resolution (H/5){\\\\texttimes}(W/5). The first row is the results of pre-, in-, and post-activation. The second row is their per-pixel absolute difference to the target image. \\n\\n(b) PSNRs achieved by pre-, in-and post-activation under different grid strides. A grid stride s means that the grid resolution is (H/s) {\\\\texttimes} (W/s). The black dashed line highlights that post-activation with stride \\\\ensuremath{\\\\approx} 8.5 can achieve the same PSNR as pre-activation with stride 2 in this example. \\n\\nFigure 4. Toy example on image fitting. The target 2D image is binary to imitate the scenario that most of the 3D space is either occupied or free. The objective is to reconstruct the target image by a low-resolution 2D grid. In each optimization step, the tunable 2D grid is queried by interpolation with pre-activation (Eq. (6a)), in-activation (Eq. (6b)), or post-activation (Eq. (6c)) to minimize the mean squared error to the target image. The result reveals that the post-activation can produce sharp boundaries even with low grid resolution (Fig. 4a) and is much better than the other two under various grid resolutions (Fig. 4b). This motivates us to model the 3D geometry directly via voxel grids with post-activation. \\n\\nqueried 3D point x: \\n\\nThe input \\\\ensuremath{\\\\delta} to the function alpha (Eq. (2b)) is omitted for simplicity. We show that the post-activation, i.e., applying all the non-linear activation after the trilinear interpolation, is capable of producing sharp surfaces (decision boundaries) with much fewer grid cells. In Fig. 3, we use a 2D grid cell as an example to show that a grid cell with post-activation can produce a sharp linear boundary, while pre- and inactivation can only produce smooth results and thus require more cells for the surface detail. In Fig. 4, we further use binary image regression as a toy example to compare their capability, which also shows that post-activation can achieve a much better efficiency in grid cell usage. \\n\\n5. Fast and direct voxel grid optimization \\n\\nWe depict an overview of our approach in Fig. 2. In Sec. 5.1, we first search the coarse geometry of a scene. In Sec. 5.2, we then reconstruct the fine detail including view-dependent effects. Hereinafter we use superscripts (c) and (f) to denote variables in the coarse and fine stages. \\n\\nTypically, a scene is dominated by free space (i.e., unoccupied space). Motivated by this fact, we aim to efficiently find the coarse 3D areas of interest before reconstructing the fine detail and view-dependent effect that require more computation resources. We can thus greatly reduce the number of queried points on each ray in the later fine stage. \\n\\nCoarse scene representation. We use a coarse denactivation (Eq. (6c)) to model scene geometry. We only model view-invariant color emissions by V (rgb)(c) \\\\ensuremath{\\\\in} in the coarse stage. A query of any 3D point x is efficient with interpolation: \\n\\n(7a) \\n\\n(7b) \\n\\nwhere c(c) \\\\ensuremath{\\\\in} R3 is the view-invariant color and \\\\ensuremath{\\\\sigma}{\\\\textasciidieresis}(c) \\\\ensuremath{\\\\in} R is the raw volume density. \\n\\nCoarse voxels allocation. We first find a bounding box (BBox) tightly enclosing the camera frustums of training views (See the red BBox in Fig. 2c for an example). Our voxel grids are aligned with the BBox. Let Lx(c) y z, L(c) , L(c) be the lengths of the BBox and M(c) be the hyperparameter for the expected total number of voxels in the coarse stage. The voxel size is s(c) = 3 L(c) x {\\\\textperiodcentered} L(c) y {\\\\textperiodcentered} Lz (c) /M(c), so there are \\n\\nCoarse-stage points sampling. On a pixel-rendering ray, we sample query points as \\n\\n(8a) \\n\\n(8b) \\n\\nwhere o is the camera center, d is the ray-casting direction, t(near) is the camera near bound, and \\\\ensuremath{\\\\delta}(c) is a hyperparameter for the step size that can be adaptively chosen according to the voxel size s(c). The query index i ranges from 1 to t(far) {\\\\textperiodcentered} d2/\\\\ensuremath{\\\\delta}(c), where t(far) is the camera far bound, so the last sampled point stops nearby the far plane. \\n\\nPrior 1: low-density initialization. At the start of training, the importance of points far from a camera is down-weighted due to the accumulated transmittance term in Eq. (2c). As a result, the coarse density voxel grid V (density)(c) could be accidentally trapped into a suboptimal {\\\\textquotedblleft}cloudy{\\\\textquotedblright} geometry with higher densities at camera near planes. We thus have to initialize V (density)(c) more carefully to ensure that all sampled points on rays are visible to the cameras at the beginning, i.e., the accumulated transmittance rates Tis in Eq. (2c) are close to 1. \\n\\nIn practice, we initialize all grid values in V (density)(c) to 0 and set the bias term in Eq. (5) to \\n\\n(9) \\n\\nwhere \\\\ensuremath{\\\\alpha}(init)(c) is a hyperparameter. Thereby, the accumulated transmittance Ti is decayed by 1 \\\\ensuremath{-} \\\\ensuremath{\\\\alpha}(init)(c) \\\\ensuremath{\\\\approx} 1 for a ray that traces forward a distance of a voxel size s(c). See supplementary material for the derivation and proof. \\n\\nPrior 2: view-count-based learning rate. There could be some voxels visible to too few training views in real-world capturing, while we prefer a surface with consistency in many views instead of a surface that can only explain few views. In practice, we set different learning rates for different grid points in V (density)(c). For each grid point indexed by j, we count the number of training views nj to which point j is visible, and then scale its base learning rate by nj/nmax, where nmax is the maximum view count over all grid points. \\n\\nTraining objective for coarse representation. The scene representation is reconstructed by minimizing the mean square error between the rendered and observed colors. To regularize the reconstruction, we mainly use background entropy loss to encourage the accumulated alpha values to concentrate on background or foreground. Please refer to supplementary material for more detail. \\n\\nGiven the optimized coarse geometry V (density)(c) in Sec. 5.1, we now can focus on a smaller subspace to reconstruct the surface details and view-dependent effects. The optimized V (density)(c) is frozen in this stage. \\n\\nFine scene representation. In the fine stage, we use V (density)(f)a higher-resolution density voxel grid \\\\ensuremath{\\\\in} with post-activated interpolation (Eq. (6c)). , where(f)N{\\\\texttimes} z (f)N{\\\\texttimes} y (f)N{\\\\texttimes} z (f)N{\\\\texttimes} y (f)1 N{\\\\texttimes}R x Note that, alternatively, it is also possible to use a more advanced data structure [18, 30, 66] to refine the voxel grid based on the current V (density)(c) but we leave that for future work. To model view-dependent color emission, we opt to use an explicit-implicit hybrid representation as we find in our prior experiments that an explicit representation tends to produce worse results, and an implicit representation entails a slower training speed. Our hybrid representation comprises (f)i) D is a hyperparameter for feature-space dimension, and ii) a shallow MLP parameteriszed by \\\\ensuremath{\\\\Theta}. Finally, queries of 3D points x and viewing-direction d are performed by \\n\\nwhere c(f) \\\\ensuremath{\\\\in} R3 is the view-dependent color emission and \\\\ensuremath{\\\\sigma}{\\\\textasciidieresis}(f) \\\\ensuremath{\\\\in} R is the raw volume density in the fine stage. Positional embedding [37] is applied on x, d for the MLP\\\\ensuremath{\\\\Theta} (rgb) . \\n\\nKnown free space and unknown space. A query point is in the known free space if the post-activated alpha value from the optimized V (density)(c) is less than the threshold \\\\ensuremath{\\\\tau} (c). Otherwise, we say the query point is in the unknown space. \\n\\nFine voxels allocation. We densely query V (density)(c) to find a BBox tightly enclosing the unknown space, where are the lengths of the BBox. The only hyper-parameter is the expected total number of voxels M (f). The (f) canN,z (f)N,y (f)L, z (f)L, y (f)L x (f) (f)voxel size and the grid dimensions Ns x then be derived automatically from M (f) as per Sec. 5.1. \\n\\nProgressive scaling. Inspired by NSVF [30], we progressively scale our voxel grid V (density)(f) and V (feat)(f). Let pg ckpt be the set of checkpoint steps. The initial number of voxels is set to M (f)/2|pg ckpt|. When reaching the training step in pg ckpt, we double the number of voxels such that the number of voxels after the last checkpoint is M (f); (f)N, z (f)N, y (f) (f)the voxel size and the grid dimensions Ns x are updated accordingly. Scaling our scene representation is much simpler. At each checkpoint, we resize our voxel grids, V (density)(f) and V (feat)(f) , by trilinear interpolation. \\n\\nFine-stage points sampling. The points sampling strategy is similar to Eq. (8) with some modifications. We first filter out rays that do not intersect with the known free space. For each ray, we adjust the near-and far-bound, t(near) and t(far) , to the two endpoints of the ray-box intersection. We do not adjust t(near) if x0 is already inside the BBox. \\n\\nFree space skipping. Querying V (density)(c) (Eq. (7a)) is faster than querying V (density)(f) (Eq. (10a)); querying for view-dependent colors (Eq. (10b)) is the slowest. We improve fine-stage efficiency by free space skipping in both training and testing. First, we skip sampled points that are in the known free space by checking the optimized V (density)(c) (Eq. (7a)). Second, we further skip sampled points in unknown space with low activated alpha value (threshold at \\\\ensuremath{\\\\tau} (f)) by querying V (density)(f) (Eq. (10a)). \\n\\nTraining objective for fine representation. We use the same training losses as the coarse stage, but we use a smaller weight for the regularization losses as we find it empirically leads to slightly better quality. \\n\\n6. Experiments \\n\\nWe choose the same hyperparameters generally for all scenes. The expected numbers of voxels are set to M (c) = 1003 and M (f) = 1603 in coarse and fine stages if not stated otherwise. The activated alpha values are initialized to be \\\\ensuremath{\\\\alpha}(init)(c) = 10\\\\ensuremath{-}6 in the coarse stage. We use a higher \\\\ensuremath{\\\\alpha}(init)(f) = 10\\\\ensuremath{-}2 as the query points are concentrated on the optimized coarse geometry in the fine stage. The points sampling step sizes are set to half of the voxel sizes, i.e., \\\\ensuremath{\\\\delta}(c) = 0.5 {\\\\textperiodcentered} s(c) and \\\\ensuremath{\\\\delta}(f) = 0.5 {\\\\textperiodcentered} s(f). The shallow MLP layer comprises two hidden layers with 128 channels. We use the Adam optimizer [20] with a batch size of 8,192 rays to optimize the coarse and fine scene representations for 10k and 20k iterations. The base learning rates are 0.1 for all voxel grids and 10\\\\ensuremath{-}3 for the shallow MLP. The exponential learning rate decay is applied. See supplementary material for detailed hyperparameter setups. \\n\\nQuantitative evaluation on the synthesized novel view. We first quantitatively compare the novel view synthesis results in Tab. 1. PSNR, SSIM [60], and LPIPS [69] are employed as evaluation metrics. Our model with M (f) = 1603 voxels already outperforms the original NeRF [37] and the improved JaxNeRF [9] re-implementation. Besides, our results are also comparable to most of the recent methods, except JaxNeRF+ [9] and Mip-NeRF [1]. Moreover, our per-scene optimization only takes about 15 minutes, while all the methods after NeRF in Tab. 1 need quite a few hours per scene. We also show our model with M (f) = 2563 voxels, which significantly improves our results under all metrics and achieves more comparable results to JaxNeRF+ and Mip-NeRF. We defer detail comparisons on the much simpler DeepVoxels [48] dataset to supplementary material, where we achieve 45.83 averaged PSNR and outperform NeRF{\\\\textquoteright}s 40.15 and IBRNet{\\\\textquoteright}s 42.93. \\n\\nTraining time comparisons. The key merit of our work is the significant improvement in convergence speed with NeRF-comparable quality. In Tab. 2, we show a training \\n\\nTable 1. Quantitative comparisons for novel view synthesis. Our method excels in convergence speed, i.e., 15 minutes per scene compared to many hours or days per scene using other methods. Besides, our rendering quality is better than the original NeRF [37] and the improved JaxNeRF [9] on the four datasets under all metrics. We also show comparable results to most of the recent methods. \\n\\ntime comparison. We also show GPU specifications after each reported time as it is the main factor affecting run-time. \\n\\nNeRF [37] with a more powerful GPU needs 1{\\\\textendash}2 days per scene to achieve 31.01 PSNR, while our method achieves a superior 31.95 and 32.80 PSNR in about 15 an 22 minutes per scene respectively. MVSNeRF [6], IBRNet [59], and NeuRay [31] also show less per-scene training time than NeRF but with the additional cost to run a generalizable cross-scene pre-training. MVSNeRF [6], after pre-training, optimizes a scene in 15 minutes as well, but the PSNR is degraded to 28.14. IBRNet [59] shows worse PSNR and longer training time than ours. NeuRay [31] originally reports time in lower-resolution (NeuRay-Lo) setup, and we receive the training time of the high-resolution (NeuRay-Hi) setup from the authors. NeuRay-Hi achieves 32.42 PSNR and requires 23 hours to train, while our method with M (f) = 2563 voxels achieves superior 32.80 in about 22 minutes. For the early-stopped NeuRay-Hi, unfortunately, only its training time is retained (early-stopped NeuRay-Lo achieves NeRF-similar PSNR). NeuRay-Hi still needs 70 minutes to train with early stopping, while we only need 15 minutes to achieve NeRF-comparable quality and do not rely on generalizable pre-training or external depth information. Mip-NeRF [1] has similar run-time to NeRF but with much better PSNRs, which also signifies using less training time to achieve NeRF{\\\\textquoteright}s PSNR. We train early-stopped Mip-NeRFs on our machine and show the averaged PSNR and training \\n\\nTable 2. Training time comparisons. We take the training time and GPU specifications reported in previous works directly. A V100 GPU can run faster and has more storage than a 2080Ti GPU. Our method achieves good PSNR in a significantly less per-scene optimization time. \\n\\ntime. The early-stopped Mip-NeRF achieves 30.85 PSNR after 6 hours of training, while we can achieve 31.95 PSNR in just 15 minutes. \\n\\nRendering speed comparisons. Improving test-time rendering speed is not the main focus of this work, but we still achieve \\\\ensuremath{\\\\sim} 45{\\\\texttimes} speedups from NeRF{\\\\textemdash}0.64 seconds versus 29 seconds per 800 {\\\\texttimes} 800 image on our machine. \\n\\nQualitative comparison. Fig. 5 shows our rendering results on the challenging parts and compare them with the results (better than NeRF{\\\\textquoteright}s) provided by PlenOctrees [66]. \\n\\nFigure 5. Qualitative comparisons on the challenging parts. Top: On ficus scene, we do not show blocking artifacts as PlenOc-tree and recover the pot better. Middle: We produce blurrier results on ship{\\\\textquoteright}s body and rigging, but we do not have the background artifacts. Bottom: On real-world captured Ignatius, we show better quality without blocking artifacts (left) and recover the color tone better (right). See supplementary material for more visualizations. \\n\\nWe mainly validate the effectiveness of the two proposed techniques{\\\\textemdash}post-activation and the imposed priors{\\\\textemdash}that enable voxel grids to model scene geometry with NeRF-comparable quality. We subsample two scenes for each dataset. See supplementary material for more detail and additional ablation studies on the number of voxels, point-sampling step size, progressive scaling, free space skipping, view-dependent colors modeling, and the losses. \\n\\nEffectiveness of the post-activation. We show in Sec. 4 that the proposed post-activated trilinear interpolation enables the discretized grid to model sharper surfaces. In Tab. 3, we compare the effectiveness of post-activation in scene reconstruction for novel view synthesis. Our grid in the fine stage consists of only 1603 voxels, where nearest-neighbor interpolation results in worse quality than trilinear interpolation. The proposed post-activation can improve the results further compared to pre-and in-activation. We find that we gain less in the real-world captured BlendedMVS and Tanks and Temples datasets. The intuitive reason is that real-world data introduces more uncertainty (e.g., inconsistent lightning, SfM error), which results in multi-view inconsistent and blurrier surfaces. Thus, the advantage is lessened for scene representations that can model sharper surfaces. We speculate that resolving the uncertainty in future work can increase the gain of the proposed post-activation. \\n\\nEffectiveness of the imposed priors. As discussed in Sec. 5.1, it is crucial to initialize the voxel grid with low density to avoid suboptimal geometry. The hyperparameter \\\\ensuremath{\\\\alpha}(init)(c) controls the initial activated alpha values via Eq. (9). In Tab. 4, we compare the quality with different \\\\ensuremath{\\\\alpha}(init)(c) and the view-count-based learning rate. Without the low-density \\n\\nTable 3. Effectiveness of the post-activation. Geometry modeling with density voxel grid can achieve better PSNRs by using the proposed post-activated trilinear interpolation. \\n\\nTable 4. Effectiveness of the imposed priors. We compare our different settings in the coarse geometry search. Top: We show their impacts on the final PSNRs after the fine stage reconstruction. Bottom: We visualize the allocated voxels by coarse geometry search on the Truck scene. Overall, low-density initialization is essential; using \\\\ensuremath{\\\\alpha}(init)(c) = 10\\\\ensuremath{-}6 and view-count-based learning rate generally achieves cleaner voxels allocation in the coarse stage and better PSNR after the fine stage. \\n\\ninitialization, the quality drops severely for all the scenes. When \\\\ensuremath{\\\\alpha}(init)(c) = 10\\\\ensuremath{-}7 , we have to train the coarse stage of some scenes for more iterations. The effective range of \\\\ensuremath{\\\\alpha}(init)(c) is scene-dependent. We find \\\\ensuremath{\\\\alpha}(init)(c) = 10\\\\ensuremath{-}6 generally works well on all the scenes in this work. Finally, using a view-count-based learning rate can further improve the results and allocate noiseless voxels in the coarse stage. \\n\\n7. Conclusion \\n\\nOur method directly optimizes the voxel grid and achieves super-fast convergence in per-scene optimization with NeRF-comparable quality{\\\\textemdash}reducing training time from many hours to 15 minutes. However, we do not deal with the unbounded or forward-facing scenes, while we believe our method can be a stepping stone toward fast convergence in such scenarios. We hope our method can boost the progress of NeRF-based scene reconstruction and its applications. Acknowledgements: This work was supported in part by the MOST grants 110-2634-F-001-009 and 110-2622-8-007-010-TE2 of Taiwan. We are grateful to National Center for High-performance Computing for providing computational resources and facilities. \\n\\nReferences \\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.txt'}),\n",
       " Document(page_content='XctNet: Reconstruction network of volumetric images from a single X-ray image \\n\\nX-ray Volumetric images \\n\\nABSTRACT  \\n\\nConventional Computed Tomography (CT) produces volumetric images by computing inverse Radon transformation using X-ray projections from different angles, which results in high dose radiation, long reconstruction time and artifacts. Biologically, prior knowledge or experience can be utilized to identify volumetric information from 2D images to certain extents. a deep learning network, XctNet, is proposed to gain this prior knowledge from 2D pixels and produce volumetric data. In the proposed framework, self-attention mechanism is used for feature adaptive optimization; multiscale feature fusion is used to further improve the reconstruction accuracy; a 3D branch generation module is proposed to generate the details of different generation fields. Comparisons are made with the state-of-arts methods using public dataset and XctNet shows significantly higher image quality as well as better accuracy (SSIM and PSNR values of XctNet are 0.8681 and 29.2823 respectively).   \\n\\n1. Introduction \\n\\nImage based 3D reconstruction, which is to infer 3D shapes from single or multiple 2D images, has been explored in the field of computer vision for decades ((\\\\ensuremath{<}\\\\ensuremath{>})Han et al., 2021) and has become the basis of many fields, such as robot navigation, 3D modeling and animation, object recognition, scene understanding, medical diagnosis, etc. However, it is not straightforward to extract volumetric information from digital images without disparity knowledge from stereo correspondence, due to the lack of approaches to derive depth information from pixels. Pro-jectional radiography, a conventional way to observe the inside of objects or bodies, is no different than normal photography, except that the pixels carry rich observations of transparent volumetric structure other than opaque surface. Specifically, in X-ray radiographs, each pixel is the line integral of attenuation data following Radon transformation in 2D space, so that the inversion of radiographs is not 2D{\\\\textendash}3D point conversion but the conversion from 2D pixel to spatial line distribution. Therefore, 3D reconstruction from radiographs is an even more chal-\\n\\nBiologically, although our eye-brain vision system does not make 3D reconstruction from plain pixels, we can still partially obtain the hidden spatial information from subtle evidence like: shadow, occlusion, light/ \\n\\nAvailable online 15 April 2022 Received 18 December 2021; Received in revised form 15 March 2022; Accepted 8 April 2022   0895-6111/{\\\\textcopyright} 2022 Elsevier Ltd. All rights reserved. \\n\\nshade, relative size, etc. The knowledge we use in this evidence-based stereo reconstruction process can be defined as prior knowledge, which plays an essential part in human vision-based judgment and identification system. When we look at photographs or images by bare eyes, the relative spatial relation of the objects or bodies could always be deduced by combining pixel information with prior knowledge. Similarly, radiologists are able to tell the spatial information of human bodies from radiographs by applying prior knowledge from anatomy and everyday practice. Therefore, from the aspect of biomimetic, prior knowledge has potential to reconstruct 3D information at least partially from radiographs theoretically. \\n\\nDeep learning, which shows great advantages over traditional methods in fitting complex nonlinear mathematical relations, has brought an evolution to numerous medical fields such as medical image segmentation, lesion area recognition, medical image registration, etc. ((\\\\ensuremath{<}\\\\ensuremath{>})Feng et al., 2020; (\\\\ensuremath{<}\\\\ensuremath{>})Schwartz et al., 2019; (\\\\ensuremath{<}\\\\ensuremath{>})Singh et al., 2020). The possibilities of 3D reconstruction from 2D radiographs have not been observed for long, but only in recent years does the attempts of the inverse mapping emerge. (\\\\ensuremath{<}\\\\ensuremath{>})Henzler et al. (2018) first, to our knowledge, applied a deep Convolutional Neural Network (CNN) to single-radiograph tomography and reconstructed 3D cranial volumes from 2D X-rays. (\\\\ensuremath{<}\\\\ensuremath{>})Kasten et al. (2020) used an end-to-end CNN for 3D reconstruction of knee bones from bi-planar X-ray images. (\\\\ensuremath{<}\\\\ensuremath{>})Shen et al. (\\\\ensuremath{<}\\\\ensuremath{>})(2019) developed a deep network system with representation, transformation, generation modules to generate volumetric tomography images from single or multiple 2D X-rays. Through the current literature research, it can be found that the current CNN-based reconstruction methods use the end-to-end network structure, which will cause a certain loss of image resolution due to the network sampling process; Secondly, the task of CT volumetric images reconstruction based on X-ray image is quite computationally expensive. Thus, this paper constructs a lightweight CNN-based reconstruction network, XctNet, which can not only improve the information loss caused by the sampling process, but also greatly reduce the required computing resources. To summarize, the contribution can be seen as follow:  \\n\\n2. Related work \\n\\nCT reconstruction is an inverse mapping mathematical process, which generates tomographic images from X-ray projection data acquired at many different angles around the patient ((\\\\ensuremath{<}\\\\ensuremath{>})Stierstorfer et al., (\\\\ensuremath{<}\\\\ensuremath{>})2004). The quality of reconstruction has a fundamental impact on the radiation dose used and the researchers are trying to find better reconstruction algorithm to ensure both the accuracy and resolution of the reconstructed image while minimizing radiation dose ((\\\\ensuremath{<}\\\\ensuremath{>})Kak and Slaney, (\\\\ensuremath{<}\\\\ensuremath{>})1987; Hsieh, 2003). \\n\\nA multi-detector spiral CT reconstruction method is proposed based on cone beam geometry ((\\\\ensuremath{<}\\\\ensuremath{>})Taguchi and Aradate, 1998). (\\\\ensuremath{<}\\\\ensuremath{>})Hu (1999) studied the scanning and reconstruction principles of multi-slice spiral CT, especially the scanning and reconstruction principles of 4-slice spiral CT, and concluded that the volume coverage speed of 4-slice spiral CT is 2{\\\\textendash}3 times that of single-slice spiral CT, which can provide the same image quality. (\\\\ensuremath{<}\\\\ensuremath{>})Schaller et al. (2001) introduced a high-quality image reconstruction approach for helical CBCT and (\\\\ensuremath{<}\\\\ensuremath{>})Flohr et al. (2003) proved its effectiveness in a 16-slice CT scanner. \\n\\nThe EOS system, originated from the Nobel prize-winning invention MWPC (the Multiwire Proportional Chamber) particle detector by Dr. Georges Charpak, is able to produce full-body stereo images of patients using biplanar low-dose X-ray scan and is regarded as the most advanced image acquisition equipment in orthopedics at present ((\\\\ensuremath{<}\\\\ensuremath{>})Melhem et al., (\\\\ensuremath{<}\\\\ensuremath{>})2016; Song et al., 2020). (\\\\ensuremath{<}\\\\ensuremath{>})Rehm et al. (2017) compared EOS imaging equipment with CT imaging equipment and showed that the EOS system can obtain high-quality images with less doses. (\\\\ensuremath{<}\\\\ensuremath{>})Post et al. (2018) proposed a three-dimensional spine classification method based on the EOS system. However, the 3D reconstruction of EOS depends on parametric models and statistical inferences from collected biplanar X-ray scans, so that the generated skeleton model is only a parametric virtual substitute and is limited in circumstances of severe skeletal malformations or abnormalities like congenital scoliosis (CS) and ankylosing spondylitis (AS). \\n\\nIn recent years, deep learning has been widely adopted in the field of medical imaging. (\\\\ensuremath{<}\\\\ensuremath{>})Meng et al. (2020) used semi-supervised learning to reconstruct high-dose volumetric images from low-dose volumetric images. (\\\\ensuremath{<}\\\\ensuremath{>})Henzler et al. (2018) proposed a deep convolution to generate 3D images from a single X-ray animal skull image. Its network architecture adopts an end-to-end structure, and compared with some previously proposed network structures, it proved that their network can achieve better reconstruction results. (\\\\ensuremath{<}\\\\ensuremath{>})Shen et al. (2019) proposed a 2D to 3D network model architecture and brought the idea of converting the 2D feature information into a spatial tensor in order to perform a 3D deconvolution. However, Shen{\\\\textquoteright}s reconstruction network includes enormous amount of parameters to update, which leads to computational inefficiency. Multiple studies on machine-learning-based 3D reconstruction have also been carried out in the field of dentistry, spine, chest, etc ((\\\\ensuremath{<}\\\\ensuremath{>})Ying et al., 2019; Bayat et al., 2020; {\\\\textasciicaron} a et al., 2020). Cavojsk{\\\\textasciiacute} The mentioned works also have limitations in generalization and tend to underperform on different datasets in practice. In this paper, a more lightweight CNN-based network is constructed to improve the reconstruction accuracy and reduce the computational cost. \\n\\n3. Methodology \\n\\nThe architecture of XctNet reconstruction network, shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 1, includes the two major parts:The X-ray feature extraction module, Multi-scale feature fusion module and the volumetric image generation \\n\\nFig. 1. Architecture of XctNet. The model contains X-ray feature extraction module, multi-scale feature fusion module and the volumetric images generation module. The input of the model is a single 2D projection image. The X-ray feature extraction module extract feature information from the input X-ray image. The multi-scale feature extraction module converts 2D features into 3D features and performs feature fusion with the corresponding 3D generation module. The volumetric images generation module, which consists of a series of New Inception module, uses the extracted feature data to generate the corresponding volumetric image. \\n\\nmodule. The details of each module and the loss function will be explained in the following sections. \\n\\nX-ray feature extraction module: The introduction of deep residual network is to solve the problem of gradient disappearance caused by too many network layers in the training process. The most representative structure of the residual network is ResNet ((\\\\ensuremath{<}\\\\ensuremath{>})He et al., 2016), which directly connects the input terminal to the following layer through the shortcut structure, thereby protecting the integrity of the transmitted data. The feature extraction module is built based on ResNet34. The input is a single X-ray image which size is 128 {\\\\texttimes} 128, the first layer of the model is composed of a convolutional layer with a kernel size of 7 {\\\\texttimes} 7 and stride 2, and the second to fifth layers are composed of four residual blocks which contains two 3 {\\\\texttimes} 3 convolutional layers. The number of channels of the convolutional layer in each residual block is kept the same to ensure that the shortcut path and the residual path can maintain the same size during the element-wise addition operation. The size of the feature representation output is 4 {\\\\texttimes} 4. In addition, through experiments, we find that when using the encoder/decoder structure for pixel level vision tasks, the convolution layer can only use local information to calculate the target pixel value. Therefore, the lack of global information will undoubtedly lead to deviation. The error caused by the convolution layer can be described by the covariance between the pixel values shown in (\\\\ensuremath{<}\\\\ensuremath{>})Eq. (1). Each pixel value xi in the feature map obtained by the convolution layer can be used as a random variable, and x is the mean value of the feature map. The similarity between the two variables can be evaluated by calculating the covariance of the two random variables. The attention mechanism is to use the similarity between pixels to improve the performance of convolution layer. \\n\\n(1) \\n\\nIn order to reduce the error caused by the convolution process, this paper introduces two attention mechanisms, CBAM (Convolutional Block Attention module) ((\\\\ensuremath{<}\\\\ensuremath{>})Woo et al., 2018) and ECA (Efficient channel Attention module) module ((\\\\ensuremath{<}\\\\ensuremath{>})Wang et al., 2020), to adaptively improve the feature extraction ability and reduce the error. Specifically, CBAM derives the attention graph from the 2D information of space and channel, then, the attention graph with the input features will be multiplied to adaptively optimize the eigenvalues. The module structure is shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 1(b). In the 2D feature extraction module, CBAM is mainly used for convolution feature extraction of the first layer and the last layer, so as to improve the ability of feature adaptive extraction on the premise of ensuring that the overall network structure is not affected. For the intermediate convolution layer, ECA module is used to improve its feature extraction performance. As a local cross-channel interaction module that does not reduce the feature dimension, ECA module obtains local cross-channel interaction information by combining each channel and its adjacent k channels. ECA module can be realized by one-dimensional convolution layer with the size of k. It is worth noting that ECA, as a lightweight module, does not add a large number of additional parameters. The network structure diagram is shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 1(c). In this paper, the feature extraction capability of the middle layer is improved by combining the residual module and ECA module. The feature map obtained by the residual module will be input into the ECA module for adaptive optimization. In addition, the feature map obtained by the residual module will also be combined with the adaptively optimized feature map through element-wise product, so as \\n\\nThe process of multi-scale convolution mainly includes two factors: feature propagation and cross-scale communication ((\\\\ensuremath{<}\\\\ensuremath{>})Feng et al., 2020). In the multi-scale feature extraction structure proposed in this paper, the input feature will be divided into high-scale feature Xhigh and low-scale feature Xlow to obtain the corresponding high-scale feature output Yhigh and low-scale feature output Ylow respectively. The multi-scale feature transform process can be seen as follow: \\n\\n(2) \\n\\nThe overall network structure is shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 1(a). Feature maps of different layers are extracted from the original network structure and converted into corresponding 3D feature maps through transform module and then the multi-scale 3D feature maps are combined with corresponding 3D feature generation layers, so as to improve the fine granularity of generation results and reduce the loss of information in the reconstruction process. For details, to convert 2D projection data to volumetric images data requires data conversion. A transform module is added to bridge the 2D feature extraction module and the 3D generation module. the multi-scale 2D features, which size is(C, H, W), are converted to (C, 1, H, W) by the dimension conversion function, after that, the converted 3D feature map, which size is (C, D, H, W) can be obtained through a deconvolution operation with a kernel size of D {\\\\texttimes} 1 {\\\\texttimes} 1. In addition, the ReLU activation function and the batch normalization function are also included to better learn the transformation relationship in the transform process. \\n\\nThe original data needs to be preprocessed before fed into the network model for training. First of all, all input data need to be resized to the same size. The 2D images and the corresponding 3D CT images used for training are resized to 128 {\\\\texttimes} 128 and 128 {\\\\texttimes} 128 {\\\\texttimes} 128 separately. In practice, 2D{\\\\textendash}3D data pairs should be composed of X-ray and CT images, owing to the lack of corresponding paired images, this paper uses digitally reconstructed radio algorithm (DRR) to generate an approximate single 2D projection image to obtain the corresponding 2D{\\\\textendash}3D data pairs. As shown in the (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 2, this article uses point source vision based DRR projection algorithm to generated 2D projection ((\\\\ensuremath{<}\\\\ensuremath{>})Moturu and Chang, 2018). The advantage of this method is that the point source can be randomly selected to obtain X-rays, which makes the data change slightly. Specifically, after the light source point is selected and the projection distance is fixed (centered on the front of the CT volumetric image), 2D projections are generated according to Beer{\\\\textquoteright}s law ((\\\\ensuremath{<}\\\\ensuremath{>})Feeman, 2010), in which the intensity loss measurement of X-rays passing through the body is modeled by Beer{\\\\textquoteright}s law. The information (spacing, size, direction) of the CT image is obtained and the image is used as the input of the DRR algorithm, and then the image is resampled by coordinate transformation. Moreover, we set the distance from the light source to the projection plane to 400 mm, and the default pixel spacing of the projection pixel plane is 0.8 {\\\\texttimes} 0.8 and set the threshold to {\\\\L} 80, and the bilinear interpolation is used to integrate each voxel plane traversed, so as to obtain the anterior-posterior positions of the 2D projected image. In order to enrich the sample size of training data, data augmentation, which includes scale change, rotation change, mirror image and translation change, brightness change, chroma change, contrast change and sharpness change, is performed before training. Moreover, the pixel-wise input data is normalized to the interval [0,1]. \\n\\nIn order to evaluate the performance of the model, we tested the trained model on the test set and used different evaluation metrics to evaluate the predicted reconstruction results. Four evaluation functions is used in this paper for model evaluation, namely: MSE (mean squared error), MAE (mean absolute error), SSIM (structural similarity) and PSNR (peak signal noise ratio). MSE and MAE are used to evaluate the deviation between the predicted reconstruction result and the target value. The smaller MSE/MAE value, the closer the reconstruction result is to the real situation. The image evaluation metric SSIM, incorporating the information of luminance, contrast and structures, is used to evaluate the degree of similarity between images. The commonly used PSNR is applied to evaluate the quality of our reconstructed volumetric images. Generally, the resultant images with better structural and higher resolution will have higher SSIM and PSNR values. Each metric value is averaged for all test samples and different methods are compared as \\n\\nFig. 2. DRR projection algorithm based on point source vision.  \\n\\nshown in (\\\\ensuremath{<}\\\\ensuremath{>})Table 2. \\n\\n4. Reconstruction experiments results \\n\\nThe input sample consists of a single 2D projection image X and volumetric CT images Y. In the training process, with the single X-ray image as input X \\\\ensuremath{\\\\in} RH{\\\\texttimes}W , the model output is volumetric images while the ground truth which is the reference standards for model training. \\n\\nIn order to verify the effectiveness of the model, we used the public dataset, the Lung Image Database Consortium image collection (LIDCIDRI) ((\\\\ensuremath{<}\\\\ensuremath{>})Armato et al., 2011), which contains 1081 CT volumetric image cases. The original data will be divided into training set, test set and verification set separately, on this basis, the original data is expanded to 59,708 cases through data augmentation. Among them, the training set is 35,825 cases, the verification set is 11,941 cases, and the test set is 11, 942 cases. At the same time, the corresponding input X of each case is generated by DRR projection. \\n\\nThe size of the input X is 128 {\\\\texttimes} 128 and the size of the ground truth YGT is 128 {\\\\texttimes} 128 {\\\\texttimes} 128. The network is trained on a device with three NVIDIA Tesla V100 graphic processing units, and the training platform is Pytorch. Training epoch is 64. As an important parameter in deep learning training, it is particularly important to select the appropriate learning rate. This paper constructs an adaptive learning rate adjustment strategy, which can modify the learning rate according to the specific situation in the training process, so as to ensure the best effect of training. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Eq. (3), the loss function used in all three trained networks was MSE. The training results were shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3. \\n\\n(3) \\n\\nWe show the reconstruction results of ResXct, CBAM/ECAXct, XctNet as well as ReconNet on the LIDC-IDRI data set. These abnormalities can further prove the effectiveness of the XctNet model. \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3, a test sample is randomly selected to show the generation results of the different model and the numbers of the selected slices are 3, 15, 35, 65 and 85. The results shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3(a){\\\\textendash}(d) are slice images which are randomly selected from the test sample; (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3(e) is the corresponding ground truth. From the overall result of reconstruction, the result generated by our XctNet is closer to the ground truth. In terms of details, the content of the slice image generated by the original version model is relatively vague; compared with the ReconNet model, the overall contour of the intermediate version model is clearer, and some internal details, such as rib areas, can be reconstructed. On the other hand, from the chest slice data at different positions, the best reconstruction detail is the bony area, while the reconstruction accuracy of internal organs in the chest is blurred to varying degrees. Besides, by randomly selecting multiple test data for analysis, it can be found that in the reconstructed volumetric data, the reconstruction performance of the middle of the volume is generally better than that of the front of the volume and the end of the volume. The main reasons for this phenomenon are as follows: First of all, owing to this paper is to reconstruct the \\n\\nFig. 3. Volumetric image examples from the test set. (a){\\\\textendash}(d) represent the results generated by the ReconNet, ResXct, CBAM/ECAXct and XctNet respectively; (e) is the ground truth. The results shown in the figure comes from different slice graphs in a volumetric image randomly selected. \\n\\nwhole thoracic cavity, we do not preprocess according to the HU values of different tissues and organs, but process the CT data of the whole thoracic cavity. Therefore, there will be some deviation for the details such as internal organs in the reconstruction process. Secondly, due to the different sources of the data sets, the quality of the thoracic cavity area can not be guaranteed to be completely consistent, resulting in the quality of the front and end volume data of the reconstructed CT volumetric image will be relatively worse than that of the middle volume. It can be seen that the main reason for this phenomenon is due to the complexity of data, but this does not mean that our model has limitations. As can be seen, compared with the above different models, XctNet has the best reconstruction performance in terms of overall contour and internal details. \\n\\nTo show the details of the differences in results between different models, as can be seen in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 4, gray values indicate areas with insignificant differences, while white and black values represent areas with large differences. It can be seen from the figure that XctNet has the smallest difference with ground truth compared with other models. \\n\\nIn order to make a quantitative analysis of XctNet and the proposed contrast network, four evaluation metric functions are used to analyze the difference between the predicted reconstructed image and the ground truth. In addition, by comparing the evaluation metric differences between the models, the effectiveness of attention mechanism and multi-scale fusion module can be further illustrated. \\n\\nIt is worth noting that the volumetric data used in this paper is composed of 128 slices of data. It is worth noting that the volumetric data used in this paper are composed of 128 slices of data. The 128 slices of volumetric data is evaluated separately by using different evaluation metric functions, and then obtain the final evaluation result by taking the average of all slices.As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table 2, our XctNet can achieve the best evaluation results, and its PSNR and SSIM can reach 29.2823 and 0.8681 respectively. Incidentally, all the evaluation metric values in (\\\\ensuremath{<}\\\\ensuremath{>})Table 2 are the mean values of the test samples. On the other hand, the evaluation results obtained by ReconNet perform better than our baseline model, RexXct, which shows that increasing the network depth is effective for the reconstruction results. In addition, the evaluation results of CBAM/ECAXct are similar to ReconNet, that is, adding lightweight attention mechanism is also an effective method to enhance the performance of the model without increasing the network depth. \\n\\nFrom the overall distribution of the evaluation results of the test set, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 5, the four violin graphs represent the results of different evaluation functions. Overall, XctNet achieves the better results in all evaluation metrics. On the other hand, as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 5(c), the distribution of PSNR values of all models are mostly concentrated near the inferior quartile, that is mainly because PSNR evaluates the gray difference between images and due to the data set used in this paper is complicated, the prediction results are usually different. From the \\n\\nFig. 5. Distribution of evaluation results of different models. (a){\\\\textendash}(d) represents the distribution of evaluation results of MAE, MSE, PSNR and SSIM on LIDC-IDRI data set respectively. It can be seen that the result distribution in (a){\\\\textendash} (c) approach to the inferior quartile and (d) approach to the superior quartile. \\n\\ncomparison results, the interquartile range (IQR) of XctNet is smaller than the other three models, which shows that XctNet model is more stable. Through the above data analysis, we can get the conclusions that self-attention mechanism and multi-scale feature fusion module can \\n\\nFig. 4. Comparison of deviation with respect to ground truth. The first column correspond to the ground truth. The second column shows the difference between ReconNet and ground truth. Other columns represent the difference between the corresponding model and the ground truth. \\n\\ngreatly improve the output accuracy of the reconstructed model. By summarizing and analyzing the training results of the four groups of models, the evaluation metrics obtained by our XctNet exceeds the ReconNet. This result confirms that without additional network depth, attention mechanism and multi-scale feature fusion module can also improve the accuracy of the model. \\n\\n5. Discussion \\n\\nThe advantages of the XctNet model is further illustrated by analyzing the semantic representation of the model. For the 2D projection image based reconstruction task, only when the feature extraction module extracts the key and useful feature information can the volumetric images be correctly reconstructed. The three models constructed in this paper contain 512 feature maps with a size of 4 {\\\\texttimes} 4. For better visualization, 16 feature maps are randomly selected to illustrate the feature representation between different models. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 6(a){\\\\textendash} (b), it can be seen that the feature map generated by the feature extraction module with self-attention mechanism is more concise than the feature map without self-attention mechanism. Comparing (b){\\\\textendash}(c), it can be seen that CBAM/ECA attention mechanism can further remove redundant features. On the other hand, from the perspective of the generated volumetric image, the image quality generated by the model with self-attention mechanism is significantly better. In other words, the feature extraction module without self-attention mechanism learns a lot of redundant information, which leads to the phenomenon of fuzzy generation results. Therefore, the conclusion can be got that the self- attention mechanism can help the model to better learn the feature information. \\n\\nIn order to further verify the performance of XctNet model, the current state of art CNN models are compared. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table 3, ReconNet model has the highest model complexity, and its FLOPs (floating point operations) reaches 1.304 {\\\\texttimes} 1012 . ResXct as our baseline model, which complexity is lower than ReconNet, and the error rate is almost the same. Our XctNet has greatly improved its error rate with only a little increase in complexity. This phenomenon shows the following two aspects. Firstly, the lightweight attention mechanism can improve the performance of the model without increasing the complexity of the model. Secondly, the New Inception module proposed in this paper can greatly reduce the amount of model calculation and generate volumetric images with richer content. \\n\\nAccording to the definition of information entropy, it represents the overall characteristics of an information source in an average sense. For image information, we can describe the amount of information contained in the image according to image entropy. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Eq. (4), x represents each pixel in the image, the image entropy reflects the average information of an image and the image entropy obtained for specific image information is unique. Therefore, the generation quality of volumetric images can be evaluated from the perspective of image \\n\\nentropy. \\n\\n(4) \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 7, two test samples are randomly selected to illustrate the distribution of image entropy comes from different model, which shows the ground truth and the entropy map of the different models. It can be seen that, the area in which the entropy map tends to be cold indicates that it contains less information and the area in which the entropy map tends to be warm indicates that it contains more information. By comparing the results of the three models with the ground truth, we can find that XctNet can get a distribution map more inclined to the ground truth by adding self-attention mechanism and multi-scale fusion module. By comparing the ReconNet and ResXct with the other two models, it further shows that the traditional end-to-end network will bring deviation in the convolution process, and also verifies the effectiveness of the improved method proposed in this paper. \\n\\nTo verify the performance of the model on clinical X-ray images, we obtained 10 original X-ray chest images through the spine surgery of the General Hospital of Shenzhen University. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 8, the reconstruction results of two groups of clinical X-ray images are shown. It can be seen that the accuracy of the reconstruction result are worse than that of the 2D projection used in this paper. The main reason for this phenomenon is that there are some differences between clinical X- ray images and 2D projections. Therefore, in the future work, more in- depth research from clinical X-rays need to conducted. However, from the generation results of ReconNet and XctNet proposed in this paper, XctNet performs better in clinical X-rays data, which also confirms that the network we constructed has considerable superiority. \\n\\n6. Conclusion \\n\\nIn this paper, we focus on the reconstruction quality of volumetric image. In order to obtain more accurate reconstruction results, a lightweight reconstruction network, XctNet, is constructed. The network structure has the following three innovations: \\n\\nFirstly, self-attention mechanism is be added to the original residual feature extraction module to remove redundant features; Secondly, a multi-scale feature fusion module is proposed in this paper to improve \\n\\nFig. 6. Feature extraction and network structure analysis. a) Feature map learned from 2D feature extraction module without attention mechanism; b) Feature map learned from 2D feature extraction module only with CBAM attention mechanism; c) Feature map learned from 2D feature extraction module with CBAM/ECA attention mechanism. \\n\\nFig. 7. The entropy map generated by different network structures. a{\\\\textendash}b) represent different test samples and their corresponding entry information. The variation of image entropy is closely related to the content contained in the image. As can be seen, the less content the image contains, the lower the image information entropy, that is, the color of the entropy map tends to be cold. \\n\\nFig. 8. Clinical X-ray reconstruction results of difference cases. a{\\\\textendash}b) represent volumetric images reconstructed from different X-ray images. The number of slices shown in the figure are 30, 60, 70, and 90. \\n\\nthe quality of reconstructed image and other details. Finally, a feature generation module called New Inception module is constructed to obtain richer feature information and more accurate reconstruction results. At the same time, there are still some problems to be solved in this paper. In the actual application scenario, the corresponding 2D projection image should be an X-ray image, but the 2D projection used in this article is projected by the DRR algorithm. To solve this problem, using style transfer algorithm may considered to solve the difference between clinical X-rays and DRR projection. In conclusion, XctNet as a lightweight framework can further improve the results of volumetric image reconstruction. \\n\\nCRediT authorship contribution statement \\n\\nZhiqiang Tan: Conceptualization, Methodology, Software, Investigation. Jun Li: Data curation, Software Huiren Tao: Resources, Validation. Shibo Li: Visualization, Writing {\\\\textendash} review \\\\& editing. Ying Hu: Supervision, Project administration. \\n\\nDeclaration of Competing Interest \\n\\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. \\n\\nAcknowledgements \\n\\nThis work was supported in part by Key-Area Research and Development Program of Guangdong Province (No.2020B0909020002), National Natural Science Foundation of China (Grant No. 62003330), Shenzhen Fundamental Research Funds (Grant No. JCYJ20200109114233670No.JCYJ20190807170407391, JCYJ20180507182415428), Natural Science Foundation of Guangdong Province (Grant No. 2019A1515011699) and Guangdong-Hong Kong- Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institute of Advanced Technology. \\n\\nReferences \\n\\nArmato, Samuel G., McLennan, Geoffrey, Bidaut, Luc M., McNitt-Gray, Michael F., Meyer, C.R., Reeves, Anthony P., Zhao, Binsheng, Aberle, Denise R., Henschke, Claudia I., Hoffman, Eric A., Kazerooni, Ella A., MacMahon, Heber, Van Beeke, Edwin J.R., Yankelevitz, David F., Biancardi, Alberto M., Bland, Peyton H., Brown, Matthew S., Engelmann, Roger M., Laderach, G.E., Max, Daniel, Pais, Richard C., Qing, D.P., Roberts, Rachael Y., Smith, Amanda R., Starkey, Adam, Batrah, Poonam, Caligiuri, Philip, Farooqi, Ali O., Gladish, Gregory W., Jude, Cecilia Matilda, Munden, Reginald F., Petkovska, Iva, Quint, Leslie E., Schwartz, Lawrence H., Sundaram, Baskaran, Dodd, Lori E., Fenimore, Charles, Gur, David, Petrick, Nicholas A., Freymann, John B., Kirby, Justin S., Hughes, Brian, Casteele, Alessi Vande, Gupte, Sangeeta, Sallamm, Maha, Heath,Michael, Kuhn, M., Dharaiya, Ekta, Burns, Richard, Fryd, David, Salganicoff, Marcos, Anand, V., Shreter, Uri, Vastagh, Stephen, Croft, Barbara Y., 2011. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules \\n\\non CT scans. Med. Phys. vol. 38, issue 2, pp. 915{\\\\textendash}31. (\\\\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\\\\ensuremath{>})Bayat, Amirhossein, Kumar Sekuboyina, Anjany, Paetzold, Johannes C., Payer, Christian, (\\\\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\\\\ensuremath{>}){\\\\textasciicaron}Stern, Darko, Urschler, Martin, Kirschke, Jan S., Menze, Bjoern H., 2020. Inferring (\\\\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\\\\ensuremath{>})the 3D standing spine posture from 2D radiographs. MICCAI. \\n\\nFeeman, Timothy G., 2010. The Mathematics of Medical Imaging. \\n\\nFeng, Ruicheng, Guan, Weipeng, Qiao, Yu, Dong, Chao, 2020. Exploring Multi-Scale Feature Propagation and Communication for Image Super Resolution. ArXiv abs/ 2008.00239, n. pag. \\n\\nHe, Kaiming, Zhang, X., Ren, Shaoqing, Sun, Jian, 2016. Deep residual learning for image recognition. In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770{\\\\textendash}8. \\n\\nHsieh, Jiang, 2003. Computed Tomography: Principles, Design, Artifacts, and Recent Advances. \\n\\nKasten, Yoni, Doktofsky, Daniel, Kovler I., 2020. End-To-End Convolutional Neural Network for 3D Reconstruction of Knee Bones from Bi-Planar X-Ray Images. ArXiv abs/2004.00871, n. pag. \\n\\nLi, Jun, Xu, Kai, Chaudhuri, Siddhartha, Yumer, Ersin, Zhang, Hao, Guibas, Leonidas J., 2017. GRASS: Generative Recursive Autoencoders for Shape Structures. ArXiv abs/ 1705.02090, n. pag. \\n\\nMoturu, Abhishek, Chang, Alex, 2018. Creation of Synthetic X-Rays to Train a Neural Network to Detect Lung Cancer. \\n\\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan, D., Vanhoucke, Vincent, Rabinovich, Andrew, 2015. Going deeper with convolutions. In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1{\\\\textendash}9. \\n\\nWang, Qilong, Wu, Banggu, Zhu, Pengfei, Li, P., Zuo, Wangmeng, Hu, Qinghua, 2020. ECA-net: efficient channel attention for deep convolutional neural networks. In: Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11531{\\\\textendash}9. \\n\\nWang, Weiyue, Huang, Qiangui, You, Suya, Yang, Chao, Neumann, Ulrich, 2017. Shape Inpainting using 3D generative adversarial network and recurrent convolutional networks. In: Proceedings of the 2017 IEEE International Conference on Computer \\n\\nVision (ICCV), pp. 2317{\\\\textendash}25. Woo, Sanghyun, Park, Jongchan, Lee, Joon-Young, Kweon, In-So, 2018. CBAM: convolutional block attention module. ECCV. \\n\\nWu, Jiajun, Zhang, Chengkai, Xue, Tianfan, Freeman, Bill, Tenenbaum, Joshua B., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. NIPS. \\n\\nWu, Zhirong, Song, Shuran, Khosla, Aditya, Yu, Fisher, Zhang, Linguang, Tang, Xiaoou, Xiao, Jianxiong, 2015. 3D shapenets: a deep representation for volumetric shapes. In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1912{\\\\textendash}20. \\n\\nYing, Xingde, Guo, Heng, Ma, Kai, Wu, Jian, Weng, Zhengxin, Zheng, Yefeng, 2019. X2CT-GAN: reconstructing CT from biplanar X-rays with generative adversarial networks. In: Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10611{\\\\textendash}20. \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.txt'}),\n",
       " Document(page_content='Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning \\n\\nABSTRACT  \\n\\nDeep learning networks have achieved remarkable progress in various tasks of medical imaging. Most of the recent success in computer vision highly depend on large amounts of carefully annotated data, whereas labelling is arduous, time-consuming and in need of expertise. In this paper, a semi-supervised learning method, Semi- XctNet, is proposed for volumetric images reconstruction from a single X-ray image. In our framework, the effect of regularization on pixel-level prediction is enhanced by introducing a transformation consistent strategy into the model. Furthermore, a multi-stage training strategy is designed to ameliorate the generalization performance of the teacher network. An assistant module is also introduced to improve the pixel quality of pseudo- labels, thereby further improving the reconstruction accuracy of the semi-supervised model. The semi-supervised method proposed in this paper has been extensively validated on the LIDC-IDRI lung cancer detection public data set. Quantitative results show that SSIM (structural similarity measurement) and PSNR (peak signal noise ratio) are 0.8384 and 28.7344 respectively. Compared with the state-of-the-arts, Semi-XctNet exhibits excellent reconstruction performance, thus demonstrating the effectiveness of our method on the task of volumetric images reconstruction network from a single X-ray image.   \\n\\n1. Introduction \\n\\nIn recent years, deep learning methods have been proven to have enormous potential in medical data processing [(\\\\ensuremath{<}\\\\ensuremath{>})2{\\\\textendash}4]. Wang et al. [(\\\\ensuremath{<}\\\\ensuremath{>})5] point out that as artificial intelligence technology continues to develop, new breakthroughs will be made in the field of tomographic imaging. In addition, Sahiner et al. [(\\\\ensuremath{<}\\\\ensuremath{>})6] provide a summary of current research in medical radiography and indicate that combining deep learning model innovations will be a key research direction for the future [(\\\\ensuremath{<}\\\\ensuremath{>})7]. The deep-learning approaches avoid playing with the pixels, but instead focus on the driving data as well as the learning architectures. In terms of 3D construction, the construction from single image is an ill-posed problem and is almost unsolvable through traditional algorithms, nonetheless, this problem is already partially settled by various teams using deep learning methods. Shen et al. [(\\\\ensuremath{<}\\\\ensuremath{>})8]. achieved the reconstruction of CT volumetric images from 2D projections for the first time \\n\\nthrough deep learning. The substantial progress can be explained as that the learning mechanism takes the advantage of prior knowledge from data and succeeds in recovering the spatial information of the pixels from images, which is in analogy with the way doctors grasp anatomical information from radiographs. \\n\\nDifferent from natural images, large-scale data is not always available due to various reasons like privacy, patient volume, the database construction of hospital and so on. In addition, the annotation of medical image data is integral to supervised learning, which requires clinical experience and is time costly. In the process of clinical diagnosis, patients usually take X-ray images for preliminary investigation, and then CT scans are performed for further diagnosis, so that the amount of X-ray data is usually greater than CT data, which makes the data for volumetric construction a partially labeled dataset. To solve this problem, this paper proposes a method for reconstructing CT volumetric images from 2D projection images based on semi-supervised learning. This method can achieve the reconstruction tasks from a large number of unpaired 2D projection images and a small fraction of paired 2D pro-jection/CT volumetric image data. \\n\\nThe semi-supervised reconstruction algorithm proposed in this paper includes three models, namely, a teacher model, an assistant model and a student model. The teacher model is used to generate pseudo-labels to train the teacher model, the assistant model is designed to enhance the details of pseudo-labels, while the student model is created for self- supervised training of volumetric image reconstruction. The main contributions of this paper are as follows.  \\n\\n2. Related work \\n\\nVarious significant research findings have been established in the field of image-based 3D reconstruction, which aim to infer the 3D structure of objects from single-view or multi-view 2D images. Nozawa et al. [(\\\\ensuremath{<}\\\\ensuremath{>})9] proposed a deep learning model for reconstructing 3D car shape from a single 2D sketch image. The model acts as a variational autoencoder deep neural network that takes a 2D sketch and generates a set of multi-view depth and mask images, forming a more efficient representation and can be efficiently fused to generate a 3D car shape. Feng et al. [(\\\\ensuremath{<}\\\\ensuremath{>})10]. proposed an efficient end-to-end deep learning framework for reconstructing the 3D structure of porous media, which could be trained by inputting 2D slices into a deep learning model, and the corresponding 3D structure could be reconstructed instantaneously. Fu et al. [(\\\\ensuremath{<}\\\\ensuremath{>})11] reviewed recent work on 3D structure reconstruction from a single image and introduced the encoder structure and training details for this task. \\n\\nCurrent approaches for volumetric data construction from single image rely on the large amount of paired data in order to obtain satisfactory reconstruction results. For medical images, labelling requires clinical prior knowledge, which includes both complex anatomical theories and practical experiences, and it also takes plenty of time. Semi- supervised learning aims to enhance the learning performance of the model by making full use of a large number of unlabeled samples under the guidance of a small number of sample labels, so as to avoid waste of data resources, and to solve the problem of poor generalization caused by few labeled data in supervised learning methods [(\\\\ensuremath{<}\\\\ensuremath{>})15]. \\n\\nLee [(\\\\ensuremath{<}\\\\ensuremath{>})16] proposed the concept of Pseudo-label by taking the target class with the highest prediction probability of unlabeled samples as the real label. The algorithm proved that training with pseudo-labels can make the entropy of unlabeled data samples smaller, so that the classification and generalization performance can be improved. Laine et al. [(\\\\ensuremath{<}\\\\ensuremath{>})17] proposed a simple and effective method for training deep neural networks in a semi-supervised environment, in which only a small part of the training data were labeled. This method introduces self-awareness and forms a consistent prediction for unknown labels under different regularization and input enhancement conditions. For unknown labels, the prediction set can be regarded as a better predictor than the network output in the recent training period, so it can be used as a training target. Sohn et al. [(\\\\ensuremath{<}\\\\ensuremath{>})18] proposed a semi-supervised learning algorithm named FixMatch by combining two common self-supervised learning methods (consistency regularization and pseudo-labeling). State--of-the-art performance was achieved on supervised learning benchmarks. In the same year, Google released a self-supervised learning framework [(\\\\ensuremath{<}\\\\ensuremath{>})19], SimCLR. This paper is not only simple in method, but also deepens our understanding of self-supervised learning and contrastive learning. On this basis, the Google team proposed SimCLR V2 [(\\\\ensuremath{<}\\\\ensuremath{>})20] for application, which mainly carries out unsupervised training on a large number of unlabeled samples, fine-tuned through a small number of labels and distills knowledge on unlabeled data. In addition, there are many advanced research results in semi-supervised learning in the field of medical image analysis. Li et al. [(\\\\ensuremath{<}\\\\ensuremath{>})21] proposed a new semi-supervised method for medical image segmentation, in which the network is optimized by a weighted combination of common supervision loss only for labeled input and regularization loss of labeled and unlabeled data. Yu et al. [(\\\\ensuremath{<}\\\\ensuremath{>})22] proposed a novel uncertainty-aware semi-supervised framework for 3D MR left atrial image segmentation, which enables student models to gradually learn from meaningful and reliable targets by exploiting uncertainty information. Wang et al. [(\\\\ensuremath{<}\\\\ensuremath{>})23] constructed a 3D medical image detection framework named FocalMix, which performed extensive experiments on two widely used lung nodule detection datasets. Results showed that FocalMix achieved substantial improvements of up to 17.3\\\\% over state-of-the-art supervised learning methods over 400 unlabeled CT scans. \\n\\nMost current deep learning-based image reconstruction tasks are based on supervised learning models, owing to data in the medical imaging field is often not available on a large scale, supervised learning for image reconstruction tasks is usually not well suited to drive clinical applications. From the above literature, it can be seen that semi- supervised learning has shown performance effects that can surpass supervised learning, so it is a feasible novel method to apply semi- supervised learning to the field of medical image reconstruction. \\n\\nFig. 1. The Schematic diagram of training process for volumetric images reconstruction. The teacher model uses labeled data for supervised training and provides pseudo-labels for unlabeled data training; the teacher model is trained based on pseudo-labels fused by different data augmentation methods and fine-tuned in the labeled data set. \\n\\nFig. 2. The main architecture of XctNet. The model contains X-ray feature extraction module, the volumetric images generation module and multi-scale feature fusion module. The feature extraction module and volumetric images generation module are connected using multi-scale feature fusion module. The input of the model is a single X-ray image. The feature extraction module can be used to extract 2D feature information and optimize features adaptively. The New Inception module used in the volumetric images generation module can obtain more detailed results. \\n\\n3. Methodology \\n\\nIn this paper, we propose a self-supervised learning-based single X- ray image reconstruction algorithm, Semi-XctNet, which aims to address the current imbalance in medical imaging data by using a small amount of X-ray/CT paired data and a large amount of unlabeled X-ray data. The labeled input and unlabeled input are single X-ray \\n\\nimage, ground truth Y \\\\ensuremath{\\\\in} R C{\\\\texttimes}H{\\\\texttimes}W is CT volumetric images. A small amount of labeled data will be input into the teacher model to perform supervised training. In addition, the framework introduces a super- resolution reconstruction network as an auxiliary module to obtain high-resolution prediction results to improve the reconstruction accuracy of the teacher model. On the other hand, semi-supervised learning based on consistent regularization has made significant progress in image classification [(\\\\ensuremath{<}\\\\ensuremath{>})24,(\\\\ensuremath{<}\\\\ensuremath{>})25]. Consequently, a learning strategy based on consistent regularization and pseudo-label is proposed for \\n\\nMain Architecture of X-ray feature extraction Module.  \\n\\nself-supervised learning of unlabeled data. As can be seen in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 1, the corresponding augmented images from unlabeled data set are obtained through different data augmentation methods and the related volumetric images are obtained through the teacher model. According to the principle of consistent regulation, the generated volumetric images is used as pseudo-label to train the student model. Furthermore, this paper proposes a multi-stage training strategy to promote the robustness of pseudo-label by continuously improving the teacher model. \\n\\nTeacher/Student modelUsing the end-to-end network model for CT volumetric images reconstruction task will lose abundant information due to the randomness of extracting X-ray feature information during down sampling operation, resulting in blurring and other phenomena in CT volumetric images reconstruction. To solve this problem, we propose a new network model, XctNet [(\\\\ensuremath{<}\\\\ensuremath{>})26], by adding a series of improved methods on the basis of the baseline model. As can be seen in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 2, a multi-scale feature fusion module is added based on the baseline model to improve the fine-grained features of the image generated by the model. On the other hand, channel attention and spatial attention mechanism [(\\\\ensuremath{<}\\\\ensuremath{>})27,(\\\\ensuremath{<}\\\\ensuremath{>})28] are taken into XctNet model. The attention module for feedforward convolutional neural network will infer relevant attention attempts from two different dimensions of channel and space in turn, thus, the obtained attention map is multiplied by the corresponding input feature map to adaptively optimize the features. In addition, XctNet, as a lightweight network model, although many novel modules are added to improve the performance of the model, it will not increase the amount of calculation. On the contrary, it can greatly progress the reconstruction performance of the model. \\n\\nAttention-based SRCNN: Under normal circumstances, the use of end-to-end neural networks for vision-related tasks will cause certain pixel loss. For generating CT volumetric images from a single X-ray image, since the X-ray only has a small amount of spatial information relationship corresponding to the CT volumetric image, a lot of pixel information will be lost in the reconstruction process. In this paper, an attention mechanism is added to the main network structure of SRCNN [(\\\\ensuremath{<}\\\\ensuremath{>})29] to enhance the reconstruction accuracy of the model and use it as an assistant model to reconstruct the generated volume images. The network mainly contains three convolution modules. Among them, the number of input channels is 128 {\\\\texttimes} 128 {\\\\texttimes} 128 and the convolution kernel and padding layer parameters of each layer adopt different sizes through calculation to ensure that the final output is consistent with the size of the input. \\n\\nAlgorithm 1. Semi-supervised learning for volumetric images reconstruction. \\n\\nTo better generate medical reconstruction images, a loss function based on similarity measurement will be used in the self-supervised learning task. The ground truth YP provided for self-supervised learning is pseudo-labels generated by teacher model, Ypre represent volumetric images generated by teacher model. The optimization used in self-supervised learning process can be seen as Eq. (\\\\ensuremath{<}\\\\ensuremath{>})(1), among them, LGCC(Ypre, YP) is the similarity measurement function, which is used to evaluate the consistent regularization in the self-supervised training process and the \\\\ensuremath{\\\\lambda}\\\\ensuremath{\\\\Vert}w\\\\ensuremath{\\\\Vert}2 is the spatial norm regularization term, which is used to constrain the spatial smoothness of the generated volumetric image and improve the anti-interference ability of the model. \\n\\n(1) \\n\\nVoxelMorph [(\\\\ensuremath{<}\\\\ensuremath{>})32] summarized the similarity of images used to measure the image, using mean squared voxel difference as the evaluation metric for images that are susceptible to grayscale distribution and contrast and the ground truth used in the self-supervised learning process is pseudo-labels. The poor quality of the pseudo-labels at the beginning of training may adversely affect the training process. Consequently, this article uses global cross-correlation (GCC) as the similarity measure function to improve the robustness of the training process, as shown in Eq. (\\\\ensuremath{<}\\\\ensuremath{>})(2). \\n\\n(2) \\n\\n(3)  \\n\\nIn the supervised learning task, the teacher and assistant models will be trained and the teacher model will be fine-tuned. Therefore, the loss function of this part includes two parts: The loss function shown in Eq. (\\\\ensuremath{<}\\\\ensuremath{>})(4) used in the teacher/agnostic model is MSE (Mean Squared Error). The assistant model uses the Smooth L1 loss function for optimization iterations, as shown in Eq. (\\\\ensuremath{<}\\\\ensuremath{>})(5) and (6). \\n\\n(4)  \\n\\n(5)  \\n\\n(6) \\n\\nIn summary, the core of the algorithm in this paper is to implement a semi-supervised learning-based CT volumetric image reconstruction task by combining GCC and an uncertainty loss function. On the other hand, a multi-stage training strategy is proposed in this paper, which can obtain more accurate generation results through multiple rounds of training, thus helping doctors to perform more accurate preoperative or intraoperative planning tasks in practical clinical applications. \\n\\n4. Experimental design \\n\\nIn order to evaluate the performance of the model, the predicted reconstruction results from the Semi-XctNet model are evaluated on the test set. This paper uses four evaluation functions for model evaluation, namely: MSE (mean squared error), MAE (mean absolute error), SSIM (structural similarity measurement) and PSNR (peak signal noise ratio). MSE and MAE are used to evaluate the deviation between the predicted reconstruction result and the target value. The image evaluation metric SSIM, incorporating the information of luminance, contrast and structures, is used to evaluate the degree of similarity between images. The commonly used PSNR is applied to evaluate the quality of our reconstructed volumetric images. Generally, reconstructed volumetric images with better structure and higher resolution will have higher SSIM and \\n\\nFig. 3. Volumetric images comparison of ReconNet, XctNet and Semi-XctNet on LIDC-IDRI data set. (a){\\\\textendash}(d) represent the ground truth of a randomly selected 2D projection and the corresponding reconstruction results from different network, respectively. \\n\\nPSNR values. \\n\\nThe model proposed in this paper are trained using Pytorch on a device with three NVIDIA Tesla V100 graphic processing units. Firstly, the baseline model is trained in the labeled data set and then the model is applied as the first iteration of teacher model to provide pseudo-labels for teacher model training. Secondly, an adaptive learning rate is adopted to adjust the learning rate during the training process according to the training situation of the network during the training process. Each round of training iterates 32 epochs, for a total of four rounds of training. Those models obtained in the three rounds is named as ResXct (baseline model), CBAM/ECAXct and XctNet. \\n\\n5. Results and discussion \\n\\nAs shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3, the performance of different models is shown in the test set. The HU (Hounsfiled Unit) value reflects the degree of tissue absorption of X-rays. Values within different ranges can represent different organs. Since our algorithm reconstructs the entire thoracic data, in order to better visualize the reconstruction effect, we adaptively display the reconstruction effect according to the HU value of the bony area. (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3 (a) reveals the ground truth of a 2D projection randomly selected from the test set and the visualization results of 3D bony areas generated for the convenience of observing the reconstruction effect. (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3 (b){\\\\textendash}(c) respectively represent the volumetric images and 3D bone region visualization results obtained based on the supervised learning model. It is worth noting that, for the convenience of comparison, the supervised learning dataset and semi-supervised dataset on which the models presented are of the same order of magnitude. The number of slices shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3 are 5, 25, 45, 65 and 85 respectively. It can be seen that the results reconstructed by XctNet network model are more accurate. (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 3 (d) shows the volumetric images and 3D visualization of bony regions based on the semi-supervised learning strategy proposed in this paper. From the reconstruction results, we can see that the details of the reconstruction results from the Semi-XctNet model are richer than \\n\\nReconNet [(\\\\ensuremath{<}\\\\ensuremath{>})8]. In order to better show the superiority of the reconstruction results of Semi-XctNet, four evaluation functions have adopted to verify the reconstruction results on the test set. Among them, the evaluation method we use is to evaluate each test set data which contain 128 \\n\\nThe Reconstruction Results Based on Semi-supervised Learning are Evaluated on the Verification Set.   \\n\\nThis paper mainly discusses how to solve the problem of difficult labeling of current medical data from the training method of semi- supervised learning. To achieve our expected results, a series of strategies are adopted to improve the reconstruction accuracy of the model. \\n\\nMulti-stage Semi-Supervised Learning: Pseudo-label is the main method used in this paper to guide model training on unlabeled data. Therefore, the accuracy of pseudo-label will largely determine the reconstruction quality of semi-supervised learning. In this paper, a multi-stage semi-supervised learning strategy is designed to endow semi-supervised learning with better performance by constantly \\n\\nMulti-stage semi-supervised learning training results.  \\n\\nFig. 4. Model performance of semi-supervised learning with different numbers of labeled data. The red part represents the performance of the model based on semi-supervised learning, and the black part represents the performance of the model based on supervised learning. \\n\\nFig. 5. Validation results of appending assistant modules. The degree of change in image entropy is closely related to the content and gray value of the image. It can be seen that the richer the content of the sliced image, the higher the image information entropy, that is, the color of the entropy map tends to be warmer. \\n\\nchanging the teacher network model. The effectiveness of this strategy is verified as shown in (\\\\ensuremath{<}\\\\ensuremath{>})Table 4. First, the baseline model is adopted to perform pre-training on the labeled dataset in the first stage. Second, the pre-trained model is used from the first stage as a teacher network to guide the training of the CBAM/ECAXct model in the second stage. This training strategy is also adopted in the subsequent rounds of training, and our final training model, Semi-XctNet, can be obtained. From the evaluation results in (\\\\ensuremath{<}\\\\ensuremath{>})Table 4, the reconstructed model is gradually increasing with the increase of the number of iterations. Therefore, it can be proved that the semi-supervised learning method based on the multi-stage training strategy can effectively improve the reconstruction performance of the model. \\n\\nThe performance of the supervised learning-based model tends to increase as the amount of data increases. This phenomenon also demonstrates that a semi-supervised learning-based model for volumetric data reconstruction can achieve satisfactory results with only a small amount of annotated data. It is worth noting that this result is not only applicable to the reconstruction task in this paper, but also to other \\n\\nFig. 6. Visualization of volumetric image reconstruction results based on 2D projection. (a){\\\\textendash}(b) are the 3D visualization results of the Semi-XctNet model combined with the assistant module and the Semi-XctNet model without the assistant module on three randomly selected volumetric data, respectively. \\n\\nmedical clinical applications where it is difficult to annotate the data. Availability of Assistant modules: To reduce the pixel loss caused by using the end-to-end neural network model for the volumetric image reconstruction task. An assistant module based on super-resolution reconstruction technique is used to enhance the image quality of pseudo-label, thereby improving the reconstruction effect based on semi-supervised learning. As shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 5, we randomly selected an example from the test set and showed some slice image data generated by the Semi-XctNet model with or without assistant module. The numbers of these slice data are 7, 43, 67, 91 and 111 respectively. From the heat map of the volumetric images, the model combined with the assistant module has a clearer division of image details. The volume data reconstructed from the network contains 128 slice data. From the extracted slices, the reconstruction quality of the whole volume data can be observed. As can be seen from (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 5 (a){\\\\textendash}(b), the overall reconstruction quality of volumetric data can be further improved in combination with the assistant module. \\n\\nFurthermore, in order to verify the specificity and sensitivity of the Semi-XctNet with assistant model, we randomly selected three groups of test samples and performed 3D visualization of their reconstruction results, which are shown in (\\\\ensuremath{<}\\\\ensuremath{>})Fig. 6. From the three sets of reconstruction results, more detailed information can be reconstructed by combining the assistant module. Specifically, the network model combined with the assistant module allows a more accurate reconstruction of the bony regions and each region (thoracic vertebrae, ribs, etc.) can also be well observed from the visualization results. Thus, it can be demonstrated that the combination of the assistant modules can greatly improve the specificity and sensitivity of the model. \\n\\nThe effectiveness of the semi-XctNet model can be verified by the various arguments mentioned above. From the performance comparison of supervised and semi-supervised learning, it can be found that semi- supervised learning has the potential to meet or even exceed the performance of supervised learning. In addition, we propose an auxiliary module to promote the accuracy of pseudo-labels, and the results are obvious from the comparison of model reconstruction performance with or without the assistant module. However, this dataset used in this paper obtained from different sources and the data format is not highly uniform. In addition, this data has only raw CT volume data and no clinical counterpart of X-ray data. Using the network model trained in this paper for real clinical applications may lead to poor generalization performance, which is one of the limitations of this paper. However, this limitation is caused by the data and is not limited by the network model proposed in this paper. Later, we will consider collecting experimental data from clinical pairs for in-depth study from a clinical perspective, or combining image style transfer algorithms to transfer features from X- rays to 2D projection images to generate the corresponding volume data. \\n\\n6. Conclusion \\n\\nIn this paper, a novel and effective semi-supervised learning algorithm is proposed for the task of reconstructing volumetric CT images from a single X-ray image. The whole framework adopts the teacher- assistant-student scheme for training and optimizes the supervised loss and self-supervised loss by combining the image similarity and uncertainty factor. To further enhance the robustness of pseudo-labels, a multi-stage training strategy is proposed to ameliorate the performance of the teacher network. On the other hand, an assistant module is added in this paper to increase the accuracy of pseudo-labels, so that the student model can be guided to generate more accurate reconstructed images. The effectiveness of our method is demonstrated by comprehensive experimental analysis on public datasets. Furthermore, our proposed semi-supervised learning reconstruction algorithm, as a general algorithm, can be applied to other semi-supervised medical image analysis tasks. \\n\\nDeclaration of competing interest \\n\\nThe authors declare no competing interest. \\n\\nAcknowledgment \\n\\nThis work was supported in part by National Natural Science Foundation of China (Grant No. 62003330, No.62050410349), Shenzhen Fundamental Research Funds (Grant No. JCYJ20220818101608019 No.JCYJ20190807170407391), Natural Science Foundation of Guang-dong Province(Grant No. 2019A1515011699) and Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institute of Advanced Technology. \\n\\nReferences \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.txt'}),\n",
       " Document(page_content='Block-NeRF: Scalable Large Scene Neural View Synthesis \\n\\nBen P. Mildenhall3 \\n\\nVincent Casser2 \\n\\nPratul Srinivasan3 \\n\\nXinchen Yan2 Jonathan T. Barron3 \\n\\nSabeek Pradhan2 Henrik Kretzschmar2 \\n\\nFigure 1. Block-NeRF is a method that enables large-scale scene reconstruction by representing the environment using multiple compact NeRFs that each fit into memory. At inference time, Block-NeRF seamlessly combines renderings of the relevant NeRFs for the given area. In this example, we reconstruct the Alamo Square neighborhood in San Francisco using data collected over 3 months. Block-NeRF can update individual blocks of the environment without retraining on the entire scene, as demonstrated by the construction on the right. Video results can be found on the project website waymo.com/research/block-nerf. \\n\\nWe present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco. \\n\\n1. Introduction \\n\\nReconstructing large-scale environments enables several important use-cases in domains such as autonomous driving [30,43,69] and aerial surveying [14,33]. For example, a high-fidelity map of the operating domain can serve as a prior for robot navigation. Large-scale scene reconstructions can be used for closed-loop robotic simulations [13]. Autonomous driving systems are commonly evaluated by re-simulating previously encountered scenarios. Any deviation from the recorded encounter, however, may change the vehicle{\\\\textquoteright}s trajectory, requiring high-fidelity novel view renderings along the altered path. Scene conditioned NeRFs can further augment simulation scenarios by changing environmental lighting conditions, such as camera exposure, weather, or time of day. \\n\\nReconstructing such large-scale environments introduces additional challenges, including the presence of transient objects (cars and pedestrians), limitations in model capacity, \\n\\nalong with memory and compute constraints. Furthermore, training data for such large environments is highly unlikely to be collected in a single capture under consistent conditions. Rather, data for different parts of the environment may need to be sourced from different data collection efforts, introducing variance in both scene geometry (e.g., construction work and parked cars), as well as appearance (e.g., weather conditions and time of day). \\n\\nWe extend NeRF with appearance embeddings and learned pose refinement to address the environmental changes and pose errors in the collected data. We additionally add exposure conditioning to provide the ability to modify the exposure during inference. We refer to this modified model as a Block-NeRF. Scaling up the network capacity of Block-NeRF enables the ability to represent increasingly large scenes. However this approach comes with a number of limitations; rendering time scales with the size of the network, networks can no longer fit on a single compute device, and updating or expanding the environment requires retraining the entire network. \\n\\nTo address these challenges, we propose dividing up large environments into individually trained Block-NeRFs, which are then rendered and combined dynamically at inference time. Modeling these Block-NeRFs independently allows for maximum flexibility, scales up to arbitrarily large environments and provides the ability to update or introduce new regions in a piecewise manner without retraining the entire environment as demonstrated in Figure 1. To compute a target view, only a subset of the Block-NeRFs are rendered and then composited based on their geographic location compared to the camera. To allow for more seamless compositing, we propose an appearance matching technique which brings different Block-NeRFs into visual alignment by optimizing their appearance embeddings. \\n\\n2. Related Work \\n\\nResearchers have been developing and refining techniques for 3D reconstruction from large image collections for decades [1,16,31,46,56,78], and much current work relies on mature and robust software implementations such as COLMAP to perform this task [54]. Nearly all of these reconstruction methods share a common pipeline: extract 2D image features (such as SIFT [37]), match these features across different images, and jointly optimize a set of 3D points and camera poses to be consistent with these matches (the well-explored problem of bundle adjustment [23,64]). Extending this pipeline to city-scale data is largely a matter of implementing highly robust and parallelized versions of these algorithms, as explored in work such as Photo Tourism [56] and Building Rome in a Day [1]. Core graphics research has also explored breaking up scenes for fast high quality rendering [36]. \\n\\nFigure 2. The scene is split into multiple Block-NeRFs that are each trained on data within some radius (dotted orange line) of a specific Block-NeRF origin coordinate (orange dot). To render a target view in the scene, the visibility maps are computed for all of the NeRFs within a given radius. Block-NeRFs with low visibility are discarded (bottom Block-NeRF) and the color output is rendered for the remaining blocks. The renderings are then merged based on each block origin{\\\\textquoteright}s distance to the target view. \\n\\nThese approaches typically output a camera pose for each input image and a sparse 3D point cloud. To get a complete 3D scene model, these outputs must be further processed by a dense multi-view stereo algorithm (e.g., PMVS [18]) to produce a dense point cloud or triangle mesh. This process presents its own scaling difficulties [17]. The resulting 3D models often contain artifacts or holes in areas with limited texture or specular reflections as they are challenging to triangulate across images. As such, they frequently require further postprocessing to create models that can be used to render convincing imagery [55]. However, this task is mainly the domain of novel view synthesis, and 3D reconstruction techniques primarily focus on geometric accuracy. \\n\\nIn contrast, our approach does not rely on large-scale SfM to produce camera poses, instead performing odome-try using various sensors on the vehicle as the images are collected [63]. \\n\\nGiven a set of input images of a given scene and their camera poses, novel view synthesis seeks to render observed scene content from previously unobserved viewpoints, allowing a user to navigate through a recreated environment with high visual fidelity. \\n\\nVolumetric Scene Representations. Recent view synthesis work has focused on unifying reconstruction and rendering and learning this pipeline end-to-end, typically using a volumetric scene representation. Methods for rendering small baseline view interpolation often use feed-forward networks to learn a mapping directly from input images to an output volume [15, 77], while methods such as Neural Volumes [35] that target larger-baseline view synthesis run a global optimization over all input images to reconstruct every new scene, similar to traditional bundle adjustment. \\n\\nNeural Radiance Fields (NeRF) [40] combines this single-scene optimization setting with a neural scene representation capable of representing complex scenes much more efficiently than a discrete 3D voxel grid; however, its rendering model scales very poorly to large-scale scenes in terms of compute. Followup work has proposed making NeRF more efficient by partitioning space into smaller regions, each containing its own lightweight NeRF network [42, 47, 48]. Unlike our method, these network ensembles must be trained jointly, limiting their flexibility. Another approach is to provide extra capacity in the form of a coarse 3D grid of latent codes [34]. This approach has also been applied to compress detailed 3D shapes into neural signed distance functions [61] and to represent large scenes using occupancy networks [45]. \\n\\nConcurrent works Mega-NeRF [65] and CityNeRF [67] utilize NeRFs to represent large scenes. Mega-NeRF splits data captured from drones into multiple partitions to train specialized NeRFs. CityNeRF learns a multi-scale representation from satellite imagery. \\n\\nWe build our Block-NeRF implementation on top of mip-NeRF [3], which improves aliasing issues that hurt NeRF{\\\\textquoteright}s performance in scenes where the input images observe the scene from many different distances. We incorporate techniques from NeRF in the Wild (NeRF-W) [38], which adds a latent code per training image to handle inconsistent scene appearance when applying NeRF to landmarks from the Photo Tourism dataset. NeRF-W creates a separate NeRF for each landmark from thousands of images, whereas our approach combines many NeRFs to reconstruct a coherent large environment from millions of images. Our model also incorporates a learned camera pose refinement which has been explored in previous works [32,58,66,70,71]. \\n\\nSome NeRF-based methods use segmentation data to \\n\\nFigure 3. Our model is an extension of the model presented in mip-NeRF [3]. The first MLP f predicts the density  for a position x in space. The network also outputs a feature vector that is concatenated with viewing direction d, the exposure level, and an appearance embedding. These are fed into a second MLP fc that outputs the color for the point. We additionally train a visibility network fv to predict whether a point in space was visible in the training views, which is used for culling Block-NeRFs during inference. \\n\\nisolate and reconstruct static [68] or moving objects (such as people or cars) [43,74] across video sequences. As we focus primarily on reconstructing the environment itself, we choose to simply mask out dynamic objects during training. \\n\\nCamera simulation has become a popular data source for training and validating autonomous driving systems on interactive platforms [2,27]. Early works [13,19,50,53] synthesized data from scripted scenarios and manually created 3D assets. These methods suffered from domain mismatch and limited scene-level diversity. Several recent works tackle the simulation-to-reality gaps by minimizing the distribution shifts in the simulation and rendering pipeline. Kar et al.[26] and Devaranjan et al.[12] proposed to minimize the scene-level distribution shift from rendered outputs to real camera sensor data through a learned scenario generation framework. Richter et al.[49] leveraged intermediate rendering buffers in the graphics pipeline to improve photorealism of synthetically generated camera images. \\n\\nTowards the goal of building photo-realistic and scalable camera simulation, prior methods [9, 30, 69] leverage rich multi-sensor driving data collected during a single drive to reconstruct 3D scenes for object injection [9] and novel view synthesis [69] using modern machine learning techniques, including image GANs for 2D neural rendering. Relying on a sophisticated surfel reconstruction pipeline, SurfelGAN [69] is still susceptible to errors in graphical reconstruction and can suffer from the limited range and vertical field-of-view of LiDAR scans. In contrast to existing efforts, our work tackles the 3D rendering problem and is capable of modeling the real camera data captured from multiple drives under varying environmental conditions, such as weather and time of day, which is a prerequisite for reconstructing large-scale areas. \\n\\n3. Background \\n\\nWe build upon NeRF [40] and its extension mip-NeRF [3]. Here, we summarize relevant parts of these methods. For details, please refer to the original papers. \\n\\nNeural Radiance Fields (NeRF) [40] is a coordinate-based neural scene representation that is optimized through a differentiable rendering loss to reproduce the appearance of a set of input images from known camera poses. After optimization, the NeRF model can be used to render previously unseen viewpoints. \\n\\nThe NeRF scene representation is a pair of multilayer perceptrons (MLPs). The first MLP f takes in a 3D position x and outputs volume density  and a feature vector. This feature vector is concatenated with a 2D viewing direction d and fed into the second MLP fc, which outputs an RGB color c. This architecture ensures that the output color can vary when observed from different angles, allowing NeRF to represent reflections and glossy materials, but that the underlying geometry represented by  is only a function of position. \\n\\nEach pixel in an image corresponds to a ray r(t)= o + td through 3D space. To calculate the color of r, NeRF randomly samples distances \\\\{ti\\\\}N i=0 along the ray and passes the points r(ti) and direction d through its MLPs to calculate i and ci. The resulting output color is \\n\\n(1) \\n\\n(2) \\n\\nThe full implementation of NeRF iteratively resamples the points ti (by treating the weights wi as a probability distribution) in order to better concentrate samples in areas of high density. \\n\\nTo enable the NeRF MLPs to represent higher frequency detail [62], the inputs x and d are each preprocessed by a componentwise sinusoidal positional encoding PE: \\n\\nwhere Lis the number of levels of positional encoding. \\n\\n(4) \\n\\nreferred to as an integrated positional encoding. \\n\\n4. Method \\n\\nTraining a single NeRF does not scale when trying to represent scenes as large as cities. We instead propose splitting the environment into a set of Block-NeRFs that can be independently trained in parallel and composited during inference. This independence enables the ability to expand the environment with additional Block-NeRFs or update blocks without retraining the entire environment (see Figure 1). We dynamically select relevant Block-NeRFs for rendering, which are then composited in a smooth manner when traversing the scene. To aid with this compositing, we optimize the appearances codes to match lighting conditions and use interpolation weights computed based on each Block-NeRF{\\\\textquoteright}s distance to the novel view. \\n\\nThe individual Block-NeRFs should be arranged such that they collectively achieve full coverage of the target environment. We typically place one Block-NeRF at each intersection, covering the intersection itself and any connected street 75\\\\% of the way until it converges into the next intersection (see Figure 1). This results in a 50\\\\% overlap between any two adjacent blocks on the connecting street segment, making appearance alignment easier between them. We make sure to train each Block-NeRF on data that is confined to a geographic area. This can be automated and only relies on basic map data, such as OpenStreetMap [22]. \\n\\nOther placement heuristics are conceivable. For example, for some of our experiments, we place Block-NeRFs along a street segment at uniform distances and define the block size to be a sphere around the origin of the blocks (see Figure 2). \\n\\nGiven that different parts of our data may be captured under different environmental conditions, we follow NeRF-W [38] and use Generative Latent Optimization [5] to optimize per-image appearance embedding vectors, as shown in Figure 3. This allows the NeRF to explain away several appearance-changing conditions, such as varying weather and lighting. We can additionally manipulate these appearance embeddings to interpolate between different conditions observed in the training data (such as cloudy versus clear skies, or \\n\\nFigure 4. The appearance codes allow the model to represent different lighting and weather conditions. \\n\\nday and night). Examples of rendering with different appearances can be seen in Figure 4. In {\\\\textsection} 4.3.3, we use test-time optimization over these embeddings to match the appearance of adjacent Block-NeRFs, which is important when combining multiple renderings. \\n\\nAlthough we assume that camera poses are provided, we find it advantageous to learn regularized pose offsets for further alignment. Pose refinement has been explored in previous NeRF based models [32,58,66,71]. These offsets are learned per driving segment and include both a translation and a rotation component. We optimize these offsets jointly with the NeRF itself, significantly regularizing the offsets in the early phase of training to allow the network to first learn a rough structure prior to modifying the poses. \\n\\nTraining images may be captured across a wide range of exposure levels, which can impact NeRF training if left unaccounted for. We find that feeding the camera exposure information to the appearance prediction part of the model allows the NeRF to compensate for the visual differences (see Figure 3). Specifically, the exposure information is processed as  PE(shutter speed  analog gain/t) where  PE is a sinusoidal positional encoding with 4 levels, and t is a scaling factor (we use 1,000 in practice). An example of different learned exposures can be found in Figure 5. \\n\\nWhile the appearance embeddings account for variation in appearance, we assume that the scene geometry is consistent across the training data. Movable objects (e.g. cars, pedestrians) typically violate this assumption. We therefore use a semantic segmentation model [10] to ignore masks of common movable objects during training. Note that this does not account for changes in otherwise static parts of the environment, e.g. construction, it accommodates most common types of geometric inconsistency. \\n\\nWhen merging multiple Block-NeRFs, it can be useful to know whether a specific region of space was visible to a given NeRF during training. We extend our model with an additional small MLP fv that is trained to learn an approximation of the visibility of a sampled point (see Figure 3). For each sample along a training ray, fv takes in the location and view direction and regresses the corresponding transmittance of the point (Ti in Equation 2). The model is trained alongside f, which provides supervision. Transmittance represents how visible a point is from a particular input camera: points in free space or on the surface of the first intersected object will have transmittance near 1, and points inside or behind the first visible object will have transmittance near 0. If a point is seen from some viewpoints but not others, the regressed transmittance value will be the average over all training cameras and lie between zero and one, indicating that the point is partially observed. Our visibility prediction is similar to the visibility fields proposed by Srinivasan et al.[57]. However, they used an MLP to predict visibility to environment lighting to recover a relightable NeRF model, while we predict visibility to training rays. \\n\\nThe visibility network is small and can be run independently from the color and density networks. This proves useful when merging multiple NeRFs, since it can help to determine whether a specific NeRF is likely to produce meaningful outputs for a given location, as explained in {\\\\textsection} 4.3.1. The visibility predictions can also be used to determine locations to perform appearance matching between two NeRFs, as detailed in {\\\\textsection} 4.3.3. \\n\\nThe environment can be composed of an arbitrary number of Block-NeRFs. For efficiency, we utilize two filtering mechanisms to only render relevant blocks for the given target viewpoint. We only consider Block-NeRFs that are within a set radius of the target viewpoint. Additionally, for each of these candidates, we compute the associated visibility. If the mean visibility is below a threshold, we discard the Block-NeRF. An example of visibility filtering is provided in Figure 2. Visibility can be computed quickly because its network is independent of the color network, and \\n\\nFigure 5. Our model is conditioned on exposure, which helps account for exposure changes present in the training data. This allows users to alter the appearance of the output images in a human-interpretable manner during inference. \\n\\nit does not need to be rendered at the target image resolution. After filtering, there are typically one to three Block-NeRFs left to merge. \\n\\nWe render color images from each of the filtered Block-NeRFs and interpolate between them using inverse distance weighting between the camera origin c and the centers xi of each Block-NeRF. Specifically, we calculate the respective weights as wi / distance(c, xi)p , where p influences the rate of blending between Block-NeRF renders. The interpolation is done in 2D image space and produces smooth transitions between Block-NeRFs. We also explore other interpolation methods in {\\\\textsection} 5.4. \\n\\nWe can control the appearance of our learned models by an appearance latent code after the Block-NeRF has been trained. These codes are randomly initialized during training and the same code therefore typically leads to different appearances when fed into different Block-NeRFs. This is undesirable when compositing as it may lead to inconsistencies between views. Given a target appearance in one of the Block-NeRFs, we match its appearance in the remaining blocks. To this end, we first select a 3D matching location between pairs of adjacent Block-NeRFs. The visibility prediction at this location should be high for both Block-NeRFs. Given the matching location, we freeze the Block-NeRF network weights and only optimize the appearance code of the target in order to reduce the ` 2 loss between the respective area renders. This optimization is quick, converging within 100 iterations. While not necessarily yielding perfect alignment, this procedure aligns most global and low-frequency attributes of the scene, such as time of day, color balance, and weather, which is a prerequisite for successful compositing. Figure 6 shows an example optimization, where appearance matching turns a daytime scene into nighttime to match the adjacent Block-NeRF. Starting from a root Block-NeRF, we propagate the optimized appearance through the scene by \\n\\niteratively optimizing the appearance of its neighbors. If multiple blocks surrounding a target Block-NeRF have already been optimized, we consider each of them when computing the loss. \\n\\n5. Results and Experiments \\n\\nIn this section we will discuss our datasets and experiments. We provide the architectural and optimization specifics in the supplement. The supplement also provides comparisons to reconstructions from COLMAP [54], a traditional Structure from Motion approach. This reconstruction is sparse and fails to represent reflective surfaces and the sky. \\n\\nWe perform experiments on datasets that we collected for novel view synthesis of large-scale scenes using data collection vehicles driving on public roads. Existing public large-scale driving datasets are not designed for the task of view synthesis. For example, some datasets lack sufficient camera coverage (e.g., KITTI [21], Cityscapes [11]) or prioritize visual diversity over repeated observations of a target area (e.g., NuScenes [7], Waymo Open Dataset [60], Argov-erse [8]). Instead, these datasets are typically designed for tasks such as object detection and tracking. \\n\\nOur dataset includes both long-term sequence data (100 s or more) and distinct sequences captured repeatedly in a particular target area over a period of several months. We use image data captured by 12 cameras, where 8 cameras mounted on the roof of the car provide a 360{\\\\textdegree} surround view, and 4 cameras located at the front of the vehicle point forward and sideways. Each camera captures images at 10 Hz and stores a scalar exposure value. The vehicle pose is known and all cameras are calibrated. We calculate the corresponding camera ray origins and directions in a common coordinate system, accounting for the rolling shutter of the cameras. As described in {\\\\textsection} 4.2.4, we use a semantic segmentation model [10] to detect movable objects. \\n\\nSan Francisco Alamo Square Dataset. We select San Francisco{\\\\textquoteright}s Alamo Square neighborhood as the target area for our scalability experiments. The dataset spans an area of approximately 960 m  570 m, and was recorded in June, July, and August of 2021. We divide this dataset into 35 Block-NeRFs. Example renderings and Block-NeRF placements can be seen in Figure 1. To best appreciate the scale of the reconstruction, please refer to supplementary videos. Each Block-NeRF was trained on data from 38 to 48 different data collection runs, adding up to a total driving time of 18 to 28 minutes each. After filtering out some redundant image captures (e.g. stationary captures), each Block-NeRF is trained on between 64,575 to 108,216 images. The overall dataset is composed of 13.4h of driving time sourced from 1,330 different data collection runs, with a total of 2,818,745 training images. \\n\\nFigure 6. When rendering scenes based on multiple Block-NeRFs, we use appearance matching to obtain a consistent appearance across the scene. Given a fixed target appearance for one of the Block-NeRFs (left image), we optimize the appearances of the adjacent Block-NeRFs to match. In this example, appearance matching produces a consistent night appearance across Block-NeRFs. \\n\\nSan Francisco Mission Bay Dataset. We choose San Francisco{\\\\textquoteright}s Mission Bay District as the target area for our baseline, block size, and placement experiments. Mission Bay is an urban environment with challenging geometry and reflective facades. We identified a long stretch on Third Street with far-range visibility, making it an interesting test case. Notably, this dataset was recorded in a single capture in November 2020, with consistent environmental conditions allowing for simple evaluation. This dataset was recorded over 100 s, in which the data collection vehicle traveled 1.08 km and captured 12,000 total images from 12 cameras. We will release this single-capture dataset to aid reproducibility. \\n\\nTable 1. Ablations of different Block-NeRF components on a single intersection in the Alamo Square dataset. We show the performance of mip-NeRF as a baseline, as well as the effect of removing individual components from our method. \\n\\nWe ablate our model modifications on a single intersection from the Alamo Square dataset. We report PSNR, SSIM, and LPIPS [76] metrics for the test image reconstructions in Table 1. The test images are split in half vertically, with the appearance embeddings being optimized on one half and tested on the other. We also provide qualitative examples in Figure 7. Mip-NeRF alone fails to properly reconstruct the scene and is prone to adding non-existent geometry and cloudy artifacts to explain the differences in appearance. When our method is not trained with appearance embeddings, these artifacts are still present. If our method is not trained with pose optimization, the resulting scene is blurrier and can contain duplicated objects due to pose misalignment. Finally, the exposure input marginally improves the reconstruction, but more importantly provides us with the ability \\n\\nto change the exposure during inference. \\n\\nTable 2. Comparison of different numbers of Block-NeRFs for reconstructing the Mission Bay dataset. Splitting the scene into multiple Block-NeRFs improves the reconstruction accuracy, even when holding the total number of weights constant (bottom section). The number of blocks determines the size of the area each block is trained on and the relative compute expense at inference time. \\n\\nWe compare performance on our Mission Bay dataset versus the number of Block-NeRFs used. We show details in Table 2, where depending on granularity, the Block-NeRF sizes range from as small as 54 m to as large as 544 m.We ensure that each pair of adjacent blocks overlaps by 50\\\\% and compare other overlap percentages in the supplement. All were evaluated on the same set of held-out test images spanning the entire trajectory. We consider two regimes, one where each Block-NeRF contains the same number of weights (top section) and one where the total number of weights across all Block-NeRFs is fixed (bottom section). In both cases, we observe that increasing the number of models improves the reconstruction metrics. In terms of computational expense, parallelization during training is trivial as each model can be optimized independently across devices. At inference, our method only requires rendering Block-NeRFs near the target view. Depending on the scene and NeRF layout, we typically render between one to three NeRFs. We report the relative compute expense in each setting without assuming any parallelization, which would also be possible and lead to an additional speed-up. We find that splitting the scene into multiple lower capacity models \\n\\nFigure 7. Model ablation results on multi segment data. Appearance embeddings help the network avoid adding cloudy geometry to explain away changes in the environment like weather and lighting. Removing exposure slightly decreases the accuracy. The pose optimization helps sharpen the results and removes ghosting from repeated objects, as observed with the telephone pole in the first row. \\n\\ncan reduce the overall computational cost as not all of the models need to be evaluated (see bottom section of Table 2). \\n\\nTable 3. Comparison of interpolation methods. For our flythrough video results, we opt for 2D inverse distance weighting (IDW) as it produces temporally consistent results. \\n\\nWe explore interpolation techniques in Table 3. The simple method of only rendering the nearest Block-NeRF to the camera requires the least amount of compute but results in harsh jumps when transitioning between blocks. These transitions can be smoothed by using inverse distance weighting (IDW) between the camera and Block-NeRF centers, as described in {\\\\textsection} 4.3.2. We also explored a variant of IDW where the interpolation was performed over projected 3D points predicted by the expected Block-NeRF depth. This method suffers when the depth prediction is incorrect, leading to artifacts and temporal incoherence. \\n\\nFinally, we experiment with weighing the Block-NeRFs based on per-pixel and per-image predicted visibility. This produces sharper reconstructions of further-away areas but is prone to temporal inconsistency. Therefore, these methods are best used only when rendering still images. We provide further details in the supplement. \\n\\n6. Limitations and Future Work \\n\\n7. Conclusion \\n\\nWe propose Block-NeRF, a method that reconstructs arbitrarily large environments using NeRFs. We demonstrate the efficacy of the method by building an entire neighborhood in San Francisco from 2.8M images, forming the largest neural scene representation to date. We accomplish this scale by splitting our representation into multiple blocks that can be optimized independently. At such a scale, the data collected will necessarily have transient objects and variations in appearance, which we account for by modifying the underlying NeRF architecture. We hope that this can inspire future work in large-scale scene reconstruction using modern neural rendering methods. \\n\\nReferences \\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.txt'}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery \\n\\nAbstract. Reconstruction of the soft tissues in robotic surgery from endoscopic stereo videos is important for many applications such as intra-operative navigation and image-guided robotic surgery automation. Previous works on this task mainly rely on SLAM-based approaches, which struggle to handle complex surgical scenes. Inspired by recent progress in neural rendering, we present a novel framework for deformable tissue reconstruction from binocular captures in robotic surgery under the single-viewpoint setting. Our framework adopts dynamic neural radiance fields to represent deformable surgical scenes in MLPs and optimize shapes and deformations in a learning-based manner. In addition to non-rigid deformations, tool occlusion and poor 3D clues from a single viewpoint are also particular challenges in soft tissue reconstruction. To overcome these di\\\\\"yculties, we present a series of strategies of tool mask-guided ray casting, stereo depth-cueing ray marching and stereo depth-supervised optimization. With experiments on DaVinci robotic surgery videos, our method significantly outperforms the current state-of-the-art reconstruction method for handling various complex non-rigid deformations. To our best knowledge, this is the first work leveraging neural rendering for surgical scene 3D reconstruction with remarkable potential demonstrated. Code is available at: (\\\\ensuremath{<}https://github.com/med-air/EndoNeRF\\\\ensuremath{>})https://github.com/med-air/EndoNeRF. \\n\\nKeywords: 3D Reconstruction {\\\\textperiodcentered} Neural Rendering {\\\\textperiodcentered} Robotic Surgery. \\n\\n1 Introduction \\n\\nSurgical scene reconstruction from endoscope stereo video is an important but di\\\\\"ycult task in robotic minimally invasive surgery. It is a prerequisite for many downstream clinical applications, including intra-operative navigation and augmented reality, surgical environment simulation, immersive education, and robotic surgery automation [(\\\\ensuremath{<}\\\\ensuremath{>})2,(\\\\ensuremath{<}\\\\ensuremath{>})12,(\\\\ensuremath{<}\\\\ensuremath{>})20,(\\\\ensuremath{<}\\\\ensuremath{>})25]. Despite much recent progress [(\\\\ensuremath{<}\\\\ensuremath{>})10,(\\\\ensuremath{<}\\\\ensuremath{>})22,(\\\\ensuremath{<}\\\\ensuremath{>})28,(\\\\ensuremath{<}\\\\ensuremath{>})29,(\\\\ensuremath{<}\\\\ensuremath{>})30,(\\\\ensuremath{<}\\\\ensuremath{>})33], several key challenges still remain unsolved. First, surgical scenes are deformable with significant topology changes, requiring dynamic reconstruction to capture a high degree of non-rigidity. Second, endoscopic videos show sparse viewpoints due to constrained camera movement in confined space, resulting in limited 3D clues of soft tissues. Third, the surgical instruments always occlude part of the soft tissues, which a{\\\\textperiodcentered}ects the completeness of surgical scene reconstruction. \\n\\nPrevious works [(\\\\ensuremath{<}\\\\ensuremath{>})1,(\\\\ensuremath{<}\\\\ensuremath{>})13] explored the e{\\\\textperiodcentered}ectiveness of surgical scene reconstruction via depth estimation. Since most of the endoscopes are equipped with stereo cameras, depth can be estimated from binocular vision. Follow-up SLAM-based methods [(\\\\ensuremath{<}\\\\ensuremath{>})23,(\\\\ensuremath{<}\\\\ensuremath{>})31,(\\\\ensuremath{<}\\\\ensuremath{>})32] fuse depth maps in 3D space to reconstruct surgical scenes under more complex settings. Nevertheless, these methods either hypothesize scenes as static or surgical tools not present, limiting their practical use in real scenarios. Recent work SuPer [(\\\\ensuremath{<}\\\\ensuremath{>})8] and E-DSSR [(\\\\ensuremath{<}\\\\ensuremath{>})11] present frameworks consisting of tool masking, stereo depth estimation and SurfelWarp [(\\\\ensuremath{<}\\\\ensuremath{>})4] to perform single-view 3D reconstruction of deformable tissues. However, all these methods track deformation based on a sparse warp field [(\\\\ensuremath{<}\\\\ensuremath{>})16], which degenerates when deformations are significantly beyond the scope of non-topological changes. \\n\\nAs an emerging technology, neural rendering [(\\\\ensuremath{<}\\\\ensuremath{>})6,(\\\\ensuremath{<}\\\\ensuremath{>})27,(\\\\ensuremath{<}\\\\ensuremath{>})26] is recently developed to break through the limited performance of traditional 3D reconstruction by leveraging di{\\\\textperiodcentered}erentiable rendering and neural networks. In particular, neural radiance fields (NeRF) [(\\\\ensuremath{<}\\\\ensuremath{>})15], a popular pioneering work of neural rendering, proposes to use neural implicit field for continuous scene representations and achieves great success in producing high-quality view synthesis and 3D reconstruction on diverse scenarios [(\\\\ensuremath{<}\\\\ensuremath{>})14,(\\\\ensuremath{<}\\\\ensuremath{>})15,(\\\\ensuremath{<}\\\\ensuremath{>})17]. Meanwhile, recent variants of NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})18,(\\\\ensuremath{<}\\\\ensuremath{>})19,(\\\\ensuremath{<}\\\\ensuremath{>})21] targeting dynamic scenes have managed to track deformations through various neural representations on non-rigid objects. \\n\\nIn this paper, we endeavor to reconstruct highly deformable surgical scenes captured from single-viewpoint stereo endoscopes. We embark on adapting the emerging neural rendering framework to the regime of deformable surgical scene reconstruction. We summarize our contributions as follows: 1) To accommodate a wide range of geometry and deformation representations on soft tissues, we leverage neural implicit fields to represent dynamic surgical scenes. 2) To address the particular tool occlusion problem in surgical scenes, we design a new mask-guided ray casting strategy for resolving tool occlusion. 3) We incorporate a depth-cueing ray marching and depth-supervised optimization scheme, using stereo prior to enable neural implicit field reconstruction for single-viewpoint input. To the best of our knowledge, this is the first work introducing cutting-edge neural rendering to surgical scene reconstruction. We evaluate our method on 6 typical in-vivo surgical scenes of robotic prostatectomy. Compared with previous methods, our results exhibit great performance gain, both quantitatively and qualitatively, on 3D reconstruction and deformation tracking of surgical scenes. \\n\\n2 Method \\n\\nGiven a single-viewpoint stereo video of a dynamic surgical scene, we aim to reconstruct 3D structures and textures of surgical scenes without occlusion of surgical instruments. We denote as a sequence of input stereo video frames, where T is the total number of frames and (I , I ) is the pair of left and time of the i-th frame is i/T . We also extract binary tool masks for the ri li right images at the i-th frame. The video duration is normalized to [0, 1]. Thus, \\n\\nFig. 1: Illustration of our proposed novel approach of neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery. \\n\\nleft views to identify the region of surgical instruments. To utilize stereo clues, we estimate coarse depth maps \\\\{Di\\\\}iT=1 for the left views from the binocular captures. We follow the modeling in D-NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})21] and represent deformable surgical scenes as a canonical neural radiance field along with a time-dependent neural displacement field (cf. Sec. (\\\\ensuremath{<}\\\\ensuremath{>})2.2). In our pipeline, each training iteration consists of the following six stages: i) randomly pick a frame for training, ii) run tool-guided ray casting (cf. Sec. (\\\\ensuremath{<}\\\\ensuremath{>})2.3) to shoot camera rays into the scene, iii) sample points along each camera ray via depth-cueing ray marching (cf. Sec. (\\\\ensuremath{<}\\\\ensuremath{>})2.4), iv) send sampled points to networks to obtain color and space occupancy of each point, v) evaluate volume rendering integral on sampled points to produce rendering results, vi) optimize the rendering loss plus depth loss to reconstruct shapes, colors and deformations of the surgical scene (cf. Sec. (\\\\ensuremath{<}\\\\ensuremath{>})2.5). The overview of key components in our approach is illustrated in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})1. We will describe the detailed methods in the following subsections. \\n\\nWe represent a surgical scene as a canonical radiance field and a time-dependent displacement field. Accordingly, each frame of the surgical scene can be regarded as a deformation of the canonical field. The canonical field, denoted as F\\\\ensuremath{\\\\Theta}(x, d), is an 8-layer MLP with network parameter \\\\ensuremath{\\\\Theta}, mapping coordinates x \\\\ensuremath{\\\\in} R3 and unit view-in directions d \\\\ensuremath{\\\\in} R3 to RGB colors c(x, d) \\\\ensuremath{\\\\in} R3 and space occupancy \\\\ensuremath{\\\\sigma}(x) \\\\ensuremath{\\\\in} R. The time-dependent displacement field G\\\\ensuremath{\\\\Phi}(x, t) is encoded in another 8-layer MLP with network parameters \\\\ensuremath{\\\\Phi} and maps input space-time coordinates (x, t) into displacement between the point x at time t and the corresponding point in the canonical field. For any time t, the color and occupancy at x can be retrieved as F\\\\ensuremath{\\\\Theta}(x + G\\\\ensuremath{\\\\Phi}(x, t), d). Compared with other dynamic modeling approaches [(\\\\ensuremath{<}\\\\ensuremath{>})18,(\\\\ensuremath{<}\\\\ensuremath{>})19], a displacement field is su\\\\\"ycient to explicitly and physically express all tissue deformations. To capture high-frequency details, we use positional encoding \\\\ensuremath{\\\\gamma}({\\\\textperiodcentered}) to map the input coordinates and time into Fourier features [(\\\\ensuremath{<}\\\\ensuremath{>})24] before feeding them to the networks. \\n\\nWith scene representations, we further leverage the di{\\\\textperiodcentered}erentiable volume rendering used in NeRF to yield renderings for supervision. The di{\\\\textperiodcentered}erentiable volume rendering begins with shooting a batch of camera rays into the surgical scene from a fixed viewpoint at an arbitrary time t. Every ray is formulated as r(s) = o + sd, where o is a fixed origin of the ray, d is the pointing direction of the ray and s is the ray parameter. In the original NeRF, rays are shot towards a batch of randomly selected pixels on the entire image plane. However, there are many pixels of surgical tools on the captured images, while our goal is to reconstruct underlying tissues. Thus, training on these tool pixels is unexpected. Our main idea for solving this issue is to bypass those rays traveling through tool pixels over the training stage. We utilize binary tool masks \\\\{Mi\\\\}Ti=1, where 0 stands for tissue pixels and 1 stands for tool pixels, to inform which rays should be neglected. In this regard, we create importance maps \\\\{V\\\\}iT=1 according to Mi and perform importance sampling to avoid shooting rays for those pixels of surgical tools. Eq. ((\\\\ensuremath{<}\\\\ensuremath{>})1) exhibits the construction of importance maps, where \\\\ensuremath{\\\\otimes} is element-wise multiplication, {\\\\textperiodcentered}F is Frobenius norm and 1 is a matrix with the same shape as Mi while filled with ones: \\n\\n(1) \\n\\nThe 1 \\\\ensuremath{-} Mi term initializes the importance of tissue pixels to 1 and the importance of tool pixels to 0. To balance the sampling rate of occluded pixels across frames, the scaling term \\\\ensuremath{\\\\Lambda} specifies higher importance scaling for those tissue areas with higher occlusion frequencies. Normalizing each importance map as V i = Vi/ViF will yield a probability mass function over the image plane. During our ray casting stage for the i-th frame, we sample pixels from the distribution V i using inverse transform sampling and cast rays towards these sampled pixels. In this way, the probability of shooting rays for tool pixels is guaranteed to be zero as the importance of tool pixels is constantly zero. \\n\\nAfter shooting camera rays over tool occlusion, we proceed ray marching to sample points in the space. Specifically, we discretize each camera ray r(s) into batch of points \\\\{xj |xj = r(sj)\\\\}m j=1 by sampling a sequence of ray steps s1 \\\\ensuremath{\\\\leq} s2 \\\\ensuremath{\\\\leq} {\\\\textperiodcentered} {\\\\textperiodcentered} {\\\\textperiodcentered} \\\\ensuremath{\\\\leq} sm. The original NeRF proposes hierarchical stratified sampling to obtain \\\\{sj\\\\}m j=1. However, this sampling strategy hardly exploits accurate 3D structures when NeRF models are trained on single-view input. Drawing inspiration from early work in iso-surface rendering [(\\\\ensuremath{<}\\\\ensuremath{>})7], we create Gaussian transfer functions with stereo depth to guide point sampling near tissue surfaces. For the i-th frame, the transfer function for a ray r(s) shooting towards pixel (u, v) is formulated as: \\n\\n(2) \\n\\nThe transfer function \\\\ensuremath{\\\\delta}(s; u, v, i) depicts an impulse distribution that continuously allocates sampling weights for every location on r(s). The impulse is centered at Di[u, v], i.e., the depth at the (u, v) pixel. The width of the impulse is controlled by the hyperparameter \\\\ensuremath{\\\\xi}, which is set to a small value to mimic Dirac delta impulse. In our ray marching, are drawn from the normalized impulse distribution By this means, sampled points are concentrated around tissue surfaces, imposing stereo prior in rendering. \\n\\nOnce we obtain the sampled points in the space, the emitted color C and optical depth D of a camera ray r(s) can be evaluated by volume rendering [(\\\\ensuremath{<}\\\\ensuremath{>})5] as: \\n\\n(3) \\n\\nTo reconstruct the canonical and displacement fields from single-view captures, we optimize the network parameters \\\\ensuremath{\\\\Theta} and \\\\ensuremath{\\\\Phi} by jointly supervising the rendered color and optical depth [(\\\\ensuremath{<}\\\\ensuremath{>})3]. Specifically, the loss function for training the networks is defined as: \\n\\n(4) \\n\\nwhere (u, v) is the location of the pixel that r(s) shoots towards, \\\\ensuremath{\\\\lambda} is a hyper-parameter weighting the depth loss. \\n\\nLast but not least, we conduct statistical depth refinement to handle corrupt stereo depth caused by fuzzy pixels and specular highlights on the images of surgical scenes. Direct supervision on the estimated depth will overfit corrupt depth in the end, leading to abrupt artifacts in reconstruction results (Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3). Our preliminary findings reveal that our model at the early training stage would produce smoother results both in color and depth since the underfitting model tends to average learned colors and occupancy. Thus, minority corrupt depth is smoothed by majority normal depth. Based on this observation, we propose to patch the corrupt depth with the output from underfitting radiance fields. Denoting D iK as the underfitting output depth maps for the i-th frame after K iterations of training, we firstly find residual maps through i = |D iK \\\\ensuremath{-} Di|, then we compute a probabilistic distribution over the residual maps. After that, we set a small number \\\\ensuremath{\\\\alpha} \\\\ensuremath{\\\\in} [0, 1] and locate those pixels with the last \\\\ensuremath{\\\\alpha}-quantile residuals. Since those located pixels statistically correspond to large residuals, we can identify them as occurrences of corrupt depth. Finally, we replace those identified corrupt depth pixels with smoother depth pixels in D iK . After this refinement procedure, the radiance fields are optimized on the patched depth maps in the subsequent training iterations, alleviating corrupt depth fitting. \\n\\n3 Experiments \\n\\nWe evaluate our proposed method on typical robotic surgery stereo videos from 6 cases of our in-house DaVinci robotic prostatectomy data. We totally extracted 6 clips with a total of 807 frames. Each clip lasts for 4\\\\ensuremath{\\\\sim}8s with 15fps. Each case is captured from stereo cameras at a single viewpoint and encompasses challenging scenes with non-rigid deformation and tool occlusion. Among the selected 6 cases, 2 cases contain traction on thin structures such as fascia, 2 cases contain significant pushing and pulling of tissue, and 2 cases contain tissue cutting, which altogether present the typical soft tissue situations in robotic surgery. For comparison, we take the most recent state-of-the-art surgical scene reconstruction method of E-DSSR [(\\\\ensuremath{<}\\\\ensuremath{>})11] as a strong comparison. For qualitative evaluation, We exhibit our reconstructed point clouds and compare textural and geometric details obtained by di{\\\\textperiodcentered}erent methods. We also conduct an ablation study on our depth-related modules through qualitative comparison. Due to clinical regulation in practice, it is impossible to collect ground truth depth for numerical evaluation on 3D structures. Following the evaluation method in [(\\\\ensuremath{<}\\\\ensuremath{>})11] and wide literature in neural rendering, we alternatively use photometric errors, including PSNR, SSIM and LPIPS, as evaluation metrics for quantitative comparisons. \\n\\nIn our implementation, we empirically set the width of the transfer function \\\\ensuremath{\\\\xi} = 1, the weight of depth loss \\\\ensuremath{\\\\lambda} = 1, depth refinement iteration K = 4000 and \\\\ensuremath{\\\\alpha}=0.1. Other training hyper-parameters follow the settings in the state-of-the-art D-NeRF [(\\\\ensuremath{<}\\\\ensuremath{>})21]. We calibrate the endoscope in advance to acquire its intrinsics. In all of our experiments, tool masks are obtained by manually labeling and coarse stereo depth maps are generated by STTR-light [(\\\\ensuremath{<}\\\\ensuremath{>})9] pretrained on Scene Flow. We optimize each model over 100K iterations on a single case. To recover explicit geometry from implicit fields, we render optimized radiance fields to RGBD maps, smooth rendered depth maps via bilateral filtering, and back-project RGBD into point clouds based on the endoscope intrinsics. \\n\\nFor qualitative evaluation, Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2 illustrates the reconstruction results of our approach and the comparison method, along with a reference to the original video. In the test case of Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2(a), the tissues are pulled by surgical instruments, yielding relatively large deformations. Benefitting from the underlying continuous scene representations, our method can reconstruct water-tight tissues without being a{\\\\textperiodcentered}ected by the tool occlusion. More importantly, per-frame deformations are captured continuously, achieving stable results over the episode of consecutive pulling. In contrast, the current state-of-the-art method [(\\\\ensuremath{<}\\\\ensuremath{>})11] could not fully track these large deformations and its reconstruction results include holes \\n\\nFig. 2: Qualitative comparisons of 2 cases, demonstrating reconstruction of soft tissues with large deformations and topology changes. \\n\\nand noisy points under such a challenging situation. We further demonstrate a more di\\\\\"ycult case in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})2(b) which includes soft tissue cutting with topology changes. From the reconstruction results, it is observed that our method manages to track the detailed cutting procedures, owing to the powerful neural \\n\\nFig. 3: Ablation study on our depth-related modules, i.e., depth-supervised loss, depth refinement and depth-cueing ray marching. \\n\\nTable 1: Quantitative evaluation on photometric errors of the dynamic reconstruction on metrics of PSNR, SSIM and LPIPS. \\n\\nrepresentation of displacement fields. In addition, it can bypass the issue of tool occlusion and recover the hidden tissues, which is cooperatively achieved by our mask-guided ray casting and the interpolation property of neural implicit fields. On the other hand, the comparison method is not able to capture these small changes on soft tissues nor patch all the tool-occluded areas. Table (\\\\ensuremath{<}\\\\ensuremath{>})1 summarizes our quantitative experiments, showing overall performance on the dataset. Our method dramatically outperforms E-DSSR by {\\\\textuparrow} 16.433 PSNR, {\\\\textuparrow} 0.295 SSIM and {\\\\textdownarrow} 0.342 LPIPS. To assess the contribution of the dynamics modeling, we also evaluate our model without neural displacement field (Ours w/o D). As expected, removing this component leads to a noticeable performance drop, which reflects the e{\\\\textperiodcentered}ectiveness of the displacement modeling. \\n\\nWe present a qualitative ablation study on our depth-related modules in Fig. (\\\\ensuremath{<}\\\\ensuremath{>})3. Without depth-supervision loss, we observe that the pipeline is not capable of learning correct geometry from single-viewpoint input. Moreover, when depth refinement is disabled, abrupt artifacts occur on the reconstruction results due to corruption in stereo depth estimation. Our depth-cueing ray marching can further diminish artifacts on 3D structures, especially for boundary points. \\n\\n4 Conclusion \\n\\nThis paper presents a novel neural rendering-based framework for dynamic surgical scene reconstruction from single-viewpoint binocular images, as well as \\n\\naddressing complex tissue deformation and tool occlusion. We adopt the cutting-edge dynamic neural radiance field method to represent surgical scenes. In addition, we propose mask-guided ray casting to handle tool occlusion and impose stereo depth prior upon the single-viewpoint situation. Our approach has achieved superior performance on various scenarios in robotic surgery data such as large elastic deformations and tissue cutting. We hope the emerging NeRF-based 3D reconstruction techniques could inspire new pathways for robotic surgery scene understanding, and empower various down-stream clinical-oriented tasks. \\n\\nAcknowledgements. This work was supported in part by CUHK Shun Hing Institute of Advanced Engineering (project MMT-p5-20), in part by Shenzhen-HK Collaborative Development Zone, and in part by Multi-Scale Medical Robotics Centre InnoHK. \\n\\nReferences \\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.txt'}),\n",
       " Document(page_content='iNeRF: Inverting Neural Radiance Fields for Pose Estimation \\n\\nFig. 1: We present iNeRF which performs mesh-free pose estimation by inverting a neural radiance field of an object or scene. The middle figure shows the trajectory of estimated poses (gray) and the ground truth pose (green) in iNeRF{\\\\textquoteright}s iterative pose estimation procedure. By comparing the observed and rendered images, we perform gradient-based optimization to estimate the camera{\\\\textquoteright}s pose without accessing the object{\\\\textquoteright}s mesh model. Click the image to play the video in a browser. \\n\\nAbstract{\\\\textemdash} We present iNeRF, a framework that performs mesh-free pose estimation by {\\\\textquotedblleft}inverting{\\\\textquotedblright} a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis {\\\\textemdash} synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation {\\\\textendash} given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view. \\n\\nThe recent advances of Neural Radiance Fields (NeRF [22]) provide a mechanism for capturing complex 3D and optical structures from only one or a few RGB images, which opens up the opportunity to apply analysis-by-synthesis to broader real-world scenarios without mesh models during training or test times. NeRF representations parameterize the density and color of the scene as a function of 3D scene coordinates. The function can either be learned from multi-view images with given camera poses [18], [22] or directly predicted by a generative model given one or few input images [45], [47]. \\n\\nHere we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model. . iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image. We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image. The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose. As illustrated in Figure (\\\\ensuremath{<}\\\\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate. \\n\\nFinally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB inputs by inverting a NeRF model inferred by pixelNeRF [47] given a single view of the object. The only prior work we are aware of that similarly provides RGB-only category-level pose estimation is the recent work of Chen et al. [3]. In Sec. (\\\\ensuremath{<}\\\\ensuremath{>})II we compare differences between [3] and our work, which mostly arise from the opportunities and challenges presented by a continuous, implicit NeRF parameterization. \\n\\nTo summarize, our primary contributions are as follows. (i) We show that iNeRF can use a NeRF model to estimate 6 DoF pose for scenes and objects with complex geometry, without the use of 3D mesh models or depth sensing {\\\\textemdash} only RGB images are used as input. (ii) We perform a thorough investigation of ray sampling and the batch sizes for gradient optimization to characterize the robustness and limitations of iNeRF. (iii) We show that iNeRF can improve NeRF by predicting the camera poses of additional images, that can then be added into NeRF{\\\\textquoteright}s training set. (iv) We show category-level pose estimation results, for unseen objects, including a real-world demonstration. \\n\\nII. RELATED WORKS \\n\\nPose Estimation from RGB Images. Classical methods for object pose estimation address the task by detecting and matching keypoints with known 3D models [1], [4], [5], [29]. Recent approaches based on deep learning have proposed to 1) directly estimate objects pose using CNN-based architectures [32], [40], [46] or 2) estimate 2D key-points [27], [35], [37], [38] and solve for pose using the PnP-RANSAC algorithm. Differentiable mesh renderers [2], [24] have also been explored for pose estimation. Although their results are impressive, all the aforementioned works require access to objects{\\\\textquoteright} 3D models during both training and testing, which significantly limits the applicability of these approaches. Recently, Chen et al. [3] address category-level object pose estimation [44], in particular they impressively estimate object shape and pose across a category from a single image. They use a single-image reconstruction with a 3D voxel-based feature volume and then estimating pose using iterative image alignment. In contrast, in our work we use continuous implicit 3D representations in the form of NeRF models, which have been empirically shown to produce more photorealistic novel-image rendering [22], [18] and scale to large, building-scale volumes [18], which we hypothesize will enable higher-fidelity pose estimation. This also presents challenges, however, due to the expensive computational cost of NeRF rendering, for which we introduce a novel importance-sampling approach in Sec. (\\\\ensuremath{<}\\\\ensuremath{>})IV-B. Another practical difference in our approach to category-level pose estimation {\\\\textendash} while [3] optimizes for shape with gradient descent, we show we can instead allow pixelNeRF to predict a NeRF model with just a forward pass of a network. Additionally, since NeRF models scale well to large \\n\\nFig. 2: An overview of our pose estimation pipeline which inverts an optimized neural radiance field (NeRF). Given an initially estimated pose, we first decide which rays to emit. Sampled points along the ray and the corresponding viewing direction are fed into NeRF{\\\\textquoteright}s volume rendering procedure to output rendered pixels. Since the whole pipeline is differentiable, we can refine our estimated pose by minimizing the residual between the rendered and observed pixels. \\n\\nscenes, we can use the same iNeRF formulation to perform localization, for example in challenging real-world LLFF scenes {\\\\textendash} this capability was not demonstrated in [3], and may be challenging due to the memory limitations of voxel representations for sufficient fidelity in large scenes. While object pose estimation methods are often separate from methods used for visual localization of a camera in a scene as in the SfM literature (i.e. [33], [41], [31]), because NeRF and iNeRF only require posed RGB images as training, iNeRF can be applied to localization as well. \\n\\nIII. BACKGROUND \\n\\nGiven a collection of N RGB images \\\\{Ii\\\\}N i=1, Ii \\\\ensuremath{\\\\in} [0, 1]H{\\\\texttimes}W {\\\\texttimes}3 with known camera poses \\\\{Ti\\\\}iN=1, NeRF learns to synthesize novel views associated with unseen camera poses. NeRF does this by representing a scene as a {\\\\textquotedblleft}radiance field{\\\\textquotedblright}: a volumetric density that models the shape of the scene, and a view-dependent color that models the appearance of occupied regions of the scene, both of which lie within a bounded 3D volume. The density \\\\ensuremath{\\\\sigma} and RGB color c of each point are parameterized by the weights \\\\ensuremath{\\\\Theta} of a multilayer perceptron (MLP) F that takes as input the 3D position of that point x = (x, y, z) and the unit-norm viewing direction of that point d = (dx, dy, dz), where (\\\\ensuremath{\\\\sigma}, c) {\\\\textleftarrow} F\\\\ensuremath{\\\\Theta}(x, d). To render a pixel, NeRF emits a camera ray from the center of the projection of a camera through that pixel on the image plane. Along the ray, a set of points are sampled for use as input to the MLP which outputs a set of densities and colors. These values are then used to approximate the image formation behind volume rendering [7] using numerical quadrature [19], producing using some sampled set of rays r \\\\ensuremath{\\\\in} R where C(r) is the observed RGB value of the pixel corresponding to ray r in some image, and C{\\\\textasciicircum}(r) is the prediction produced from neural volume rendering. To improve rendering efficiency one may train two MLPs: one {\\\\textquotedblleft}coarse{\\\\textquotedblright} and one {\\\\textquotedblleft}fine{\\\\textquotedblright}, where the coarse model serves to bias the samples that are used for the fine model. For more details, we refer readers to Mildenhall et al. [22]. \\n\\nAlthough NeRF originally needs to optimize the representation for every scene independently, several extensions [28], [39], [45], [47] have been proposed to directly predict a continuous neural scene representation conditioned on one or few input images. In our experiments, we show that iNeRF can be used to perform 6D pose estimation with either an optimized or predicted NeRF model. \\n\\nIV. INERF FORMULATION \\n\\nWe now present iNeRF, a framework that performs 6 DoF pose estimation by {\\\\textquotedblleft}inverting{\\\\textquotedblright} a trained NeRF. Let us assume that the NeRF of a scene or object parameterized by \\\\ensuremath{\\\\Theta} has already been recovered and that the camera intrinsics are known, but the camera pose T of an image observation I are as-yet undetermined. Unlike NeRF, which optimizes \\\\ensuremath{\\\\Theta} using a set of given camera poses and image observations, we instead solve the inverse problem of recovering the camera pose T given the weights \\\\ensuremath{\\\\Theta} and the image I as input: \\n\\n(1) \\n\\nTo solve this optimization, we use the ability from NeRF to take some estimated camera pose T \\\\ensuremath{\\\\in} SE(3) in the coordinate frame of the NeRF model and render a corresponding image observation. We can then use the same photometric loss function L as was used in NeRF (Sec. (\\\\ensuremath{<}\\\\ensuremath{>})III), but rather than backpropagate to update the weights \\\\ensuremath{\\\\Theta} of the MLP, we instead update the pose T to minimize L. The overall procedure is shown in Figure (\\\\ensuremath{<}\\\\ensuremath{>})2. While the concept of inverting a NeRF to perform pose estimation can be concisely stated, it is not obvious that such a problem can be practically solved to a useful degree. The loss function L is non-convex over the 6DoF space of SE(3), and full-image NeRF renderings are computationally expensive, particularly if used in the loop of an optimization procedure. Our formulation and experimentation (Sec. (\\\\ensuremath{<}\\\\ensuremath{>})V) aim to address these challenges. In the next sections, we discuss (i) the gradient-based SE(3) optimization procedure, (ii) ray sampling strategies, and (iii) how to use iNeRF{\\\\textquoteright}s predicted poses to improve NeRF. \\n\\nLet \\\\ensuremath{\\\\Theta} be the parameters of a trained and fixed T{\\\\textasciicircum}i the estimated camera pose at current optimization i, I the observed image, and L(T{\\\\textasciicircum}i | I,\\\\ensuremath{\\\\Theta}) be the to train the fine model in NeRF. We employ gradient-based optimization to solve for T{\\\\textasciicircum} as defined in Equation ensure that the estimated pose T{\\\\textasciicircum}i continues to lie SE(3) manifold during gradient-based optimization, rameterize T{\\\\textasciicircum}i with exponential coordinates. Given pose estimate T{\\\\textasciicircum}0 \\\\ensuremath{\\\\in} SE(3) from the camera frame model frame, we represent T{\\\\textasciicircum}i as: \\n\\nwhere S = [\\\\ensuremath{\\\\omega},\\\\ensuremath{\\\\nu}]T represents the screw axis, \\\\ensuremath{\\\\theta} the magnitude, [w] represents the skew-symmetric 3 {\\\\texttimes} 3 matrix of w, and K(S,\\\\ensuremath{\\\\theta}) = (I\\\\ensuremath{\\\\theta}+ (1 \\\\ensuremath{-} cos \\\\ensuremath{\\\\theta})[\\\\ensuremath{\\\\omega}] + (\\\\ensuremath{\\\\theta}\\\\ensuremath{-} sin \\\\ensuremath{\\\\theta})[\\\\ensuremath{\\\\omega}]2)\\\\ensuremath{\\\\nu} [14]. With this parameterization, our goal is to solve the optimal relative transformation from an initial estimated pose T0: \\n\\n(2) \\n\\nWe iteratively differentiate the loss function through the MLP to obtain the gradient \\\\ensuremath{\\\\nabla}S\\\\ensuremath{\\\\theta}L(e[S]\\\\ensuremath{\\\\theta}T0 | I,\\\\ensuremath{\\\\Theta}) that is used to update the estimated relative transformation. We use Adam optimizer [9] with an exponentially decaying learning rate (See Supplementary for parameters). For each observed image, we initialize S\\\\ensuremath{\\\\theta} near 0, where each element is drawn at random from a zero-mean normal distribution N (0,\\\\ensuremath{\\\\sigma} = 10\\\\ensuremath{-}6). In practice, parameterizing with e[S]\\\\ensuremath{\\\\theta} T0 rather than T0 e[S]\\\\ensuremath{\\\\theta} results in a center-of-rotation at the initial estimate{\\\\textquoteright}s center, rather than at the camera frame{\\\\textquoteright}s center. This alleviates coupling between rotations and translations during optimization. \\n\\nIn a typical differentiable render-and-compare pipeline, one would want to leverage the gradients contributed by all of the output pixels in the rendered image [43]. However, with NeRF, each output pixel{\\\\textquoteright}s value is computed by weighing the values of n sampled points along each ray r \\\\ensuremath{\\\\in} R during ray marching, so given the amount of sampled rays in a batch b = |R|, then O(bn) forward/backward passes of the underlying NeRF MLP will be queried. Computing and backpropagating the loss of all pixels in an image (i.e., , b = HW, where H and W represent the height and width of a high-resolution image) therefore require significantly more memory than is present on any commercial GPU. While we may perform multiple forward and backward passes to accumulate these gradients, this becomes prohibitively slow to perform each step of our already-iterative optimization procedure. In the following, we explore strategies for selecting a sampled set of rays R for use in evaluating the loss function L at each optimization step. In our experiments we find that we are able to recover accurate poses while sampling only \\n\\nFig. 3: An illustration of 3 sampling strategies. The input image and the rendering corresponding to the estimated pose of the scene are averaged. We use x to represent sampled pixels on the background; + to represent sampled pixels that are covered by both rendered and observed images; o to represent sampled pixels that are only covered by either the rendered or the input image. When performing random sampling (left) many sampled pixels are x, which provide no gradients for updating the pose. For {\\\\textquotedblleft}interest point{\\\\textquotedblright} sampling (middle) some of the sampled pixels are already aligned and therefore provide little information. For {\\\\textquotedblleft}interest region{\\\\textquotedblright} sampling, many sampled pixels are o, which helps pose estimation achieve higher accuracy and faster convergence. \\n\\nb = 2048 rays per gradient step, which corresponds to a single forward/backward pass that fits within GPU memory and provides 150{\\\\texttimes} faster gradient steps on a 640 {\\\\texttimes} 480 image. \\n\\nmorphological dilation for I iterations to enlarge the sampled region. In practice, we find this to speed up the optimization when the batch size of rays is small. Note that if I is set to a large number, Interest Region Sampling falls back to Random Sampling. \\n\\nIn addition to using iNeRF to perform pose estimation given a trained NeRF, we also explore using the estimated poses to feed back into training the NeRF representation. Specifically, we first (1) train a NeRF given a set of training RGB images with known camera poses \\\\{(Ii,Ti)\\\\}i=1 , yield-Ntrain ing NeRF parameters \\\\ensuremath{\\\\Theta}train. We then (2) use iNeRF to take in additional unknown-pose observed images and solve for estimated poses Given these estimated poses, we can then (3) use the self-supervised pose labels to add into the training set. This procedure allows NeRF to be trained in a semi-supervised setting. \\n\\nV. RESULTS \\n\\nWe first conduct extensive experiments on the synthetic dataset from NeRF [22] and the real-world complex scenes from LLFF [21] to evaluate iNeRF for 6DoF pose estimation. Specifically, we study how the batch size of rays and sampling strategy affect iNeRF. We then show that iNeRF can improve NeRF by estimating the camera poses of images with unknown poses and using these images as additional training data for NeRF. Finally, we show that iNeRF works well in tandem with pixelNeRF [47] which predicts a NeRF model conditioned on a single RGB image. We test our method for category-level object pose estimation in both simulation and the real world. We found that iNeRF achieving competitive results against feature-based methods without accessing object mesh models during either training or test time. \\n\\nTABLE I: Benchmark on Fern scene. NeRFs trained with pose labels generated by iNeRF can achieve higher PSNR. \\n\\nFig. 5: iNeRF can be used to improve NeRF by augmenting training data with images whose camera poses are unknown. We present an ablation study using 25\\\\% and 50\\\\% of training images to train NeRF models. These models are compared with models trained using 100\\\\% of the training images but where a fraction of that data use estimated poses from iNeRF rather than ground-truth poses from the dataset. \\n\\ntest set, I0 is selected randomly from one of the 251 views and the other image I1 is selected from views whose rotation and translation are within 30-degree from I0. At test time, our method uses a pre-trained pixelNeRF to predict a NeRF model conditioned on image I0. Then, we apply iNeRF to align against I1 for estimating the relative pose T01 . \\n\\nTABLE II: Quantitative results for the ShapeNet Cars dataset. We report performance using the mean and median of the translation and rotation error. A prediction is defined as an outlier when either the translation error or the rotation error is larger than 20{\\\\textopenbullet} . \\n\\nrequires a segmented image as input, we use PointRend to remove the background for frames that pixelNeRF takes inputs. In this iterative tracking setting, iNeRF only requires less than 10 iterations of optimization to converge which enables tracking at approximately 1Hz. \\n\\nVI. LIMITATIONS AND FUTURE WORK \\n\\nWhile iNeRF has shown promising results on pose estimation, it is not without limitations. Both lighting and occlusion can severely affect the performance of iNeRF and are not modeled by our current formulation. One potential solution is to model appearance variation using transient latent codes as was done in NeRF-W [18] when training NeRFs, and jointly optimize these appearance codes alongside camera pose within iNeRF. Also, currently iNeRF takes around 20 seconds to run 100 optimization steps, which prevents it from being practical for real-time use. We expect that this issue may be mitigated with recent improvements in NeRF{\\\\textquoteright}s rendering speed [13]. \\n\\n(b) Results on real-world scenes from the LLFF dataset. \\n\\nFig. 6: (a) Quantitative results on the synthetic dataset. {\\\\textquotedblleft}s{\\\\textquotedblright} stands for sampling strategy and {\\\\textquotedblleft}b{\\\\textquotedblright} stands for the batch size. Applying Interest Region Sampling (s=region) improves the accuracy by 15\\\\% across various batch sizes. (b) Quantitative results on LLFF. Interest Region Sampling is always applied and we show the effect of various batch sizes on performance. Larger batch sizes can improve accuracy while reducing the number of gradient steps needed for convergence. \\n\\nFig. 8: Qualitative results of pose tracking in real-world images without the need for mesh/CAD model. In the left column, we show input video frames at different time steps. At each time t, iNeRF leverages a NeRF model inferred by pixelNeRF based on input frame at time t \\\\ensuremath{-} 1 to estimate the object{\\\\textquoteright}s pose. In the right column, we show the resulting reconstructed frames and the estimated poses at each time step. The background has been masked out using PointRend [10] before feeding the frame into pixelNeRF. The views are rotations about the view-space vertical axis.Click the image to play the video in a browser. \\n\\nVII. CONCLUSION \\n\\nWe have presented iNeRF, a framework for mesh-free, RGB-only pose estimation that works by inverting a NeRF model. We have demonstrated that iNeRF is able to perform accurate pose estimation using gradient-based optimization. We have thoroughly investigated how to best construct mini-batches of sampled rays for iNeRF and have demonstrated its performance on both synthetic and real datasets. Lastly, we have shown how iNeRF can perform category-level object pose estimation and track pose for novel object instances \\n\\nFig. 9: Histogram of pose errors on real-world scenes from the LLFF dataset. \\n\\nwith an image conditioned generative NeRF model. \\n\\nVIII. IMPLEMENTATION DETAILS \\n\\nThe Y channel is not considered in the computation of loss. \\n\\nIX. HISTOGRAM OF POSE ERRORS \\n\\nWe visualize the histogram of pose errors, before and after iNeRF optimization, on the LLFF dataset in Figure (\\\\ensuremath{<}\\\\ensuremath{>})9 using the data from Section 5.2. The data is generated by applying random perturbations within [\\\\ensuremath{-}40, 40] degrees for rotation and [\\\\ensuremath{-}0.1, 0.1] meters along each axis for translation. Note that when the batch size is 2048, more than 70\\\\% of the data has \\\\ensuremath{<} 5{\\\\textopenbullet} and \\\\ensuremath{<} 5 cm error after iNeRF is applied. \\n\\nX. MORE ANALYSIS IN SELF-SUPERVISED NERF \\n\\nFor the Fern scene, we found that when only 10\\\\% of labeled camera poses are used, it worsens the PSNR from 18.5 to 15.64. The results show that having enough labels for a good initalization is important. \\n\\nREFERENCES \\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.txt'}),\n",
       " Document(page_content='X2CT-GAN: Reconstructing CT from Biplanar X-Rays with Generative Adversarial Networks \\n\\nComputed tomography (CT) can provide a 3D view of the patient{\\\\textquoteright}s internal organs, facilitating disease diagnosis, but it incurs more radiation dose to a patient and a CT scanner is much more cost prohibitive than an X-ray machine too. Traditional CT reconstruction methods require hundreds of X-ray projections through a full rotational scan of the body, which cannot be performed on a typical X-ray machine. In this work, we propose to reconstruct CT from two orthogonal X-rays using the generative adversarial network (GAN) framework. A specially designed generator network is exploited to increase data dimension from 2D (X-rays) to 3D (CT), which is not addressed in previous research of GAN. A novel feature fusion method is proposed to combine information from two X-rays. The mean squared error (MSE) loss and adversarial loss are combined to train the generator, resulting in a high-quality CT volume both visually and quantitatively. Extensive experiments on a publicly available chest CT dataset demonstrate the effectiveness of the proposed method. It could be a nice enhancement of a low-cost X-ray machine to provide physicians a CT-like 3D volume in several niche applications. \\n\\n1. Introduction \\n\\nImmediately after its discovery by Wilhelm Rntgen in 1895, X-ray found wide applications in clinical practice. It is the first imaging modality enabling us to non-invasively see through a human body and diagnose changes of internal anatomies. However, all tissues are projected onto a 2D image, overlaying each other. While bones are clearly visible, soft tissues are often difficult to discern. Computed tomography (CT) is an imaging modality that reconstructs a 3D volume from a set of X-rays (usually, at least 100 images) captured in a full rotation of the X-ray apparatus around the body. One prominent advantage of CT is that tissues \\n\\nFigure 1. Illustration of the proposed method. The network takes 2D biplanar X-rays as input and outputs a 3D CT volume. \\n\\nare presented in the 3D space, which completely solves the overlaying issue. However, a CT scan incurs far more radiation dose to a patient (depending on the number of X-rays acquired for CT reconstruction). Moreover, a CT scanner is often much more cost prohibitive than an X-ray machine, making its less accessible in developing countries [37]. \\n\\nThe purpose of this work is not to replace CT with X-rays. Though the proposed method can reconstruct the general structure accurately, small anatomies still suffer from some artifacts. However, the proposed method may find some niche applications in clinical practice. For example, we can measure the size of major organs (e.g., lungs, heart, and liver) accurately, or diagnose ill-positioned organs on the reconstructed CT scan. It may also be used for dose planning in radiation therapy, or pre-operative planning and intra-operative guidance in minimally invasive intervention. It could be a nice enhancement of a low-cost X-ray machine as physicians may also get a CT-like 3D volume that has certain clinical values. \\n\\nThough the proposed network can also be used to reconstruct CT from a single X-ray, we argue that using bipla-nar X-rays is a more practical solution. First, CT reconstruction from a single X-ray subjects to too much ambiguity while biplanar X-rays offer additional information from both views that is complementary to each other. More accurate results, 4 dB improvement in peak signal-to-noise ratio (PSNR), are achieved in our comparison experiment. Second, biplanar X-ray machines are already clinically available, which can capture two orthogonal X-ray images simultaneously. And, it is also clinically practicable to capture two orthogonal X-rays with a mono-planar machine, by rotating the X-ray apparatus to a new orientation for the second X-ray imaging. \\n\\nOne practical issue to train X2CT-GAN is lacking of paired X-ray and CT 1 . It is expensive to collect such paired data from patients and it is also unethical to subject patients to additional radiation doses. In this work, we train the network with synthesized X-rays generated from large public-available chest CT datasets [1]. Given a CT volume, we simulate two X-rays, one from the posterior-anterior (PA) view and the other from the lateral view, using the digitally reconstructed radiographs (DRR) technology [28]. Although DRR synthesized X-rays are quite photo-realistic, there still exits a gap between real and synthesized X-rays, especially in finer anatomy structures, e.g., blood vessels. Therefore we further resort CycleGAN [41] to learn the genuine X-ray style that can be transferred to the synthesized data. More information about the style transfer operation can be found in supplement materials. \\n\\nFigure 2. Overview of the X2CT-GAN model. RL and PL are abbreviations of the reconstruction loss and projection loss. \\n\\nTo summarize, we make the following contributions: \\n\\n2. Related Work \\n\\nCross-Modality Transfer A DL based model often suffers from lacking enough training data so as to fall into a suboptimal point during training or even overfit the small dataset. To alleviate this problem, synthetic data has been used to boost the training process [33, 39]. So synthesizing realistic images close to the target distribution is a critic premise. Previous research such as pix2pix [17] could do the pixel level image to image transfer and CycleGAN [41] has the ability to learn the mapping between two unpaired datasets. In medical imaging community, quite some efforts have been put into this area to transfer a source modality to a target modality, e.g., 3T MRI to 7T MRI [3], MRI to CT [5, 30], MRI and CT bidirectional transfer [39] etc. Our approach differs from the previous cross-modality transfer works in two ways. First, in all the above works, the dimensions of the input and output are consistent, e.g., 2D to \\n\\nFigure 3. Network architecture of the X2CT-GAN generator. Two encoder-decoder networks with the same architecture work in parallel for posterior-anterior (PA) and lateral X-rays, respectively. Another fusion network between these two encoder-decoder networks is responsible for fusing information coming from two views. For more details about Connection-A, B and C, please refer to Fig. 4. \\n\\n2D or 3D to 3D. Here, we want to transfer 2D X-rays to a 3D volume. To handle this challenge, we propose X2CTGAN, which incorporates two mechanisms to increase the data dimension. Second, our goal is to reconstruct accurate 3D anatomy from biplanar X-rays with clinical values instead of enriching the training set. A photo-realistic image (e.g., one generated from pure noise input [11]) may already be beneficial for training. However, our application further requires the image to be anatomically accurate as well. \\n\\n3. Objective Functions of X2CT-GAN \\n\\nGAN [11] is a recent proposal to effectively train a generative model that has demonstrated the ability to capture real data distribution. Conditional GAN [29], as an extension of the original GAN, further improves the data generation process by conditioning the generative model on additional inputs, which could be class labels, partial data, or even data from a different modality. Inspired by the successes of conditional GANs, we propose a novel solution to train a generative model that can reconstruct a 3D CT volume from biplanar 2D X-rays. In this section, we first introduce several loss functions that are used to constrain the generative model. \\n\\nThe original intention of GAN is to learn deep generative models while avoiding approximating many intractable probabilistic computations that arise in other strategies, i.e., maximum likelihood estimation. The learning procedure is a two-player game, where a discriminator D and a generator G would compete with each other. The ultimate goal is to learn a generator distribution pG(x) that matches the real data distribution pdata(x). An ideal generator could generate samples that are indistinguishable from the real samples by the discriminator. More formally, the minmax game is summarized by the following expression: \\n\\n(1) \\n\\nwhere z is sampled from a noise distribution. \\n\\nAs we want to learn a non-linear mapping from X-rays to CT, the generated CT volume should be consistent with the semantic information provided by the input X-rays. After trying different mutants of the conditional GAN, we find out that LSGAN [27] is more suitable for our task and apply it to guide the training process. The conditional LSGAN loss is defined as: \\n\\nwhere x is composed of two orthogonal biplanar X-rays, and y is the corresponding CT volume. Compared to the original objective function defined in Eq. (1), LSGAN replaces the logarithmic loss with a least-square loss, which helps to stabilize the adversarial training process and achieve more realistic details. \\n\\nThe conditional adversarial loss tries to make prediction look real. However, it does not guarantee that G can generate a sample maintaining the structural consistency with the input. Moreover, CT scans, different from natural images that have more diversity in color and shape, require higher precision of internal structures in 3D. Consequently, an additional constraint is required to enforce the reconstructed CT to be voxel-wise close to the ground truth. Some previous work has combined the reconstruction loss [32] with the adversarial loss and got positive improvements. We also follow this strategy and acquire a high PSNR as shown in Table 1. Our reconstruction loss is defined as MSE: \\n\\n(3) \\n\\nThe aforementioned reconstruction loss is a voxel-wise loss that enforces the structural consistency in the 3D space. To improve the training efficiency, more simple shape priors could be utilized as auxiliary regularizations. Inspired by [18], we impel 2D projections of the predicted volume to match the ones from corresponding ground-truth in different views. Orthogonal projections, instead of perspective projections, are carried out to simplify the process as this auxiliary loss focuses only on the general shape consistency, not the X-ray veracity. We choose three orthogonal projection planes (axial, coronal, and sagittal, as shown in Fig. 2, following the convention in the medical imaging community). Finally, the proposed projection loss is defined as below: \\n\\n(4) \\n\\nwhere the Pax, Pco and Psa represent the projection in the axial, coronal, and sagittal plane, respectively. The L1 distance is used to enforce sharper image boundaries. \\n\\nGiven the definitions of the adversarial loss, reconstruction loss, and projection loss, our final objective function is formulated as: \\n\\n(5) \\n\\n4. Network Architecture of X2CT-GAN \\n\\nIn this section, we introduce our proposed network designs that are used in the 3D CT reconstruction task from 2D biplanar X-rays. Similar to other 3D GAN architectures, our method involves a 3D generator and a 3D discriminator. These two models are alternatively trained with the supervision defined in previous section. \\n\\nThe proposed 3D generator, as illustrated in Fig. 3, consists of three individual components: two encoder-decoder networks with the same architecture working in parallel for posterior-anterior (PA) and lateral X-rays respectively, and a fusion network. The encoder-decoder network aims to learn the mapping from the input 2D X-ray to the target 3D CT in the feature space, and the fusion network is responsible for reconstructing the 3D CT volume with the fused biplanar information from the two encoder-decoder networks. Since the training process in our reconstruction task involves circulating information between input and output from two different modalities and dimensionalities, several modifica-tions of the network architecture are made to adapt to the challenge. \\n\\nDensely Connected Encoder Dense connectivity [15] has a compelling advantage in the feature extraction process. To optimally utilize information from 2D X-ray images, we embed dense modules to generator{\\\\textquoteright}s encoding path. As shown in Fig. 3, each dense module consists of a down-sampling block (2D convolution with stride=2), a densely connected convolution block and a compressing block (output channels halved). The cascaded dense modules encode different level information of the input image and pass it to the decoder along different shortcut paths. \\n\\nBridging 2D Encoder and 3D Decoder Some existing encoder-decoder networks [17, 25] link encoder and decoder by means of convolution. There is no obstacle in a pure 2D or 3D encode-decode process, but our special 2D to 3D mapping procedure requires a new design to bridge the information from two dimensionalities. Motivated by [40], we extend fully connected layer to a new connection module, named Connection-A (Fig. 4a), to bridge the 2D encoder and 3D decoder in the middle of our generator. To better utilize skip connections in the 2D-3D generator, we design another novel connection module, named Connection-B (Fig. 4b), to shuttle low-level features from encoder to decoder. \\n\\nTo be more specific, Connection-A achieves the 2D-3D conversion through fully-connected layers, where the last encoder layer{\\\\textquoteright}s output is flattened and elongated to a 1D \\n\\nFigure 4. Different types of connections. Connection-A and Connection-B aim to increase dimensionality of feature maps, and Connection-C is for fusing information from two different views. \\n\\nvector that is further reshaped to 3D. However, most of the 2D spatial information gets lost during such conversion so that we only use Connection-A to link the last encoder layer and first decoder layer. For the rest of skip connections, we use Connection-B and take following steps: 1) enforce the channel number of the encoder being equal to the one on the corresponding decoder side by a basic 2D convolution block; 2) expand the 2D feature map to a pseudo-3D one by duplicating the 2D information along the third axis; 3) use a basic 3D convolution block to encode the pseudo-3D feature map. The abundant low-level information shuttled across two parts of the network imposes strong correlations on the shape and appearance between input and output. \\n\\nFeature Fusion of Biplanar X-rays As a common sense, a 2D photograph from frontal view could not retain lateral information of the object and vice versa. In our task, we resort biplanar X-rays captured from two orthogonal directions, where the complementary information could help the generative model achieve more accurate results. Two encoder-decoder networks in parallel extract features from each view while the third decoder network is set to fuse the extracted information and output the reconstructed volume. As we assume the biplanar X-rays are captured within a negligible time interval, meaning no data shift caused by patient motions, we can directly average the extracted features after transforming them into the same coordinate space, as shown in Fig. 4c. Any structural inconsistency between two decoders{\\\\textquoteright} outputs will be captured by the fusion network and back-propagated to two networks. \\n\\nThe generator and discriminator are trained alternatively following the standard process [11]. We use the Adam solver [20] to train our networks. The initial learning rate of Adam is 2e-4, momentum parameters \\\\ensuremath{\\\\beta}1 =0.5 and \\\\ensuremath{\\\\beta}2 =0.99. After training 50 epochs, we adopt a linear learning rate decay policy to decrease the learning rate to 0. We train our model for a total of 100 epochs. \\n\\nAs instance normalization [34] has been demonstrated to be superior to batch normalization [16] in image generation tasks, we use instance normalization to regularize intermediate feature maps of our generator. At inference time, we observe that better generating results could be obtained if we use the statistics of the test batch itself instead of the running average of training batches, as suggested in [17]. Constrained by GPU memory limit, the batch size is set to one in all our experiments. \\n\\n5. Experiments \\n\\nIn this section, we introduce an augmented dataset built on LIDC-IDRI [1]. We evaluate the proposed X2CT-GAN model with several widely used metrics, e.g., peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index. To demonstrate the effectiveness of our method, we reproduce a baseline model named 2DCNN [13]. Fair comparisons and comprehensive analysis are given to demonstrate the improvement of our proposed method over the baseline and other mutants. Finally, we show the real-world X-ray evaluation results of X2CT-GAN. Input images to X2CT-GAN are resized to 128 {\\\\texttimes} 128 pixels, while the input of 2DCNN is 256 {\\\\texttimes} 256 pixels as suggested by [13]. The output of all models is set to 128 {\\\\texttimes} 128 {\\\\texttimes} 128 voxels. \\n\\nCT and X-ray Paired Dataset Ideally, to train and validate the proposed CT reconstruction approach, we need a large dataset with paired X-rays and corresponding CT reconstructions. Furthermore, the X-ray machine needs to be calibrated to get an accurate projection matrix. However, no such dataset is available and it is very costly to collect such real paired dataset. Therefore, we take a real CT volume \\n\\nFigure 5. DRR [28] simulated X-rays. (a) and (c) are simulated PA view X-rays of two subjects, (b) and (d) are the corresponding lateral views. \\n\\nand use the digitally reconstructed radiographs (DRR) technology [28] to synthesize corresponding X-rays, as shown in Fig. 5. It is much cheaper to collect such synthesized datasets to train our networks. To be specific, we use the publicly available LIDC-IDRI dataset [1], which contains 1,018 chest CT scans. The heterogeneous of imaging protocols result in different capture ranges and resolutions. For example, the number of slices varies a lot for different volumes. The resolution inside a slice is isotropic but also varies for different volumes. All these factors lead to a nontrivial reconstruction task from 2D X-rays. To simplify, we first resample the CT scans to the 1 {\\\\texttimes} 1 {\\\\texttimes} 1 mm3 resolution. Then, a 320 {\\\\texttimes} 320 {\\\\texttimes} 320 mm3 cubic area is cropped from each CT scan. We randomly select 916 CT scans for training and the rest 102 CT scans are used for testing. \\n\\nMapping from Real to Synthetic X-rays Although DRR synthetic X-rays are quite photo-realistic, there is still a gap between the real and synthetic X-rays, especially for those subtle anatomical structures, e.g., blood vessels. Since our networks are trained with synthesized X-rays, a sub-optimal result will be obtained if we directly feed a real X-ray into the network. We propose to perform style transfer to map real X-rays to the synthesized style. Without paired dataset of real and synthesized X-rays, we exploit CycleGAN [41] to learn the mapping. We collected 200 real X-rays and randomly selected 200 synthetic X-rays from the training set of the paired LIDC dataset. \\n\\nPSNR is often used to measure the quality of reconstructed digital signals [31]. Conventionally, CT value is recorded with 12 bits, representing a range of [0, 4095] (the actual Hounsfield unit equals the CT value minus 1024) [4], which makes PSNR an ideal criterion for image quality evaluation. \\n\\nSSIM is a metric to measure the similarity of two images, including brightness, contrast and structure [36]. Compared to PSNR, SSIM can match human{\\\\textquoteright}s subjective evaluation better. \\n\\nWe first qualitatively evaluate CT reconstruction results shown in Fig. 6, where X2CT-CNN is the proposed network \\n\\nFigure 6. Reconstructed CT scans from different approaches. 2DCNN is our reproduced baseline model [13]; X2CT-CNN is our generator network optimized by the MSE loss alone and X2CT-GAN is our GAN-based model optimized by total objective. {\\\\textquoteleft}+S{\\\\textquoteright} means single-view X-ray input and {\\\\textquoteleft}+B{\\\\textquoteright} means biplanar X-rays input. The first row demonstrates axial slices generated by different models. The last two rows are 3D renderings of generated CT scans in the PA and lateral view, respectively. \\n\\nsupervised only by the reconstruction loss while X2CTGAN is the one trained with full objectives; {\\\\textquoteleft}+S{\\\\textquoteright} means single-view X-ray input and {\\\\textquoteleft}+B{\\\\textquoteright} means biplanar X-rays input. For comparison, we also reproduce the method proposed in [13] (referred as 2DCNN in Fig. 6) as the baseline, one of very few published works tackling the X-ray to CT reconstruction problem using deep learning. Since 2DCNN is designed to deal with single X-ray input, no bi-planar results are shown. From the visual quality evaluation, it is obvious to see the differences. First of all, 2DCNN and X2CT-CNN generate very blurry volumes while X2CTGAN maintains small anatomical structures. Secondly, though missing reconstruction details, X2CT-CNN+S generates sharper boundaries of large organs (e.g., heart, lungs and chest wall) than 2DCNN. Last but not least, models trained with biplanar X-rays outperform the ones trained with single view X-ray. More reconstructed CT slices could be found in Fig. 8. \\n\\nQuantitative results are summarized in Table 1. Biplanar inputs significantly improve the reconstruction accuracy, about 4 dB improvement for both X2CT-CNN and X2CTGAN, compared to single X-ray input. It is well known that the GAN models often sacrifice MSE-based metrics to achieve visually better results. This phenomenon is also observed here. However, by tuning the relative weights of the voxel-level MSE loss and semantic-level adversarial loss \\n\\nTable 1. Quantitative results. 2DCNN is our reproduced model from [13]; X2CT-CNN is our generator network optimized by the MSE loss alone; and X2CT-GAN is our GAN-based model optimized by total objective. {\\\\textquoteleft}+S{\\\\textquoteright} means single-view X-ray input and {\\\\textquoteleft}+B{\\\\textquoteright} means biplanar X-rays input. \\n\\nis our cost function, we can make a reasonable trade-off. For example, there is only 1.1 dB decrease in PSNR from X2CT-CNN+B to X2CT-GAN+B, while the visual image quality is dramatically improved as shown in Fig. 6. We argue that visual image quality is as important as (if not more important than) PSNR in CT reconstruction since eventually the images will be read visually by a physician. \\n\\nAnalysis of Proposed Connection Modules To validate the effectiveness of proposed connection modules, we also perform an ablation study in the setting of X2CT-CNN. As shown in Table 2, single view input with Connection-B achieves 0.7 dB improvement in PSNR. The biplanar \\n\\nTable 2. Evaluation of different connection modules. {\\\\textquoteleft}XC{\\\\textquoteright} denotes X2CT-CNN model without the proposed Connection-B and Connection-C module. {\\\\textquoteleft}+S{\\\\textquoteright} means the model{\\\\textquoteright}s input is a single-view X-ray and {\\\\textquoteleft}+B{\\\\textquoteright} means biplanar X-rays. {\\\\textquoteleft}CB{\\\\textquoteright} and {\\\\textquoteleft}CC{\\\\textquoteright} denote Connection-B and Connection-C respectively as shown in Fig. 4. \\n\\nTable 3. Evaluation of different settings in the GAN framework. {\\\\textquoteleft}RL{\\\\textquoteright} and {\\\\textquoteleft}PL{\\\\textquoteright} denote the reconstruction and projection loss, respectively. {\\\\textquoteleft}CD{\\\\textquoteright} means that input X-ray information is fed to the discriminator to achieve a conditional GAN. \\n\\ninput, even without skip connections, surpasses the single view due to the complementary information injected to the network. And in our biplanar model, Connection-B and Connection-C are interdependent so that we regard them as one module. As can be seen, the biplanar model with this module surpasses other combinations by a large margin both in PSNR and SSIM. \\n\\nDifferent Settings in GAN Framework The effects of different settings in the GAN framework are summarized in Table 3. As the first row shows, adversarial loss alone performs poorly on PSNR and SSIM due to the lack of strong constraints. The most significant improvement comes from the reconstruction loss being added to the GAN framework. Projection loss and the conditional information bring additional improvement slightly. \\n\\nSince the ultimate goal is to reconstruct a CT scan from real X-rays, we finally evaluate our model on real-world data, despite the model is trained on synthetic data. As we have no corresponding 3D CT volumes for real X-rays, only qualitative evaluation is conducted. Visual results are presented in Fig. 7, we could see that the reconstructed lung and surface structures are quite plausible. \\n\\nFigure 7. CT reconstruction from real-world X-rays. Two subjects are shown here. The first and second columns are real X-rays in two views. The following two columns are transformed X-rays by CycleGAN [41]. The last two columns show 3D renderings of reconstructed internal structures and surfaces. Dotted ellipses highlight regions of high-quality anatomical reconstruction. \\n\\n \\n\\nFigure 8. Examples of reconstructed CT slices (a) and corresponding groundtruth (b). As could be seen, our method reconstructs the shape and appearance of major anatomical structures accurately. \\n\\n6. Conclusions \\n\\nIn this paper, we explored the possibility of reconstructing a 3D CT scan from biplanar 2D X-rays in an end-to-end fashion. To solve this challenging task, we combined the reconstruction loss, the projection loss and the adversarial loss in the GAN framework. Moreover, a specially designed generator network is exploited to increase the data dimension from 2D to 3D. Our experiments qualitatively and quantitatively demonstrate that biplanar X-rays are superior to single view X-ray in the 3D reconstruction process. For future work, we will collaborate physicians to evaluate the clinical value of the reconstructed CT scans, including measuring the size of major organs and dose planning in radiation therapy, etc. \\n\\nReferences \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.txt'}),\n",
       " Document(page_content='pixelNeRF: Neural Radiance Fields from One or Few Images \\n\\nWe propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixel-NeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pix-elNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf. \\n\\n1. Introduction \\n\\nWe study the problem of synthesizing novel views of a scene from a sparse set of input views. This long-standing problem has recently seen progress due to advances in differentiable neural rendering [27, 20, 24, 39]. Across these approaches, a 3D scene is represented with a neural network, which can then be rendered into 2D views. Notably, the recent method neural radiance fields (NeRF) [27] has shown impressive performance on novel view synthesis of a specific scene by implicitly encoding volumetric density and color through a neural network. While NeRF can render photorealistic novel views, it is often impractical as it requires a large number of posed images and a lengthy per-scene optimization. \\n\\nIn this paper, we address these shortcomings by proposing pixelNeRF, a learning framework that enables predicting NeRFs from one or several images in a feed-forward manner. Unlike the original NeRF network, which does not make use of any image features, pixelNeRF takes spatial image features aligned to each pixel as an input. This image conditioning allows the framework to be trained on a set of multi-view images, where it can learn scene priors to perform view synthesis from one or few input views. In contrast, NeRF is unable to generalize and performs poorly when few input images are available, as shown in Fig. 1. \\n\\nSpecifically, we condition NeRF on input images by first computing a fully convolutional image feature grid from the input image. Then for each query spatial point x and viewing direction d of interest in the view coordinate frame, we sample the corresponding image feature via projection and bilinear interpolation. The query specification is sent along with the image features to the NeRF network that outputs density and color, where the spatial image features are fed to each layer as a residual. When more than one image is available, the inputs are first encoded into a latent representation in each camera{\\\\textquoteright}s coordinate frame, which are then pooled in an intermediate layer prior to predicting the color and density. The model is supervised with a reconstruction loss between a ground truth image and a view rendered using conventional volume rendering techniques. This framework is illustrated in Fig. 2. \\n\\nPixelNeRF has many desirable properties for few-view novel-view synthesis. First, pixelNeRF can be trained on a dataset of multi-view images without additional supervision such as ground truth 3D shape or object masks. Second, pixelNeRF predicts a NeRF representation in the camera coordinate system of the input image instead of a canonical coordinate frame. This is not only integral for generalization to unseen scenes and object categories [41, 37], but also for flexibility, since no clear canonical coordinate system exists on scenes with multiple objects or real scenes. Third, it is fully convolutional, allowing it to preserve the spatial alignment between the image and the output 3D representation. Lastly, pixelNeRF can incorporate a variable number of posed input views at test time without requiring any test-time optimization. \\n\\nWe conduct an extensive series of experiments on synthetic and real image datasets to evaluate the efficacy of our framework, going beyond the usual set of ShapeNet experiments to demonstrate its flexibility. Our experiments show that pixelNeRF can generate novel views from a single image input for both category-specific and category-agnostic settings, even in the case of unseen object categories. Further, we test the flexibility of our framework, both with a new multi-object benchmark for ShapeNet, where pixel-NeRF outperforms prior approaches, and with simulation-to-real transfer demonstration on real car images. Lastly, we test capabilities of pixelNeRF on real images using the DTU dataset [14], where despite being trained on under 100 scenes, it can generate plausible novel views of a real scene from three posed input views. \\n\\n2. Related Work \\n\\nNovel View Synthesis. The long-standing problem of novel view synthesis entails constructing new views of a scene from a set of input views. Early work achieved photore-alistic results but required densely captured views of the scene [19, 11]. Recent work has made rapid progress to-\\n\\nTable 1: A comparison with prior works reconstructing neural scene representations. The proposed approach learns a scene prior for one or few-view reconstruction using only multi-view 2D image supervision. Unlike previous methods in this regime, we do not require a consistent canonical space across the training corpus. Moreover, we incorporate local image features to preserve local information which is in contrast to methods that compress the structure and appearance into a single latent vector such as Occupancy Networks (ONet) [25] and DVR [28]. \\n\\nward photorealism for both wider ranges of novel views and sparser sets of input views, by using 3D representations based on neural networks [27, 23, 26, 38, 42, 7]. However, because these approaches fit a single model to each scene, they require many input views and substantial optimization time per scene. \\n\\nThere are methods that can predict novel view from few input views or even single images by learning shared priors across scenes. Methods in the tradition of [35, 3] use depth-guided image interpolation [54, 10, 32]. More recently, the problem of predicting novel views from a single image has been explored [44, 47, 36, 5]. However, these methods employ 2.5D representations, and are therefore limited in the range of camera motions they can synthesize. In this work we infer a 3D volumetric NeRF representation, which allows novel view synthesis from larger baselines. \\n\\nSitzmann et al. [39] introduces a representation based on a continuous 3D feature space to learn a prior across scene instances. However, using the learned prior at test time requires further optimization with known absolute camera poses. In contrast, our approach is completely feed-forward and only requires relative camera poses. We offer extensive comparisons with this approach to demonstrate the advantages our design affords. Lastly, note that concurrent work [43] adds image features to NeRF. A key difference is that we operate in view rather than canonical space, which makes our approach applicable in more general settings. Moreover, we extensively demonstrate our method{\\\\textquoteright}s performance in few-shot view synthesis, while GRF shows very limited quantitative results for this task. \\n\\nMost single-view 3D reconstruction methods condition neural 3D representations on input images. The majority employs global image features [29, 6, 28, 25, 8], which, while memory efficient, cannot preserve details that are present in the image and often lead to retrieval-like results. Spatially-aligned local image features have been shown to achieve detailed reconstructions from a single view [49, 33]. However, both of these methods require 3D supervision. Our method is inspired by these approaches, but only requires multi-view supervision. \\n\\nWithin existing methods, the types of scenes that can be reconstructed are limited, particularly so for object-centric approaches (e.g. [46, 21, 12, 45, 38, 53, 25, 49, 28]). CoReNet [31] reconstructs scenes with multiple objects via a voxel grid with offsets, but it requires 3D supervision including the identity and placement of objects. In comparison, we formulate a scene-level learning framework that can in principle be trained to scenes of arbitrary structure. \\n\\nViewer-centric 3D reconstruction For the 3D learning task, prediction can be done either in a viewer-centered coordinate system, i.e. view space, or in an object-centered coordinate system, i.e. canonical space. Most existing methods [49, 25, 28, 39] predict in canonical space, where all objects of a semantic category are aligned to a consistent orientation. While this makes learning spatial regularities easier, using a canonical space inhibits prediction performance on unseen object categories and scenes with more than one object, where there is no pre-defined or well-defined canonical pose. PixelNeRF operates in view-space, which has been shown to allow better reconstruction of unseen object categories in [37, 2], and discourages the memorization of the training set [41]. We summarize key aspects of our approach relative to prior work in Table 1. \\n\\n3. Background: NeRF \\n\\nWe first briefly review the NeRF representation [27]. A NeRF encodes a scene as a continuous volumetric radiance field f of color and density. Specifically, for a 3D point x \\\\ensuremath{\\\\in}R3 and viewing direction unit vector d \\\\ensuremath{\\\\in}R3 , f returns a differential density {\\\\textperiodcentered} and RGB color c: f(x,d) = ({\\\\textperiodcentered}, c). \\n\\nThe volumetric radiance field can then be rendered into a 2D image via \\n\\n(1) \\n\\nwhere ds  handles occlusion. For a target view with pose P, a camera ray can be parameter-\\n\\nized as r(t) = o + td, with the ray origin (camera center) o \\\\ensuremath{\\\\in}R3 and ray unit direction vector d \\\\ensuremath{\\\\in}R3 . The integral is computed along r between pre-defined depth bounds [tn, tf ]. In practice, this integral is approximated with numerical quadrature by sampling points along each pixel ray. \\n\\nThe rendered pixel value for camera ray r can then be compared against the corresponding ground truth pixel value, C(r), for all the camera rays of the target view with pose P. The NeRF rendering loss is thus given by \\n\\n(2) \\n\\nwhere R(P) is the set of all camera rays of target pose P. \\n\\nLimitations While NeRF achieves state of the art novel view synthesis results, it is an optimization-based approach using geometric consistency as the sole signal, similar to classical multiview stereo methods [1, 34]. As such each scene must be optimized individually, with no knowledge shared between scenes. Not only is this time-consuming, but in the limit of single or extremely sparse views, it is unable to make use of any prior knowledge of the world to accelerate reconstruction or for shape completion. \\n\\n4. Image-conditioned NeRF \\n\\nTo overcome the NeRF representation{\\\\textquoteright}s inability to share knowledge between scenes, we propose an architecture to condition a NeRF on spatial image features. Our model is comprised of two components: a fully-convolutional image encoder E, which encodes the input image into a pixel-aligned feature grid, and a NeRF network f which outputs color and density, given a spatial location and its corresponding encoded feature. We choose to model the spatial query in the input view{\\\\textquoteright}s camera space, rather than a canonical space, for the reasons discussed in {\\\\textsection}2. We validate this design choice in our experiments on unseen object categories ({\\\\textsection}5.2) and complex unseen scenes ({\\\\textsection}5.3). The model is trained with the volume rendering method and loss described in {\\\\textsection}3. \\n\\nIn the following, we first present our model for the single view case. We then show how this formulation can be easily extended to incorporate multiple input images. \\n\\nWe now describe our approach to render novel views from one input image. We fix our coordinate system as the view space of the input image and specify positions and camera rays in this coordinate system. \\n\\nGiven a input image I of a scene, we first extract a feature volume W = E(I). Then, for a point on a camera ray x, we retrieve the corresponding image feature by projecting x onto the image plane to the image coordinates {\\\\textasciicaron}(x) using \\n\\nFigure 2: Proposed architecture in the single-view case. For a query point x along a target camera ray with view direction d, a corresponding image feature is extracted from the feature volume W via projection and interpolation. This feature is then passed into the NeRF network f along with the spatial coordinates. The output RGB and density value is volume-rendered and compared with the target pixel value. The coordinates x and d are in the camera coordinate system of the input view. \\n\\nknown intrinsics, then bilinearly interpolating between the pixelwise features to extract the feature vector W({\\\\textasciicaron}(x)). The image features are then passed into the NeRF network, along with the position and view direction (both in the input view coordinate system), as \\n\\n(3) \\n\\nwhere ({\\\\textperiodcentered}) is a positional encoding on x with 6 exponentially increasing frequencies introduced in the original NeRF [27]. The image feature is incorporated as a residual at each layer; see {\\\\textsection}5 for more information. We show our pipeline schematically in Fig. 2. \\n\\nIn the few-shot view synthesis task, the query view direction is a useful signal for determining the importance of a particular image feature in the NeRF network. If the query view direction is similar to the input view orientation, the model can rely more directly on the input; if it is dissimilar, the model must leverage the learned prior. Moreover, in the multi-view case, view directions could serve as a signal for the relevance and positioning of different views. For this reason, we input the view directions at the beginning of the NeRF network. \\n\\nMultiple views provide additional information about the scene and resolve 3D geometric ambiguities inherent to the single-view case. We extend our model to allow for an arbitrary number of views at test time, which distinguishes our method from existing approaches that are designed to only use single input view at test time. [8, 53] Moreover, our formulation is independent of the choice of world space and the order of input views. \\n\\nIn the case that we have multiple input views of the scene, we assume only that the relative camera poses are known. For purposes of explanation, an arbitrary world coordinate system can be fixed for the scene. We denote the ith input image as I(i) and its associated camera transform from the world space to its view space as \\n\\nFor a new target camera ray, we transform a query point x, with view direction d, into the coordinate system of each input view i with the world to camera transform as \\n\\n(4) \\n\\nTo obtain the output density and color, we process the coordinates and corresponding features in each view coordinate frame independently and aggregate across the views within the NeRF network. For ease of explanation, we denote the initial layers of the NeRF network as f1, which process inputs in each input view space separately, and the final layers as f2, which process the aggregated views. \\n\\nWe encode each input image into feature volume For the view-space point x(i) , we extract the corresponding image feature from the feature volume W(i) at the projected image coordinate {\\\\textasciicaron}(x(i)). We then pass these inputs into f1 to obtain intermediate vectors: \\n\\n(5) \\n\\nThe intermediate V(i) are then aggregated with the average pooling operator and passed into a the final layers, denoted as f2, to obtain the predicted density and color: \\n\\n(6) \\n\\nIn the single-view special case, this simplifies to Equation 3 with f = f2 {\\\\textopenbullet}f1, by considering the view space as the world space. An illustration is provided in the supplemental. \\n\\n5. Experiments \\n\\nBaselines For ShapeNet benchmarks, we compare quantitatively and qualitatively to SRN [39] and DVR [28], the current state-of-the-art in few-shot novel-view synthesis and 2D-supervised single-view reconstruction respectively. We use the 2D multiview-supervised variant of DVR. In the category-agnostic setting ({\\\\textsection}5.1.2), we also include grayscale rendering of SoftRas [21] results. 1 In the experiments with multiple ShapeNet objects, we compare with SRN, which can also model entire scenes. \\n\\nFor the experiment on the DTU dataset, we compare to NeRF [27] trained on sparse views. Because NeRF is a test-time optimization method, we train a separate model for each scene in the test set. \\n\\nMetrics We report the standard image quality metrics PSNR and SSIM [55] for all evaluations. We also include LPIPS [52], which more accurately reflects human perception, in all evaluations except in the category-specific setup ({\\\\textsection}5.1.1). In this setting, we exactly follow the protocol of SRN [39] to remain comparable to prior works [40, 48, 9, 8, 43], for which source code is unavailable. \\n\\nImplementation Details For the image encoder E, to capture both local and global information effectively, we extract a feature pyramid from the image. We use a ResNet34 backbone pretrained on ImageNet for our experiments. Features are extracted prior to the first 4 pooling layers, upsam-pled using bilinear interpolation, and concatenated to form latent vectors of size 512 aligned to each pixel. \\n\\nTo incorporate a point{\\\\textquoteright}s corresponding image feature into the NeRF network f, we choose a ResNet architecture with a residual modulation rather than simply concatenating the feature vector with the point{\\\\textquoteright}s position and view direction. Specifically, we feed the encoded position and view direction through the network and add the image feature as a residual at the beginning of each ResNet block. We train an independent linear layer for each block residual, in a similar manner as AdaIn and SPADE [13, 30], a method previously used with success in [25, 28]. Please refer to the supplemental for additional details. \\n\\nWe first evaluate our approach on category-specific and category-agnostic view synthesis tasks on ShapeNet. \\n\\nWe perform one-shot and two-shot view synthesis on the {\\\\textquotedblleft}chair{\\\\textquotedblright} and {\\\\textquotedblleft}car{\\\\textquotedblright} classes of ShapeNet, using the protocol and dataset introduced in [39]. The dataset contains 6591 \\n\\nFigure 3: Category-specific single-view reconstruction benchmark. We train a separate model for cars and chairs and compare to SRN. The corresponding numbers may be found in Table 2. \\n\\nFigure 4: Category-specific 2-view reconstruction benchmark. We provide two views (left) to each model, and show two novel view renderings in each case (right). Please also refer to Table 2. \\n\\nTable 2: Category-specific 1-and 2-view reconstruction. Methods marked * do not require canonical poses at test time. In all cases, a single model is trained for each category and used for both 1-and 2-view evaluation. Note ENR is a 1-view only model. \\n\\nTable 3: Ablation studies for ShapeNet chair reconstruction. We show the benefit of using local features over a global code to condition the NeRF network (\\\\ensuremath{-}Local vs Full), and of providing view directions to the network (\\\\ensuremath{-}Dirs vs Full). \\n\\nFigure 5: Category-agnostic single-view reconstruction. Going beyond the SRN benchmark, we train a single model to the 13 largest ShapeNet categories; we find that our approach produces superior visual results compared to a series of strong baselines. In particular, the model recovers fine detail and thin structure more effectively, even for outlier shapes. Quite visibly, images on monitors and tabletop textures are accurately reproduced; baselines representing the scene as a single latent vector cannot preserve such details of the input image. SRN{\\\\textquoteright}s test-time latent inversion becomes less reliable as well in this setting. The corresponding quantitative evaluations are available in Table 4. Due to space constraints, we show objects with interesting properties here. Please see the supplemental for sampled results. \\n\\nchairs and 3514 cars with a predefined split across object instances. All images have resolution 128 {\\\\texttimes}128. \\n\\nA single model is trained for each object class with 50 random views per object instance, randomly sampling either one or two of the training views to encode. For testing, We use 251 novel views on an Archimedean spiral for each object in the test set of object instances, fixing 1-2 informative views as input. We report our performance in comparison with state-of-the-art baselines in Table 2, and show selected qualitative results in Fig. 4. We also include the quantitative results of baselines TCO [40] and dGQN [9] reported in [39] where applicable, and the values available in the recent works ENR [8] and GRF [43] in this setting. \\n\\nPixelNeRF achieves noticeably superior results despite solving a problem significantly harder than SRN because we: 1) use feed-forward prediction, without test-time optimization, 2) do not use ground-truth absolute camera poses at test-time, 3) use view instead of canonical space. \\n\\nWhile we found appreciable improvements over baselines in the simplest category-specific benchmark, our method is by no means constrained to it. We show in Table 4 and Fig. 5 that our approach offers a much greater advantage in the category-agnostic setting of [21, 28], where we train a single model to the 13 largest categories of ShapeNet. Please see the supplemental for randomly sampled results. \\n\\nWe follow community standards for 2D-supervised methods on multiple ShapeNet categories [28, 16, 21] and use the renderings and splits from Kato et al. [16], which provide 24 fixed elevation views of 64 {\\\\texttimes}64 resolution for each object instance. During both training and evaluation, a random view is selected as the input view for each object and shared across all baselines. The remaining 23 views are used as target views for computing metrics (see {\\\\textsection}5). \\n\\nTaking a step towards reconstruction in less controlled capture scenarios, we perform experiments on ShapeNet data in three more challenging setups: 1) unseen object categories, 2) multiple-object scenes, and 3) simulation-to-real \\n\\nFigure 6: Generalization to unseen categories. We evaluate a model trained on planes, cars, and chairs on 10 unseen ShapeNet categories. We find that the model is able to synthesize reasonable views even in this difficult case. \\n\\nFigure 7: 360{\\\\textopenbullet}view prediction with multiple objects. We show qualitative results of our method compared with SRN on scenes composed of multiple ShapeNet chairs. We are easily able to handle this setting, because our prediction is done in view space; in contrast, SRN predicts in canonical space, and struggles with scenes that cannot be aligned in such a way. \\n\\nTable 5: Image quality metrics for challenging ShapeNet tasks. (Left) Average metrics on 10 unseen categories for models trained on only planes, cars, and chairs. See the supplemental for a breakdown by category. (Right) Average metrics for two-view reconstruction for scenes with multiple ShapeNet chairs. \\n\\nFigure 8: Results on real car photos. We apply the car model from {\\\\textsection} 5.1.1 directly to images from the Stanford cars dataset [18]. The background has been masked out using PointRend [17]. The views are rotations about the view-space vertical axis. \\n\\ntransfer on car images. In these settings, successful reconstruction requires geometric priors; recognition or retrieval alone is not sufficient. \\n\\nGeneralization to novel categories. We first aim to reconstruct ShapeNet categories which were not seen in training. \\n\\nUnlike the more standard category-agnostic task described in the previous section, such generalization is impossible with semantic information alone. The results in Table 5 and Fig. 6 suggest our method learns intrinsic geometric and appearance priors which are fairly effective even for objects quite distinct from those seen during training. \\n\\nWe loosely follow the protocol used for zero-shot cross-category reconstruction from [53, ?]. Note that our baselines [39, 28] do not evaluate in this setting, and we adapt them for the sake of comparison. We train on the airplane, car, and chair categories and test on 10 categories unseen during training, continuing to use the Kato et al. renderings described in {\\\\textsection}5.1.2. \\n\\nMultiple-object scenes. We further perform few-shot 360{\\\\textopenbullet}reconstruction for scenes with multiple randomly placed and oriented ShapeNet chairs. In this setting, the network cannot rely solely on semantic cues for correct object placement and completion. The priors learned by the network must be applicable in an arbitrary coordinate system. We show in Fig. 7 and Table 5 that our formulation allows us to perform well on these simple scenes without additional design modifications. In contrast, SRN models scenes in a canonical space and struggles on held-out scenes. \\n\\nWe generate training images composed with 20 views randomly sampled on the hemisphere and render test images composed of a held out test set of chair instances, with 50 views sampled on an Archimedean spiral. During training, we randomly encode two input views; at test-time, we fix two informative views across the compared methods. In the supplemental, we provide example images from our dataset as well as additional quantitative results and qualitative comparisons with varying numbers of input views. \\n\\nSim2Real on Cars. We also explore the performance of pixelNeRF on real images from the Stanford cars dataset [18]. We directly apply car model from {\\\\textsection}5.1.1 without any fine-tuning. As seen in Fig. 8, the network trained on synthetic data effectively infers shape and texture of the real cars, suggesting our model can transfer beyond the synthetic domain. \\n\\nSynthesizing the 360{\\\\textopenbullet}background from a single view is nontrivial and out of the scope for this work. For this demonstration, the off-the-shelf PointRend [17] segmentation model is used to remove the background. \\n\\nFinally, we demonstrate that our method is applicable for few-shot wide baseline novel-view synthesis on real scenes in the DTU MVS dataset [14]. Learning a prior for view synthesis on this dataset poses significant challenges: not only does it consist of more complex scenes, without clear semantic similarities across scenes, it also contains inconsistent backgrounds and lighting between scenes. More-\\n\\nFigure 9: Wide baseline novel-view synthesis on a real image dataset. We train our model to distinct scenes in the DTU MVS dataset [14]. Perhaps surprisingly, even in this case, our model is able to infer novel views with reasonable quality for held-out scenes without further test-time optimization, all from only three views. Note the train/test sets share no overlapping scenes. \\n\\nFigure 10: PSNR of few-shot feed-forward DTU reconstruction. We show the quantiles of PSNR on DTU for our method and NeRF, given 1, 3, 6, or 9 input views. Separate NeRFs are trained per scene and number of input views, while our method requires only a single model trained with 3 encoded views. \\n\\nover, under 100 scenes are available for training. We found that the standard data split introduced in MVSNet [51] contains overlap between scenes of the training and test sets. Therefore, for our purposes, we use a different split of 88 training scenes and 15 test scenes, in which there are no shared or highly similar scenes between the two sets. Images are down-sampled to a resolution of 400 {\\\\texttimes}300. \\n\\nWe train one model across all training scenes by encoding 3 random views of a scene. During test time, we choose a set of fixed informative input views shared across all instances. We show in Fig. 9 that our method can perform view synthesis on the held-out test scenes. We further quantitatively compare the performance of our feed-forward model with NeRF optimized to the same set of input views in Fig. 10. Note that training each of 60 NeRFs took 14 hours; in contrast, pixelNeRF is applied to new scenes immediately without any test-time optimization. \\n\\n6. Discussion \\n\\nWe have presented pixelNeRF, a framework to learn a scene prior for reconstructing NeRFs from one or a few images. Through extensive experiments, we have established that our approach can be successfully applied in a variety of settings. We addressed some shortcomings of NeRF, but there are challenges yet to be explored: 1) Like NeRF, our rendering time is slow, and in fact, our runtime increases linearly when given more input views. Further, some methods (e.g. [28, 21]) can recover a mesh from the image enabling fast rendering and manipulation afterwards, while NeRF-based representations cannot be converted to meshes very reliably. Improving NeRF{\\\\textquoteright}s efficiency is an important research question that can enable real-time applications. 2) As in the vanilla NeRF, we manually tune ray sampling bounds tn, tf and a scale for the positional encoding. Making NeRF-related methods scale-invariant is a crucial challenge. 3) While we have demonstrated our method on real data from the DTU dataset, we acknowledge that this dataset was captured under controlled settings and has matching camera poses across all scenes with limited viewpoints. Ultimately, our approach is bottlenecked by the availability of large-scale wide baseline multi-view datasets, limiting the applicability to datasets such as ShapeNet and DTU. Learning a general prior for 360{\\\\textopenbullet}scenes in-the-wild is an exciting direction for future work. \\n\\nAcknowledgements \\n\\nWe thank Shubham Goel and Hang Gao for comments on the text. We also thank Emilien Dupont and Vincent Sitzmann for helpful discussions. \\n\\nReferences \\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loader\n",
    "loaders = [TextLoader(filename) for filename in filenames]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.014,\n",
       " 'namespaces': {'': {'vector_count': 1400}},\n",
       " 'total_vector_count': 1400}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_KEY, \n",
    "    environment=PINECONE_ENV, \n",
    ")\n",
    "\n",
    "index_name = \"document\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print(\"Index does not exist: \", index_name)\n",
    "    \n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "# index.delete(deleteAll='true', namespace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.vectorstores.pinecone.Pinecone"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch = Pinecone.from_texts([text.page_content for text in texts], \n",
    "                                embeddings, \n",
    "                                index_name=index_name)\n",
    "type(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the llm model for our qa session\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up the query \n",
    "query = \"What is NeRF?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "# Run the QA chain with your query to get the answer\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' NeRF is a neural scene representation that combines a single-scene '\n",
      " 'optimization setting with a neural scene representation capable of '\n",
      " 'representing complex scenes much more efficiently than a discrete 3D voxel '\n",
      " 'grid.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process the response from the QA chain \n",
    "# and isolate result and source docs and page numbers\n",
    "def parse_response(response):\n",
    "    print(response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source_name in response[\"source_documents\"]:\n",
    "        print(source_name.metadata['source'], \"page #:\", source_name.metadata['page'])\n",
    "\n",
    "# Set up the retriever on the pinecone vectorstore\n",
    "# Make sure to set include_metadata = True\n",
    "retriever = docsearch.as_retriever(include_metadata=True, metadata_key = 'source')\n",
    "\n",
    "# Set up the RetrievalQA chain with the retriever\n",
    "# Make sure to set return_source_documents = True\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain to get the response\n",
    "query = \"How are NeRF and iNeRF different?\"\n",
    "response = qa_chain(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'query': 'How are NeRF and iNeRF different?',\n",
      " 'result': ' NeRF optimizes a set of given camera poses and image observations '\n",
      "           'to recover a scene or object parameterized by , while iNeRF '\n",
      "           'solves the inverse problem of recovering the camera pose T given '\n",
      "           'the weights  and the image I as input.',\n",
      " 'source_documents': [Document(page_content='and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.', metadata={}),\n",
      "                      Document(page_content='Here we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model. . iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image. We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image. The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose. As illustrated in Figure (\\\\ensuremath{<}\\\\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate.', metadata={}),\n",
      "                      Document(page_content='V. RESULTS \\n\\nWe first conduct extensive experiments on the synthetic dataset from NeRF [22] and the real-world complex scenes from LLFF [21] to evaluate iNeRF for 6DoF pose estimation. Specifically, we study how the batch size of rays and sampling strategy affect iNeRF. We then show that iNeRF can improve NeRF by estimating the camera poses of images with unknown poses and using these images as additional training data for NeRF. Finally, we show that iNeRF works well in tandem with pixelNeRF [47] which predicts a NeRF model conditioned on a single RGB image. We test our method for category-level object pose estimation in both simulation and the real world. We found that iNeRF achieving competitive results against feature-based methods without accessing object mesh models during either training or test time. \\n\\nTABLE I: Benchmark on Fern scene. NeRFs trained with pose labels generated by iNeRF can achieve higher PSNR.', metadata={}),\n",
      "                      Document(page_content='Although NeRF originally needs to optimize the representation for every scene independently, several extensions [28], [39], [45], [47] have been proposed to directly predict a continuous neural scene representation conditioned on one or few input images. In our experiments, we show that iNeRF can be used to perform 6D pose estimation with either an optimized or predicted NeRF model. \\n\\nIV. INERF FORMULATION \\n\\nWe now present iNeRF, a framework that performs 6 DoF pose estimation by {\\\\textquotedblleft}inverting{\\\\textquotedblright} a trained NeRF. Let us assume that the NeRF of a scene or object parameterized by \\\\ensuremath{\\\\Theta} has already been recovered and that the camera intrinsics are known, but the camera pose T of an image observation I are as-yet undetermined. Unlike NeRF, which optimizes \\\\ensuremath{\\\\Theta} using a set of given camera poses and image observations, we instead solve the inverse problem of recovering the camera pose T given the weights \\\\ensuremath{\\\\Theta} and the image I as input: \\n\\n(1)', metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "pprint(type(response))\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
