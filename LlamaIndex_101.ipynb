{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env LLAMA_INDEX_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir='data', \n",
    "    required_exts=['.txt']\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(response='NeRF stands for Neural Radiance Fields. It is a framework '\n",
      "                  'that is used for view synthesis, which involves '\n",
      "                  'synthesizing photorealistic novel views of real-world '\n",
      "                  'scenes or objects. NeRF represents the density and color of '\n",
      "                  'a scene or object as a function of 3D scene coordinates. It '\n",
      "                  'can be learned from multi-view images with given camera '\n",
      "                  'poses or directly predicted by a generative model given one '\n",
      "                  'or few input images. NeRF has been shown to be effective '\n",
      "                  'for tasks such as mesh-free pose estimation and large-scale '\n",
      "                  'scene reconstruction.',\n",
      "         source_nodes=[NodeWithScore(node=TextNode(id_='12808301-86f7-4b00-94ed-36370555f6bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1182bbcf-0b0f-429b-abbe-faab21d08475', node_type=None, metadata={}, hash='4756ec13f81a627f763c5189900a5b73816f87311d08cec2b71ed37818758e85'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='87379fd0-ae8a-45a6-b70b-a23fbab328c2', node_type=None, metadata={}, hash='8d57005a4053fa438381b5a830fbf8f4b05eab8ab0de8c6497aeb1f132147846')}, hash='6c34c097b22afafc8b68d9f0cffdf2a12746512bc9e8e8a6b0bd3108782cbadd', text='iNeRF: Inverting Neural Radiance Fields for Pose Estimation \\n\\nFig.1: We present iNeRF which performs mesh-free pose estimation by inverting a neural radiance field of an object or scene.The middle figure shows the trajectory of estimated poses (gray) and the ground truth pose (green) in iNeRF{\\\\textquoteright}s iterative pose estimation procedure.By comparing the observed and rendered images, we perform gradient-based optimization to estimate the camera{\\\\textquoteright}s pose without accessing the object{\\\\textquoteright}s mesh model.Click the image to play the video in a browser.Abstract{\\\\textemdash} We present iNeRF, a framework that performs mesh-free pose estimation by {\\\\textquotedblleft}inverting{\\\\textquotedblright} a Neural Radiance Field (NeRF).NeRFs have been shown to be remarkably effective for the task of view synthesis {\\\\textemdash} synthesizing photorealistic novel views of real-world scenes or objects.In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation {\\\\textendash} given an image, find the translation and rotation of a camera relative to a 3D object or scene.Our method assumes that no object mesh models are available during either training or test time.Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image.In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset.We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF.Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.The recent advances of Neural Radiance Fields (NeRF [22]) provide a mechanism for capturing complex 3D and optical structures from only one or a few RGB images, which opens up the opportunity to apply analysis-by-synthesis to broader real-world scenarios without mesh models during training or test times.NeRF representations parameterize the density and color of the scene as a function of 3D scene coordinates.The function can either be learned from multi-view images with given camera poses [18], [22] or directly predicted by a generative model given one or few input images [45], [47].Here we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model..iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image.We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image.The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose.As illustrated in Figure (\\\\ensuremath{<}\\\\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate.Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB inputs by inverting a NeRF model inferred by pixelNeRF [47] given a single view of the object.The only prior work we are aware of that similarly provides RGB-only category-level pose estimation is the recent work of Chen et al.[3].In Sec.(\\\\ensuremath{<}\\\\ensuremath{>})II we compare differences between [3] and our work, which mostly arise from the opportunities and challenges presented by a continuous, implicit NeRF parameterization.To summarize, our primary contributions are as follows.(i) We show that iNeRF can use a NeRF model to estimate 6 DoF pose for scenes and objects with complex geometry, without the use of 3D mesh models or depth sensing {\\\\textemdash} only RGB images are used as input.(ii) We perform a thorough investigation of ray sampling and the batch sizes for gradient optimization to characterize the robustness and limitations of iNeRF.(iii) We show that iNeRF can improve NeRF by predicting the camera poses of additional images, that can then be added into NeRF{\\\\textquoteright}s training set.(iv) We show category-level pose estimation results, for unseen objects, including a real-world demonstration.II.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8329973081616556),\n",
      "                       NodeWithScore(node=TextNode(id_='ecb99451-a9e8-4979-9efb-959036a97c43', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4aa02ec1-63ac-40a3-b861-69ee25bea768', node_type=None, metadata={}, hash='8289c9142f645b34b0e93c00232c36b7dd744d8a3495cd3caacb6fb641478ded'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='80faec12-e58d-41a2-ae62-38eb5e7fc4a1', node_type=None, metadata={}, hash='b0a902c86a445e8f9cbc488170641858ef5100de8975425c57189bb78eed1216')}, hash='c7beea896c20bcb8296fb0c20f18d3b5d3b8368c9132e135ffdc75df5ecd41db', text='Block-NeRF: Scalable Large Scene Neural View Synthesis \\n\\nBen P. Mildenhall3 \\n\\nVincent Casser2 \\n\\nPratul Srinivasan3 \\n\\nXinchen Yan2 Jonathan T. Barron3 \\n\\nSabeek Pradhan2 Henrik Kretzschmar2 \\n\\nFigure 1.Block-NeRF is a method that enables large-scale scene reconstruction by representing the environment using multiple compact NeRFs that each fit into memory.At inference time, Block-NeRF seamlessly combines renderings of the relevant NeRFs for the given area.In this example, we reconstruct the Alamo Square neighborhood in San Francisco using data collected over 3 months.Block-NeRF can update individual blocks of the environment without retraining on the entire scene, as demonstrated by the construction on the right.Video results can be found on the project website waymo.com/research/block-nerf.We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments.Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs.This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment.We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions.We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined.We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.1.Introduction \\n\\nReconstructing large-scale environments enables several important use-cases in domains such as autonomous driving [30,43,69] and aerial surveying [14,33].For example, a high-fidelity map of the operating domain can serve as a prior for robot navigation.Large-scale scene reconstructions can be used for closed-loop robotic simulations [13].Autonomous driving systems are commonly evaluated by re-simulating previously encountered scenarios.Any deviation from the recorded encounter, however, may change the vehicle{\\\\textquoteright}s trajectory, requiring high-fidelity novel view renderings along the altered path.Scene conditioned NeRFs can further augment simulation scenarios by changing environmental lighting conditions, such as camera exposure, weather, or time of day.Reconstructing such large-scale environments introduces additional challenges, including the presence of transient objects (cars and pedestrians), limitations in model capacity, \\n\\nalong with memory and compute constraints.Furthermore, training data for such large environments is highly unlikely to be collected in a single capture under consistent conditions.Rather, data for different parts of the environment may need to be sourced from different data collection efforts, introducing variance in both scene geometry (e.g., construction work and parked cars), as well as appearance (e.g., weather conditions and time of day).We extend NeRF with appearance embeddings and learned pose refinement to address the environmental changes and pose errors in the collected data.We additionally add exposure conditioning to provide the ability to modify the exposure during inference.We refer to this modified model as a Block-NeRF.Scaling up the network capacity of Block-NeRF enables the ability to represent increasingly large scenes.However this approach comes with a number of limitations; rendering time scales with the size of the network, networks can no longer fit on a single compute device, and updating or expanding the environment requires retraining the entire network.To address these challenges, we propose dividing up large environments into individually trained Block-NeRFs, which are then rendered and combined dynamically at inference time.Modeling these Block-NeRFs independently allows for maximum flexibility, scales up to arbitrarily large environments and provides the ability to update or introduce new regions in a piecewise manner without retraining the entire environment as demonstrated in Figure 1.To compute a target view, only a subset of the Block-NeRFs are rendered and then composited based on their geographic location compared to the camera.To allow for more seamless compositing, we propose an appearance matching technique which brings different Block-NeRFs into visual alignment by optimizing their appearance embeddings.2.Related Work \\n\\nResearchers have been developing and refining techniques for 3D reconstruction from large image collections for decades [1,16,31,46,56,78], and much current work relies on mature and robust software implementations such as COLMAP to perform this task [54].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8322262565384327)],\n",
      "         metadata={'12808301-86f7-4b00-94ed-36370555f6bc': {},\n",
      "                   'ecb99451-a9e8-4979-9efb-959036a97c43': {}})\n"
     ]
    }
   ],
   "source": [
    "eng = index.as_query_engine()\n",
    "rsp = eng.query(\"What is NeRF?\")\n",
    "pprint(rsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Queries and Events Using Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/docstore.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/docstore.json\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/index_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/index_store.json\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/vector_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/vector_store.json\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/graph_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/graph_store.json\n"
     ]
    }
   ],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/docstore.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/docstore.json\n",
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/index_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/index_store.json\n",
      "DEBUG:llama_index.vector_stores.simple:Loading llama_index.vector_stores.simple from ./storage/vector_store.json.\n",
      "Loading llama_index.vector_stores.simple from ./storage/vector_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/vector_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/vector_store.json\n",
      "DEBUG:llama_index.graph_stores.simple:Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/tmquan/Repos/ResearchGPT/storage/graph_store.json\n",
      "open file: /Users/tmquan/Repos/ResearchGPT/storage/graph_store.json\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
