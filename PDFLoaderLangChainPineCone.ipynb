{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import time \n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_KEY = os.getenv(\"PINECONE_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmquan/opt/anaconda3/envs/rsgpt/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "import langchain\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf',\n",
       " 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf',\n",
       " 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf',\n",
       " 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf',\n",
       " 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf',\n",
       " 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf',\n",
       " 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf',\n",
       " 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf',\n",
       " 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf',\n",
       " 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf',\n",
       " 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf',\n",
       " 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf',\n",
       " 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf',\n",
       " 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf',\n",
       " 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf',\n",
       " 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf',\n",
       " 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf',\n",
       " 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf',\n",
       " 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf',\n",
       " 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect the file\n",
    "datadir = \"data\"\n",
    "filenames = sorted(glob.glob(os.path.join(datadir, r\"*.pdf\")))\n",
    "display(filenames)\n",
    "display(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='MedNeRF: Medical Neural Radiance Fields for\\nReconstructing 3D-aware CT-Projections from a Single X-ray\\nAbril Corona-Figueroa1, Jonathan Frawley1, Sam Bond-Taylor1,\\nSarath Bethapudi2, Hubert P. H. Shum1, Chris G. Willcocks1\\nAbstract— Computed tomography (CT) is an effective med-\\nical imaging modality, widely used in the field of clinical\\nmedicine for the diagnosis of various pathologies. Advances in\\nMultidetector CT imaging technology have enabled additional\\nfunctionalities, including generation of thin slice multiplanar\\ncross-sectional body imaging and 3D reconstructions. However,\\nthis involves patients being exposed to a considerable dose\\nof ionising radiation. Excessive ionising radiation can lead to\\ndeterministic and harmful effects on the body. This paper\\nproposes a Deep Learning model that learns to reconstruct\\nCT projections from a few or even a single-view X-ray. This is\\nbased on a novel architecture that builds from neural radiance\\nfields, which learns a continuous representation of CT scans by\\ndisentangling the shape and volumetric depth of surface and\\ninternal anatomical structures from 2D images. Our model is\\ntrained on chest and knee datasets, and we demonstrate qual-\\nitative and quantitative high-fidelity renderings and compare\\nour approach to other recent radiance field-based methods.\\nOur code and link to our datasets are available at https:\\n//github.com/abrilcf/mednerf\\nClinical relevance— Our model is able to infer the anatomical\\n3D structure from a few or a single-view X-ray, showing future\\npotential for reduced ionising radiation exposure during the\\nimaging process.\\nI. INTRODUCTION\\n3D medical imaging often involves joining multiple 2D\\nslices from CT or Magnetic Resonance Imaging (MRI), and\\npart of their workflow consists of specifying values for the\\nposition of the patient, the imaging source, and the detector.\\nThe quality and accuracy of a CT 3D representation require\\nhundreds of X-ray projections with a thin slice thickness\\n[1]. Moreover, this process exposes patients to more ionising\\nradiation than typical X-rays and requires the patient to\\nremain immobile for up to more than 1 hour, depending on\\nthe type of test [2]. Continuous 3D representations would\\ngive radiologists optics of every point in the internal anatomy\\ncaptured. While such representations are useful, there are\\npractical challenges in CT due to the increased radiation\\nexposure, angle-dependent structures, and time consumption\\n[3].\\nEarlier approaches in medical image reconstruction used\\nanalytic and iterative methods [4], [5] on given input data.\\nHowever, they often encounter mismatches between the\\nmathematical model and physical properties of the imaging\\nsystem. Instead, several recent approaches leverage deep\\n1Corona-Figueroa, Frawley, Bond-Taylor, Shum and Willcocks are with\\nthe Computer Science Department, Durham University, Durham, DH1 3LE,\\nUK abril.corona-figueroa@durham.ac.uk\\n2Bethapudi is with the County Durham and Darlington NHS Foundation\\nTrust, Durham, DL3 6HX, UK\\nlearning [6] for sparse view reconstruction [7], [8], [9],\\n3D CT reconstruction from 2D images [10], and anomaly\\ndetection [11]. These deep learning approaches solved the\\nmismatches between the mathematical model and imaging\\nsystem and reported improved reconstructions by fine-tuning\\nstate-of-the-art architectures. However, they require a large\\namount of training data, which may be difficult to meet in\\nthe medical domain where acquiring expert annotations is\\nboth cost and time prohibitive.\\nThe Neural Radiance Fields (NeRF) [12] model is a\\nrecent reformulation for estimating a 3D volumetric rep-\\nresentation from images. Such representations encode the\\nradiance field and density of the scene in the parameters\\nof a neural network. The neural network learns to synthesize\\nnew views via volume rendering from point samples along\\ncast rays. However, these representations are often captured\\nin controlled settings [13]. First, the scene is taken by a\\nset of fixed cameras within a short time frame. Second, all\\ncontent in the scene is static and real images often need\\nmasking. These constraints prohibit the direct application of\\nNeRF to the medical domain, where the imaging system\\ngreatly differs from conventional cameras, and the images\\nare captured over a long time frame hampering the patient’s\\nstillness. Moreover, the overlapping of anatomical structures\\nin medical images hinders the definition of edges which\\ncannot be easily solved with masking. These aspects explain\\nwhy the NeRF approach especially shows successes for\\n“natural images”.\\nTo address these challenges, we propose MedNeRF, a\\nmodel that adapts Generative Radiance Fields (GRAF) [14]\\nin the medical domain to render CT projections given a few\\nor even a single-view X-ray. Our approach not only synthe-\\nsizes realistic images, but also capture the data manifold and\\nprovides a continuous representation of how the attenuation\\nand volumetric depth of anatomical structures vary with the\\nviewpoint without 3D supervision. This is achieved via a\\nnew discriminator architecture that provides a stronger and\\nmore comprehensive signal to GRAF when dealing with CT\\nscans.\\nClosest to our goal are [8], [9], which both train a\\ncoordinate-based network in sinograms of low-dose CT of\\nphantom objects and apply it to the sparse-view tomography\\nreconstruction problem. In contrast to [8], we learn multiple\\nrepresentations in a single model by randomly feeding data of\\ndifferent medical instances instead of separately optimizing\\nfor each collection of images. For testing [9] reconstruction\\nability, they integrate it into reconstruction methods and use\\n2022 44th Annual International Conference of\\nthe IEEE Engineering in Medicine & Biology Society (EMBC)\\nScottish Event Campus, Glasgow, UK, July 11-15, 2022\\n978-1-7281-2782-8/22/$31.00 ©2022 IEEE\\n3843\\n2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) | 978-1-7281-2782-8/22/$31.00 ©2022 IEEE | DOI: 10.1109/EMBC48229.2022.9871757\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='at least 60 views. Different from their methods, we do not\\nrely on additional reconstruction algorithms, and we only\\nrequire multiple views during training.\\nWe render CT projections of our two datasets of digitally\\nreconstructed radiographs (DRR) from chest and knee. We\\nqualitative and quantitative demonstrate high-fidelity render-\\nings and compare our approach to other recent radiance\\nfield-based methods. Furthermore, we render CT projections\\nof a medical instance given a single-view X-ray and show\\nthe effectiveness of our model to cover surface and internal\\nstructures.\\nII. METHODS\\nA. Dataset Preparation\\nTo train our models, we generate DRRs instead of col-\\nlecting paired X-rays and corresponding CT reconstructions,\\nwhich would expose patients to more radiation. Furthermore,\\nDRR generation removes patient data and enables control\\nin capture ranges and resolutions. We generated DRRs by\\nusing 20 CT chest scans from [15], [16] and five CT knee\\nscans from [17], [18]. These scans cover a diverse group of\\npatients at different contrast types showing both normal and\\nabnormal anatomy. The radiation source and imaging panel\\nare assumed to rotate around the vertical-axis, generating a\\nDRR of 128×128 resolution at every five degrees, resulting\\nin 72 DRRs for each object. During training we use the whole\\nset of 72 DRRs (a fifth of all views within a full 360-degree\\nvertical rotation) per patient and let the model render the rest.\\nOur work did not involve experimental procedures on human\\nsubjects or animals and thus did not require Institutional\\nReview Board approval.\\nB. GRAF Overview\\nGRAF [14] is a model that builds from NeRF and defines\\nit within an Generative Adversarial Network (GAN). It\\nconsists of a generator Gθ that predicts an image patch\\nP pred and a discriminator Dϕ that compares the predicted\\npatch to a patch P real extracted from a real image. GRAF\\nhas shown an effective capacity to disentangle 3D shape and\\nviewpoint of objects from 2D images alone, in contrast to\\nthe original NeRF [12] and similar approaches such as [19].\\nTherefore, we aim to translate GRAF’s methods to our task,\\nand in subsection II-C we describe our new discriminator\\narchitecture, which allows us to disentangle 3D properties\\nfrom DRRs.\\nWe consider the experimental setting to obtain the radia-\\ntion attenuation response instead of the color used in natural\\nimages. To obtain the attenuation response at a pixel location\\nfor an arbitrary projection K with pose ξ, first, we consider\\na pattern ν = (u, s) to sample R X-ray beams within a\\nK × K image-patch P . Then, we sample N 3D points xi\\nr\\nalong the X-ray beam r originating from the pixel location\\nand ordered between the near and far planes of the projection\\n(Fig. 1a).\\nThe object representation is encoded in a multi-layer\\nperceptron (MLP) that takes as input a 3D position x =\\n(x, y, z) and a viewing direction d = (θ, ϕ), and produces\\n?\\n ν = (u, s)\\nK\\n{xi}= (x, y, z)\\nr\\n p ξ\\nγ\\na)\\nb)\\n c)\\nd)\\nzs\\n∫\\nc\\nc\\nhθ\\nσθ\\ncθ\\nq\\nza\\n{cr,σr}\\ngθ\\ngθ\\ngθ\\nGθ(zs, za, ξ, ν)\\nPredicted patch\\nFig. 1.\\nAn overview of GRAF’s generator.\\nas output a density scalar σ and a pixel value c. To learn\\nhigh-frequency features, the input is mapped into a 2L-\\ndimensional representation (Fig. 1b):\\nγ(p) = ..., cos(2jπp), sin(2jπp), ...\\n(1)\\nwhere p represents the 3D position or viewing direction, for\\nj = 0, ..., m − 1.\\nFor modeling the shape and appearance of anatomical\\nstructures, let zs ∼ ps and za ∼ pa be the latent codes\\nsampled from a standard Gaussian distribution, respectively\\n(Fig. 1c). To obtain the density prediction σ, the shape\\nencoding q is transformed to volume density through a\\ndensity head σθ. Then, the network gθ(·) operates on a shape\\nencoding q = (γ(x), zs) that is later concatenated with the\\npositional encoding of d and appearance code za (Fig. 1c):\\n(γ(x), zs) �→ q\\n(2)\\n(q(x, zs), γ(d), za) �→ c\\n(3)\\nq(x, zs) �→ σ\\n(4)\\nThe final pixel response cr is computed by the compositing\\noperation (Fig. 1c):\\ncr =\\nN\\n�\\ni=1\\nci\\nrαi\\nr exp (−\\ni−1\\n�\\nj=1\\nσj\\nrδj\\nr)\\n(5)\\nwhere αi\\nr = 1−exp (−σi\\nrδi\\nr) is the alpha compositing value\\nof sampled point i and δi\\nr =∥ xi+1\\nr\\n− xi\\nr ∥2 is the distance\\nbetween the adjacent sampled points.\\nIn this way, both the density and pixel values are computed\\nat each sampled point along the beam r with network gθ.\\nFinally, combining the results of all R beams, the generator\\nGθ predicts an image patch P pred, as illustrated in Fig. 1d.\\nC. MedNeRF\\nWe investigate how we can adapt GRAF to the medical\\ndomain and apply it to render a volumetric representation\\nfrom DRRs. Leveraging a large dataset, GRAF’s discrimi-\\nnator Dϕ is able to continuously provide useful signals to\\ntrain the generator Gθ. However, medical datasets like those\\nconsidered in our problem are generally small, which causes\\ntwo sequential issues:\\nThe lack of real information to the generator: In GRAF\\n(and in GAN in general), the only source of features of\\n3844\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 1, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='the training data contributing to the generator is the indirect\\ngradient transferred from the discriminator. We find that the\\nsingle convolutional feedback from GRAF’s discriminator\\npoorly conveys refined features from DRRs resulting in\\ninaccurate volumetric estimation.\\nBrittle adversarial training: With a limited training dataset,\\nthe generator or discriminator may fall into ill-posed settings\\nsuch as mode collapse, which would lead to generating a\\nlimited number of instances and consequently, a suboptimal\\ndata distribution estimation. While some works have applied\\ndata augmentation techniques to leverage more data in the\\nmedical domain, some transformations could mislead the\\ngenerator to learn the infrequent or even non-existent aug-\\nmented data distribution [20]. We find that naively applying\\nclassic data augmentation works less favorably than our\\nadopted framework.\\n1) Self-supervised Learning for High-Fidelity Synthesis:\\nTo allow richer feature-maps covering from the DRRs such\\nthat it produces more comprehensive signals to train Gθ,\\nwe replace GRAF’s discriminator architecture with recent\\nadvancements in self-supervised approaches. We allow Dϕ\\nto learn useful global and local features training it on a\\npretext task, in particular, the self-supervision method based\\non auto-encoding [21]. Different from [21], we only use\\ntwo decoders for the feature-maps on scales: f 1 on 322\\nand f 2 on 82 (Fig. 2a). We find that this choice allows\\nbetter performance and enables a correct volumetric depth\\nestimation. Dϕ must therefore not only discriminate P pred\\npredicted from Gθ but also extract comprehensive features\\nfrom real image patches P real that enable the decoders to\\nresemble the data distribution.\\nTo assess global structure in decoded patches from Dϕ, we\\nuse the Learned Perceptual Image Patch Similarity (LPIPS)\\nmetric [22]. We compute the weighted pairwise image\\ndistance between two VGG16 feature spaces, where the\\npretrained weights are fit to better match human perceptual\\njudgments. The additional discriminator loss is therefore:\\nLr = Ef∼D(p), p∼P\\n� 1\\nwhd ∥ ϕi(G(f)) − ϕi(T (p)) ∥2\\n�\\n(6)\\nwhere ϕi(·) denotes the ith layer output of a pretrained\\nVGG16 network, and w, h, and d stand for the width, height\\nand depth of a feature space, respectively. Let G be the\\nprocessing on the intermediate feature-maps f from Dϕ,\\nand T the processing on real image patches. When coupled\\nwith this additional reconstruction loss, the network learns\\nrepresentations that transfer across tasks.\\n2) Improving Learning via Data Augmentation:\\nWe improve learning of Gθ and Dϕ by adopting the\\nData Augmentation Optimized for GAN (DAG) framework\\n[20] in which a data augmentation transformation Tk (Fig.\\n2b) is applied using multiple discriminator heads {Dk}.\\nTo further reduce memory usage, we share all layers of\\nDϕ except the last layers corresponding to each head (Fig.\\nDΦ\\nSD\\nSD  : Simple Decoder\\nSD\\nP\\n,Ppart\\nr\\n(\\n(\\n,\\n)\\n)\\n+\\nTk\\nT1\\nD1\\nDk\\nT2(Gθ(zs, za, ξ, ν))\\nT2(I, ν)\\nTk(I, ν)\\nTk(Gθ(zs, za, ξ, ν))\\n...\\n(I, ν)\\nGθ(zs, za, ξ, ν)\\nReal patch\\nPredicted patch\\nShared\\nweights\\nShared\\nweights\\na)\\nb)\\nc)\\nFig. 2.\\nAn overview of our discriminator with self-supervised learning and\\nDAG.\\n2c). Because applying differentiable and invertible data aug-\\nmentation transformations Tk has the Jenssen-Shannon (JS)\\npreserving property [20]:\\nJS(pTk\\nd\\n∥ pTk\\ng ) = JS(pd ∥ pg)\\n(7)\\nwhere pTk\\nd\\nis the transformed training data distribution and\\npTk\\ng\\nthe transformed distribution captured by Gθ. By using a\\ntotal of four transformations combining flipping and rotation,\\nwe encourage optimization to the original data distribution,\\nwhich also brings the most performance boost. These choices\\nallow our model to benefit from not only JS(pd ∥ pg) but\\nalso JS(pTk\\nd\\n∥ pTk\\ng ), thereby improving the learning of Gθ and\\ngeneralization of Dϕ. Furthermore, using multiple discrim-\\ninators with weight-sharing provides learning regularization\\nof Dϕ.\\nReplacing GRAF’s logistic objective with a hinge loss, we\\nthen define our overall loss as below:\\nL(θ, {ϕk}) = L(θ, ϕ0) +\\nλ\\nn − 1\\nn\\n�\\nk=1\\nL(θ, ϕk)\\n(8)\\nL(θ, ϕk) =\\nEzs∼ps,za∼pa,ξ∼pξ,ν∼pν [f(Dϕ(Gθ(zs, za, ξ, ν)))]\\n+ EI∼pD,ν∼pν [f(−Dϕ(I, ν))] + Lr\\n(9)\\nwhere f(u) = max(0, 1+u). We optimize this loss with n =\\n4, where k = 0 corresponds to the identity transformation\\nand λ = 0.2 (as in [20]).\\n3) Volumetric Rendering from a Single View X-ray:\\nAfter training a model, we reconstruct the complete X-\\nray projections within a full vertical rotation of a medical\\ninstance given a single view X-ray. We follow the relaxed\\nreconstruction formulation in [23], which fits the generator\\nto a single image. Then, we allow the parameters of the\\ngenerator Gθ to be slightly fine-tuned along with the shape\\nand appearance latent vectors zs and za. The distortion and\\nperception tradeoff is well known in GAN methods [24] and\\ntherefore we modify our generation objective by adding the\\ndistortion Mean Square Error (MSE) loss, which incentivises\\n3845\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 2, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='Ours\\nGround Truth\\ngiven X-ray\\nreconstructed X-ray projections\\ncontrast\\ntransformation\\nFig. 3.\\nKnee renderings from continuous viewpoint rotations showing tissue and bone. Given a single-view X-ray from a CT, we can generate the complete\\nset of CT-projections within a full vertical rotation by slightly fine-tuning a pretrained model along with the shape and appearance latent codes.\\nTABLE I. Quantitative results based on PSNR and SSIM of rendered\\nX-ray projections with single-view X-ray input.\\nDataset\\n↑ PSNR (dB) (µ ± σ)\\n↑ SSIM (µ ± σ)\\nKnee\\n30.17 ± 1.93\\n0.670 ± 0.040\\nChest\\n28.54 ± 0.79\\n0.462 ± 0.082\\na balance between blurriness and accuracy:\\nLgen = λ1Lr(V GG16) + λ2LMSE(G) + λ3LNLLL(zs, za)\\n(10)\\nwhere NLLL corresponds to the negative log-likelihood loss\\nand the tuned hyperparameters lr = 0.0005, β1 = 0, β2 =\\n0.999, λ1 = 0.3, λ2 = 0.1 and λ3 = 0.3.\\nOnce the model locates an optimal combination of zs and\\nza, we replicate them and use them to render the rest of\\nthe X-ray projections by continuously controlling the angle\\nviewpoint.\\nIII. RESULTS\\nHere we provide an evaluation of MedNeRF on our\\ndatasets. We compare our model’s results to the ground\\ntruth, two baselines, perform an ablation study, and show\\nqualitative and quantitative evaluations. We train all models\\nfor 100,000 iterations with a batch size of 8. Projection\\nparameters (u, v) are chosen to evenly sample points on the\\nsurface of a sphere, specifically a slight horizontal elevation\\nof 70-85 degrees and umin = 0, umax = 1 for a full 360-\\ndegree vertical rotation. However, we only provide a fifth of\\nthe views (72-views each at five degrees) during training and\\nlet the model render the rest.\\nA. Reconstruction from Single View X-ray\\nWe evaluate our model’s representation for 3D-aware DRR\\nsynthesis given a single-view X-ray as input. We find that\\ndespite the implicit linear network’s limited capacity, our\\nmodel can disentangle 3D anatomy identity and attenuation\\nresponse of different medical instances, which are retrieved\\nthrough the described reconstruction reformulation in II-C.3.\\nOurs\\nGRAF\\npixelNeRF\\nChest\\nKnee\\nVolumetric map\\nVolumetric map\\nAttenuation\\nAttenuation\\nnear\\nfar\\nFig. 4.\\nVolumetric maps and attenuation renderings on our dataset.\\nOur model can also facilitate distinguishing bone from tissue\\nvia a contrast transformation, as it renders a brighter pixel\\nvalue for denser structures (e.g. bone) (Fig. 3).\\nTable I summarises our results based on the peak signal-to-\\nnoise ratio (PSNR) and structural similarity (SSIM), which\\nmeasure the quality of reconstructed signals and human\\nsubjective similarity, respectively. We find that our generative\\nloss can achieve a reasonable perception-distortion curve\\nin renderings and show consistency with the location and\\nvolumetric depth of anatomical structures at continuous\\nviewpoints compared to the ground truth.\\nB. 2D DRR Rendering\\nWe evaluate our model on the task of 2D rendering and\\ncompare it to pixelNeRF [19], and GRAF [14] baseline,\\nwherein the original architecture is used. Our model can\\nmore accurately estimate volumetric depth compared to\\n3846\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 3, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='TABLE II. FID and KID analysis comparing other methods.\\nChest dataset\\nKnee dataset\\nMethod\\n↓ FID (µ ± σ)\\n↓ KID (µ ± σ)\\n↓ FID (µ ± σ)\\n↓ KID (µ ± σ)\\nGRAF [14]\\n68.25 ± 0.954\\n0.053 ± 0.0008\\n76.70 ± 0.302\\n0.058 ± 0.0001\\npixelNeRF [19]\\n112.96 ± 2.356\\n0.084 ± 0.0012\\n166.40 ± 2.153\\n0.158 ± 0.0010\\nOurs\\n60.26 ± 0.322\\n0.041 ± 0.0005\\n76.12 ± 0.193\\n0.052 ± 0.0004\\nTABLE III. FID and KID analysis of ablations of our model.\\nChest dataset\\nAblation\\n↓ FID (µ ± σ)\\n↓ KID (µ ± σ)\\nMedNeRF - 3 SD, logistic loss, classic DA\\n84.85 ± 1.025\\n0.069 ± 0.0031\\nMedNeRF - 2 SD, logistic loss, classic DA\\n67.73 ± 0.712\\n0.051 ± 0.0006\\nMedNeRF - 2 SD, hinge loss, classic DA\\n65.34 ± 0.353\\n0.045 ± 0.0004\\nMedNeRF - 2 SD, hinge loss, DAG\\n60.26 ± 0.322\\n0.041 ± 0.0005\\nGRAF and pixelNeRF (Fig. 4). For each category, we find\\nan unseen target instance with a similar view direction\\nand shape. Volumetric depth estimation is given by bright\\ncolors (far) and dark colors (near). Lacking a perceptual\\nloss, GRAF is not incentivized to produce high-frequency\\ntextures. In contrast, we find our model renders a more\\ndetailed internal structure with varied attenuation. GRAF\\nproduces a consistent attenuation response, but seems to\\nbe unable to distinguish the anatomical shape from the\\nbackground. Our self-supervised discriminator enables the\\ngenerator to disentangle shape and background by rendering\\na brighter color for the background and a darker color for\\nthe shape, while GRAF renders a bright or dark color for\\nboth.\\nWe find pixelNeRF produces blurred attenuation render-\\nings for all datasets, and volumetric maps tend to exhibit\\nstrong color shifts (Fig. 4). We believe these artifacts are due\\nto the see-through nature of the dataset, compared to solid-\\nlike natural objects on which NeRFs are trained. This data\\ncharacteristic impairs not only volumetric maps but also fine\\nanatomical structures. In contrast, our model is better able\\nto render both volumetric depth and attenuation response.\\nWe also find pixelNeRF is sensitive to slight changes in\\nprojection parameters, hampering optimization for the knee\\ncategory. Our model produces a consistent 3D geometry and\\ndoes not rely on explicit projection matrices.\\nTable II compares image quality based on Frechet Incep-\\ntion Distance (FID) and Kernel Inception Distance (KID)\\nmetrics, in which lower values mean better. Optimizing\\npixelNeRF on our datasets leads to particularly poor results\\nthat are unable to compete with the GRAF baseline and our\\nmodel. In contrast, our model outperforms the baselines on\\nFID and KID metrics for all datasets.\\nC. Ablation Study\\nWe evaluate our model with three ablations (Table III):\\nwherein an additional simple decoder (SD) is included; the\\nadversarial logistic loss is replaced by its hinge version; and\\nwherein the non-classical DAG approach is adopted. We find\\nthat that the DAG approach brings the most performance\\nboost compared to naively applying classical DA, while\\nthe use of a hinge loss performs slightly better than its\\nlogistic version. However, an additional decoder in our self-\\nsupervised discriminator can lead to a significant drop in\\nperformance.\\nIV. CONCLUSION\\nWe have presented a novel Deep Learning architecture\\nbased on Neural Radiance Fields for learning a continuous\\nrepresentation of CT scans. We learn a medical category\\nencoding of the attenuation response of a set of 2D DRRs\\nin the weights of a generator. Furthermore, we have found\\nthat a stronger and more comprehensive signal from our\\ndiscriminator allows generative radiance fields to model 3D-\\naware CT-projections. Experimental evaluation demonstrates\\nsignificant qualitative and quantitative reconstructions and\\nimprovements over other Neural Radiance Field approaches.\\nWhilst the proposed model may not replace CT entirely, the\\nfunctionality of generating 3D-aware CT-projections from X-\\nrays has great potential for clinical use in osseous trauma,\\nskeletal evaluation in dysplasia and for orthopaedic pre-\\nsurgical planning. This could cut down on the radiation dose\\ngiven to patients, with significant economic implications such\\nas bringing down the cost of investigations.\\nACKNOWLEDGMENT\\nThis work is partially supported by the Mexican Council\\nof Science and Technology (CONACyT).\\nREFERENCES\\n[1] Paul Suetens, Visualization for diagnosis and therapy, p. 190–218,\\nCambridge University Press, 2 edition, 2009.\\n[2] Pechin Lo, Bram van Ginneken, Joseph M. Reinhardt, Tarunashree\\nYavarna, Pim A. de Jong, Benjamin Irving, Catalin Fetita, Margarete\\nOrtner, Rˆomulo Pinho, Jan Sijbers, Marco Feuerstein, Anna Fabijan-\\nska, Christian Bauer, Reinhard Beichel, Carlos S. Mendoza, Rafael\\nWiemker, Jaesung Lee, Anthony P. Reeves, Silvia Born, Oliver Wein-\\nheimer, Eva M. van Rikxoort, Juerg Tschirren, Ken Mori, Benjamin\\nOdry, David P. Naidich, Ieneke Hartmann, Eric A. Hoffman, Mathias\\nProkop, Jesper H. Pedersen, and Marleen de Bruijne, “Extraction of\\nairways from ct (exact’09),” IEEE Transactions on Medical Imaging,\\nvol. 31, no. 11, pp. 2093–2107, 2012.\\n[3] Mary Coffey and Aude Vaandering, “Patient setup for pet/ct acquisi-\\ntion in radiotherapy planning,” Radiotherapy and Oncology, vol. 96,\\nno. 3, pp. 298–301, 2010, PET in Radiotherapy Planning.\\n[4] Tri Huynh, Yaozong Gao, Jiayin Kang, Li Wang, Pei Zhang, Jun\\nLian, and Dinggang Shen, “Estimating ct image from mri data using\\nstructured random forest and auto-context model,” IEEE Transactions\\non Medical Imaging, vol. 35, no. 1, pp. 174–183, 2016.\\n[5] Shoulie Xie, Weimin Huang, Tao Yang, Dajun Wu, and Huiying\\nLiu, “Compressed sensing based image reconstruction with projection\\nrecovery for limited angle cone-beam ct imaging,”\\nin 2020 42nd\\nAnnual International Conference of the IEEE Engineering in Medicine\\nBiology Society (EMBC), 2020, pp. 1307–1310.\\n[6] Ge Wang, Jong Chu Ye, Klaus Mueller, and Jeffrey A. Fessler,\\n“Image reconstruction is a new frontier of machine learning,” IEEE\\nTransactions on Medical Imaging, vol. 37, no. 6, pp. 1289–1296, 2018.\\n[7] Yinsheng Li, Ke Li, Chengzhu Zhang, Juan Montoya, and Guang-\\nHong Chen, “Learning to reconstruct computed tomography images\\ndirectly from sinogram data under a variety of data acquisition\\nconditions,” IEEE Transactions on Medical Imaging, vol. 38, no. 10,\\npp. 2469–2481, 2019.\\n[8] David B. Lindell, Julien N. P. Martel, and Gordon Wetzstein, “Autoint:\\nAutomatic integration for fast neural volume rendering,” in Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), June 2021, pp. 14556–14565.\\n[9] Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, and Ulugbek S.\\nKamilov, “Coil: Coordinate-based internal learning for tomographic\\nimaging,” IEEE Transactions on Computational Imaging, vol. 7, pp.\\n1400–1412, 2021.\\n3847\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 4, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='[10] Xingde Ying, Heng Guo, Kai Ma, Jian Wu, Zhengxin Weng, and\\nYefeng Zheng, “X2ct-gan: Reconstructing ct from biplanar x-rays with\\ngenerative adversarial networks,” in 2019 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), 2019, pp. 10611–\\n10620.\\n[11] Bao Nguyen, Adam Feldman, Sarath Bethapudi, Andrew Jennings, and\\nChris G. Willcocks, “Unsupervised region-based anomaly detection\\nin brain mri with adversarial image inpainting,” in 2021 IEEE 18th\\nInternational Symposium on Biomedical Imaging (ISBI), 2021, pp.\\n1127–1131.\\n[12] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.\\nBarron, Ravi Ramamoorthi, and Ren Ng, “Nerf: Representing scenes\\nas neural radiance fields for view synthesis,” in Computer Vision –\\nECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-\\nMichael Frahm, Eds., Cham, 2020, pp. 405–421, Springer International\\nPublishing.\\n[13] Ricardo\\nMartin-Brualla,\\nNoha\\nRadwan,\\nMehdi\\nS.\\nM.\\nSajjadi,\\nJonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth,\\n“NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\\nCollections,” in CVPR, 2021.\\n[14] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger,\\n“Graf: Generative radiance fields for 3d-aware image synthesis,” in\\nAdvances in Neural Information Processing Systems (NeurIPS), 2020.\\n[15] E B Tsai, S Lungren, M P Hershman, M Roshkovan, L Colak,\\nE Erickson, B J Shih, G Stein, A Kalpathy-Cramer, J Shen, J Hafez,\\nM A F John, S Rajiah, P Pogatchnik, B P Mongan, J T Altinmakas,\\nE Ranschaert, E R Kitamura, and C C. Wu,\\n“Medical imaging\\ndata resource center (midrc) - rsna international covid open research\\ndatabase (ricord) release 1b - chest ct covid-[dataset],” The Cancer\\nImaging Archive, 2021.\\n[16] K Clark, B Vendt, K Smith, J Freymann, J Kirby, P Koppel, S Moore,\\nS Phillips, D Maffitt, M Pringle, L Tarbox, and F Prior,\\n“The\\ncancer imaging archive (tcia): Maintaining and operating a public\\ninformation repository,” Journal of Digital Imaging, vol. 26, no. 6,\\npp. 1045—1057, December 2013.\\n[17] Michael D Harris, Adam J Cyr, Azhar A Ali, Clare K Fitzpatrick,\\nPaul J Rullkoetter, Lorin P Maletsky, and Kevin B Shelburne,\\n“A\\ncombined experimental and computational approach to subject-specific\\nanalysis of knee joint laxity,” Journal of biomechanical engineering,\\nvol. 138, no. 8, August 2016.\\n[18] Azhar A Ali, Sami S Shalhoub, Adam J Cyr, Clare K Fitzpatrick,\\nLorin P Maletsky, Paul J Rullkoetter, and Kevin B Shelburne, “Vali-\\ndation of predicted patellofemoral mechanics in a finite element model\\nof the healthy and cruciate-deficient knee,” Journal of biomechanics,\\nvol. 49, no. 2, pp. 302—309, January 2016.\\n[19] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa, “pix-\\nelNeRF: Neural radiance fields from one or few images,” in CVPR,\\n2021.\\n[20] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien\\nNguyen, and Ngai-Man Cheung,\\n“On data augmentation for gan\\ntraining,” IEEE Transactions on Image Processing, vol. 30, pp. 1882–\\n1897, 2021.\\n[21] Bingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal,\\n“Towards faster and stabilized {gan} training for high-fidelity few-\\nshot image synthesis,”\\nin International Conference on Learning\\nRepresentations, 2021.\\n[22] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and\\nOliver Wang,\\n“The unreasonable effectiveness of deep features as\\na perceptual metric,”\\nin Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), June 2018.\\n[23] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy,\\nand Ping Luo, “Exploiting deep generative prior for versatile image\\nrestoration and manipulation,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence, pp. 1–1, 2021.\\n[24] Yochai Blau and Tomer Michaeli, “The perception-distortion tradeoff,”\\nin Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), June 2018.\\n3848\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:25:19 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'file_path': 'data/Corona-Figueroa et al. - 2022 - MedNeRF Medical Neural Radiance Fields for Recons.pdf', 'page': 5, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray', 'author': '', 'subject': '2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC);2022; ; ;10.1109/EMBC48229.2022.9871757', 'keywords': '', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64); modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': \"D:20220823114122-07'00'\", 'modDate': \"D:20220905062819-04'00'\", 'trapped': ''}),\n",
       " Document(page_content='Plenoxels: Radiance Fields without Neural Networks\\nSara Fridovich-Keil⇤\\nAlex Yu⇤\\nMatthew Tancik\\nQinhong Chen\\nBenjamin Recht\\nAngjoo Kanazawa\\nUC Berkeley\\nAbstract\\nWe introduce Plenoxels (plenoptic voxels), a system for\\nphotorealistic view synthesis. Plenoxels represent a scene as\\na sparse 3D grid with spherical harmonics. This representa-\\ntion can be optimized from calibrated images via gradient\\nmethods and regularization without any neural components.\\nOn standard, benchmark tasks, Plenoxels are optimized two\\norders of magnitude faster than Neural Radiance Fields with\\nno loss in visual quality. For video and code, please see\\nhttps://alexyu.net/plenoxels.\\n1. Introduction\\nA recent body of research has capitalized on implicit,\\ncoordinate-based neural networks as the 3D representation\\nto optimize 3D volumes from calibrated 2D image super-\\nvision. In particular, Neural Radiance Fields (NeRF) [28]\\ndemonstrated photorealistic novel viewpoint synthesis, cap-\\nturing scene geometry as well as view-dependent effects.\\nThis impressive quality, however, requires extensive com-\\nputation time for both training and rendering, with training\\nlasting more than a day and rendering requiring 30 sec-\\nonds per frame, on a single GPU. Multiple subsequent pa-\\npers [9, 10, 21, 37, 38, 59] reduced this computational cost\\nfor rendering, but single GPU training still requires multiple\\nhours, a bottleneck that limits the practical application of\\nphotorealistic volumetric reconstruction.\\nIn this paper, we show that we can train a radiance ﬁeld\\nfrom scratch, without neural networks, while maintaining\\nNeRF quality and reducing optimization time by two orders\\nof magnitude. We provide a custom CUDA [31] implemen-\\ntation that capitalizes on the model simplicity to achieve\\nsubstantial speedups. Our typical optimization time on a\\nsingle Titan RTX GPU is 11 minutes on bounded scenes\\n(compared to roughly 1 day for NeRF, more than a 100⇥\\nspeedup) and 27 minutes on unbounded scenes (compared\\n* Authors contributed equally to this work.\\nNeRF\\nPlenoxel\\nTraining Time (minutes)\\nPSNR\\nNeRF\\nPlenoxel\\n10 min\\n5 min\\n1 min\\n20\\n25\\n30\\n35\\n60\\n50\\n40\\n30\\n20\\n10\\n0\\n100x Faster Convergence\\nFigure 1. Plenoxel: Plenoptic Volume Elements for fast optimiza-\\ntion of radiance ﬁelds. We show that direct optimization of a fully\\nexplicit 3D model can match the rendering quality of modern neural\\nbased approaches such as NeRF while optimizing over two orders\\nof magnitude faster.\\nto roughly 4 days for NeRF++ [60], again more than a 100⇥\\nspeedup). Although our implementation is not optimized for\\nfast rendering, we can render novel viewpoints at interactive\\nrates (15 fps). If faster rendering is desired, our optimized\\nPlenoxel model can be converted into a PlenOctree [59].\\nSpeciﬁcally, we propose an explicit volumetric represen-\\ntation, based on a view-dependent sparse voxel grid without\\nany neural networks. Our model can render photorealistic\\nnovel viewpoints and be optimized end-to-end from cali-\\nbrated 2D photographs, using the differentiable rendering\\nloss on training views along with a total variation regularizer.\\n5501\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='We call our model Plenoxel for plenoptic volume elements,\\nas it consists of a sparse voxel grid in which each voxel\\nstores density and spherical harmonic coefﬁcients, which\\nmodel view dependence [1]. By interpolating these coefﬁ-\\ncients, Plenoxels achieve a continuous model of the plenoptic\\nfunction [1]: the light at every position and in every direc-\\ntion inside a volume. To achieve high resolution on a single\\nGPU, we prune empty voxels and follow a coarse to ﬁne\\noptimization strategy. Although our core model is a bounded\\nvoxel grid, we show that unbounded scenes can be modeled\\nby using normalized device coordinates (for forward-facing\\nscenes) or by surrounding our grid with multisphere images\\nto encode the background (for 360◦ scenes).\\nOur method reveals that photorealistic volumetric recon-\\nstruction can be approached using standard tools from in-\\nverse problems: a data representation, a forward model, a\\nregularization function, and an optimizer. Our method shows\\nthat each of these components can be simple and state of\\nthe art results can still be achieved. Our experiments suggest\\nthe key element of Neural Radiance Fields is not the neural\\nnetwork but the differentiable volumetric renderer.\\n2. Related Work\\nClassical Volume Reconstruction.\\nWe begin with a brief\\noverview of classical methods for volume reconstruction,\\nfocusing on those which ﬁnd application in our work. The\\nmost common classical methods for volume rendering are\\nvoxel grids [7,14,19,22,43–45,53,54] and multi-plane im-\\nages (MPIs) [27,34,48,49,58,62]. Voxel grids are capable of\\nrepresenting arbitrary topologies but can be memory limited\\nat high resolution. One approach for reducing the memory\\nrequirement for voxel grids is to encode hierarchical struc-\\nture, for instance using octrees [12,40,52,55] (see [17] for\\na survey); we use an even simpler sparse array structure.\\nUsing these grid-based representations combined with some\\nform of interpolation [54] produces a continuous represen-\\ntation that can be arbitrarily resized using standard signal\\nprocessing methods (see [32] for reference). We combine\\nthis classical sampling and interpolation paradigm with the\\nforward volume rendering formula introduced by Max [24]\\n(based on work from Kajiya and Von Herzen [13] and used\\nin NeRF) to directly optimize a 3D grid from indirect 2D\\nobservations. We further extend these classical approaches\\nby modeling view dependence, which we accomplish by\\noptimizing spherical harmonic coefﬁcients for each color\\nchannel at each voxel. Spherical harmonics are a standard\\nbasis for functions over the sphere, and have been used pre-\\nviously to represent view dependence [4,36,47,58,59].\\nNeural Volume Reconstruction.\\nRecently, dramatic im-\\nprovements in neural volume reconstruction have renewed\\ninterest in this direction. Neural representations were ﬁrst\\nused to model occupancy [6,23,26] and signed distance to\\nan object’s surface [33, 50], and perform novel view syn-\\nthesis from 3D point clouds [2, 18, 42, 57]. Several papers\\nextended this idea to model a 3D scene using only calibrated\\n2D image supervision via a differentiable volume rendering\\nformulation [22,28,45,46]. NeRF [28] in particular produces\\nimpressive results but requires more than a day for full train-\\ning, and about half an minute to render a full 800 ⇥ 800\\nimage, because every rendered pixel requires evaluating a\\ncoordinate-based MLP at hundreds of sample locations along\\nthe corresponding ray. Many papers have since extended the\\ncapabilities of NeRF, including modeling the background\\nin 360◦ views [60] and incorporating anti-aliasing for mul-\\ntiscale rendering [3]. We extend our Plenoxel method to\\nunbounded 360◦ scenes using a background model inspired\\nby NeRF++ [60].\\nOf these methods, Neural Volumes [22] is the most simi-\\nlar to ours in that it uses a voxel grid with interpolation, but\\noptimizes this grid through a convolutional neural network\\nand applies a learned warping function to improve the ef-\\nfective resolution (of a 1283 grid). We show that the voxel\\ngrid can be optimized directly and high resolution can be\\nachieved by pruning and coarse to ﬁne optimization, without\\nany neural networks or warping functions.\\nAccelerating NeRF.\\nIn light of the substantial computa-\\ntional requirements of NeRF for both training and rendering,\\nmany recent papers have proposed methods to improve efﬁ-\\nciency, particularly for rendering. Among these methods are\\nsome that achieve speedup by subdividing the 3D volume\\ninto regions that can be processed more efﬁciently [21,37].\\nOther speedup approaches have focused on a range of com-\\nputational and pre- or post-processing methods to remove\\nbottlenecks in the original NeRF formulation. JAXNeRF [8],\\na JAX [5] reimplementation of NeRF, offers a speedup for\\nboth training and rendering via parallelization across many\\nGPUs or TPUs. AutoInt [20] restructures the coordinate-\\nbased MLP to compute ray integrals exactly, for more than\\n10⇥ faster rendering with a small loss in quality. Learned\\nInitializations [51] employs meta-learning on many scenes to\\nstart from a better MLP initialization, for both > 10⇥ faster\\ntraining and better priors when per-scene data is limited.\\nOther methods [15,29,35] achieve speedup by predicting a\\nsurface or sampling near the surface, reducing the number\\nof samples necessary for rendering each ray.\\nAnother approach is to pretrain a NeRF (or similar model)\\nand then extract it into a different data structure that can\\nsupport fast inference [9,10,38,59]. In particular, PlenOc-\\ntrees [59] extracts a NeRF variant into a sparse voxel grid\\nin which each voxel represents view-dependent color us-\\ning spherical harmonic coefﬁcients. Because the extracted\\nPlenOctree can be further optimized, this method can speed\\nup training by roughly 3⇥, and because it uses an efﬁcient\\nGPU octree implementation without any MLP evaluations,\\n5502\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Spherical\\nHarmonics\\nb) Trilinear Interpolation\\na) Sparse Voxel Grid\\n�\\nRay Distance\\nTraining\\nImage\\nd) Optimization\\nc) Volumetric Rendering\\nminimize\\n��\\nPredicted\\nColor\\nFigure 2. Overview of our sparse Plenoxel model. Given a set of images of an object or a scene, we optimize a (a) sparse voxel (“Plenoxel”)\\ngrid with density and spherical harmonic coefﬁcients at each voxel. To render a ray, we (b) compute the color and opacity of each sample point\\nvia trilinear interpolation of the neighboring voxel coefﬁcients. We integrate the color and opacity of these samples using (c) differentiable\\nvolume rendering, following the recent success of NeRF [28]. The voxel coefﬁcients can then be (d) optimized using the standard MSE\\nreconstruction loss relative to the training images, along with a total variation regularizer.\\nit achieves > 3000⇥ rendering speedup. Our method ex-\\ntends PlenOctrees to perform end-to-end optimization of a\\nsparse voxel representation with spherical harmonics, offer-\\ning much faster training (two orders of magnitude speedup\\ncompared to NeRF). Our Plenoxel model is a generalization\\nof PlenOctrees to support sparse plenoptic voxel grids of\\narbitrary resolution (not necessary powers of two) with the\\nability to perform trilinear interpolation, which is easier to\\nimplement with this sparse voxel structure.\\n3. Method\\nOur model is a sparse voxel grid in which each occupied\\nvoxel corner stores a scalar density σ and a vector of spheri-\\ncal harmonic (SH) coefﬁcients for each color channel. From\\nhere on we refer to this representation as Plenoxels. The\\ndensity and color at an arbitrary position and viewing direc-\\ntion are determined by trilinearly interpolating the values\\nstored at the neighboring voxels and evaluating the spherical\\nharmonics at the appropriate viewing direction. Given a set\\nof calibrated images, we optimize our model directly using\\nthe rendering loss on training rays. Our model is illustrated\\nin Fig. 2 and described in detail below.\\n3.1. Volume Rendering\\nWe use the same differentiable model for volume render-\\ning as in NeRF, where the color of a ray is approximated by\\nintegrating over samples taken along the ray:\\nˆC(r) =\\nN\\nX\\ni=1\\nTi\\n�\\n1 − exp(−σiδi)\\n�\\nci\\n(1)\\nwhere\\nTi = exp\\n0\\n@−\\ni−1\\nX\\nj=1\\nσjδj\\n1\\nA\\n(2)\\nTi represents how much light is transmitted through ray r\\nto sample i, (1 − exp(−σiδi)) denotes how much light is\\ncontributed by sample i, σi denotes the density of sample\\ni, and ci denotes the color of sample i, with distance δi\\nto the next sample. Although this formula is not exact (it\\nassumes single-scattering [13] and constant values between\\nsamples [24]), it is differentiable and enables updating the\\n3D model based on the error of each training ray.\\n3.2. Voxel Grid with Spherical Harmonics\\nSimilar to PlenOctrees [59], we use a sparse voxel grid\\nfor our geometry model. However, for simplicity and ease\\nof implementing trilinear interpolation, we do not use an\\noctree for our data structure. Instead, we store a dense 3D\\nindex array with pointers into a separate data array con-\\ntaining values for occupied voxels only. Like PlenOctrees,\\neach occupied voxel stores a scalar density σ and a vector\\nof spherical harmonic coefﬁcients for each color channel.\\nSpherical harmonics form an orthogonal basis for functions\\ndeﬁned over the sphere, with low degree harmonics encoding\\nsmooth (more Lambertian) changes in color and higher de-\\ngree harmonics encoding higher-frequency (more specular)\\neffects. The color of a sample ci is simply the sum of these\\nharmonic basis functions for each color channel, weighted\\nby the corresponding optimized coefﬁcients and evaluated\\nat the appropriate viewing direction. We use spherical har-\\nmonics of degree 2, which requires 9 coefﬁcients per color\\nchannel for a total of 27 harmonic coefﬁcients per voxel.\\nWe use degree 2 harmonics because PlenOctrees found that\\nhigher order harmonics confer only minimal beneﬁt.\\nPlenoxel grid uses trilinear interpolation to deﬁne a con-\\ntinuous plenoptic function throughout the volume. This is\\nin contrast to PlenOctrees, which assumes that the density\\nand spherical harmonic coefﬁcients remain constant inside\\n5503\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='each voxel. This difference turns out to be an important fac-\\ntor in successfully optimizing the volume, as we discuss\\nbelow. All coefﬁcients (for density and spherical harmonics)\\nare optimized directly, without any special initialization or\\npretraining with a neural network.\\n3.3. Interpolation\\nThe density and color at each sample point along each\\nray are computed by trilinear interpolation of density and\\nharmonic coefﬁcients stored at the nearest 8 voxels. We ﬁnd\\nthat trilinear interpolation signiﬁcantly outperforms a sim-\\npler nearest neighbor interpolation; see Tab. 1. The beneﬁts\\nof interpolation are twofold: interpolation increases the effec-\\ntive resolution by representing sub-voxel variations in color\\nand density, and interpolation produces a continuous func-\\ntion approximation that is critical for successful optimization.\\nBoth of these effects are evident in Tab. 1: doubling the res-\\nolution of a nearest-neighbor-interpolating Plenoxel closes\\nmuch of the gap between nearest neighbor and trilinear inter-\\npolation at a ﬁxed resolution, yet some gap remains due to\\nthe difﬁculty of optimizing a discontinuous model. Indeed,\\nwe ﬁnd that trilinear interpolation is more stable with respect\\nto variations in learning rate compared to nearest neighbor\\ninterpolation (we tuned the learning rates separately for each\\ninterpolation method in Tab. 1, to provide close to the best\\nnumber possible for each setup).\\nPSNR \"\\nSSIM \"\\nLPIPS #\\nTrilinear, 2563\\n30.57\\n0.950\\n0.065\\nTrilinear, 1283\\n28.46\\n0.926\\n0.100\\nNearest Neighbor, 2563\\n27.17\\n0.914\\n0.119\\nNearest Neighbor, 1283\\n23.73\\n0.866\\n0.176\\nTable 1. Ablation over interpolation method. Results are averaged\\nover the 8 NeRF synthetic scenes. We ﬁnd that trilinear interpola-\\ntion provides dual beneﬁts of improving effective resolution and\\nimproving optimization, such that trilinear interpolation at resolu-\\ntion 1283 outperforms nearest neighbor interpolation at 2563.\\n3.4. Coarse to Fine\\nWe achieve high resolution via a coarse-to-ﬁne strategy\\nthat begins with a dense grid at lower resolution, optimizes,\\nprunes unnecessary voxels, reﬁnes the remaining voxels by\\nsubdividing each in half in each dimension, and continues\\noptimizing. For example, in the synthetic case, we begin\\nwith 2563 resolution and upsample to 5123. We use trilinear\\ninterpolation to initialize the grid values after each voxel\\nsubdivision step. In fact, we can resize between arbitrary\\nresolutions using trilinear interpolation. Voxel pruning is per-\\nformed using the method from PlenOctrees [59], which ap-\\nplies a threshold to the maximum weight Ti(1−exp(−σiδi))\\nof each voxel over all training rays (or, alternatively, to the\\ndensity value in each voxel). Due to trilinear interpolation,\\nnaively pruning can adversely impact the the color and den-\\nsity near surfaces since values at these points interpolate with\\nthe voxels in the immediate exterior. To solve this issue, we\\nperform a dilation operation so that a voxel is only pruned if\\nboth itself and its neighbors are deemed unoccupied.\\n3.5. Optimization\\nWe optimize voxel densities and spherical harmonic coef-\\nﬁcients with respect to the mean squared error (MSE) over\\nrendered pixel colors, with total variation (TV) regulariza-\\ntion [41]. Speciﬁcally, our base loss function is:\\nL = Lrecon + λT V LT V\\n(3)\\nWhere the MSE reconstruction loss Lrecon and the total\\nvariation regularizer LT V are:\\nLrecon =\\n1\\n|R|\\nX\\nr2R\\nkC(r) − ˆC(r)k2\\n2\\nLT V = 1\\n|V|\\nX\\nv2V\\nd2[D]\\nq\\n∆2x(v, d) + ∆2y(v, d) + ∆2z(v, d)\\nwith ∆2\\nx(v, d) shorthand for the squared difference between\\nthe dth value in voxel v := (i, j, k) and the dth value in voxel\\n(i + 1, j, k) normalized by the resolution, and analogously\\nfor ∆2\\ny(v, d) and ∆2\\nz(v, d), where D is the total number\\nof density and spherical harmonic (SH) coefﬁcients stored\\nat each voxel. In practice we use different weights for SH\\ncoefﬁcients and σ values. These weights are ﬁxed for each\\nscene type (bounded, forward-facing, and 360◦).\\nFor faster iteration, we use a stochastic sample of the rays\\nR to evaluate the MSE term and a stochastic sample of the\\nvoxels V to evaluate the TV term in each optimization step.\\nWe use the same learning rate schedule as JAXNeRF and\\nMip-NeRF [3,8], but tune the initial learning rate separately\\nfor density and harmonic coefﬁcients. The learning rate is\\nﬁxed for all scenes in all datasets in the main experiments.\\nDirectly optimizing voxel coefﬁcients is a challenging\\nproblem for several reasons: there are many values to op-\\ntimize (the problem is high-dimensional), the optimization\\nobjective is nonconvex due to the rendering formula, and\\nthe objective is poorly conditioned. Poor conditioning is\\ntypically best resolved by using a second order optimiza-\\ntion algorithm (e.g. as recommended in [30]), but this is\\npractically challenging to implement for a high-dimensional\\noptimization problem because the Hessian is too large to\\neasily compute and invert in each step. Instead, we use RM-\\nSProp [11] to ease the ill-conditioning problem without the\\nfull computational complexity of a second-order method.\\n3.6. Unbounded Scenes\\nWith minor modiﬁcations, Plenoxels extend to real, un-\\nbounded scenes, both forward-facing and 360◦. For forward-\\n5504\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Full\\nNo SH TV\\nNo σ TV\\nNo TV\\nFigure 3. Ablation over TV regularization. Clear artifacts are\\nvisible in the forward-facing scenes without TV on both σ and SH\\ncoefﬁcients, although PSNR does not always reﬂect this.\\nfacing scenes, we use normalized device coordinates, as\\ndeﬁned in the original NeRF paper [28].\\nBackground model. For 360◦ scenes, we augment our\\nsparse voxel grid foreground representation with a multi-\\nsphere image (MSI) background model, which also uses\\nlearned voxel colors and densities with trilinear interpola-\\ntion within and between spheres. Note that this is effectively\\nthe same as our foreground model, except the voxels are\\nwarped into spheres using the simple equirectangular projec-\\ntion (voxels index over sphere angles ✓ and φ). We place 64\\nspheres linearly in inverse radius from 1 to 1 (we pre-scale\\nthe inner scene to be approximately contained in the unit\\nsphere). To conserve memory, we store only rgb channels for\\nthe colors (only zero-order SH) and store all layers sparsely\\nby using density thresholding as in our main model. This is\\nsimilar to the background model in NeRF++ [60].\\n3.7. Regularization\\nWe illustrate the importance of TV regularization in Fig. 3.\\nIn addition to TV regularization, which encourages smooth-\\nness and is used on all scenes, for certain types of scenes we\\nalso use additional regularizers.\\nOn the real, forward-facing and 360◦ scenes, we use a\\nsparsity prior based on the Cauchy loss from SNeRG [10]:\\nLs = λs\\nX\\ni,k\\nlog\\n�\\n1 + 2σ(ri(tk))2�\\n(4)\\nwhere σ(ri(tk)) denotes the density of sample k along train-\\ning ray i. In each minibatch of optimization on forward-\\nfacing scenes, we evaluate this loss term at each sample\\non each active ray. This is also similar to the sparsity loss\\nused in PlenOctrees [59] and encourages voxels to be empty,\\nwhich helps to save memory.\\nOn the real, 360◦ scenes, we also use a beta distribution\\nregularizer on the accumulated foreground transmittance of\\neach ray in each minibatch. This loss term, following Neu-\\nral Volumes [22], promotes a clear foreground-background\\ndecomposition by encouraging the foreground to be either\\nIteration (Batch #)\\nGradient Sparsity (% Voxels)\\n0\\n10\\n20\\n30\\n100\\n101\\n102\\n103\\n104\\nReal 360°\\nReal Forward-facing\\nSynthetic\\nFigure 4. Gradient sparsity. The gradient becomes very sparse\\nspatially within the ﬁrst 12800 batches (one epoch for the synthetic\\nscenes), with as few as 1% of the voxels updating per batch in the\\nsynthetic case. This enables efﬁcient training via sparse parameter\\nupdates. The solid lines show the mean and the shaded regions\\nshow the full range of values among all scenes of each type.\\nfully opaque or empty. This beta loss is:\\nLβ = λβ\\nX\\nr\\n(log(TF G(r)) + log(1 − TF G(r)))\\n(5)\\nwhere r are the training rays and TF G(r) is the accumulated\\nforeground transmittance (between 0 and 1) of ray r.\\n3.8. Implementation\\nSince sparse voxel volume rendering is not well-\\nsupported in modern autodiff libraries, we created a cus-\\ntom PyTorch CUDA [31] extension library to achieve fast\\ndifferentiable volume rendering. We also provide a slower,\\nhigher-level JAX [5] implementation. The speed of our im-\\nplementation is possible in large part because the gradient\\nof our Plenoxel model becomes very sparse very quickly, as\\nshown in Fig. 4. Within the ﬁrst 1-2 minutes of optimization,\\nfewer than 10% of the voxels have nonzero gradients.\\n4. Results\\nWe present results on synthetic, bounded scenes; real, un-\\nbounded, forward-facing scenes; and real, unbounded, 360◦\\nscenes. We include time trial comparisons with prior work,\\nshowing dramatic speedup in training compared to all prior\\nmethods (alongside real-time rendering). Quantitative com-\\nparisons are presented in Tab. 2, and visual comparisons\\nare shown in Fig. 1, Fig. 6, Fig. 7, and Fig. 8. Our method\\nachieves quality results after even the ﬁrst epoch of optimiza-\\ntion, less than 1.5 minutes, as shown in Fig. 5.\\nWe also present the results from various ablation studies\\nof our method. In the main text we present average results\\n(PSNR, SSIM [56], and VGG LPIPS [61]) over all scenes\\nof each type; full quantitative and visual results on each\\n5505\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Figure 5. 1 minute, 20 seconds. Results on the synthetic scenes after 1 epoch of optimization, an average of 1 minute and 20 seconds.\\nPSNR \" SSIM \" LPIPS #\\nTrain Time\\nOurs\\n31.71\\n0.958\\n0.049\\n11 mins\\nNV [22]\\n26.05\\n0.893\\n0.160\\n>1 day\\nJAXNeRF [8,28]\\n31.85\\n0.954\\n0.072\\n1.45 days\\nOurs\\n26.29\\n0.839\\n0.210\\n24 mins\\nLLFF [27]\\n24.13\\n0.798\\n0.212\\n—*\\nJAXNeRF [8,28]\\n26.71\\n0.820\\n0.235\\n1.62 days\\nOurs\\n20.40\\n0.696\\n0.420\\n27 mins\\nNeRF++ [60]\\n20.49\\n0.648\\n0.478\\n⇠4 days\\nTable 2. Results. Top: average over the 8 synthetic scenes from\\nNeRF; Middle: the 8 real, forward-facing scenes from NeRF; Bot-\\ntom: the 4 real, 360◦ scenes from Tanks and Temples [16]. 4 of the\\nsynthetic scenes train in under 10 minutes. *LLFF requires pretrain-\\ning a network to predict MPIs for each view, and then can render\\nnovel scenes without further training; this pretraining is amortized\\nacross all scenes so we do not include it in the table.\\nscene, and full experimental details (hyperparameters, etc.)\\nare included in the supplement.\\n4.1. Synthetic Scenes\\nOur synthetic experiments use the 8 scenes from NeRF:\\nchair, drums, ﬁcus, hotdog, lego, materials, mic, and ship.\\nEach scene includes 100 ground truth training views with\\n800 ⇥ 800 resolution, from known camera positions dis-\\ntributed randomly in the upper hemisphere facing the object.\\nEach scene is evaluated on 200 test views, also with resolu-\\ntion 800 ⇥ 800 and known inward-facing camera positions in\\nthe upper hemisphere. We provide quantitative comparisons\\nin Tab. 2 and visual comparisons in Fig. 6.\\nWe compare our method to Neural Volumes (NV) [22], a\\nprior grid-based method with a 3D convolutional network,\\nand JAXNeRF [8, 28]. For Neural Volumes we use values\\nreported in [28]; for JAXNeRF we report results from our\\nown rerunning, ﬁxing its centered pixel bug [3]. Our method\\nachieves comparable quality compared to the best baseline,\\nwhile training in an average of 11 minutes per scene on a\\nsingle GPU and supporting interactive rendering.\\n4.2. Real Forward-Facing Scenes\\nWe extend our method to unbounded, forward-facing\\nscenes by using normalized device coordinates (NDC), as\\nGround Truth\\nJAXNeRF [8,28]\\nPlenoxels\\nFigure 6. Synthetic, bounded scenes. Example results on the lego\\nand ship synthetic scenes from NeRF [28].\\nderived in NeRF [28]. Our method is otherwise identical to\\nthe version we use on bounded, synthetic scenes, except that\\nwe use TV regularization (with a stronger weight) throughout\\nthe optimization. This change is likely necessary because of\\nthe reduced number of training views for these scenes, as\\ndescribed in Sec. 4.4.\\nOur forward-facing experiments use the same 8 scenes\\nas in NeRF, 5 of which are originally from LLFF [27]. Each\\nscene consists of 20 to 60 forward-facing images captured\\nby a handheld cell phone with resolution 1008 ⇥ 756, with\\n7\\n8 of the images used for training and the remaining 1\\n8 of the\\nimages reserved as a test set.\\nWe compare our method to Local Light Field Fusion\\n(LLFF) [27], a prior method that uses a 3D convolutional\\nnetwork to predict a grid for each input view, and JAXNeRF.\\nWe provide quantitative comparisons in Tab. 2 and visual\\ncomparisons in Fig. 7.\\n4.3. Real 360◦ Scenes\\nWe extend our method to real, unbounded 360◦ scenes by\\nsurrounding our sparse voxel grid with an multi-sphere im-\\nage (MSI, based on multi-plane images introduced by [62])\\nbackground model, in which each background sphere is also\\na simple voxel grid with trilinear interpolation (both within\\neach sphere and between adjacent layers).\\nOur 360◦ experiments use 4 scenes from the Tanks and\\nTemples dataset [16]: M60, playground, train, and truck. For\\n5506\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Ground Truth\\nJAXNeRF [8,28]\\nPlenoxels\\nFigure 7. Real, forward-facing scenes. Example results on the\\nfern and orchid forward-facing scenes from NeRF.\\neach scene, we use the same train/test split as [39].\\nWe compare our method to NeRF++ [60], which aug-\\nments NeRF with a background model to represent un-\\nbounded scenes. We present quantitative comparisons in\\nTab. 2 and visual comparisons in Fig. 8.\\n4.4. Ablation Studies\\nIn this section, we perform extensive ablation studies\\nof our method to understand which features are core to its\\nsuccess, with such a simple model. In Tab. 1, we show that\\ncontinuous (in our case, trilinear) interpolation is responsible\\nfor dramatic improvement in ﬁdelity compared to nearest\\nneighbor interpolation (i.e. constant within each voxel) [59].\\nIn Tab. 3, we consider how our method handles a dramatic\\nreduction in training data, from 100 views to 25 views, on the\\n8 synthetic scenes. We compare our method to NeRF and ﬁnd\\nthat, despite its lack of complex neural priors, by increasing\\nTV regularization our method can outperform NeRF even\\nin this limited data regime. This ablation also sheds light on\\nwhy our model performs better with higher TV regularization\\non the real forward-facing scenes compared to the synthetic\\nscenes: the real scenes have many fewer training images,\\nand the stronger regularizer helps our optimization extend\\nsmoothly to sparsely-supervised regions.\\nWe also ablate over the resolution of our Plenoxel grid\\nin Tab. 4 and the rendering formula in Tab. 5. The rendering\\nformula from Max [24] yields a substantial improvement\\ncompared to that of Neural Volumes [22], perhaps because it\\nis more physically accurate (as discussed further in the sup-\\nplement). The supplement also includes ablations over the\\nlearning rate schedule and optimizer demonstrating Plenoxel\\noptimization to be robust to these hyperparameters.\\n5. Discussion\\nWe present a method for photorealistic scene modeling\\nand novel viewpoint rendering that produces results with\\ncomparable ﬁdelity to the state-of-the-art, while taking or-\\nders of magnitude less time to train. Our method is also\\nPSNR \" SSIM \" LPIPS #\\nOurs: 100 images (low TV)\\n31.71\\n0.958\\n0.050\\nNeRF: 100 images [28]\\n31.01\\n0.947\\n0.081\\nOurs: 25 images (low TV)\\n26.88\\n0.911\\n0.099\\nOurs: 25 images (high TV)\\n28.25\\n0.932\\n0.078\\nNeRF: 25 images [28]\\n27.78\\n0.925\\n0.108\\nTable 3. Ablation over the number of views. By increasing our\\nTV regularization, we exceed NeRF ﬁdelity even when the number\\nof training views is only a quarter of the full dataset. Results are\\naveraged over the 8 synthetic scenes from NeRF.\\nResolution\\nPSNR \"\\nSSIM \"\\nLPIPS #\\n5123\\n31.71\\n0.958\\n0.050\\n2563\\n30.57\\n0.950\\n0.065\\n1283\\n28.46\\n0.926\\n0.100\\n643\\n26.11\\n0.892\\n0.139\\n323\\n23.49\\n0.859\\n0.174\\nTable 4. Ablation over the Plenoxel grid resolution. Results are\\naveraged over the 8 synthetic scenes from NeRF.\\nRendering Formula\\nPSNR \" SSIM \" LPIPS #\\nMax [24], used in NeRF [28]\\n30.57\\n0.950\\n0.065\\nNeural Volumes [22]\\n27.54\\n0.906\\n0.201\\nTable 5. Comparison of different rendering formulas. We com-\\npare the rendering formula from Max [24] (used in NeRF and our\\nmain method) to the one used in Neural Volumes [22], which uses\\nabsolute instead of relative transmittance. Results are averaged over\\nthe 8 synthetic scenes from NeRF.\\nstrikingly straightforward, shedding light on the core ele-\\nments that are necessary for solving 3D inverse problems: a\\ndifferentiable forward model, a continuous representation (in\\nour case, via trilinear interpolation), and appropriate regular-\\nization. We acknowledge that the ingredients for this method\\nhave been available for a long time, however nonlinear opti-\\nmization with tens of millions of variables has only recently\\nbecome accessible to the computer vision practitioner.\\nLimitations and Future Work.\\nAs with any underdeter-\\nmined inverse problem, our method is susceptible to artifacts.\\nOur method exhibits different artifacts than neural methods,\\nas shown in Fig. 9, but both methods achieve similar quality\\nin terms of standard metrics (as presented in Sec. 4). Future\\nwork may be able to adjust or mitigate these remaining arti-\\nfacts by studying different regularization priors and/or more\\nphysically accurate differentiable rendering functions.\\nAlthough we report all of our results for each dataset with\\na ﬁxed set of hyperparameters, there is no optimal a priori\\n5507\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Ground Truth\\nNeRF++ [60]\\nPlenoxels\\nFigure 8. Real, 360◦ scenes. Example results on the playground and truck 360◦ scenes from Tanks and Temples [16].\\nGround Truth\\nJAXNeRF [8,28]\\nPlenoxels\\nFigure 9. Artifacts. JAXNeRF and Plenoxel exhibit slightly dif-\\nferent artifacts, as shown here in the specularities in the synthetic\\ndrums scene. Note that some artifacts are unavoidable for any\\nunderdetermined inverse problem, but the speciﬁc artifacts vary\\ndepending on the priors induced by the model and regularizer.\\nsetting of the TV weight λT V . Better results may be obtained\\nby tuning this parameter on a scene-by-scene basis, which\\nis possible due to our fast training time. This is expected\\nbecause the scale, smoothness, and number of training views\\nvaries between scenes.\\nOur method should extend naturally to support multi-\\nscale rendering with proper anti-aliasing through voxel cone-\\ntracing, similar to the modiﬁcations in Mip-NeRF [3]. An-\\nother easy addition is tone-mapping to account for white\\nbalance and exposure changes [42], which we expect would\\nhelp especially in the real 360◦ scenes. A hierarchical data\\nstructure (such as an octree) may provide additional speedup\\ncompared to our sparse array implementation, provided that\\ndifferentiable interpolation is preserved.\\nSince our method is two orders of magnitude faster than\\nNeRF, we believe that it may enable downstream applica-\\ntions currently bottlenecked by the performance of NeRF–for\\nexample, multi-bounce lighting and 3D generative models\\nacross large databases of scenes. By combining our method\\nwith additional components such as camera optimization and\\nlarge-scale voxel hashing, it may enable a practical pipeline\\nfor end-to-end photorealistic 3D reconstruction.\\nAcknowledgements\\nWe note that Utkarsh Singhal and Sara Fridovich-Keil pre-\\nviously tried a related idea with point clouds. Additionally,\\nwe would like to thank Ren Ng for helpful suggestions and\\nHang Gao for reviewing the paper draft. The project is gener-\\nously supported in part by the CONIX Research Center, spon-\\nsored by DARPA; Google research faculty award to Angjoo\\nKanazawa; Benjamin Recht’s ONR awards N00014-20-1-\\n2497 and N00014-18-1-2833, NSF CPS award 1931853,\\nand the DARPA Assured Autonomy program (FA8750-18-\\nC-0101). SFK and MT are supported by the NSF GRFP.\\n5508\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] Edward H. Adelson and James R. Bergen. The plenoptic\\nfunction and the elements of early vision. In Computational\\nModels of Visual Processing, pages 3–20. MIT Press, 1991. 2\\n[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\\nUlyanov, and Victor Lempitsky. Neural point-based graphics.\\nIn Computer Vision–ECCV 2020: 16th European Conference,\\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XXII\\n16, pages 696–712. Springer, 2020. 2\\n[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\\nMip-NeRF: A multiscale representation for anti-aliasing neu-\\nral radiance ﬁelds, 2021. 2, 4, 6, 8, 1\\n[4] R. Basri and D.W. Jacobs. Lambertian reﬂectance and lin-\\near subspaces. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 25(2):218–233, 2003. 2\\n[5] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James\\nJohnson, Chris Leary, Dougal Maclaurin, George Necula,\\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne,\\nand Qiao Zhang.\\nJAX: composable transformations of\\nPython+NumPy programs, 2018. 2, 5\\n[6] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for\\ngenerative shape modeling, 2019. 2\\n[7] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin\\nChen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach for\\nsingle and multi-view 3d object reconstruction, 2016. 2\\n[8] Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan.\\nJaxNeRF: an efﬁcient JAX implementation of NeRF, 2020.\\n2, 4, 6, 7, 8, 5, 9, 10\\n[9] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie\\nShotton, and Julien Valentin. Fastnerf: High-ﬁdelity neural\\nrendering at 200fps, 2021. 1, 2\\n[10] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\\nJonathan T. Barron, and Paul Debevec. Baking neural ra-\\ndiance ﬁelds for real-time view synthesis, 2021. 1, 2, 5\\n[11] Geoffrey Hinton. RMSProp. 4, 1, 2\\n[12] Christian H¨ane, Shubham Tulsiani, and Jitendra Malik. Hier-\\narchical surface prediction for 3d object reconstruction, 2017.\\n2\\n[13] James T. Kajiya and Brian P Von Herzen. Ray tracing volume\\ndensities. In Proceedings of the 11th Annual Conference on\\nComputer Graphics and Interactive Techniques, SIGGRAPH\\n’84, page 165–174, New York, NY, USA, 1984. Association\\nfor Computing Machinery. 2, 3\\n[14] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learning\\na multi-view stereo machine, 2017. 2\\n[15] Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari\\nPulli, and Gordon Wetzstein. Neural lumigraph rendering,\\n2021. 2\\n[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\\nKoltun. Tanks and temples: Benchmarking large-scale scene\\nreconstruction. ACM Trans. Graph., 36(4), July 2017. 6, 8, 4\\n[17] Aaron Knoll. A survey of octree volume rendering methods,\\n2006. 2\\n[18] Christoph Lassner and Michael Zollh¨ofer. Pulsar: Efﬁcient\\nsphere-based neural rendering. In IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), June 2021.\\n2\\n[19] M. Levoy. Display of surfaces from volume data. IEEE\\nComputer Graphics and Applications, 8(3):29–37, 1988. 2\\n[20] David B. Lindell, Julien N. P. Martel, and Gordon Wetzstein.\\nAutoint: Automatic integration for fast neural volume render-\\ning, 2021. 2\\n[21] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\\nChristian Theobalt. Neural sparse voxel ﬁelds, 2021. 1, 2\\n[22] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural\\nvolumes. ACM Transactions on Graphics, 38(4):1–14, Jul\\n2019. 2, 5, 6, 7, 3\\n[23] Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R.\\nChan, Marco Monteiro, and Gordon Wetzstein. Acorn: Adap-\\ntive coordinate networks for neural scene representation, 2021.\\n2\\n[24] N. Max.\\nOptical models for direct volume rendering.\\nIEEE Transactions on Visualization and Computer Graphics,\\n1(2):99–108, 1995. 2, 3, 7\\n[25] Duane Merrill and NVIDIA Corporation. CUB: Cooperative\\nprimitives for CUDA C++, 2021. 1\\n[26] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space, 2019. 2\\n[27] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\\nAbhishek Kar. Local light ﬁeld fusion: Practical view synthe-\\nsis with prescriptive sampling guidelines, 2019. 2, 6, 8\\n[28] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance ﬁelds for view syn-\\nthesis. In European conference on computer vision, pages\\n405–421. Springer, 2020. 1, 2, 3, 5, 6, 7, 8, 9, 10\\n[29] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, J. H. Mueller,\\nC. R. A. Chaitanya, A. Kaplanyan, and M. Steinberger. Don-\\nerf: Towards real-time rendering of compact neural radiance\\nﬁelds using depth oracle networks. Computer Graphics Fo-\\nrum, 40(4):45–59, Jul 2021. 2\\n[30] Jorge Nocedal and Stephen J. Wright. Numerical Optimiza-\\ntion. Springer, New York, NY, USA, second edition, 2006.\\n4\\n[31] NVIDIA, P´eter Vingelmann, and Frank H.P. Fitzek. Cuda,\\nrelease: 10.2.89, 2020. 1, 5\\n[32] Alan V. Oppenheim and Ronald W. Schafer. Discrete-Time\\nSignal Processing. Prentice Hall Press, USA, 3rd edition,\\n2009. 2\\n[33] Jeong Joon Park, Peter Florence, Julian Straub, Richard New-\\ncombe, and Steven Lovegrove. DeepSDF: Learning continu-\\nous signed distance functions for shape representation, 2019.\\n2\\n[34] Eric Penner and Li Zhang. Soft 3d reconstruction for view\\nsynthesis. ACM Transactions on Graphics (TOG), 36:1 – 11,\\n2017. 2\\n[35] Martin Piala and Ronald Clark. TermiNeRF: Ray termination\\nprediction for efﬁcient neural rendering, 2021. 2\\n5509\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[36] Ravi Ramamoorthi and Pat Hanrahan. On the relationship\\nbetween radiance and irradiance: determining the illumina-\\ntion from images of a convex lambertian object. JOSA A,\\n18(10):2448–2459, 2001. 2\\n[37] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,\\nKwang Moo Yi, and Andrea Tagliasacchi. Derf: Decomposed\\nradiance ﬁelds, 2020. 1, 2\\n[38] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\\nGeiger. KiloNeRF: Speeding up neural radiance ﬁelds with\\nthousands of tiny mlps, 2021. 1, 2\\n[39] Gernot Riegler and Vladlen Koltun. Free view synthesis,\\n2020. 7\\n[40] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Oct-\\nnet: Learning deep 3d representations at high resolutions,\\n2017. 2\\n[41] Leonid I Rudin and Stanley Osher. Total variation based im-\\nage restoration with free local constraints. In Proceedings of\\n1st International Conference on Image Processing, volume 1,\\npages 31–35. IEEE, 1994. 4\\n[42] Darius R¨uckert, Linus Franke, and Marc Stamminger. Adop:\\nApproximate differentiable one-pixel point rendering, 2021.\\n2, 8\\n[43] Seitz, Steven, Kutulakos, and Kiriakos. A theory of shape by\\nspace carving. 01 2000. 2\\n[44] S.M. Seitz and C.R. Dyer. Photorealistic scene reconstruction\\nby voxel coloring. In Proceedings of IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition,\\npages 1067–1073, 1997. 2\\n[45] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\\nNießner, Gordon Wetzstein, and Michael Zollh¨ofer. Deepvox-\\nels: Learning persistent 3d feature embeddings, 2019. 2\\n[46] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein.\\nScene representation networks: Continuous 3d-structure-\\naware neural scene representations, 2020. 2\\n[47] Peter-Pike Sloan, Jan Kautz, and John Snyder.\\nPrecom-\\nputed radiance transfer for real-time rendering in dynamic,\\nlow-frequency lighting environments. ACM Trans. Graph.,\\n21(3):527–536, July 2002. 2\\n[48] Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron,\\nRavi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing\\nthe boundaries of view extrapolation with multiplane images,\\n2019. 2\\n[49] Richard Szeliski and Polina Golland. Stereo matching with\\ntransparency and matting. International Journal of Computer\\nVision, 32:45–61, 2004. 2\\n[50] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis,\\nCharles Loop, Derek Nowrouzezahrai, Alec Jacobson, Mor-\\ngan McGuire, and Sanja Fidler. Neural geometric level of\\ndetail: Real-time rendering with implicit 3d shapes, 2021. 2\\n[51] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\\nSchmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren\\nNg. Learned initializations for optimizing coordinate-based\\nneural representations, 2021. 2\\n[52] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\\nOctree generating networks: Efﬁcient convolutional architec-\\ntures for high-resolution 3d outputs, 2017. 2\\n[53] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jiten-\\ndra Malik. Multi-view supervision for single-view reconstruc-\\ntion via differentiable ray consistency, 2017. 2\\n[54] Craig Upson and Michael Keeler. V-buffer: Visible volume\\nrendering. SIGGRAPH Comput. Graph., 22(4):59–64, jun\\n1988. 2\\n[55] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,\\nand Xin Tong.\\nO-cnn.\\nACM Transactions on Graphics,\\n36(4):1–11, Jul 2017. 2\\n[56] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simon-\\ncelli. Image quality assessment: from error visibility to struc-\\ntural similarity. IEEE Transactions on Image Processing,\\n13(4):600–612, 2004. 5\\n[57] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\\nJohnson. Synsin: End-to-end view synthesis from a single\\nimage, 2020. 2\\n[58] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\\nview synthesis with neural basis expansion, 2021. 2\\n[59] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\\nneural radiance ﬁelds. In ICCV, 2021. 1, 2, 3, 4, 5, 7\\n[60] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\\nKoltun. NeRF++: Analyzing and improving neural radiance\\nﬁelds, 2020. 1, 2, 5, 6, 7, 8, 4, 11\\n[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. In CVPR, 2018. 5\\n[62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magniﬁcation: Learning view syn-\\nthesis using multiplane images, 2018. 2, 6\\n5510\\n', metadata={'source': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'file_path': 'data/Fridovich-Keil et al. - 2022 - Plenoxels Radiance Fields without Neural Networks.pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'Plenoxels: Radiance Fields Without Neural Networks', 'author': 'Sara Fridovich-Keil;  Alex Yu;  Matthew Tancik;  Qinhong Chen;  Benjamin Recht;  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Knowledge-Based Systems 236 (2022) 107680\\nContents lists available at ScienceDirect\\nKnowledge-Based Systems\\njournal homepage: www.elsevier.com/locate/knosys\\nX-CTRSNet: 3D cervical vertebra CT reconstruction and segmentation\\ndirectly from 2D X-ray images\\nRongjun Ge a,d, Yuting He b,c,d, Cong Xia e, Chenchu Xu f, Weiya Sun b,c,d, Guanyu Yang b,c,d,\\nJunru Li g, Zhihua Wang c, Hailing Yu c, Daoqiang Zhang a,∗, Yang Chen b,c,d,∗∗, Limin Luo b,c,d,\\nShuo Li h, Yinsu Zhu i,∗\\na College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China\\nb Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Laboratory of Image Science and Technology,\\nSoutheast University, Nanjing, China\\nc School of Computer Science and Engineering, Southeast University, Nanjing, China\\nd Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing, China\\ne Jiangsu Key Laboratory of Molecular and Functional Imaging, Department of Radiology, Zhongda Hospital, Medical School of Southeast\\nUniversity, Nanjing, China\\nf School of Computer Science and Technology, Anhui University, Hefei, China\\ng College of Software Engineering, Southeast University, Nanjing, China\\nh Department of Medical Imaging, Western University, London, ON, Canada\\ni Department of Radiology, the First Affiliated Hospital of Nanjing Medical University, Nanjing, China\\na r t i c l e\\ni n f o\\nArticle history:\\nReceived 13 January 2021\\nReceived in revised form 30 October 2021\\nAccepted 30 October 2021\\nAvailable online 11 November 2021\\nKeywords:\\nAI-based medical assistant tool\\n2D-to-3D\\nReconstruction\\nSegmentation\\na b s t r a c t\\nOrthogonal 2D cervical vertebra (C-vertebra) X-ray images have the advantages of high imaging\\nefficiency, low radiation risk, easy operation and low cost for rapid primary clinical diagnoses.\\nEspecially in emergency departments, this technique is known to be significantly useful in triage\\nfor trauma patients. However, the technique can only provide overlapping anatomic information\\nfrom limited projection views and is unable to visually exhibit full-view anatomy and precise stereo\\nstructures without further CT examination. To promote ‘‘once is enough\" for visualizing 3D anatomy\\n& structures and reducing repetitive radiation as much as possible, we proposed X-CTRSNet for 2D X-\\nray images. This is the first powerful work that simultaneously and accurately enables 3D C-vertebra\\nCT reconstruction and segmentation directly from orthogonally anteroposterior- and lateral-view 2D\\nX-ray images. X-CTRSNet combines the reciprocally coupled SpaDRNet for reconstruction & MulSISNet\\nfor segmentation, and a RSC Learning for tasks consistency. The experiment shows that X-CTRSNet\\nsuccessfully reconstructs and segments the 3D C-vertebra CT from the 2D X-ray images with a PSNR\\nof 24.58 dB, an SSIM of 0.749, and an average Dice of 80.44%. All these findings reveal the great\\npotential of X-CTRSNet in clinical imaging and diagnosis to facilitate emergency triage by enabling\\nprecise 3D reconstruction and segmentation on 2D X-ray images.\\n© 2021 Elsevier B.V. All rights reserved.\\n1. Introduction\\nAccurate 3D cervical vertebra (C-vertebra) CT reconstruction\\nand segmentation directly from orthogonal 2D C-vertebra X-\\nray images is clinically significant to distinctly enable a detailed\\n3D imaging diagnosis basis for clinicians, and effectively re-\\nduce repetitive radiation for patients, especially in assessing\\n∗\\nCorresponding authors.\\n∗∗ Corresponding author at: Jiangsu Provincial Joint International Research\\nLaboratory of Medical Information Processing, Laboratory of Image Science and\\nTechnology, Southeast University, Nanjing, China.\\nE-mail addresses:\\ndqzhang@nuaa.edu.cn (D. Zhang),\\nchenyang.list@seu.edu.cn (Y. Chen), zhuyinsu@njmu.edu.cn (Y. Zhu).\\nC-vertebra disease and surgery, and facilitating emergency triage.\\nBecause it is equipped with the efficiency in 2D X-ray imaging,\\nthe rich anatomic structure in 3D CT volume, and the intuitive\\nvisualization in 3D segmentation, detailed as following: (1) 2D\\nC-vertebra X-ray images have the advantages of high imaging\\nspeed, low radiation risk, easy operation, low cost and portable\\ndevelopment, and are widely used for fast primary clinical di-\\nagnoses [1–4]. But it can only provided overlapping anatomic\\ninformation in a 2D plane from the limited projection view [5–\\n7], as the anteroposterior- (AP) and lateral- (LA) view X-ray\\nimages in Figs. 1 (a1) & (a2) show. (2) Compared with 2D X-\\nrays, 3D C-vertebra computed tomography (CT) scans are capable\\nof providing full-view anatomy and precise stereo structures of\\npathologies [8,9]. It enables the volumetric data [10] as Fig. 1(b),\\nhttps://doi.org/10.1016/j.knosys.2021.107680\\n0950-7051/© 2021 Elsevier B.V. All rights reserved.\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nFig. 1. 3D C-vertebra computed tomography (CT) scan can provide full-view anatomy and enable precise stereo structure, compared to the 2D X-ray images.\\nand the no-overlapping distinct anatomic structure from different\\nviews [11–13] such as the axial, coronal and sagittal planes in\\nFig. 1 (b1), (b2) & (b3). However, additional pricey CT scans may\\ncause repetitive radiation and unnecessary medical resources\\noccupation of over-treatment, and itself also has a high-dose\\nradiation risk due to multi-slice dense projection [4]. Besides, for\\nthe rapid triage of trauma patients in emergency department, it\\nalso needs too much time in the race against time, and may over-\\nwhelm medical resources in a short time [14]. (3) For efficient\\nimage interpretation, surgical planning and objective assessment,\\n3D C-vertebra segmentation directly enables the precise stereo\\nbiological structures, as shown in Fig. 1(c). It effectively reflects\\nthe morphological shapes, relative locations, and physiological\\ncurves for the C-vertebras that has highly flexible anatomy vul-\\nnerable to injuries and degeneration. Therefore, it is of great\\nclinical contribution to improve clinical diagnosis efficiency and\\nspeed up emergency triage, that with only rapid 2D X-ray image\\ninputs and achieving 3D C-vertebra anatomy and structures as far\\nas possible. It does not aim to replace CT examination completely,\\nbut can provide more 3D diagnostic basis on primary 2D X-ray\\nimaging without additional time costs.\\nAlthough 3D C-vertebra CT reconstruction and segmentation\\ndirectly from 2D X-ray images is clinically urgent, and deep learn-\\ning has achieved great success in lots of clinical related tasks [15–\\n21], the issue still has never been investigated. Some existing\\nworks [22–24] have attempted on 3D reconstruction from 2D X-\\nray images, but they ignored the inherent spatiality and lacked\\ninteractive segmentation. While these works show promising\\nresults on their issues, they still have difficulties to be applied\\nhere, due to the following reasons. (1) The C-vertebra has a high\\nrequirement of spatiality for stereo correspondence. Because it\\nis important for the C-vertebra spatial evaluation of dislocation,\\nphysiological curves, and distance measurements [25]. But the ex-\\nisting works [22–24] just using only the 1D feature vector for 2D\\nto 3D transformation ignore the inherent spatiality. (2) The lack\\nof an interactive scheme that promotes multi-task learning with\\nsegmentation is needed to ensure reconstruction–segmentation\\nconsistency. So that can reciprocally make the shape constraint\\nand enhance the biological CT anatomical texture for the recon-\\nstructed 3D CT, and strengthen the precise segmentation robust-\\nness on the reconstructed image. (3) Besides, for 3D reconstructed\\nvolumes from 2D projected images, content consistency is needed\\nto express the anatomy, which is beyond the abilities of the\\nformer used voxel-to-voxel local optimization lacking assem-\\nbled expressiveness. (4) The C-vertebra has multi-scale structure\\ncomponents such as the vertebral body, spinous process, trans-\\nverse process, foramen transversarium, etc., and pathologically\\ncorrelated locations inter C-vertebras, which requires multi-scale\\nstereoscopical information for precise segmentation.\\nAs shown in Fig. 2, we proposed the first powerful work,\\nX-CTRSNet, which simultaneously and accurately enables 3D C-\\nvertebra CT reconstruction and segmentation directly from widely\\naccessible AP and LA view 2D X-ray images. X-CTRSNet is com-\\nposed\\nof\\nthree\\nelements,\\nincluding\\nSpatial\\nDecomposition-\\nReconstruction Net (SpaDRNet), Multi-scale Space Interoperabil-\\nity\\nSegmentation\\nNet\\n(MulSISNet),\\nand\\nReconstruction–\\nsegmentation Consistency (RSC) Learning. The effects of these\\nthree specially-designed elements can be summarized as follows:\\n(1) SpaDRNet (Sect. 2.1) is used to achieve the 3D C-vertebra\\nCT reconstruction directly from AP & LA projected 2D planes,\\nas shown in Fig. 3(b). It consists of progressive 2D-to-3D con-\\nversion to multi-scale decompose the compressed space to the\\ncorresponding stereo location from the overlapped domain, hi-\\nerarchical 3D fusion to interpretively sort out the 3D spatial\\ninformation in consistency, and multi-view vgg loss to expres-\\nsively guide the reconstructed scene content learning. And (2)\\nMulSISNet (Sect. 2.2) further enables 3D C-vertebra semantic\\nsegmentation on reconstructed CT, and promotes shape con-\\nstraints to SpaDRNet, as shown in Fig. 3(c). It comprehensively\\nextracts rich stereo structure covering from the small-scale de-\\ntails to the large-scale distribution with the information densely\\ninteroperated among multi-scale 3D feature. Interactively, (3)\\nRSC Learning (Sect. 2.3) enhances reconstruction–segmentation\\nconsistent with the ground truth (GT) for the multi-task learning,\\nas shown in Fig. 3(d). It promotes segmenting on the CT ground\\ntruth (GT), and optimize the divergence of the reconstructed CT’s\\nsegmentation with it.\\nThe contributions of this work are summarized as following:\\n• For the first time, the proposed X-CTRSNet promotes ‘‘once\\nis enough\" for achieving 3D C-vertebra reconstruction and\\nsegmentation directly from AP and LA view 2D X-ray im-\\nages. It efficiently gains detailed 3D imaging diagnosis basis\\nwith only 2D X-ray imaging, so that effectively improves\\nclinical diagnosis and imaging efficiency of C-vertebra, as\\nwell as greatly reduces repetitive radiation.\\n• The newly designed SpaDRNet enables 2D-to-3D reconstruc-\\ntion of C-vertebra from overlapping 2D projections for clear\\n3D anatomy. It novelly designs the progressive 2D-to-3D\\nconversion to multi-scale decompose the compressed space\\nto the spatially corresponding stereo location from the over-\\nlapped 2D domain, and the hierarchical 3D fusion to inter-\\npretively sort out the multi-scale 3D spatial information in\\nconsistency. During network learning, it creatively devel-\\nops multi-view vgg loss to guide the reconstructed scene\\ncontent of expressing anatomy, for volumetric images.\\n2\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nFig. 2. Workflow diagram of X-CTRSNet.\\nFig. 3. X-CTRSNet is achieved via reciprocally coupled SpaDRNet of reconstruction & MulSISNet of Segmentation, and a RSC Learning of tasks consistency, to\\nsimultaneously enable 3D C-vertebra CT reconstruction and segmentation directly from the 2D X-ray images.\\n• The novel MulSISNet promotes 3D segmentation from re-\\nconstructed CT to distill the 3D structure of the C-vertebra,\\nand establishes shape constraints to SpaDRNet. It robustly\\nextracts the multi-scale stereo structures and distribution\\nfor the fine-gained spatial representation, and densely inter-\\noperates among the multi-scale 3D features to fully exploit\\nthe learned information for enhancing each other and com-\\npensating the lost. Besides, it also feeds back the segmented\\nshape error to guide SpaDRNet reconstruction.\\n• The creative RSC Learning enhances the multi-task learning\\nof reconstruction and segmentation. It forcefully introduces\\nthe CT GT to strengthen the segmentation effectiveness,\\nreducing the misleading of MulSISNet from the early-stage\\nunstable reconstruction; and deeply feeds back the diver-\\ngence between two segmentations for the reconstructed\\nbiological CT anatomy.\\n2. Methodology\\nAs shown in Fig. 3(a), the proposed X-CTRSNet is conducted\\non the AP and LA views 2D X-ray images to directly make the\\n3D C-vertebra CT reconstruction and segmentation. So that it\\nachieves the full-view anatomy and precise stereo structure, mak-\\ning up the shortage in 2D imaging. It is built by three collaborate\\nelements: (1) SpaDRNet (R, Section 2.1) combines progressive 2D-\\nto-3D multi-paths, hierarchical 3D fusion and multi-view vgg loss\\nto decompose the overlapped 2D X-ray images into reconstruct-\\ning the detailed 3D CT. (2) MulSISNet (S, Section 2.2) extracts the\\n3\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nrobust multi-scale stereo features for the reciprocal 3D semantic\\nsegmentation on reconstructed CT and shape constraint feedback.\\n(3) RSC Learning (LRSC, Section 2.3) promotes segmenting on the\\nCT GT and optimizes the divergence between two segmentations\\nto drive the reconstruction–segmentation consistency. Given the\\nAP & LA views 2D X-ray images xAP&xLA, the GT of 3D CT yCT and\\nsegmentation ySeg, the target of X-CTRSNet is formulated as:\\nmin\\nR,S LX−CTRSNet = LR(R) + LS(R, S) + LRSC(R, S)\\n(1)\\n2.1. SpaDRNet for 3D CT reconstruction from 2D X-ray images\\nOur SpaDRNet in Fig. 3(b) innovatively uses progressive 2D-\\nto-3D conversion, hierarchical 3D fusion, and multi-view vgg\\nloss, to decompose the latent dimension space in the overlapped\\n2D projection, and spatially correspondingly reconstruct into 3D\\nCT images, directly from the AP and LA views X-ray images.\\n2.1.1. Progressive 2D-to-3D conversion\\nAs shown in Fig. 3(b), the progressive 2D-to-3D conversion\\nexploits the 2D collateral multi-paths and the view alignment, to\\nextract the 3D spatial information existing in 2D projection.\\n(1) The 2D collateral multi-paths progressively extract the pro-\\njected information in multi-scale with spatial correspondence\\ninstead of spatialless 1D abstraction. The beginnings of each path\\nFeat2D,(S,1), S ∈ {\\n1, 1\\n2, 1\\n4, 1\\n8\\n}\\nfor different scales, are calculated as:\\nFeat2D,(1,1) = SeqConv2D(I)\\nFeat2D,( 1\\n2 ,1) = SeqConv2D(DConv(Feat2D,(1,1)))\\nFeat2D,( 1\\n4 ,1) = SeqConv2D(DConv(Feat2D,( 1\\n2 ,1)))\\nFeat2D,( 1\\n8 ,1) = SeqConv2D(DConv(Feat2D,( 1\\n4 ,1)))\\n(2)\\nwhere SeqConv2D(·) means sequentially conducts the 2D 3 × 3\\nconvolution with Leaky ReLU activation function. DConv(·) is the\\ndown-sampling operation by using the 3 × 3 convolutions with\\nstride 2, and I is the inputting 2D X-ray images.\\nIn addition, the multi-scale fusion [26] is used during the scale\\nprogression for information compensation and representation en-\\nhancement. This procedure can be formulated as:\\nFeat2D,(1,l+1) = SeqConv2D(Feat2D,(1,l) + U(Feat2D,( 1\\n2 ,l−1))\\n+ U(Feat2D,( 1\\n4 ,l−2)) + U(Feat2D,( 1\\n8 ,l−3)))\\nFeat2D,( 1\\n2 ,l+1) = SeqConv2D(Feat2D,( 1\\n2 ,l) + D(Feat2D,(1,l+1))\\n+ U(Feat2D,( 1\\n4 ,l−1)) + U(Feat2D,( 1\\n8 ,l−2)))\\nFeat2D,( 1\\n4 ,l+1) = SeqConv2D(Feat2D,( 1\\n4 ,l) + D(Feat2D,(1,l+2))\\n+ D(Feat2D,( 1\\n2 ,l+1)) + U(Feat2D,( 1\\n8 ,l−1)))\\nFeat2D,( 1\\n8 ,l+1) = SeqConv2D(Feat2D,( 1\\n8 ,l) + D(Feat2D,(1,l+3))\\n+ D(Feat2D,( 1\\n2 ,l+2)) + D(Feat2D,( 1\\n4 ,l+1)))\\n(3)\\nwhere l is the layer number of the corresponding path, D(·) is the\\ndown-sampling operation composed of consecutive strided 3 × 3\\nconvolutions with stride 2, U(·) represents the simple nearest\\nneighbor sampling following a 1 × 1 convolution.\\nProgressively, the 2D overlapping is spatially converted into\\n3D decomposition, extracting the projected dimension and keep-\\ning the plane spatiality, as 128 × 128 × 1 → 128 × 128 × 16 →\\n128 × 128 × 32 → 128 × 128 × 64 → 128 × 128 × 128 (the first\\npath for instance), so that promotes the 2D–3D spatial correspon-\\ndence in conversion. At the end, a further 2D convolution with\\npixelshuffle [27,28] is used to convert into 128 × 128 × (128 · C)\\nto enlarge the feature channels for more expression in the 3D\\ndomain, formulated as:\\nFeat3D,S = PS(Feat2D,(S,L))\\n(4)\\nwhere PS(·) is pixelshuffle operation.\\nThanks to these, the 2D collateral multi-paths are able to\\neffectively decompose the overlapped anatomical structures.\\n(2) The view alignment combines the AP and the LA informa-\\ntion to construct the stereo anatomical structure. It transforms\\nthe AP and LA features to be consistent in orientation, and inte-\\ngrates the orthogonal spatial anatomy. The further 3D convolu-\\ntion with multi-scale fusion is used to enhance the correlation of\\nthe voxel for the stereo structure.\\nIn detail, the 3D features with projected information ex-\\ntracted from AP and LA views are firstly permuted to be assigned\\nto each other and 3D CT orientation. For LA-view 3D feature\\nFeat3D\\nLA (XLA, YLA, ZLA), the first dimension XLA represents the length\\ndimension of LA-view X-ray image, and corresponds to the width\\ndimension YCT of 3D CT data. Such real-word geometric corre-\\nspondence of all dimensions between 3D features of X-ray images\\nand CT data can be described as:\\nXLA ↔ YCT\\nYLA ↔ ZCT\\nZLA ↔ XCT\\nXAP ↔ XCT\\nYAP ↔ ZCT\\nZAP ↔ YCT\\n(5)\\nwhere XLA, YLA, ZLA are dimensions in LA-view 3D feature Feat3D\\nLA\\n(XLA, YLA, ZLA), XAP, YAP, ZAP mean dimensions in AP-view 3D fea-\\nture Feat3D\\nAP (XAP, YAP, ZAP), and XCT , YCT , ZCT denote dimensions\\nin CT feature Feat3D\\nCT (XCT, YCT, ZCT ). Therefore, to promote view\\nalignment, the permutations of Feat3D\\nLA and Feat3D\\nAP are formulated\\nas:\\nFeat′3D\\nLA = P(Feat3D\\nLA , [ZLA, XLA, YLA])\\nFeat′3D\\nAP = P(Feat3D\\nAP, [XAP, ZAP, YAP])\\n(6)\\nwhere P is the permutation operation, according to the order [·].\\nThen, the permuted 3D features Feat′3D\\nLA and Feat′3D\\nAP are concate-\\nnated along channel as:\\nFeat3D\\nCT = Concat(Feat′3D\\nLA , Feat′3D\\nAP)\\n(7)\\nAnd a following multi-scale 3D fusion is conducted to fur-\\nther merge the multi-view decomposed stereo information and\\nintegrate multi-scale structure. Given the permuted 3D features\\nFeat\\n3D, 1\\nCT 8\\n, Feat\\n3D, 1\\nCT 4\\n, Feat\\n3D, 1\\nCT 2\\nand Feat3D,1\\nCT\\nfrom the multi-paths\\nwith scales of\\n1\\n8,\\n1\\n4,\\n1\\n2 and 1, respectively, the procedures are\\ndepicted as\\nFeat′3D,1\\nCT\\n= SeqConv3D(Feat3D,1\\nCT\\n+ U(Feat\\n3D, 1\\n2\\nCT\\n) + U(Feat\\n3D, 1\\n4\\nCT\\n) + U(Feat\\n3D, 1\\n8\\nCT\\n))\\nFeat′3D, 1\\nCT 2\\n= SeqConv3D(Feat\\n3D, 1\\nCT 2\\n+ D(Feat3D,1\\nCT\\n) + U(Feat\\n3D, 1\\n4\\nCT\\n) + U(Feat\\n3D, 1\\n8\\nCT\\n))\\nFeat′3D, 1\\nCT 4\\n= SeqConv3D(Feat\\n3D, 1\\nCT 4\\n+ D(Feat3D,1\\nCT\\n) + D(Feat\\n3D, 1\\n2\\nCT\\n) + U(Feat\\n3D, 1\\n8\\nCT\\n))\\nFeat′3D, 1\\nCT 8\\n= SeqConv3D(Feat\\n3D, 1\\nCT 8\\n+ D(Feat3D,1\\nCT\\n) + D(Feat\\n3D, 1\\n2\\nCT\\n) + D(Feat\\n3D, 1\\n4\\nCT\\n))\\n(8)\\nwhere SeqConv3D(·) represents sequentially conducting the 3D\\n3 × 3 × 3 convolution with Leaky ReLU activation function. D(·)\\nis the down-sampling operation composed of consecutive strided\\n3 × 3 × 3 convolutions with stride 2, U(·) represents the simple\\nnearest neighbor sampling following a 1 × 1 × 1 convolution.\\n2.1.2. Hierarchical 3D fusion\\nTo interpretively sort out the 3D spatial information in con-\\nsistency, the hierarchical 3D fusion in Fig. 3(b) gradually merges\\nthe 3D features between the adjacent scale expressions. The pro-\\ncedure uses the transpose convolution to up-sample for arranging\\n4\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nthe adjacent scales in a learnable way, and fuse the grouped\\nadjacent scales with successive 3D convolutions, formulaically\\ndescribed as:\\nFeat′′3D,1\\nCT\\n= SeqConv3D(Concat(Feat′3D,1\\nCT , UConv(Feat′′3D, 1\\nCT 2\\n)))\\nFeat′′3D, 1\\nCT 2\\n= SeqConv3D(Concat(Feat′3D, 1\\nCT 2\\n, UConv(Feat′′3D, 1\\nCT 4\\n)))\\nFeat′′3D, 1\\nCT 4\\n= SeqConv3D(Concat(Feat′3D, 1\\nCT 4\\n, UConv(Feat′3D, 1\\nCT 8\\n)))\\n(9)\\nwhere is UConv(·) is the up-sampling operation by using the\\n3 × 3 × 3 transpose convolution with stride 2.\\nHierarchical 3D fusion thus explicitly interprets the consistent\\nmulti-scale structure in 3D C-vertebra CT, and reconstructs the\\nstereo anatomy according to the strong spatial relation inter\\nadjacent scale.\\n2.1.3. Multi-view vgg loss\\nFor the assembled expressiveness of the voxels in 3D recon-\\nstructed CT, multi-view vgg loss is creatively developed on all\\nplanes along the axial, the coronal and the sagittal views. It guides\\ncontent consistency among the voxels in the 3D scene to express\\nthe anatomy. It is directly transferred from the widely accepted\\npre-trained VGGNet. With the pre-trained VGGNet [29], the loss\\nextracts the high-level feature representations of expressing [30]\\nfor the scene interpretation, and further constraints in multi-\\nviews for the stereo context. Given the reconstructed 3D CT ˆyCT\\nand its GT yCT , multi-view vgg loss is defined as:\\nlossMV−vgg = lossA−vgg(ˆyCT ) + lossC−vgg(ˆyCT ) + lossS−vgg(ˆyCT )\\n= 1\\nL\\n∑\\ni\\n\\ued79\\ued79vgg(ˆyi\\nA) − vgg(yi\\nA)\\n\\ued79\\ued792\\n2\\n+ 1\\nW\\n∑\\nj\\n\\ued79\\ued79\\ued79vgg(ˆy\\nj\\nC) − vgg(y\\nj\\nC)\\n\\ued79\\ued79\\ued79\\n2\\n2\\n+ 1\\nH\\n∑\\nk\\n\\ued79\\ued79vgg(ˆyk\\nS) − vgg(yk\\nS)\\n\\ued79\\ued792\\n2\\n(10)\\nwhere ˆyi\\nA, ˆy\\nj\\nC and ˆyk\\nS are planes exported from ˆyCT along the axial,\\nthe coronal and the sagittal views. L, W and H are the length,\\nwidth and height of ˆyCT .\\nFurthermore, the loss function of SpaDRNet consists of\\nlossMV−vgg for anatomy expression and lossMAE for voxel details,\\nas follows:\\nLR(R) = lossMV−vgg + lossMAE\\n= lossMV−vgg +\\n\\ued79\\ued79ˆyCT − yCT\\n\\ued79\\ued79\\n(11)\\n2.2. MulSISNet for 3D semantic segmentation on reconstructed CT\\nOur MulSISNet in Fig. 3(c) robustly utilizes multi-scale 3D\\nextraction and interoperation to interactively make the 3D C-\\nvertebra semantic segmentation from the reconstructed CT, and\\ntransfers the shape constraint to SpaDRNet.\\n2.2.1. MulSISNet architecture\\nThe MulSISNet architecture is inspired by the robust repre-\\nsentation of HRNet [26], and the efficient architecture of 3D\\nUNet [31]. As shown Fig. 3(c), MulSISNet further lengthens each\\nencoded layer to connect the decoder, instead of skip connection,\\nfor avoiding the low-level representation of larger size feature\\nwith shallow extraction and the information loss of smaller size\\nfeature with downsample in 3D UNet. So that it develops 3D\\nmulti-scale paths in 3D UNet for the different-range stereo struc-\\nture correlation acquirement for C-vertebra shape. To make full\\nuse of the learned information and compensate the lost of the\\nfeature extraction in each scale path, MulSISNet deploys the inter-\\noperation among multi-scales paths to enhance the representa-\\ntion of each other. During the interoperation, MulSISNet develops\\nthe multi-scale fusion [26] into stereo by using 3 × 3 × 3\\nconvolution with stride = 2 to downsample, nearest neighbor\\ninterpolation+1 × 1 × 1 convolution to upsample, and summa-\\ntion to merge the arranged multi-scales. The procedure can be\\nformulated as:\\nFeat\\n3D,(1,l+1)\\nSeg\\n= SeqConv3D(Feat\\n3D,(1,l)\\nSeg\\n+ U(Feat\\n3D,( 1\\n2 ,l−1)\\nSeg\\n)\\n+ U(Feat\\n3D,( 1\\n4 ,l−2)\\nSeg\\n) + U(Feat\\n3D,( 1\\n8 ,l−3)\\nSeg\\n))\\nFeat\\n3D,( 1\\n2 ,l+1)\\nSeg\\n= SeqConv3D(Feat\\n3D,( 1\\n2 ,l)\\nSeg\\n+ D(Feat\\n3D,(1,l+1)\\nSeg\\n)\\n+ U(Feat\\n3D,( 1\\n4 ,l−1)\\nSeg\\n) + U(Feat\\n3D,( 1\\n8 ,l−2)\\nSeg\\n))\\nFeat\\n3D,( 1\\n4 ,l+1)\\nSeg\\n= SeqConv3D(Feat\\n3D,( 1\\n4 ,l)\\nSeg\\n+ D(Feat\\n3D,(1,l+2)\\nSeg\\n)\\n+ D(Feat\\n3D,( 1\\n2 ,l+1)\\nSeg\\n) + U(Feat\\n3D,( 1\\n8 ,l−1)\\nSeg\\n))\\nFeat\\n3D,( 1\\n8 ,l+1)\\nSeg\\n= SeqConv3D(Feat\\n3D,( 1\\n8 ,l)\\nSeg\\n+ D(Feat\\n3D,(1,l+3)\\nSeg\\n)\\n+ D(Feat3D,( 1\\n2 ,l+2)) + D(Feat\\n3D,( 1\\n4 ,l+1)\\nSeg\\n))\\n(12)\\nand\\nFeat\\n3D,(1,1)\\nSeg\\n= SeqConv3D(ˆyCT )\\nFeat\\n3D,( 1\\n2 ,1)\\nSeg\\n= SeqConv3D(DConv(Feat\\n3D,(1,1)\\nSeg\\n))\\nFeat\\n3D,( 1\\n4 ,1)\\nSeg\\n= SeqConv3D(DConv(Feat\\n3D,( 1\\n2 ,1)\\nSeg\\n))\\nFeat\\n3D,( 1\\n8 ,1)\\nSeg\\n= SeqConv3D(DConv(Feat\\n3D,( 1\\n4 ,1)\\nSeg\\n))\\n(13)\\nFinally, the multi-scale stereo features are transmitted into the\\ndecoder to summarize the semantics for segmentation, formu-\\nlaically described as:\\nFeat′3D,1\\nSeg = SeqConv3D(Concat(Feat\\n3D,(1,5)\\nSeg\\n, UConv(Feat′3D, 1\\nSeg 2\\n)))\\nFeat′3D, 1\\nSeg 2\\n= SeqConv3D(Concat(Feat\\n3D,( 1\\n2 ,4)\\nSeg\\n, UConv(Feat′3D, 1\\nSeg 4\\n)))\\nFeat′3D, 1\\nSeg 4\\n= SeqConv3D(Concat(Feat\\n3D,( 1\\n4 ,3)\\nSeg\\n, UConv(Feat\\n3D,( 1\\n8 ,2)\\nSeg\\n)))\\n(14)\\n2.2.2. Dice loss\\nReciprocally, on the basis of the reconstructed CT, there is a\\nstrong relation inter the tasks, so that MulSISNet further trans-\\nfers the segmentation learning to make shape constraint on the\\nreconstruction in SpaDRNet. The segmentation learning loss is\\ncalculated with dice as Eq. (15), where ˆy\\nm,n,p\\nSeg\\nand y\\nm,n,p\\nSeg\\nare voxel\\nsegmentation and its GT at (m, n, p).\\nLS(R, S) = 1 − dice(ˆySeg, ySeg)\\n= 1 −\\n2 ∑\\nm,n,p ˆy\\nm,n,p\\nSeg\\n· y\\nm,n,p\\nSeg\\n∑\\nm,n,p ˆy\\nm,n,p\\nSeg\\n+ ∑\\nm,n,p y\\nm,n,p\\nSeg\\n(15)\\n2.3. RSC learning for reconstruction–segmentation consistency\\nOur RSC Learning in Fig. 3(d) creatively introduces CT GT-to-\\nsegmentation learning, to drive the reconstruction–segmentation\\nconsistency of the interactive tasks.\\n(1) RSC Learning on segmentation: It strengthens the seg-\\nmentation by enforcing the segmentation learning on the CT GT\\nduring X-CTRSNet training, so that robustly reduces the mislead-\\ning and mess from the early-stage unstable reconstruction, and\\nstabilizes the learning direction of MulSISNet. Moreover, with\\nsuch segmentation optimization on CT GT, it further guides the\\neffectiveness of MulSISNet on real CT data, thus refining the\\nsegmentation feedback and driving consistency with CT.\\n5\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nFig. 4. X-CTRSNet anatomically enables a 3D C-vertebra physiological structure\\nillustration.\\n(2) RSC Learning on reconstruction: It enhances the recon-\\nstruction as penalizing the reconstruction training with the di-\\nvergence between the two segmentations of reconstructed CT and\\nCT GT, so that deeply reinforces the reconstructed biological CT\\nanatomical texture. By penalizing the inter-segmentations diver-\\ngence, it can converge the reconstructed CT to the CT with the\\nreal CT consistent segmentation. Through beneficial interaction,\\nRSC Learning enables X-CTRSNet the consistently and precisely\\ncoupled tasks reconstruction–segmentation.\\nGiven the segmentations ˆySeg, ˆyGTCT−Seg of the reconstructed CT\\nand CT GT, the loss function is defined as:\\nLRSC(R, S) = dice(ˆyGTCT−Seg, ySeg) + dice(ˆySeg, ˆyGTCT−Seg)\\n(16)\\n3. Experiments and analysis\\n3.1. Materials and configurations\\nClinical data from 69 patients were used for the evaluation.\\nThe segmentation GT of C-vertebras (ordered as C1,C2,C3,C4,C5,\\nC6 and C7) is labeled by two radiologists with cross-check. Specif-\\nically, Instance Normalization and Group Normalization are used\\nin SpaDRNet of reconstruction and MulSISNet of segmentation,\\nrespectively. The network is implemented using Tensorflow with\\nthe Adam optimizer. The initial learning rate is set as 10−3. Ten-\\nfolder cross validation is adopted in the performance evaluation\\nand comparison. The dataset is divided into 10 groups. In the\\nfirst nine groups, there were 7 patients in each group. And the\\nlast group contains 6 patients. In each validation, nine groups are\\nused to train the network, and the last group is used for test. The\\nprocedure was repeated 10 times, until all the subjects have been\\nprocessed.\\nStructural similarity index (SSIM) [32] and peak signal to\\nnoise ratio (PSNR) are employed to evaluate the reconstruction\\nperformance, as well as Dice coefficient (Dice) [33] is used for\\nsegmentation assessment. SSIM is defined as\\nSSIM(yCT, ˆyCT ) =\\n(2µyCT µˆyCT + c1)(2cov(yCT, ˆyCT ) + c2)\\n(µ2\\nyCT + µ2\\nˆyCT + c1)(σ 2\\nyCT + σ 2\\nˆyCT + c2) ,\\n(17)\\nwhere µ means average, σ is standard deviation, cov(·) denotes\\ncovariance, c represents variables to stabilize the division with\\nweak denominator.\\nPSNR is calculated as\\nPSNR = 10 · log10(\\nMAX 2\\nyCT\\n1\\nLWH\\n∑L,W,H\\nm=1,n=1,p=1[ˆy\\nm,n,p\\nCT\\n− y\\nm,n,p\\nCT\\n]2 ),\\n(18)\\nAnd Dice has definition of\\nDice =\\n2\\n⏐⏐ySeg ∩ ˆySeg\\n⏐⏐\\n⏐⏐ySeg\\n⏐⏐ +\\n⏐⏐ˆySeg\\n⏐⏐ · 100%\\n(19)\\n3.2. Results and analysis\\n3.2.1. Overall performance\\nAs the last row in Table 1 shows, the proposed X-CTRSNet\\nsuccessfully achieves high-performance 3D C-vertebra CT recon-\\nstruction and segmentation directly from the 2D X-ray images.\\nIt gains a high SSIM of 0.749 and a high PSNR of 24.58 dB for\\nthe reconstructed CT, as well as an average Dice up to 80.44% for\\nthe seven segmented C-vertebras (C1,C2,C3,C4,C5,C6 and C7). So\\nthat it anatomically enables a 3D cervical vertebra physiological\\nstructure illustration, as shown in Fig. 4, with ‘‘once is enough\"\\nfast 2D imaging to speed up the diagnostic procedure and reduce\\nthe repetitive radiation.\\n3.2.2. Ablation study\\nAs shown in Table 1, the innovative components designed for\\nX-CTRSNet, including SpaDRNet, lossMV−vgg, MulSISNet and RSC\\nLearning, enable robust improvements. By using SpaDRNet, the\\nanatomical structure from the overlapped 2D X-ray images are ef-\\nfectively decomposed layer-by-layer as shown in Fig. 5, thanks to\\nits progressive 2D–3D conversion with spatial correspondence. By\\nusing lossMV−vgg, the performance of reconstruction gains 2.11%\\nimprovement in SSIM and 0.31 dB improvement in PSNR. It is\\nbeneficial from the guidance of lossMV−vgg for content consistency\\namong voxels in 3D scenes to express biological anatomy. By\\nusing MulSISNet, an accurate segmentation is further enabled for\\n3D morphology extraction, and meanwhile enhance 3D CT recon-\\nstruction. It is contributed by the multi-scale 3D extraction and\\ninteroperation in MulSISNet, as well as utilizing the reciprocal re-\\nlation inter tasks. By using RSC Learning, the best performance in\\nboth reconstruction and segmentation is further achieved, as RSC\\nLearning promotes the reconstruction–segmentation consistency\\nof the interactive tasks with CT GT-to-segmentation learning.\\n3.2.3. Comparison experiments\\nAs shown in Figs. 2 & 3, X-CTRSNet achieves superior accu-\\nracy in both tasks compared to the state-of-the-art methods: (1)\\nSIT [22], PSR [23] and X2CT-GAN [24] for reconstruction, as well\\nas (2) 3D UNet [31], DSN [34], DenseBiasNet [35], CS2-Net [36]\\nand ConResNet [37] for after-reconstruction 3D segmentation.\\nIn the reconstruction comparison (Table 2), X-CTRSNet im-\\nproved the SSIM by 13.78% on average for accurate anatomical\\nstructures, and increased the PSNR by 2.27 dB for clearly readable\\nimaging. As shown in Fig. 6, it visually enables clear and ro-\\nbust full-view biologic structures without overlapping that have\\nlegible distribution, shape and explicitly readable anatomical tex-\\nture, and further promotes the precise 3D segmentation of the\\nC-vertebras morphology on the reconstructed stereo CT data.\\nSo that X-CTRSNet distinctly provides the detailed 3D imag-\\ning diagnosis basis from the 2D X-ray imaging characterized by\\nlow radiation risk. But the compared ones behave poorly, which\\ncauses rough shape of C-vertebras groups, and fails on each\\ndetailed C-vertebra and the interlock relation among C-vertebras.\\nIn the segmentation comparison, as shown in Table 3, our\\nproposed method still effectively improves the average Dice with\\n3.25%, and comprehensive promotes the more precise segmenta-\\ntion for all C-vertebras C1 to C7. Visually in Fig. 7, it is robust to\\nsegment the multi-scale structure components distributed among\\nC-vertebras. As the multi-scale structure components (circled by\\ndotted line in Fig. 7) including foramen transversarium, spinous\\nprocess, and vertebral body which cause difficulties to the com-\\npared methods. Our X-CTRSNet still precisely segments them for\\nthe morphology extraction, thanks to the multi-scale path and\\ninteroperation in its sub module MulSISNet.\\nFurthermore, besides the above accuracy comparison in both\\ntasks, the comparison of model complexities is also made. As\\n6\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nFig. 5. SpaDRNet effectively decomposes the anatomical structure from the overlapped 2D X-ray images, by progressive 2D–3D conversion with spatial correspondence.\\nFig. 6. X-CTRSNet achieves the reconstruction of the detailed CT anatomy of legible distribution, shape and explicitly readable anatomical texture, and thus further\\npromotes the precise 3D structure segmentation.\\nTable 1\\nX-CTRSNet successfully achieves the accurate reconstruction and segmentation, contributed to its innovative components.\\nshown in the last 3 columns in Figs. 2 & 3, compared with the\\nknown methods, our proposed method gains acceptable perfor-\\nmance in model complexities including the number of train-\\nable parameters, floating point operations (FLOPs), and inference\\nspeed, especially when combined with our high-quality recon-\\nstruction and segmentation results. (1) Significantly, during the\\ninference, our method only needs 0.592 s to conduct both the 3D\\nCT reconstruction and segmentation from two 2D X-ray images\\n7\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\nTable 2\\nX-CTRSNet gains superior accuracy in reconstruction compared to the state-of-the-art method, with the acceptable\\nmodel complexity.\\nSSIM\\nPSNR (dB)\\nParameters\\nnumber\\nFLOPs\\nInference speed\\nSIT [22]\\n0.631 ± 0.058\\n22.00 ± 1.82\\n8,354,112\\n36,970,987,528\\n0.040 s/patient\\nPSR [23]\\n0.653 ± 0.049\\n21.90 ± 1.77\\n519,432,132\\n43,572,466,660\\n0.177 s/patient\\nX2CT-GAN [24]\\n0.691 ± 0.051\\n23.04 ± 1.70\\n23,564,691\\n227,531,788,978\\n0.210 s/patient\\nX-CTRSNet\\n0.749 ± 0.043\\n24.58 ± 1.56\\n22,992,322\\n216,830,517,469\\n0.182 s/patient\\nTable 3\\nX-CTRSNet gains superior accuracy in after-reconstruction 3D segmentation compared to the state-of-the-art method, with the acceptable model complexity.\\nDice (%)\\nParameters\\nnumber\\nFLOPs\\nInference speed\\nC1\\nC2\\nC3\\nC4\\nC5\\nC6\\nC7\\nMean\\n3D UNet [31]\\n68.65 ±\\n12.51\\n72.50 ±\\n10.30\\n73.29 ±\\n10.18\\n72.83 ±\\n12.14\\n68.08 ±\\n14.48\\n71.88 ±\\n11.85\\n76.67 ±\\n8.98\\n71.98 ±\\n11.49\\n1,755,416\\n601,140,127,338\\n0.350 s/patient\\nDSN [34]\\n71.82 ±\\n11.16\\n82.33 ±\\n9.26\\n77.44 ±\\n8.67\\n72.97 ±\\n11.45\\n73.98 ±\\n13.24\\n80.24 ±\\n7.32\\n82.61 ±\\n6.03\\n77.34 ±\\n9.59\\n7,028,344\\n513,911,708,845\\n0.269 s/patient\\nDenseBiasNet [35]\\n72.75 ±\\n10.14\\n81.24 ±\\n9.88\\n81.35 ±\\n6.15\\n79.44 ±\\n9.38\\n76.59 ±\\n10.45\\n79.54 ±\\n7.83\\n81.25 ±\\n6.97\\n78.88 ±\\n8.69\\n1,568,820\\n756,871,554,273\\n0.446 s/patient\\nCS2-Net [36]\\n72.94 ±\\n9.65\\n80.94 ±\\n9.23\\n77.97 ±\\n14.59\\n77.61 ±\\n13.99\\n78.72 ±\\n10.21\\n79.96 ±\\n6.80\\n82.56 ±\\n5.60\\n78.67 ±\\n10.01\\n5,831,628\\n594,944,273,005\\n0.329 s/patient\\nConResNet [37]\\n71.62 ±\\n10.98\\n81.20 ±\\n9.24\\n79.84 ±\\n8.79\\n79.71 ±\\n9.86\\n78.11 ±\\n10.70\\n79.93 ±\\n7.59\\n83.02 ±\\n5.38\\n79.06 ±\\n8.93\\n19,175,488\\n1,583,526,133,185\\n0.780 s/patient\\nX-CTRSNet\\n74.71 ±\\n8.39\\n83.25 ±\\n7.21\\n81.89 ±\\n5.83\\n80.15 ±\\n8.42\\n78.37 ±\\n9.75\\n80.91 ±\\n6.45\\n83.80 ±\\n4.88\\n80.44 ±\\n7.27\\n4,410,136\\n658,383,183,305\\n0.410 s/patient\\nFig. 7. X-CTRSNet shows superiority to precisely segment C-vertebra from the interactively reconstructed 3D CT. For the cervical vertebra that has multi-scale structure\\ncomponents, it still makes robust segmentation. As the foramen transversarium, spinous process, and vertebral body of the multi-scale structure components (circled\\nby dotted line) cause difficulties to the compared ones, X-CTRSNet precisely segment them for the morphology extraction.\\nfor one patient on a laptop with one Nvidia RTX 3080 GPU and\\nan Intel i9 CPU. As can be seen, our method just takes less\\nthan a second for processing, so that remarkably saves time\\nin clinical 3D CT imaging and analysis, as well as reduces un-\\nwanted repetitive radiation of excessive examination, especially\\nfor the triage of emergency department. (2) Besides, in clinical\\napplications, the accuracy of the method is a more important\\npriority [38]. The results of the accuracy comparison show that\\nthe performance of our method is significantly better than those\\nof other known methods, gaining 13.78% improvements in SSIM\\nfor accurate anatomical structure, and increasing PSNR by 2.27dB\\nfor clearly readable imaging, as well as improving the average\\nDice with 3.25% for precise segmentation. Especially compared\\nwith SIT and DSN which have the lowest model complexities for\\nthe reconstruction and the 3D segmentation, respectively, our\\nmethod achieves 18.7% higher SSIM, 2.58dB higher PSNR and\\n3.10% higher Dice to achieve the best model accuracy. Combining\\nboth the accuracy and complexities of our method, our method\\nhas great potential to effectively and quickly make 3D CT re-\\nconstruction and segmentation directly from 2D X-ray images in\\nclinical.\\n8\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\n4. Conclusion\\nIn this paper, we propose X-CTRSNet, the first powerful work\\nto simultaneously and accurately enable 3D C-vertebra CT re-\\nconstruction and segmentation directly from 2D X-ray images.\\nThe method is innovatively achieved by the following compo-\\nnents: (1) SpaDRNet for the overlapped anatomy decomposition\\nand reconstructing into the pathological information detailed 3D\\nCT; (2) MulSISNet for the multi-scale stereo structure extraction\\nand the further segmentation on the reconstructed CT, where\\nthe shape constrains are interpretively fed back; and (3) RSC\\nLearning for the reconstruction–segmentation consistency in the\\ninteractive multi-tasks. Extensive experiments on reconstruction\\nand segmentation reveal ‘‘once is enough\" with X-CTRSNet to\\nimprove the diagnosis efficiency of 2D X-ray imaging and avoid\\nthe repetitive radiation of overtreatment in clinical.\\nCRediT authorship contribution statement\\nRongjun Ge: Conceptualization, Methodology, Software, Val-\\nidation, Writing – original draft, Writing – review & editing.\\nYuting He: Formal analysis, Investigation. Cong Xia: Resources,\\nData curation. Chenchu Xu: Methodology. Weiya Sun: Valida-\\ntion. Guanyu Yang: Formal analysis. Junru Li: Validation. Zhihua\\nWang: Validation. Hailing Yu: Validation. Daoqiang Zhang: Su-\\npervision, Project administration. Yang Chen: Project administra-\\ntion, Funding acquisition. Limin Luo: Supervision. Shuo Li: Super-\\nvision, Conceptualization. Yinsu Zhu: Resources, Data curation,\\nWriting – review & editing.\\nDeclaration of competing interest\\nThe authors declare that they have no known competing finan-\\ncial interests or personal relationships that could have appeared\\nto influence the work reported in this paper.\\nAcknowledgments\\nThis study was funded by the Fundamental Research Funds\\nfor the Central University, China (No. NS2021067); the National\\nNatural Science Foundation, China (No. 62101249, 61871117,\\n62171123 and 81871444); the China Postdoctoral Science Foun-\\ndation (No. 2021TQ0149); the Natural Science Foundation of\\nJiangsu Province (No. BK20210291); the State’s Key Project of\\nResearch\\nand\\nDevelopment\\nPlan\\n(No.\\n2017YFC0109202,\\n2018YFA0704102).\\nReferences\\n[1] S. Ehara, G.Y. El-Khoury, C.R. Clark, Radiologic evaluation of dens fracture.\\nRole of plain radiography and tomography, Spine 17 (5) (1992) 475–479.\\n[2] C. Bach, I. Steingruber, S. Peer, R. Peer-Kühberger, W. Jaschke, M. Ogon,\\nRadiographic evaluation of cervical spine trauma, Arch. Orthop. Trauma\\nSurg. 121 (7) (2001) 385–387.\\n[3] K. Ofori, S.W. Gordon, E. Akrobortu, A.A. Ampene, E.O. Darko, Estimation\\nof adult patient doses for selected X-ray diagnostic examinations, J. Radiat.\\nRes. Appl. Sci. 7 (4) (2014) 459–462.\\n[4] D.J. Brenner, E.J. Hall, Computed tomography-an increasing source of\\nradiation exposure, N. Engl. J. Med. 357 (22) (2007) 2277–2284.\\n[5] G.Y. El-Khoury, M.H. Kathol, W.W. Daniel, Imaging of acute injuries of the\\ncervical spine: value of plain radiography, CT, and MR imaging, AJR Am. J.\\nRoentgenol. 164 (1) (1995) 43–50.\\n[6] K.C. Kim, H.C. Cho, T.J. Jang, J.M. Choi, J.K. Seo, Automatic detection and\\nsegmentation of lumbar vertebrae from X-ray images for compression\\nfracture evaluation, Comput. Methods Programs Biomed. (2020) 105833.\\n[7] M.S. Kang, J.W. Lee, H.Y. Zhang, Y.E. Cho, Y.M. Park, Diagnosis of cervical\\nOPLL in lateral radiograph and MRI: is it reliable? Korean J. Spine 9 (3)\\n(2012) 205.\\n[8] M. Yamazaki, T. Akazawa, A. Okawa, M. Koda, Usefulness of three-\\ndimensional full-scale modeling of surgery for a giant cell tumor of the\\ncervical spine, Spinal Cord 45 (3) (2007) 250–253.\\n[9] S.A. Stratemann, J.C. Huang, K. Maki, D.C. Hatcher, A.J. Miller, Evaluating\\nthe mandible with cone-beam computed tomography, Am. J. Orthod.\\nDentofacial Orthop. 137 (4) (2010) S58–S70.\\n[10] T. Izumi, T. Hirano, K. Watanabe, A. Sano, T. Ito, N. Endo, Three-dimensional\\nevaluation of volume change in ossification of the posterior longitudinal\\nligament of the cervical spine using computed tomography, Eur. Spine J.\\n22 (11) (2013) 2569–2574.\\n[11] C. Mouhanna-Fattal, M. Papadopoulos, J. Bouserhal, A. Tauk, N. Bassil-\\nNassif, A. Athanasiou, Evaluation of upper airway volume and craniofacial\\nvolumetric structures in obstructive sleep apnoea adults: a descriptive\\nCBCT study, Int. Orthod. 17 (4) (2019) 678–686.\\n[12] B. Qiu, J. Guo, J. Kraeima, H.H. Glas, R.J. Borra, M.J. Witjes, P.M. van Ooijen,\\nAutomatic segmentation of the mandible from computed tomography\\nscans for 3D virtual surgical planning using the convolutional neural\\nnetwork, Phys. Med. Biol. 64 (17) (2019) 175020.\\n[13] I. Barngkgei, E. Joury, A. Jawad, An innovative approach in osteoporosis\\nopportunistic screening by the dental practitioner: the use of cervical\\nvertebrae and cone beam computed tomography with its viewer program,\\nOral Surg. Oral Med. Oral Pathol. Oral Radiol. 120 (5) (2015) 651–659.\\n[14] B. Müller, D. Evangelopoulos, K. Bias, A. Wildisen, H. Zimmermann, A.K.\\nExadaktylos, Can S-100B serum protein help to save cranial CT resources\\nin a peripheral trauma centre? A study and consensus paper, Emerg. Med.\\nJ. 28 (11) (2011) 938–940.\\n[15] R. Ge, G. Yang, y. Chen, L. Luo, C. Feng, H. Zhang, S. Li, PV-LVNet: Direct\\nleft ventricle multitype indices estimation from 2D echocardiograms of\\npaired apical views with deep neural networks, Med. Image Anal. 58 (2019)\\n101554.\\n[16] Z. Wu, R. Ge, M. Wen, G. Liu, Y. Chen, P. Zhang, X. He, J. Hua, L. Luo, S. Li,\\nELNet: Automatic classification and segmentation for esophageal lesions\\nusing convolutional neural network, Med. Image Anal. 67 (2021) 101838.\\n[17] Y. He, T. Li, R. Ge, J. Yang, Y. Kong, J. Zhu, H. Shu, G. Yang, S. Li, Few-shot\\nlearning for deformable medical image registration with perception-\\ncorrespondence decoupling and reverse teaching, IEEE J. Biomed. Health\\nInf. (2021).\\n[18] R. Ge, G. Yang, Y. Chen, L. Luo, C. Feng, H. Ma, J. Ren, S. Li, K-Net:\\nintegrate left ventricle segmentation and direct quantification of paired\\necho sequence, IEEE Trans. Med. Imaging 39 (5) (2020) 1690–1702.\\n[19] R. Ge, T. Shen, Y. Zhou, C. Liu, L. Zhang, B. Yang, Y. Yan, J.-L. Coatrieux,\\nS. Li, Convolutional squeeze-and-excitation network for ECG arrhythmia\\ndetection, Artif. Intell. Med. 121 (2021) 102181.\\n[20] D. Hu, W. Wu, M. Xu, Y. Zhang, J. Liu, R. Ge, Y. Chen, L. Luo, G. Coatrieux,\\nSISTER: Spectral-image similarity-based tensor with enhanced-sparsity re-\\nconstruction for sparse-view multi-energy CT, IEEE Trans. Comput. Imaging\\n6 (2019) 477–490.\\n[21] G. Luo, W. Wang, C. Tam, K. Wang, S. Cao, H. Zhang, B. Chen, S.\\nLi, Dynamically constructed network with error correction for accurate\\nventricle volume estimation, Med. Image Anal. 64 (2020) 101723.\\n[22] P. Henzler, V. Rasche, T. Ropinski, T. Ritschel, Single-image tomography:\\n3D volumes from 2D cranial X-Rays, in: Computer Graphics Forum, vol.\\n37, (2) Wiley Online Library, 2018, pp. 377–388.\\n[23] L. Shen, W. Zhao, L. Xing, Patient-specific reconstruction of volumetric\\ncomputed tomography images from a single projection view via deep\\nlearning, Nat. Biomed. Eng. 3 (11) (2019) 880–888.\\n[24] X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, Y. Zheng, X2CT-GAN: reconstructing\\nCT from biplanar X-rays with generative adversarial networks, in: Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern Recognition,\\n2019, pp. 10619–10628.\\n[25] K.T. Johnson, W.N. Al-Holou, R.C. Anderson, T.J. Wilson, T. Karnati, M.\\nIbrahim, H.J. Garton, C.O. Maher, Morphometric analysis of the developing\\npediatric cervical spine, J. Neurosurg. Pediatr. 18 (3) (2016) 377–389.\\n[26] K. Sun, B. Xiao, D. Liu, J. Wang, Deep high-resolution representation\\nlearning for human pose estimation, in: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2019, pp. 5693–5703.\\n[27] W. Shi, J. Caballero, F. Huszár, J. Totz, A.P. Aitken, R. Bishop, D. Rueckert,\\nZ. Wang, Real-time single image and video super-resolution using an\\nefficient sub-pixel convolutional neural network, in: Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.\\n1874–1883.\\n[28] R. Ge, G. Yang, C. Xu, Y. Chen, L. Luo, S. Li, Stereo-correlation and noise-\\ndistribution aware ResVoxGAN for dense slices reconstruction and noise\\nreduction in thick low-dose CT, in: International Conference on Medical\\nImage Computing and Computer-Assisted Intervention, Springer, 2019, pp.\\n328–338.\\n9\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='R. Ge, Y. He, C. Xia et al.\\nKnowledge-Based Systems 236 (2022) 107680\\n[29] K.\\nSimonyan,\\nA.\\nZisserman,\\nVery\\ndeep\\nconvolutional\\nnetworks\\nfor\\nlarge-scale image recognition, 2014, arXiv preprint arXiv:1409.1556.\\n[30] J. Johnson, A. Alahi, L. Fei-Fei, Perceptual losses for real-time style trans-\\nfer and super-resolution, in: European Conference on Computer Vision,\\nSpringer, 2016, pp. 694–711.\\n[31] O. Çiçek, A. Abdulkadir, S.S. Lienkamp, T. Brox, O. Ronneberger, 3D U-Net:\\nlearning dense volumetric segmentation from sparse annotation, in: Inter-\\nnational Conference on Medical Image Computing and Computer-Assisted\\nIntervention, Springer, 2016, pp. 424–432.\\n[32] Z. Wang, A.C. Bovik, H.R. Sheikh, E.P. Simoncelli, Image quality assessment:\\nfrom error visibility to structural similarity, IEEE Trans. Image Process. 13\\n(4) (2004) 600–612.\\n[33] L.R. Dice, Measures of the amount of ecologic association between species,\\nEcology 26 (3) (1945) 297–302.\\n[34] Q. Dou, L. Yu, H. Chen, Y. Jin, X. Yang, J. Qin, P.-A. Heng, 3D deeply\\nsupervised network for automated segmentation of volumetric medical\\nimages, Med. Image Anal. 41 (2017) 40–54.\\n[35] Y. He, G. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J.-L. Dillenseger,\\nP. Shao, S. Zhang, et al., Dpa-densebiasnet: Semi-supervised 3d fine\\nrenal artery segmentation with dense biased network and deep priori\\nanatomy, in: International Conference on Medical Image Computing and\\nComputer-Assisted Intervention, Springer, 2019, pp. 139–147.\\n[36] L. Mou, Y. Zhao, H. Fu, Y. Liu, J. Cheng, Y. Zheng, P. Su, J. Yang, L. Chen, A.F.\\nFrangi, M. Akiba, J. Liu, CS2-Net: Deep learning segmentation of curvilinear\\nstructures in medical imaging, Med. Image Anal. 67 (2021) 101874.\\n[37] J. Zhang, Y. Xie, Y. Wang, Y. Xia, Inter-slice context residual learning for\\n3D medical image segmentation, IEEE Trans. Med. Imaging 40 (2) (2021)\\n661–672.\\n[38] C. Zhang, H. Shu, G. Yang, F. Li, Y. Wen, Q. Zhang, J.-L. Dillenseger, J.-L.\\nCoatrieux, HIFUNet: Multi-class segmentation of uterine regions from MR\\nimages using global convolutional networks for HIFU surgery planning,\\nIEEE Trans. Med. Imaging 39 (11) (2020) 3309–3320.\\n10\\n', metadata={'source': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'file_path': 'data/Ge et al. - 2022 - X-CTRSNet 3D cervical vertebra CT reconstruction .pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'Knowledge-Based Systems, 236 (2022) 107680. doi:10.1016/j.knosys.2021.107680', 'keywords': '', 'creator': 'Elsevier', 'producer': 'pdfTeX', 'creationDate': \"D:20211228084907+00'00'\", 'modDate': \"D:20211228084907+00'00'\", 'trapped': ''}),\n",
       " Document(page_content='Reconstruction of 3D CT from A Single X-ray\\nProjection View Using CVAE-GAN\\n1st Ling Jiang\\nImage Processing Center\\nBeihang University\\nBeijing, China\\njiangling@buaa.edu.cn\\n2nd Mengxi Zhang\\nImage Processing Center\\nBeihang University\\nBeijing, China\\n3250664648@qq.com\\n3rd Ran Wei\\nChinese Cancer Center\\nChinese Academy of Medical Sciences\\nBeijing, China\\nwei cn00@163.com\\n4th Bo Liu\\nImage Processing Center\\nBeihang University\\nBeijing, China\\nbo.liu@buaa.edu.cn\\n5th Xiangzhi Bai\\nImage Processing Center\\nBeihang University\\nBeijing, China\\njackybxz@buaa.edu.cn\\n6th Fugen Zhou\\nImage Processing Center\\nBeihang University\\nBeijing, China\\nzhfugen@buaa.edu.cn\\nAbstract—Computed tomography can provide a 3D view of the\\npatient’s internal anatomy. However, traditional CT reconstruc-\\ntion methods require hundreds of X-ray projections through a\\nfull rotational scan of the body, which cannot be performed on a\\ntypical X-ray machine. In order to deal with the impact of organ\\nmovement caused by respiration in radiotherapy on the accuracy\\nof radiotherapy, we propose to reconstruct CT from a single X-\\nray projection view using the conditional variational autoencoder.\\nConditional variational autoencoder encodes the features of a\\n2D X-ray projection. The decoder decodes the hidden variables\\nencoded by the encoder and increase data dimension from 2D\\n(X-rays) to 3D (CT) to generates a corresponding 3D CT. In\\naddition, we use the discriminator to distinguish the generated\\n3D CT from the real 3D CT to make the generated 3D CT more\\nrealistic. We demonstrate the feasibility of the approach with 3D\\nCT of two patients with lung cancer.\\nIndex Terms—CT reconstruction, X-ray projection, VAE, GAN\\nI. INTRODUCTION\\nThe precise knowledge of tumor position is important\\nfor intra-operative image-guidance of various treatment. For\\nexample, in radiotherapy of lung cancer , the respiratory\\nmotion causes the change of the position of the tumor and\\nsurrounding tissues, which leads to the uncertainty of the\\nradiation dose. It is very necessary to obtain the actual 3D\\nanatomical information during the treatment to guide the\\ntreatment and analyse the actual dose distribution.\\nX-ray projections enable us to observe the human body\\nin real time and non-invasively. However, the anatomy is\\nprojected onto a plane in X-ray projections. The human\\ntissues overlay each other and reduces the visibility. Computed\\ntomography (CT) imaging can generate 3D volume with high\\nspatial resolution of the internal anatomy of the human body.\\nHowever, standard CT reconstruction algorithms need a set\\nof X-ray projections rotationally obtained around the patient,\\nwhich are unavailable during the treatment.\\nWith patient-specific prior knowledge, data-driven deep\\nlearning methods can represent the features of anatomical\\nstructures in a standardized manner and infer the correspond-\\ning 3D volume from the 2D projection. Recently, methods\\nbased on deep learning have been used to generate structurally\\nconsistent information in the case of irreversible imaging with\\nincomplete information. Shen et al. [1] proposed PatRecon-\\nNet to learn the feature-space transformation between a 2D\\nprojection and a 3D CT and derive 3D CT from an anterior-\\nposterior projection. However, convolutional neural network\\n(CNN) may produces anatomically inaccurate synthesis of 3D\\nCT as they provide no guarantee on the anatomical plausibility\\nof their outcome. Generative adversarial network (GAN) [2]\\nhas been used for cross-modal transformation [3]–[5]. Ying et\\nal. [6] proposed X2CT-GAN to synthesize full 3D CT from\\n2D X-rays. However, as GAN is trained to approximate the\\nprobabilistic distribution of the training dataset implicitly, it\\nmay hallucinate a plausible structure that reasonably exists in\\nthe training dataset, instead of generating a structure extrap-\\nolated from the input. Hence, the resulting image structure is\\noften irrelevant to the input and unrealistic. Cerrolaza et al.\\n[7] used a conditional variational autoencoder(CVAE) [8] to\\ngenerate a 3D skull model from 2D ultrasound images, but it\\nwas unable to reconstruct a complete 3D CT. The robustness\\nof technology in response to changes in anatomical structures\\nis still a concern in clinical applications.\\nFollowing these observations, we propose a method to\\nreconstruct 3D CT from a single X-ray projection image. To\\nsummarize, we make the following contributions:\\n• We propose a 2D to 3D conditional variational autoen-\\ncoder(CVAE) to generate 3D CT from 2D X-ray.\\n• We add adversarial loss to differentiate the distribution\\nof generated 3D volume from ground truth 3D volume,\\nthe capability of our method to generate more realistic\\n3D CT is significantly improved.\\n978-1-6654-2608-4/21/$31.00 ©2021 IEEE\\n2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE) | 978-1-6654-2608-4/21/$31.00 ©2021 IEEE | DOI: 10.1109/ICMIPE53131.2021.9698875\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='Fig. 1. The architecture of generator.\\nII. NETWORK ARCHITECTURE\\nOur network, used in reconstruction of volumetric computed\\ntomography images from a single projection view, is similar\\nto other GAN architectures, which involves a generator and a\\ndiscriminator.\\nA. Generator\\nTo perform reconstruction of volumetric computed tomog-\\nraphy images from a single projection view, the design of\\nour structure follows the conditional variational autoencoder\\nframework.\\nEncoder In the encoder part, the information of 2D X-ray\\nimage is encoded. The encoder has 2 branch, one of which\\nencodes the 3D CT samples and the other encodes the a\\n2D X-ray image as conditional variable. The outputs of each\\nbranch are concatenated and mapped to two separate fully-\\nconnected layers to generate µ(Y, X) and Σ(Y, X), which\\nwill be combined with Σ to create z. Dense connectivity [9]\\nhas a compelling advantage in the feature extraction process.\\nTo optimally utilize information from 2D X-ray images, we\\nembed dense modules to generator’s encoding path. Each\\ndense module consists of a down-sampling block, a densely\\nconnected convolution block and a compressing block. The\\ncascaded dense modules encode different level information of\\nthe input image and pass it to the decoder along different\\nshortcut paths.\\nSuppose Y represent a 3D CT volume and X representing\\nthe corresponding 2D X-ray view projected from a certain\\nangle. A generative model, which learns the conditional dis-\\ntribution P(Y |X), can produces a close approximation of\\nYi for a given observation Xi. In the context of variational\\nautoencoders, the generative process is modeled by means\\nof a latent d-dimensional variable z, with a known simple\\ndistribution (typically z ∼ N(0, I)). With this latent variable,\\nthe model can generate new instances of the target structure\\nby randomly sampling values of z. However, it would be very\\ndifficult to directly infer P(Y |X) without sampling a large\\nnumber of z values. Hence, a new function Q(z|X, Y ), which\\ncan generate values of z likely to produce Y with label X, is\\nintroduced. Using Bayes’ rule, there is\\nlogP(Y |X)\\n≥logP(Y |X) − DKL[Q(z|X, Y )||P(z|Y, X)]\\n=logP(Y |X) − Ez∼Q[logQ(z|X, Y ) − logP(z|Y, X)]\\n=logP(Y |X) − Ez∼Q[logQ(z|X, Y ) − logP(Y |z, X)\\n− logP(z|X) + logP(Y |X)]\\n=Ez∼Q[logP(Y |z, X) + logP(z|X) − logQ(z|X, Y )]\\n=Ez∼Q[logP(Y |z, X)] − DKL[Q(z|X, Y )||P(z|X)]\\n(1)\\nwhere\\nDKL[a||b]\\n=\\nEz∼Q[log(a) − log(b))]\\nrepre-\\nsents the Kullback-Leibler(KL) divergence, Q(z|Y, X)\\n=\\nN(z|µ(Y, X), Σ(Y, X)) where µ and Σ are arbitrary, deter-\\nministic functions learned from the data.\\nSince P(z|X)\\n∼\\nN(0, 1), this choice of Q(z|Y, X)\\nallows us to compute DKL[Q(z|Y, X)||P(z|X)] as the\\nKL-divergence between two Gaussian distribution, which\\nhas\\na\\nclosed-form\\nsolution.\\nAs\\nQ\\nis\\nassumed\\nas\\na\\nhigh-capacity function which can approximate P(z|Y, X),\\nDKL[Q(z|X, Y )||P(z|Y, X)]\\nwill\\ntend\\nto\\n0.\\nTherefore,\\nP(Y |X) can be directly optimized through optimizing the\\nright hand side of (1) via stochastic gradient descent. Dur-\\ning the training time, we use the reparameterization trick\\nto make the sampling of z differentiable with respect to µ\\nand Σ, and define zi = µ(Y i, Xi) + ηΣ(Y i, Xi), where\\nη ∼ N(0, I).Based on equation (1), the reconstruction network\\ncan be implemented. The function Q takes the form of the\\nencoder, encoding Y and X into a d-dimensional latent space\\nz, via µ and Σ.\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 1, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='Fig. 2. The loss of network.\\nDecoder In the decoder part, the conditional dependency\\non X is explicitly modeled by the concatenation of z with the\\nvector representation of X. Then, the fully connected layer’s\\noutput is reshaped to 3D. Through the model-training process,\\nthe transformation module learns the underlying relationship\\nbetween feature representations across dimension and reshape\\n2D features to 3D features, making it possible to generate a\\nvolumetric CT image from a 2D projection. However, as most\\nof the 2D spatial information gets lost during such conversion,\\nwe use skip connection between the encoder and the decoder.\\nIt enforces the channel number of the encoder being equal\\nto the one on the corresponding decoder side by a basic 2D\\nconvolution block, expand the 2D feature map to a pseudo-\\n3D one by duplicating the 2D information along the third axis\\nand use a basic 3D convolution block to encode the pseudo-3D\\nfeature map. The abundant low-level information across two\\nparts of the network imposes strong correlations on the shape\\nand appearance between input and output. At test time, the\\ndecoder operates as a generative reconstruction network given\\nthe 2D X-ray image X, generating 3D CT volume by sampling\\nz ∼ N(0, I). In particular, we generate the highest-confidence\\nprediction with z = 0.\\nB. Discriminator\\nPatchGANs [10] have been used frequently in recent works\\ndue to the good generalization property. We adopt a similar\\narchitecture in our discriminator network from 3D Patch Dis-\\ncriminator [10]. It consists of 3D convolution layer, instance\\nnormalization layer and rectified linear unit. The proposed\\ndiscriminator architecture improves the discriminative capacity\\ninherited from the PatchGAN framework and can distinguish\\nreal or fake 3D volumes.\\nIII. LOSS FUNCTIONS\\nIn this section, we introduce loss functions that are used to\\nconstrain the proposed network.\\nA. Adversarial Loss\\nThe intention of GAN is to learn deep generative models\\nwhile avoiding approximating intractable probabilistic com-\\nputations that arise in other strategies such as maximum\\nlikelihood estimation. In the learning procedure, a discrimi-\\nnator D and a generator G would compete with each other\\nto learn a generator distribution pG(x) that matches the real\\ndata distribution pdata(x). An ideal generator could generate\\nsamples that are indistinguishable from the real samples by the\\ndiscriminator. More formally, the minmax game is summarized\\nby the following expression:\\nmin\\nG max\\nD V (G, D) =Ex∼Pdata[logD(x)]+\\nEz∼noise[log(1 − D(G(z)))]\\n(2)\\nwhere z is sampled from a noise distribution.\\nTo learn a non-linear mapping from X-rays to CT, the\\ngenerated CT volume should be consistent with the semantic\\ninformation provided by the input X-rays. Therefore, LSGAN\\n[11] is more suitable for our task. The conditional LSGAN\\nloss is defined as:\\nLLSGAN(D) = 1\\n2[Ey∼p(CT )(D(y|x) − 1)2+\\nEx∼p(Xray)(D(G(x)|x) − 0)2]\\n(3)\\nLLSGAN(G) = 1\\n2[Ex∼p(Xray)(D(G(x)|x) − 1)2]\\n(4)\\nwhere x is a single X-ray projection, and y is the correspond-\\ning CT volume. Compared to the original objective function\\ndefined in (2), LSGAN replaces the logarithmic loss with\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 2, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='a least-square loss, which helps to stabilize the adversarial\\ntraining process and achieve more realistic details.\\nB. KL Loss\\nIn conditional variational autoencoder, Kullback-Leibler di-\\nvergence constrains the distribution of hidden variables. Since\\nthe conditional distribution is defined as a multidimensional\\nGaussian distribution N(0, I), the KL loss is defined as [7]:\\nLkl = DKL[z||N(0, I)] + DKL[zi||N(0, I)]\\n(5)\\nC. Reconstruction Loss\\nThe conditional adversarial loss can not guarantee that the\\noutput generated by generator has the structural consistency\\nwith the input. Moreover, CT scans require higher precision\\nof internal structures in 3D. Consequently, to enforce the\\nreconstructed CT to be voxel-wise close to the ground truth,\\nthe reconstruction loss is defined as MSE [1]:\\nLre = Ex,y||y − G(x)||2\\n2\\n(6)\\nD. Projection Loss\\nTo improve the training efficiency, simple shape priors\\ncould be utilized as auxiliary regularizations. Therefore, 2D\\nprojections of the predicted volume are enforced to match\\nthe ones from corresponding ground-truth on axial, coronal\\nand sagittal planes. As this auxiliary loss focuses only on the\\ngeneral shape consistency, it can use orthogonal projections\\ninstead of perspective projections to simplify the process. The\\nproposed projection loss is defined as [6]:\\nLpj = 1\\n3[Ex,y||Pax(y) − Pax(G(x))||1+\\nEx,y||Pco(y) − Pco(G(x))||1+\\nEx,y||Psa(y) − Psa(G(x))||1]\\n(7)\\nwhere the Pax, Pco and Psa represent the projection in the\\naxial, coronal and sagittal plane, respectively. The L1 distance\\nis used to enforce sharper image boundaries.\\nE. Total Loss\\nGiven the definitions of the adversarial loss, KL loss,\\nreconstruction loss and projection loss, the final loss function\\nis formulated as:\\nD∗ = argmin\\nD λ1LLSGAN(D)\\n(8)\\nG∗ = argmin\\nG [λ1LLSGAN(G)+λ4Lkl+λ2Lre+λ3Lpj] (9)\\nwhere λ1, λ2 and λ3 control the relative importance of\\ndifferent loss terms. In reconstruction of 3D CT from X-ray\\nprojection, the adversarial loss is important to encourage local\\nrealism of the synthesized CT, but global shape consistency\\nshould be prioritized during the optimization process.\\nIV. EXPERIMENTS\\nIn this section, we introduce an augmented dataset built on\\na ten-phase lung 4D CT scan of two patients. We evaluate the\\nproposed model with several widely used metrics, e.g., peak\\nsignaltonoise ratio (PSNR) and structural similarity (SSIM).\\nTo demonstrate the effectiveness of our method, we compare\\nour method with other methods including PatRecon and X2CT.\\nFair comparisons and comprehensive analysis are given to\\ndemonstrate the improvement of our proposed method.\\nA. Datasets\\nAs there is no large dataset with paired 2D X-ray projection\\nimages and corresponding 3D CT, we use real 3D CT to\\nsynthesize corresponding X-ray projections through digitally\\nreconstructed radiographs technology [12].\\nTo be specific, two ten-phase lung 4D CT scans of two\\npatients for radiation therapy treatment planning are selected.\\nThe tumor diameter of patient 1 is about 40mm, and the\\ntumor diameter of patient 2 is about 10mm, which represents\\nlarge tumors and small tumors respectively. For each patients,\\na respiratory motion model based on principal component\\nanalysis (PCA) is constructed, which can described anatomical\\ndeformation induced by breathing using a linear combination\\nof principal components and corresponding coefficients. Then,\\n1080 3D-CTs in different phases are generated by sampling the\\nPCA coefficients. Next, to avoid the difficulty of reconstruction\\nof 3D CT from 2D X-ray projection caused by the heteroge-\\nneous of imaging protocols, we first resample the CT scans\\nto 1×1×1 mm3 resolution. Then, a cubic area is cropped from\\neach CT scan. For each 3D CT, we synthesize corresponding\\nX-rays from four different projection angles (ie, 0°, 30°, 60°,\\nand 90°). Among all the paired data of CT-DRR, 60% are\\nselected for training, 20% for verification, and the rest 20%\\nfor testing. Pytorch [13] is used to build a neural network, and\\nAdam [14] is used as the network optimizer.\\nB. Metrics\\nMAE Mean Absolute Error(MAE) is L1-norm error be-\\ntween the reconstructed image and the real image, which\\nis commonly used to estimate the difference between the\\nprediction and groundtruth images.\\nPSNR Peak Signal-to-Noise Ratio(PSNR) is often used to\\nmeasure the quality of reconstructed image. Conventionally,\\nCT value is recorded with 12 bits, representing a range of [0,\\n4095] (the actual Hounsfield unit equals the CT value minus\\n1024), which makes PSNR an ideal criterion for image quality\\nevaluation.\\nSSIM Structural SIMilarity(SSIM) is a metric of the overall\\nsimilarity of two images, including brightness, contrast and\\nstructure. SSIM can match human’s subjective evaluation\\nbetter.\\nC. Qualitative Results\\nThe visible results of CT reconstruction are shown in 3.\\nFrom the visual quality evaluation, it is obvious that the\\nproposed method can reconstruct a complete 3D CT from a\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 3, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='Fig. 3. CT scans reconstructed from DRRs from different projection angles.\\nTABLE I\\nQUANTITATIVE RESULTS.\\nAngle\\nMethod\\nPatient 1\\nPatient 2\\nMAE\\nPSNR\\nSSIM\\nMAE\\nPSNR\\nSSIM\\n0°\\nX2CT\\n0.011\\n38.608\\n0.932\\n0.016\\n35.859\\n0.861\\nPatRecon\\n0.018\\n35.262\\n0.814\\n0.017\\n35.908\\n0.851\\nOurs\\n0.012\\n38.830\\n0.931\\n0.016\\n36.265\\n0.864\\n30°\\nX2CT\\n0.024\\n32.323\\n0.780\\n0.027\\n31.526\\n0.709\\nPatRecon\\n0.040\\n28.860\\n0.814\\n0.040\\n29.025\\n0.727\\nOurs\\n0.023\\n32.571\\n0.811\\n0.026\\n31.630\\n0.743\\n60°\\nX2CT\\n0.019\\n34.166\\n0.855\\n0.027\\n31.516\\n0.728\\nPatRecon\\n0.042\\n28.250\\n0.700\\n0.038\\n29.559\\n0.751\\nOurs\\n0.020\\n35.357\\n0.852\\n0.025\\n32.244\\n0.751\\n90°\\nX2CT\\n0.014\\n36.741\\n0.913\\n0.027\\n32.224\\n0.755\\nPatRecon\\n0.018\\n35.293\\n0.837\\n0.049\\n27.439\\n0.657\\nOurs\\n0.013\\n36.949\\n0.910\\n0.025\\n32.253\\n0.755\\nAverage\\nX2CT\\n0.017\\n35.459\\n0.870\\n0.024\\n32.781\\n0.763\\nPatRecon\\n0.029\\n31.916\\n0.791\\n0.036\\n30.483\\n0.746\\nOurs\\n0.017\\n35.927\\n0.876\\n0.023\\n33.098\\n0.778\\nsingle X-ray projection image from various angles, accurately\\nreconstruct the tiny anatomical structure and obtain a clear\\nimage boundary. It can be seen from the figure that the 0° X-\\nray projection can reconstruct the 3D CT better. The possible\\nreason is that body thickness along the sagittal axis is smaller\\nthan body thickness along other axes and the visibility of the\\n0° X-ray projection is better.\\nD. Quantitative Results\\nQuantitative results are summarized in Table 1. Compared\\nwith the existing X2CT [6] and PatRecon [1], the proposed\\nmethod has lower mean absolute error, higher peak signal-\\nto-noise ratio and structure similarity, which represent higher\\nreconstruction accuracy. Moreover, by tuning the weights of\\nthe voxel-level MSE loss and semantic-level adversarial loss\\nin cost function, we can make a reasonable trade-off between\\nthe visual image quality and qualitative results.\\nV. CONCLUSIONS\\nIn this paper, we explored the possibility of reconstructing\\na 3D CT scan from a single 2D X-rays at different projection\\nangles in an end to end network. In order to solve this\\nchallenging task, we use a variational autoencoder to learn\\nthe features of the input 2D X-ray projection image, and\\nconstruct a decoder for 3D CT reconstruction. Moreover, a\\nGAN structure is adopted to make the generated 3D CT more\\nrealistic. Experiments have proved that this method can obtain\\naccurate 3D CT using a single X-ray projection image. The\\nproposed method can be used for tumor motion control and\\ndynamic dose assessment of radiotherapy, which has high\\napplication value.\\nACKNOWLEDGMENT\\nThis work was supported by the National Key R&D\\nProgram of China under Grant No. 2018YFA0704100 and\\n2018YFA0704101, the National Natural Science Foundation\\nof China under Grant Nos. 61601012.\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 4, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='REFERENCES\\n[1] L. Shen, W. Zhao, and L. Xing, “Patient-specific reconstruction of\\nvolumetric computed tomography images from a single projection view\\nvia deep learning,” Nature biomedical engineering, vol. 3, no. 11,\\npp. 880–888, 2019.\\n[2] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,”\\narXiv preprint arXiv:1406.2661, 2014.\\n[3] D. Nie, R. Trullo, J. Lian, C. Petitjean, S. Ruan, Q. Wang, and D. Shen,\\n“Medical image synthesis with context-aware generative adversarial\\nnetworks,” in International conference on medical image computing and\\ncomputer-assisted intervention, pp. 417–425, Springer, 2017.\\n[4] K. Bahrami, F. Shi, I. Rekik, and D. Shen, “Convolutional neural net-\\nwork for reconstruction of 7t-like images from 3t mri using appearance\\nand anatomical features,” in Deep Learning and Data Labeling for\\nMedical Applications, pp. 39–47, Springer, 2016.\\n[5] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting mul-\\ntimodal medical volumes with cycle-and shape-consistency generative\\nadversarial network,” in Proceedings of the IEEE conference on com-\\nputer vision and pattern Recognition, pp. 9242–9251, 2018.\\n[6] X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, and Y. Zheng, “X2ct-\\ngan: reconstructing ct from biplanar x-rays with generative adversarial\\nnetworks,” in Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 10619–10628, 2019.\\n[7] J. J. Cerrolaza, Y. Li, C. Biffi, A. Gomez, M. Sinclair, J. Matthew,\\nC. Knight, B. Kainz, and D. Rueckert, “3d fetal skull reconstruction\\nfrom 2dus via deep conditional generative networks,” in International\\nConference on Medical Image Computing and Computer-Assisted Inter-\\nvention, pp. 383–391, Springer, 2018.\\n[8] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\\npreprint arXiv:1312.6114, 2013.\\n[9] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\\nconnected convolutional networks,” in Proceedings of the IEEE confer-\\nence on computer vision and pattern recognition, pp. 4700–4708, 2017.\\n[10] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\\nwith conditional adversarial networks,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, pp. 1125–1134,\\n2017.\\n[11] Y. Jin, H. Li, Q. Dou, H. Chen, J. Qin, C.-W. Fu, and P.-A. Heng, “Multi-\\ntask recurrent convolutional network with correlation loss for surgical\\nvideo analysis,” Medical image analysis, vol. 59, p. 101572, 2020.\\n[12] N. Milickovic, D. Baltas, S. Giannouli, M. Lahanas, and N. Zamboglou,\\n“Ct imaging based digitally reconstructed radiographs and their applica-\\ntion in brachytherapy,” Physics in Medicine & Biology, vol. 45, no. 10,\\np. 2787, 2000.\\n[13] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in\\npytorch,” 2017.\\n[14] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\\narXiv preprint arXiv:1412.6980, 2014.\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'file_path': 'data/Jiang et al. - 2021 - Reconstruction of 3D CT from A Single X-ray Projec.pdf', 'page': 5, 'total_pages': 6, 'format': 'PDF 1.6', 'title': 'Reconstruction of 3D CT from A Single X-ray Projection View Using CVAE-GAN', 'author': '', 'subject': '2021 IEEE International Conference on Medical Imaging Physics and Engineering (ICMIPE);2021; ; ;10.1109/ICMIPE53131.2021.9698875', 'keywords': '', 'creator': 'TeX', 'producer': 'pdfTeX-1.40.23; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20211228084001Z', 'modDate': \"D:20220209102847-05'00'\", 'trapped': 'False'}),\n",
       " Document(page_content='BARF\\n: Bundle-Adjusting Neural Radiance Fields\\nChen-Hsuan Lin1\\nWei-Chiu Ma2\\nAntonio Torralba2\\nSimon Lucey1,3\\n1Carnegie Mellon University\\n2Massachusetts Institute of Technology\\n3The University of Adelaide\\nhttps://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF\\nAbstract\\nNeural Radiance Fields (NeRF) [31] have recently gained\\na surge of interest within the computer vision community for\\nits power to synthesize photorealistic novel views of real-\\nworld scenes. One limitation of NeRF, however, is its re-\\nquirement of accurate camera poses to learn the scene rep-\\nresentations. In this paper, we propose Bundle-Adjusting\\nNeural Radiance Fields (BARF) for training NeRF from im-\\nperfect (or even unknown) camera poses — the joint problem\\nof learning neural 3D representations and registering cam-\\nera frames. We establish a theoretical connection to classical\\nimage alignment and show that coarse-to-ﬁne registration is\\nalso applicable to NeRF. Furthermore, we show that naïvely\\napplying positional encoding in NeRF has a negative impact\\non registration with a synthesis-based objective. Experi-\\nments on synthetic and real-world data show that BARF can\\neffectively optimize the neural scene representations and re-\\nsolve large camera pose misalignment at the same time. This\\nenables view synthesis and localization of video sequences\\nfrom unknown camera poses, opening up new avenues for\\nvisual localization systems (e.g. SLAM) and potential appli-\\ncations for dense 3D mapping and reconstruction.\\n1. Introduction\\nHumans have strong capabilities of reasoning about 3D\\ngeometry through our vision from the slightest ego-motion.\\nWhen watching movies, we can immediately infer the 3D\\nspatial structures of objects and scenes inside the videos.\\nThis is because we have an inherent ability of associating\\nspatial correspondences of the same scene across continuous\\nobservations, without having to make sense of the relative\\ncamera or ego-motion. Through pure visual perception, not\\nonly can we recover a mental 3D representation of what we\\nare looking at, but meanwhile we can also recognize where\\nwe are looking at the scene from.\\nSimultaneously solving for the 3D scene representation\\nfrom RGB images (i.e. reconstruction) and localizing the\\ngiven camera frames (i.e. registration) is a long-standing\\nchicken-and-egg problem in computer vision — recovering\\nNeRF\\nImages + accurate camera poses\\n3D scene representation\\nBARF (ours)\\nImages + imperfect camera poses\\n3D scene representation\\n+ registered camera poses\\n˝\\n˝\\n˝\\nFigure 1: Training NeRF requires accurate camera poses for\\nall images. We present BARF for learning 3D scene repre-\\nsentations from imperfect (or even unknown) camera poses\\nby jointly optimizing for registration and reconstruction.\\nthe 3D structure requires observations with known camera\\nposes, while localizing the cameras requires reliable corre-\\nspondences from the reconstruction. Classical methods such\\nas structure from motion (SfM) [17, 44] or SLAM [13, 32]\\napproach this problem through local registration followed\\nby global geometric bundle adjustment (BA) on both the\\nstructure and cameras. SfM and SLAM systems, however,\\nare sensitive to the quality of local registration and easily\\nfall into suboptimal solutions. In addition, the sparse nature\\nof output 3D point clouds (often noisy) limits downstream\\nvision tasks that requires dense geometric reasoning.\\nClosely related to 3D reconstruction from imagery is the\\nproblem of view synthesis. Though not primarily purposed\\nfor recovering explicit 3D structures, recent advances on\\nphotorealistic view synthesis have opted to recover an inter-\\nmediate dense 3D-aware representation (e.g. depth [15, 61],\\nmulti-plane images [71, 51, 55], or volume density [27, 31]),\\nfollowed by neural rendering techniques [14, 29, 47, 54] to\\n15741\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='synthesize the target images. In particular, Neural Radiance\\nFields (NeRF) [31] have demonstrated its remarkable ability\\nfor high-ﬁdelity view synthesis. NeRF encodes 3D scenes\\nwith a neural network mapping 3D point locations to color\\nand volume density. This allows the scenes to be represented\\nwith compact memory footprint without limiting the reso-\\nlution of synthesized images. The optimization process of\\nthe network is constrained to obey the principles of classical\\nvolume rendering [23], making the learned representation\\ninterpretable as a continuous 3D volume density function.\\nDespite its notable ability for photorealistic view synthe-\\nsis and 3D scene representation, a hard prerequisite of NeRF\\n(as well as other view synthesis methods) is accurate cam-\\nera poses of the given images, which is typically obtained\\nthrough auxiliary off-the-shelf algorithms. One straightfor-\\nward way to circumvent this limitation is to additionally\\noptimize the pose parameters with the NeRF model via back-\\npropagation. As discussed later in the paper, however, naïve\\npose optimization with NeRF is sensitive to initialization. It\\nmay lead to suboptimal solutions of the 3D scene representa-\\ntion, degrading the quality of view synthesis.\\nIn this paper, we address the problem of training NeRF\\nrepresentations from imperfect camera poses — the joint\\nproblem of reconstructing the 3D scene and registering the\\ncamera poses (Fig. 1). We draw inspiration from the suc-\\ncess of classical image alignment methods and establish\\na theoretical connection, showing that coarse-to-ﬁne regis-\\ntration is also critical to NeRF. Speciﬁcally, we show that\\npositional encoding [57] of input 3D points plays a cru-\\ncial role — as much as it enables ﬁtting to high-frequency\\nfunctions [53], positional encoding is also more susceptible\\nto suboptimal registration results. To this end, we present\\nBundle-Adjusting NeRF (BARF), a simple yet effective strat-\\negy for coarse-to-ﬁne registration on coordinate-based scene\\nrepresentations. BARF can be regarded as a type of photo-\\nmetric BA [8, 2, 26] using view synthesis as the proxy objec-\\ntive. Unlike traditional BA, however, BARF can learn scene\\nrepresentations from scratch (i.e. from randomly initialized\\nnetwork weights), lifting the reliance of local registration\\nsubprocedures and allowing for more generic applications.\\nIn summary, we present the following contributions:\\n• We establish a theoretical connection between classical\\nimage alignment to joint registration and reconstruc-\\ntion with Neural Radiance Fields (NeRF).\\n• We show that susceptibility to noise from positional\\nencoding affects the basin of attraction for registration,\\nand we present a simple strategy for coarse-to-ﬁne\\nregistration on coordinate-based scene representations.\\n• Our proposed BARF can successfully recover scene\\nrepresentations from imperfect camera poses, allowing\\nfor applications such as view synthesis and localization\\nof video sequences from unknown poses.\\n2. Related Work\\nStructure from motion (SfM) and SLAM. Given a set of\\ninput images, SfM [37, 38, 48, 49, 1, 62] and SLAM [33,\\n13, 32, 64] systems aim to recover the 3D structure and the\\nsensor poses simultaneously. These can be classiﬁed into (a)\\nindirect methods that rely on keypoint detection and match-\\ning [6, 32] and (b) direct methods that exploit photometric\\nconsistency [2, 12]. Modern pipelines following the indirect\\nroute have achieved tremendous success [44]; however, they\\noften suffer at textureless regions and repetitive patterns,\\nwhere distinctive keypoints cannot be reliably detected. Re-\\nsearchers have thus sought to use neural networks to learn\\ndiscriminative features directly from data [10, 35, 11].\\nDirect methods, on the other hand, do not rely on such dis-\\ntinctive keypoints — every pixel can contribute to maximiz-\\ning photometric consistency, leading to improved robustness\\nin sparsely textured environments [59]. They can also be\\nnaturally integrated into deep learning frameworks through\\nimage reconstruction losses [70, 58, 66]. Our method BARF\\nlies under the broad umbrella of direct methods, as BARF\\nlearns 3D scene representations from RGB images while also\\nlocalizing the respective cameras. However, unlike classical\\nSfM and SLAM that represent 3D structures with explicit\\ngeometry (e.g. point clouds), BARF encodes the scenes as\\ncoordinate-based representations with neural networks.\\nView synthesis. Given a set of posed images, view synthe-\\nsis attempts to simulate how a scene would look like from\\nnovel viewpoints [5, 24, 52, 19]. The task has been closely\\ntied to 3D reconstruction since its introduction [7, 72, 18].\\nResearchers have investigated blending pixel colors based\\non depth maps [4] or leveraging proxy geometry to warp\\nand composite the synthesized image [22]. However, since\\nthe problem is inherently ill-posed, there are still multiple\\nrestrictions and assumptions on the synthesized viewpoints.\\nState-of-the-art methods have capitalized on neural net-\\nworks to learn both the scene geometry and statistical priors\\nfrom data. Various representations have been explored in this\\ndirection, e.g. depth [15, 61, 42, 43], layered depth [56, 46],\\nmulti-plane images [71, 51, 55], volume density [27, 31],\\nand mesh sheets [20]. Unfortunately, these view synthesis\\nmethods still require the camera poses to be known a priori,\\nlargely limiting their applications in practice. In contrast, our\\nmethod BARF is able to effectively learn 3D representations\\nthat encodes the underlying scene geometry from imperfect\\nor even unknown camera poses.\\nNeural Radiance Fields (NeRF). Recently, Mildenhall et\\nal. [31] proposed NeRF to synthesize novel views of static,\\ncomplex scenes from a set of posed input images. The key\\nidea is to model the continuous radiance ﬁeld of a scene with\\na multi-layer perceptron (MLP), followed by differentiable\\nvolume rendering to synthesize the images and backpropa-\\ngate the photometric errors. NeRF has drawn wide attention\\n25742\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='across the vision community [68, 34, 40, 36, 65] due to its\\nsimplicity and extraordinary performance. It has also been\\nextended on many fronts, e.g. reﬂectance modeling for pho-\\ntorealistic relighting [3, 50] and dynamic scene modeling\\nthat integrates the motion of the world [25, 63, 39]. Recent\\nworks have also sought to exploit a large corpus of data to\\npretrain the MLP, enabling the ability to infer the radiance\\nﬁeld from a single image [16, 67, 41, 45].\\nWhile impressive results have been achieved by the above\\nNeRF-based models, they have a common drawback — the\\nrequirement of posed images. Our proposed BARF allows\\nus to circumvent such requirement. We show that with a\\nsimple coarse-to-ﬁne bundle adjustment technique, we can\\nrecover from imperfect camera poses (including unknown\\nposes of video sequences) and learn the NeRF representation\\nsimultaneously. Concurrent to our work, NeRF-- [60] intro-\\nduced an empirical, two-stage pipeline to estimate unknown\\ncamera poses. Our method BARF, in contrast, is motivated\\nby mathematical insights and can recover the camera poses\\nwithin a single course of optimization, allowing for direct\\nutilities for various NeRF applications and extensions.\\n3. Approach\\nWe unfold this paper by motivating with the simpler 2D\\ncase of classical image alignment as an example. Then we\\ndiscuss how the same concept is also applicable to the 3D\\ncase, giving inspiration to our proposed BARF.\\n3.1. Planar Image Alignment (2D)\\nLet x ∈ R2 be the 2D pixel coordinates and I : R2 → R3\\nbe the imaging function. Image alignment aims to ﬁnd the\\nrelative geometric transformation which minimizes the pho-\\ntometric error between two images I1 and I2. The problem\\ncan be formulated with a synthesis-based objective:\\nmin\\np\\n�\\nx\\n∥I1(W(x; p)) − I2(x)∥2\\n2 ,\\n(1)\\nwhere W : R2 → R2 is the warp function parametrized by\\np ∈ RP (with P as the dimensionality). As this is a non-\\nlinear problem, gradient-based optimization is the method\\nof choice: given the current warp state p, warp updates ∆p\\nare iteratively solved for and updated to the solution via\\np ← p + ∆p. Here, ∆p can be written in a generic form of\\n∆p = −A(x; p)\\n�\\nx\\nJ(x; p)⊤�\\nI1(W(x; p)) − I2(x)\\n�\\n, (2)\\nwhere J ∈ R3×P is termed the steepest descent image, and\\nA is a generic transformation which depends on the choice\\nof the optimization algorithm. The seminal Lucas-Kanade\\nalgorithm [28] approaches the problem using Gauss-Newton\\noptimization, i.e. A(x; p) = (�\\nx J(x; p)⊤J(x; p))−1; al-\\nternatively, one could also choose ﬁrst-order optimizers such\\n𝑓! 𝑥\\n𝑓\" 𝑥\\n𝑓! 𝑥\\n𝑓\" 𝑥\\n(a) smooth signal\\n(b) complex signal\\nFigure 2: Predicting alignment from signal differences.\\nConsider two 1D signals where f1(x) = f2(x + c) differs\\nby an offset c. When solving for alignment, smoother sig-\\nnals can predict more coherent displacements than complex\\nsignals, which easily results in suboptimal alignment.\\nas (stochastic) gradient descent which can be more naturally\\nincorporated into modern deep learning frameworks, where\\nA would correspond to a scalar learning rate.\\nThe steepest descent image J can be expanded as\\nJ(x; p) = ∂I1(W(x; p))\\n∂W(x; p)\\n∂W(x; p)\\n∂p\\n,\\n(3)\\nwhere ∂W(x;p)\\n∂p\\n∈ R2×P is the warp Jacobian constraining\\nthe pixel displacements with respect to the predeﬁned warp.\\nAt the heart of gradient-based registration are the image\\ngradients ∂I(x)\\n∂x\\n∈ R3×2 modeling a local per-pixel linear\\nrelationship between appearance and spatial displacements,\\nwhich is classically estimated via ﬁnite differencing. The\\noverall warp update ∆p can be more effectively estimated\\nfrom pixel value differences if the per-pixel predictions are\\ncoherent (Fig. 2), i.e. the image signals are smooth. However,\\nas natural images are typically complex signals, gradient-\\nbased registration on raw images is susceptible to subopti-\\nmal solutions if poorly initialized. Therefore, coarse-to-ﬁne\\nstrategies have been practiced by blurring the images at ear-\\nlier stages of registration, effectively widening the basin of\\nattraction and smoothening the alignment landscape.\\nImages as neural networks. An alternative formulation of\\nthe problem is to learn a coordinate-based image representa-\\ntion with a neural network while also solving for the warp p.\\nWriting the network as f : R2 → R3 and denoting Θ as its\\nparameters, one can instead choose to optimize the objective\\nmin\\np,Θ\\n�\\nx\\n�\\n∥f(x; Θ) − I1(x)∥2\\n2\\n+ ∥f(W(x; p); Θ) − I2(x)∥2\\n2\\n�\\n,\\n(4)\\nor alternatively, one may choose to solve for warp parameters\\n35743\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='p1 and p2 respectively for both images I1 and I2 through\\nmin\\np1,p2,Θ\\nM\\n�\\ni=1\\n�\\nx\\n∥f(W(x; pi); Θ) − Ii(x)∥2\\n2 ,\\n(5)\\nwhere M = 2 is the number of images. Albeit similar\\nto (1), the image gradients become the analytical Jacobian\\nof the network ∂f(x)\\n∂x\\ninstead of numerical estimation. By\\nmanipulating the network f, this also enables more princi-\\npled control of the signal smoothness for alignment without\\nhaving to rely on heuristic blurring on images, making these\\nforms generalizable to 3D scene representations (Sec. 3.2).\\n3.2. Neural Radiance Fields (3D)\\nWe discuss the 3D case of recovering the 3D scene repre-\\nsentation from Neural Radiance Fields (NeRF) [31] jointly\\nwith the camera poses. To signify the analogy to Sec. 3.1,\\nwe deliberately overload the notations x as 3D points, W as\\ncamera pose transformations, and f as the network in NeRF.\\nNeRF encodes a 3D scene as a continuous 3D representa-\\ntion using an MLP f : R3 → R4 to predict the RGB color\\nc ∈ R3 and volume density σ ∈ R for each input 3D point\\nx ∈ R3. This can be summarized as y = [c; σ]⊤ = f(x; Θ),\\nwhere Θ is the network parameters1. NeRF assumes an\\nemission-only model, i.e. the rendered color of a pixel is de-\\npendent only on the emitted radiance of 3D points along the\\nviewing ray, without considering external lighting factors.\\nWe ﬁrst formulate the rendering operation of NeRF in the\\ncamera view space. Given pixel coordinates u ∈ R2 and\\ndenoting its homogeneous coordinates as ¯u = [u; 1]⊤ ∈ R3,\\nwe can express a 3D point xi along the viewing ray at depth\\nzi as xi = zi¯u. The RGB color ˆI at pixel location u is\\nextracted by volume rendering via\\nˆI(u) =\\n� zfar\\nznear\\nT(u, z)σ(z¯u)c(z¯u)dz ,\\n(6)\\nwhere T(u, z) = exp\\n�\\n−\\n� z\\nznear σ(z′¯u)dz′�\\n, and znear and zfar\\nare bounds on the depth range of interest. We refer our\\nreaders to Levoy [23] and Mildenhall et al. [31] for a more\\ndetailed treatment on volume rendering. In practice, the\\nabove integral formulations are approximated numerically\\nvia quadrature on discrete N points at depth {z1, . . . , zN}\\nsampled along the ray. This involves N evaluations of the\\nnetwork f, whose output {y1, . . . , yN} are further compos-\\nited through volume rendering. We can summarize the ray\\ncompositing function as g : R4N → R3 and rewrite ˆI(u) as\\nˆI(u) = g (y1, . . . , yN). Note that g is differentiable but de-\\nterministic, i.e. there are no learnable parameters associated.\\nUnder a 6-DoF camera pose parametrized by p ∈ R6, a\\n3D point x in the camera view space can be transformed to\\n1In practice, f is also conditioned on the viewing direction [31] for\\nmodeling view-dependent effects, which we omit here for simplicity.\\nthe 3D world coordinates through a 3D rigid transformation\\nW : R3 → R3. Therefore, the synthesized RGB value at\\npixel u becomes a function of the camera pose p as\\nˆI(u; p) = g\\n�\\nf(W(z1¯u; p); Θ), . . . , f(W(zN ¯u; p); Θ)\\n�\\n. (7)\\nGiven M images {Ii}M\\ni=1, our goal is to optimize NeRF and\\nthe camera poses {pi}M\\ni=1 over the synthesis-based objective\\nmin\\np1,...,pM,Θ\\nM\\n�\\ni=1\\n�\\nu\\n��ˆI(u; pi, Θ) − Ii(u)\\n��2\\n2 ,\\n(8)\\nwhere ˆI also depends on the network parameters Θ.\\nOne may notice the analogy between the synthesis-based\\nobjectives of 2D image alignment (5) and NeRF (8). Simi-\\nlarly, we can also derive the “steepest descent image” as\\nJ(u; p) =\\nN\\n�\\ni=1\\n∂g(y1, . . . , yN)\\n∂yi\\n∂yi(p)\\n∂xi(p)\\n∂W(zi¯u; p)\\n∂p\\n, (9)\\nwhich is formed via backpropagation in practice. The lin-\\nearization (9) is also analogous to the 2D case of (3), where\\nthe Jacobian of the network ∂y\\n∂x =\\n∂f(x)\\n∂x\\nlinearly relates\\nthe change of color c and volume density σ with 3D spatial\\ndisplacements. To solve for effective camera pose updates\\n∆p through backpropagation, it is also desirable to control\\nthe smoothness of f for predicting coherent geometric dis-\\nplacements from the sampled 3D points {x1, . . . , xN}.\\n3.3. On Positional Encoding and Registration\\nThe key of enabling NeRF to synthesize views with high\\nﬁdelity is positional encoding [57], a deterministic mapping\\nof input 3D coordinates x to higher dimensions of different\\nsinusoidal frequency bases2. We denote γ : R3 → R3+6L as\\nthe positional encoding with L frequency bases, deﬁned as\\nγ(x) =\\n�\\nx, γ0(x), γ1(x), . . . , γL−1(x)\\n�\\n∈ R3+6L , (10)\\nwhere the k-th frequency encoding γk(x) is\\nγk(x) =\\n�\\ncos(2kπx), sin(2kπx)\\n�\\n∈ R6 ,\\n(11)\\nwith the sinusoidal functions operating coordinate-wise. The\\nspecial case of L = 0 makes γ an identity mapping function.\\nThe network f is thus a composition of f(x) = f ′ ◦ γ(x),\\nwhere f ′ is the subsequent learnable MLP. Positional en-\\ncoding allows coordinate-based neural networks, which are\\ntypically bandwidth limited, to represent signals of higher\\nfrequency with faster convergence behaviors [53].\\nThe Jacobian of the k-th positional encoding γk is\\n∂γk(x)\\n∂x\\n= 2kπ ·\\n�\\n− sin(2kπx), cos(2kπx)\\n�\\n,\\n(12)\\n2Although we focus on 3D input coordinates here, positional encoding\\nis also directly applicable to 2D image coordinates in Sec. 3.1 as well.\\n45744\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='which ampliﬁes the gradient signals from the MLP f ′ by\\n2kπ with its direction changing at the same frequency. This\\nmakes it difﬁcult to predict effective updates ∆p, since gra-\\ndient signals from the sampled 3D points are incoherent (in\\nterms of both direction and magnitude) and can easily can-\\ncel out each other. Therefore, naïvely applying positional\\nencoding can become a double-edged sword to NeRF for the\\ntask of joint registration and reconstruction.\\n3.4. Bundle-Adjusting Neural Radiance Fields\\nWe describe our proposed BARF, a simple yet effective\\nstrategy for coarse-to-ﬁne registration for NeRF. The key\\nidea is to apply a smooth mask on the encoding at different\\nfrequency bands (from low to high) over the course of opti-\\nmization, which acts like a dynamic low-pass ﬁlter. Inspired\\nby recent work of learning coarse-to-ﬁne deformation ﬂow\\nﬁelds [36], we weigh the k-th frequency component of γ as\\nγk(x; α) = wk(α) ·\\n�\\ncos(2kπx), sin(2kπx)\\n�\\n,\\n(13)\\nwhere the weight wk is deﬁned as\\nwk(α) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n0\\nif α < k\\n1 − cos((α − k)π)\\n2\\nif 0 ≤ α − k < 1\\n1\\nif α − k ≥ 1\\n(14)\\nand α ∈ [0, L] is a controllable parameter proportional to\\nthe optimization progress. The Jacobian of γk thus becomes\\n∂γk(x; α)\\n∂x\\n= wk(α) · 2kπ ·\\n�\\n− sin(2kπx), cos(2kπx)\\n�\\n. (15)\\nWhen wk(α) = 0, the contribution to the gradient from the\\nk-th (and higher) frequency component is nulliﬁed.\\nStarting from the raw 3D input x (α = 0), we gradually\\nactivate the encodings of higher frequency bands until full\\npositional encoding is enabled (α = L), equivalent to the\\noriginal NeRF model. This allows BARF to discover the\\ncorrect registration with an initially smooth signal and later\\nshift focus to learning a high-ﬁdelity scene representation.\\n4. Experiments\\nWe validate the effectiveness of our proposed BARF with\\na simple experiment of 2D planar image alignment, and show\\nhow the same coarse-to-ﬁne registration strategy can be gen-\\neralized to NeRF [31] for learning 3D scene representations.\\n4.1. Planar Image Alignment (2D)\\nWe choose a representative image from ImageNet [9],\\nshown in Fig. 3. Given M = 5 patches from the image\\ngenerated with homography perturbations (Fig. 3(a)), we aim\\nto ﬁnd the homography warp parameters p ∈ R8 for each\\npatch (Fig. 3(b)) while also learning the neural representation\\nof the entire image with a network f by optimizing (5). We\\ninitialize all M patches with a center crop (Fig. 3(c)), and we\\nanchor the warp of the ﬁrst patch as identity so the recovered\\nimage would be implicitly aligned to the raw image. We\\nparametrize homography warps with the sl(3) Lie algebra.\\nExperimental settings. We investigate how positional en-\\ncoding impacts this problem by comparing networks with\\nnaïve (full) positional encoding and without any encoding.\\nWe use a simple ReLU MLP for f with four 256-dimensional\\nhidden units, and we use the Adam optimizer [21] to opti-\\nmize both the network weights and the warp parameters for\\n5000 iterations with a learning rate of 0.001. For BARF, we\\nlinearly adjust α for the ﬁrst 2000 iterations and activate all\\nfrequency bands (L = 8) for the remaining iterations.\\nResults. We visualize the registration results in Fig. 4. Align-\\nment with full positional encoding results in suboptimal\\nregistration with ghostly artifacts in the recovered image rep-\\nresentation. On the other hand, alignment without positional\\nencoding achieves decent registration results, but cannot re-\\ncover the image with sufﬁcient ﬁdelity. BARF discovers the\\nprecise geometric warps with the image representation opti-\\nmized with high ﬁdelity, quantitatively reﬂected in Table 1.\\nThe image alignment experiment demonstrates the general\\nadvantage of BARF for coordinate-based representations.\\n4.2. NeRF (3D): Synthetic Objects\\nWe investigate the problem of learning 3D scene repre-\\nsentations with Neural Radiance Fields (NeRF) [31] from\\nimperfect camera poses. We experiment with the 8 synthetic\\nobject-centric scenes provided by Mildenhall et al. [31],\\nwhich consists of M = 100 rendered images with ground-\\ntruth camera poses for each scene for training.\\nExperimental settings. We parametrize the camera poses p\\nwith the se(3) Lie algebra and assume known intrinsics. For\\neach scene, we synthetically perturb the camera poses with\\nadditive noise δp ∼ N(0, 0.15I), which corresponds to a\\nstandard deviation of 14.9° in rotation and 0.26 in transla-\\ntional magnitude (Fig. 5(a)). We optimize the objective in (8)\\njointly for the scene representation and the camera poses. We\\nevaluate BARF mainly against the original NeRF model with\\nnaïve (full) positional encoding; for completeness, we also\\ncompare with the same model without positional encoding.\\nImplementation details. We follow the architectural set-\\ntings from the original NeRF [31] with some modiﬁcations.\\nWe train a single MLP with 128 hidden units in each layer\\nand without additional hierarchical sampling for simplicity.\\nWe resize the images to 400 × 400 pixels and randomly sam-\\nple 1024 pixel rays at each optimization step. We choose\\nN = 128 sample for numerical integration along each ray,\\nand we use the softplus activation on the volume density out-\\nput σ for improved stability. We use the Adam optimizer and\\ntrain all models for 200K iterations, with a learning rate of\\n5×10−4 exponentially decaying to 1×10−4 for the network\\nf and 1×10−3 decaying to 1×10−5 for the poses p. For\\n55745\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(c) ground-truth warps\\n(b) initialization\\n(a) image patches given for optimization \\nFigure 3: Given image patches color-coded in (a), we\\naim to recover the alignment and the neural represen-\\ntation of the entire image, with the patches initialized\\nto center crops shown in (b) and the corresponding\\nground-truth warps shown in (c).\\npositional encoding\\nsl(3) error\\npatch PSNR\\nnaïve (full)\\n0.2949\\n23.41\\nwithout\\n0.0641\\n24.72\\nBARF (coarse-to-ﬁne)\\n0.0096\\n35.30\\nTable 1: Quantitative results of planar image align-\\nment. BARF optimizes for more accurate alignment\\nand patch reconstruction compared to the baselines.\\n(a) naïve pos. enc.\\n(b) w/o pos. enc.\\n(c) BARF\\nFigure 4: Qualitative results of the planar image alignment experi-\\nment. We visualize the optimized warps (top row), the patch recon-\\nstructions in corresponding colors (middle row), and recovered image\\nrepresentation from f (bottom row). BARF is able to recover accurate\\nalignment and high-ﬁdelity image reconstruction, while baselines re-\\nsult in suboptimal alignment with naïve positional encoding and blurry\\nreconstruction without any encoding. Best viewed in color.\\n(b) full positional encoding\\n(c) BARF (ours)\\n(a) initial camera poses\\nperturbed/optimized \\ncamera poses\\nground-truth \\ncamera poses\\ntranslational error\\nFigure 5: Visual comparison of the initial and optimized\\ncamera poses (Procrustes aligned) for the chair scene. BARF\\nsuccessfully realigns all the camera frames while NeRF naïve\\npositional encoding gets stuck at suboptimal solutions.\\nBARF, we linearly adjust α from iteration 20K to 100K and\\nactivate all frequency bands (up to L = 10) subsequently.\\nEvaluation criteria. We measure the performance in two\\naspects: pose error for registration and view synthesis quality\\nfor the scene representation. Since both the scene and cam-\\nera poses are variable up to a 3D similarity transformation,\\nwe evaluate the quality of registration by pre-aligning the op-\\ntimized poses to the ground truth with Procrustes analysis on\\nthe camera locations. For evaluating view synthesis, we run\\nan additional step of test-time photometric optimization on\\nthe trained models [26, 65] to factor out the pose error that\\nmay contaminate the view synthesis quality. We report the\\naverage rotation and translation errors for pose and PSNR,\\nSSIM and LPIPS [69] for view synthesis.\\nResults. We visualize the results in Fig. 6 and report the\\nquantitative results in Table 2. BARF takes the best of both\\nworlds of recovering the neural scene representation with the\\ncamera pose successfully registered, while naïve NeRF with\\nfull positional encoding ﬁnds suboptimal solutions. Fig. 5\\nshows that BARF can achieve near-perfect registration for\\nthe synthetic scenes. Although the NeRF model without\\npositional encoding can also successfully recover alignment,\\nthe learned scene representations (and thus the synthesized\\nimages) lack the reconstruction ﬁdelity. As a reference, we\\nalso compare the view synthesis quality against standard\\nNeRF models trained under ground-truth poses, showing\\nthat BARF can achieve comparable view synthesis quality in\\nall metrics, albeit initialized from imperfect camera poses.\\n65746\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='full pos. enc.\\nw/o pos. enc.\\nBARF (ours)\\nreference NeRF\\nground truth\\nlego\\nficus\\nFigure 6: Qualitative results of NeRF on synthetic scenes. We visualize the image synthesis (top) and the expected depth\\nthrough ray compositing (bottom). BARF achieves comparable synthesis quality to the reference NeRF (trained under perfect\\ncamera poses), while full positional encoding results in suboptimal registration, leading to synthesis artifacts.\\nScene\\nCamera pose registration\\nView synthesis quality\\nRotation (°) ↓\\nTranslation ↓\\nPSNR ↑\\nSSIM ↑\\nLPIPS ↓\\nfull\\nw/o\\nBARF\\nfull\\nw/o\\nBARF\\nfull\\nw/o\\nBARF\\nref.\\nfull\\nw/o\\nBARF\\nref.\\nfull\\nw/o\\nBARF\\nref.\\npos.enc. pos.enc.\\npos.enc. pos.enc.\\npos.enc. pos.enc.\\nNeRF pos.enc. pos.enc.\\nNeRF pos.enc. pos.enc.\\nNeRF\\nChair\\n7.186\\n0.110\\n0.096 16.638\\n0.555\\n0.428\\n19.02\\n30.22\\n31.16 31.91\\n0.804\\n0.942\\n0.954 0.961\\n0.223\\n0.065\\n0.044 0.036\\nDrums\\n3.208\\n0.057\\n0.043\\n7.322\\n0.255\\n0.225\\n20.83\\n23.56\\n23.91 23.96\\n0.840\\n0.893\\n0.900 0.902\\n0.166\\n0.116\\n0.099 0.095\\nFicus\\n9.368\\n0.095\\n0.085 10.135\\n0.430\\n0.474\\n19.75\\n25.58\\n26.26 26.68\\n0.836\\n0.922\\n0.934 0.941\\n0.182\\n0.070\\n0.058 0.051\\nHotdog\\n3.290\\n0.225\\n0.248\\n6.344\\n1.122\\n1.308\\n28.15\\n34.00\\n34.54 34.91\\n0.923\\n0.967\\n0.970 0.973\\n0.083\\n0.040\\n0.032 0.029\\nLego\\n3.252\\n0.108\\n0.082\\n4.841\\n0.391\\n0.291\\n24.23\\n26.35\\n28.33 29.28\\n0.876\\n0.880\\n0.927 0.942\\n0.102\\n0.112\\n0.050 0.037\\nMaterials\\n6.971\\n0.845\\n0.844 15.188\\n2.678\\n2.692\\n16.51\\n26.86\\n27.84 28.48\\n0.747\\n0.926\\n0.936 0.944\\n0.294\\n0.068\\n0.058 0.049\\nMic\\n10.554\\n0.081\\n0.071 22.724\\n0.356\\n0.301\\n15.10\\n30.93\\n31.18 31.98\\n0.788\\n0.968\\n0.969 0.971\\n0.334\\n0.050\\n0.048 0.044\\nShip\\n5.506\\n0.095\\n0.075\\n7.232\\n0.354\\n0.326\\n22.12\\n26.78\\n27.50 28.00\\n0.755\\n0.833\\n0.849 0.858\\n0.255\\n0.175\\n0.132 0.118\\nMean\\n6.167\\n0.202\\n0.193 11.303\\n0.768\\n0.756\\n22.12\\n26.78\\n27.50 29.40\\n0.821\\n0.917\\n0.930 0.936\\n0.205\\n0.087\\n0.065 0.057\\nTable 2: Quantitative results of NeRF on synthetic scenes. BARF successfully optimizes for camera registration (with less\\nthan 0.2° rotation error) while still consistently achieving high-quality view synthesis that is comparable to the reference NeRF\\nmodels (trained under perfect camera poses). Translation errors are scaled by 100.\\n4.3. NeRF (3D): Real-World Scenes\\nWe investigate the challenging problem of learning neural\\n3D representations with NeRF on real-world scenes, where\\nthe camera poses are unknown. We consider the LLFF\\ndataset [30], which consists of 8 forward-facing scenes with\\nRGB images sequentially captured by hand-held cameras.\\nExperimental settings. We parametrize the camera poses\\np with se(3) following Sec. 4.2 but initialize all cameras\\nwith the identity transformation, i.e. pi = 0 ∀i. We assume\\nknown camera intrinsics (provided by the dataset). We com-\\npare against the original NeRF model with naïve positional\\nencoding, and we use the same evaluation criteria described\\nin Sec. 4.2. However, we note that the camera poses pro-\\nvided in LLFF are also estimations from SfM packages [44];\\ntherefore, the pose evaluation is at most an indication of how\\nwell BARF agrees with classical geometric pose estimation.\\nImplementation details. We follow the same architectural\\nsettings from the original NeRF [31] and resize the images to\\n480×640 pixels. We train all models for 200K iterations and\\nrandomly sample 2048 pixel rays at each optimization step,\\nwith a learning rate of 1×10−3 for the network f decaying\\nto 1×10−4, and 3×10−3 for the pose p decaying to 1×10−5.\\nWe linearly adjust α for BARF from iteration 20K to 100K\\nand activate all bands (up to L = 10) subsequently.\\nResults. The quantitative results (Table 3) show that the\\nrecovered camera poses from BARF highly agrees with those\\nestimated from off-the-shelf SfM methods (visualized in\\nFig. 8), demonstrating the ability of BARF to localize from\\nscratch. Furthermore, BARF can successfully recover the 3D\\nscene representation with high ﬁdelity (Fig. 7). In contrast,\\nNeRF with naïve positional encoding diverge to incorrect\\ncamera poses, which in turn results in poor view synthesis.\\nThis highlights the effectiveness of BARF utilizing a coarse-\\nto-ﬁne strategy for joint registration and reconstruction.\\n75747\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='full pos. enc.\\nBARF (ours)\\nreference NeRF\\nground truth\\nimage\\ndepth\\nimage\\ndepth\\nimage\\ndepth\\nN/A\\nN/A\\nN/A\\nfern\\nt-rex\\norchids\\nFigure 7: Qualitative results of NeRF on real-world scenes from unknown camera poses. Compared to a reference NeRF\\nmodel trained with camera poses provided from SfM [44], BARF can effectively optimize for the poses jointly with the scene\\nrepresentation. NeRF models with full positional encoding diverge to incorrect localization and hence poor synthesis quality.\\nScene\\nCamera pose registration\\nView synthesis quality\\nRotation (°) ↓\\nTranslation ↓\\nPSNR ↑\\nSSIM ↑\\nLPIPS ↓\\nfull\\nBARF\\nfull\\nBARF\\nfull\\nBARF\\nref.\\nfull\\nBARF\\nref.\\nfull\\nBARF\\nref.\\npos.enc.\\npos.enc.\\npos.enc.\\nNeRF pos.enc.\\nNeRF pos.enc.\\nNeRF\\nFern\\n74.452\\n0.191 30.167 0.192\\n9.81\\n23.79 23.72\\n0.187\\n0.710 0.733\\n0.853\\n0.311 0.262\\nFlower\\n2.525\\n0.251\\n2.635\\n0.224\\n17.08\\n23.37 23.24\\n0.344\\n0.698 0.668\\n0.490\\n0.211 0.244\\nFortress\\n75.094\\n0.479 33.231 0.364\\n12.15\\n29.08 25.97\\n0.270\\n0.823 0.786\\n0.807\\n0.132 0.185\\nHorns\\n58.764\\n0.304 32.664 0.222\\n8.89\\n22.78 20.35\\n0.158\\n0.727 0.624\\n0.805\\n0.298 0.421\\nLeaves\\n88.091\\n1.272 13.540 0.249\\n9.64\\n18.78 15.33\\n0.067\\n0.537 0.306\\n0.782\\n0.353 0.526\\nOrchids\\n37.104\\n0.627 20.312 0.404\\n9.42\\n19.45 17.34\\n0.085\\n0.574 0.518\\n0.806\\n0.291 0.307\\nRoom\\n173.811 0.320 66.922 0.270\\n10.78\\n31.95 32.42\\n0.278\\n0.940 0.948\\n0.871\\n0.099 0.080\\nT-rex\\n166.231 1.138 53.309 0.720\\n10.48\\n22.55 22.12\\n0.158\\n0.767 0.739\\n0.885\\n0.206 0.244\\nMean\\n84.509\\n0.573 31.598 0.331\\n11.03\\n23.97 22.56\\n0.193\\n0.722 0.665\\n0.787\\n0.238 0.283\\nTable 3: Quantitative results of NeRF on the LLFF forward-facing scenes\\nfrom unknown camera poses. BARF can optimize for accurate camera poses\\n(with an average < 0.6° rotation error) and high-ﬁdelity scene representations,\\nenabling novel view synthesis whose quality is comparable to reference NeRF\\nmodel trained under SfM poses. Translation errors are scaled by 100.\\n(a) full pos. enc.\\n(b) BARF (ours)\\ntop-down (aerial) view\\nfrontal-facing view\\nCOLMAP (SfM)\\nNeRF / BARF\\nFigure 8: Visualization of optimized camera\\nposes from the fern scene (Procrustes aligned).\\nResults from BARF highly agrees with SfM,\\nwhereas the baseline poses are suboptimal.\\n5. Conclusion\\nWe present Bundle-Adjusting Neural Radiance Fields\\n(BARF), a simple yet effective strategy for training NeRF\\nfrom imperfect camera poses. By establishing a theoretical\\nconnection to classical image alignment, we demonstrate that\\ncoarse-to-ﬁne registration is necessary for joint registration\\nand reconstruction with coordinate-based scene representa-\\ntions. Our experiments show that BARF can effectively learn\\nthe 3D scene representations from scratch and resolve large\\ncamera pose misalignment at the same time.\\nDespite the intriguing results at the current stage, BARF\\nhas similar limitations to the original NeRF formulation [31]\\n(e.g. slow optimization and rendering, rigidity assumption,\\nsensitivity to dense 3D sampling), as well as reliance on\\nheuristic coarse-to-ﬁne scheduling strategies. Nevertheless,\\nsince BARF keeps a close formulation to NeRF, many of the\\nlatest advances on improving NeRF are potentially transfer-\\nable to BARF as well. We believe BARF opens up exciting\\navenues for rethinking visual localization for SfM/SLAM\\nsystems and self-supervised dense 3D reconstruction frame-\\nworks using view synthesis as a proxy objective.\\n85748\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian\\nSimon, Brian Curless, Steven M Seitz, and Richard Szeliski.\\nBuilding rome in a day. ACM Communications, 2011. 2\\n[2] Hatem Alismail, Brett Browning, and Simon Lucey. Photo-\\nmetric bundle adjustment for vision-based slam. In Asian\\nConference on Computer Vision, pages 324–341. Springer,\\n2016. 2\\n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-\\nron, Ce Liu, and Hendrik Lensch. Nerd: Neural reﬂectance\\ndecomposition from image collections. arXiv, 2020. 3\\n[4] Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung,\\nand George Drettakis. Depth synthesis and local warps for\\nplausible image-based navigation. TOG, 2013. 2\\n[5] Shenchang Eric Chen and Lance Williams. View interpola-\\ntion for image synthesis. In Proceedings of the 20th annual\\nconference on Computer graphics and interactive techniques,\\n1993. 2\\n[6] Andrew J Davison, Ian D Reid, Nicholas D Molton, and\\nOlivier Stasse. Monoslam: Real-time single camera slam.\\nTPAMI, 2007. 2\\n[7] Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod-\\neling and rendering architecture from photographs: A hybrid\\ngeometry-and image-based approach. In Proceedings of the\\n23rd annual conference on Computer graphics and interactive\\ntechniques, 1996. 2\\n[8] Amaël Delaunoy and Marc Pollefeys. Photometric bundle\\nadjustment for dense multi-view 3d modeling. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 1486–1493, 2014. 2\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\\nFei-Fei. Imagenet: A large-scale hierarchical image database.\\nIn 2009 IEEE conference on computer vision and pattern\\nrecognition, pages 248–255. Ieee, 2009. 5\\n[10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\\nnovich. Superpoint: Self-supervised interest point detection\\nand description. In CVPR, 2018. 2\\n[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net:\\nA trainable cnn for joint detection and description of local\\nfeatures. arXiv, 2019. 2\\n[12] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct\\nsparse odometry. TPAMI, 2017. 2\\n[13] Jakob Engel, Thomas Schöps, and Daniel Cremers. Lsd-slam:\\nLarge-scale direct monocular slam. In European conference\\non computer vision, pages 834–849. Springer, 2014. 1, 2\\n[14] SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse,\\nFabio Viola, Ari S Morcos, Marta Garnelo, Avraham Ru-\\nderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor,\\net al. Neural scene representation and rendering. Science,\\n360(6394):1204–1210, 2018. 1\\n[15] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-\\nVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and\\nRichard Tucker. Deepview: View synthesis with learned\\ngradient descent. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, pages 2367–2376,\\n2019. 1, 2\\n[16] Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang,\\nand Jia-Bin Huang. Portrait neural radiance ﬁelds from a\\nsingle image. arXiv, 2020. 3\\n[17] Richard Hartley and Andrew Zisserman. Multiple View Geom-\\netry in Computer Vision. Cambridge University Press, ISBN:\\n0521540518, second edition, 2004. 1\\n[18] Peter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes\\nKopf. Casual 3d photography. TOG, 2017. 2\\n[19] Benno Heigl, Reinhard Koch, Marc Pollefeys, Joachim Den-\\nzler, and Luc Van Gool. Plenoptic modeling and rendering\\nfrom image sequences taken by a hand-held camera. 1999. 2\\n[20] Ronghang Hu and Deepak Pathak. Worldsheet: Wrapping the\\nworld in a 3d sheet for view synthesis from a single image.\\narXiv, 2020. 2\\n[21] Diederik Kingma and Jimmy Ba.\\nAdam: A method for\\nstochastic optimization.\\nIn International Conference on\\nLearning Representations, 2015. 5\\n[22] Johannes Kopf, Michael F Cohen, and Richard Szeliski. First-\\nperson hyper-lapse videos. TOG, 2014. 2\\n[23] Marc Levoy. Efﬁcient ray tracing of volume data. ACM\\nTransactions on Graphics (TOG), 9(3):245–261, 1990. 2, 4\\n[24] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering. In Pro-\\nceedings of the 23rd annual conference on Computer graphics\\nand interactive techniques, 1996. 2\\n[25] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\\nNeural scene ﬂow ﬁelds for space-time view synthesis of\\ndynamic scenes. arXiv, 2020. 3\\n[26] Chen-Hsuan Lin, Oliver Wang, Bryan C Russell, Eli Shecht-\\nman, Vladimir G Kim, Matthew Fisher, and Simon Lucey.\\nPhotometric mesh optimization for video-aligned 3d object\\nreconstruction. In IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), 2019. 2, 6\\n[27] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural\\nvolumes: Learning dynamic renderable volumes from images.\\narXiv preprint arXiv:1906.07751, 2019. 1, 2\\n[28] Bruce D. Lucas and Takeo Kanade. An iterative image reg-\\nistration technique with an application to stereo vision. In\\nProceedings of the 7th International Joint Conference on Arti-\\nﬁcial Intelligence - Volume 2, IJCAI’81, pages 674–679, 1981.\\n3\\n[29] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues\\nHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-\\nBrualla. Neural rerendering in the wild. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 6878–6887, 2019. 1\\n[30] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\\nAbhishek Kar. Local light ﬁeld fusion: Practical view synthe-\\nsis with prescriptive sampling guidelines. ACM Transactions\\non Graphics (TOG), 38(4):1–14, 2019. 7\\n[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance ﬁelds for view synthe-\\nsis. In European conference on computer vision, 2020. 1, 2,\\n4, 5, 7, 8\\n95749\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[32] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D\\nTardos. Orb-slam: a versatile and accurate monocular slam\\nsystem. IEEE transactions on robotics, 31(5):1147–1163,\\n2015. 1, 2\\n[33] Richard A Newcombe, Steven J Lovegrove, and Andrew J\\nDavison. Dtam: Dense tracking and mapping in real-time. In\\nICCV, 2011. 2\\n[34] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\\ning scenes as compositional generative neural feature ﬁelds.\\narXiv, 2020. 3\\n[35] Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.\\nLf-net: Learning local features from images. arXiv, 2018. 2\\n[36] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen\\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-\\nMartin Brualla. Deformable neural radiance ﬁelds. arXiv\\npreprint arXiv:2011.12948, 2020. 3, 5\\n[37] Marc Pollefeys, Reinhard Koch, and Luc Van Gool. Self-\\ncalibration and metric reconstruction inspite of varying and\\nunknown intrinsic camera parameters. International Journal\\nof Computer Vision, 32(1):7–25, 1999. 2\\n[38] Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank\\nVerbiest, Kurt Cornelis, Jan Tops, and Reinhard Koch. Visual\\nmodeling with a hand-held camera. International Journal of\\nComputer Vision, 59(3):207–232, 2004. 2\\n[39] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\\nFrancesc Moreno-Noguer. D-nerf: Neural radiance ﬁelds\\nfor dynamic scenes. arXiv, 2020. 3\\n[40] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,\\nKwang Moo Yi, and Andrea Tagliasacchi. Derf: Decom-\\nposed radiance ﬁelds. arXiv, 2020. 3\\n[41] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio\\nFerrari. Sharf: Shape-conditioned radiance ﬁelds from a\\nsingle view. arXiv, 2021. 3\\n[42] Gernot Riegler and Vladlen Koltun. Free view synthesis. In\\nECCV, 2020. 2\\n[43] Gernot Riegler and Vladlen Koltun. Stable view synthesis.\\narXiv, 2020. 2\\n[44] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited. In Proceedings of the IEEE confer-\\nence on computer vision and pattern recognition, pages 4104–\\n4113, 2016. 1, 2, 7, 8\\n[45] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\\nGeiger. Graf: Generative radiance ﬁelds for 3d-aware image\\nsynthesis. arXiv, 2020. 3\\n[46] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\\nHuang. 3d photography using context-aware layered depth\\ninpainting. In CVPR, 2020. 2\\n[47] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein.\\nScene representation networks: Continuous 3d-structure-\\naware neural scene representations. In Advances in Neural\\nInformation Processing Systems, 2016. 1\\n[48] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo\\ntourism: exploring photo collections in 3d. In SIGGRAPH.\\n2006. 2\\n[49] Noah Snavely, Steven M Seitz, and Richard Szeliski. Mod-\\neling the world from internet photo collections. IJCV, 2008.\\n2\\n[50] Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\\nTancik, Ben Mildenhall, and Jonathan T Barron. Nerv: Neu-\\nral reﬂectance and visibility ﬁelds for relighting and view\\nsynthesis. arXiv, 2020. 3\\n[51] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi\\nRamamoorthi, Ren Ng, and Noah Snavely. Pushing the bound-\\naries of view extrapolation with multiplane images. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 175–184, 2019. 1, 2\\n[52] Richard Szeliski and Polina Golland. Stereo matching with\\ntransparency and matting. In ICCV, 1998. 2\\n[53] Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara\\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\\nmamoorthi, Jonathan T Barron, and Ren Ng. Fourier features\\nlet networks learn high frequency functions in low dimen-\\nsional domains. In Advances in Neural Information Process-\\ning Systems, 2020. 2, 4\\n[54] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitz-\\nmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-\\nBrualla, Tomas Simon, Jason Saragih, Matthias Nießner, et al.\\nState of the art on neural rendering. In Computer Graphics\\nForum, volume 39, pages 701–727. Wiley Online Library,\\n2020. 1\\n[55] Richard Tucker and Noah Snavely. Single-view view synthe-\\nsis with multiplane images. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 551–560, 2020. 1, 2\\n[56] Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer-\\nstructured 3d scene inference via view synthesis. In ECCV,\\n2018. 2\\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. Advances in Neural\\nInformation Processing Systems, 30:5998–6008, 2017. 2, 4\\n[58] Chaoyang Wang, José Miguel Buenaposada, Rui Zhu, and\\nSimon Lucey. Learning depth from monocular videos using\\ndirect methods. In CVPR, 2018. 2\\n[59] Rui Wang, Martin Schworer, and Daniel Cremers. Stereo\\ndso: Large-scale direct sparse visual odometry with stereo\\ncameras. In ICCV, 2017. 2\\n[60] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\\ntor Adrian Prisacariu. Nerf −−: Neural radiance ﬁelds with-\\nout known camera parameters. arXiv, 2021. 3\\n[61] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\\nJohnson. Synsin: End-to-end view synthesis from a single\\nimage. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, pages 7467–7477, 2020.\\n1, 2\\n[62] Changchang Wu et al. Visualsfm: A visual structure from\\nmotion system. 2\\n[63] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\\nKim. Space-time neural irradiance ﬁelds for free-viewpoint\\nvideo. arXiv, 2020. 3\\n[64] Anqi Joyce Yang, Can Cui, Ioan Andrei Bârsan, Raquel Urta-\\nsun, and Shenlong Wang. Asynchronous multi-view SLAM.\\nIn ICRA, 2021. 2\\n105750\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[65] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto\\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting\\nneural radiance ﬁelds for pose estimation. arXiv preprint\\narXiv:2012.05877, 2020. 3, 6\\n[66] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning\\nof dense depth, optical ﬂow and camera pose. In CVPR, 2018.\\n2\\n[67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\\npixelnerf: Neural radiance ﬁelds from one or few images. In\\nCVPR, 2021. 3\\n[68] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\\nKoltun. Nerf++: Analyzing and improving neural radiance\\nﬁelds. arXiv, 2020. 3\\n[69] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition, pages\\n586–595, 2018. 6\\n[70] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\\nLowe. Unsupervised learning of depth and ego-motion from\\nvideo. In CVPR, 2017. 2\\n[71] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magniﬁcation: Learning view\\nsynthesis using multiplane images. In SIGGRAPH, 2018. 1,\\n2\\n[72] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\\nSimon Winder, and Richard Szeliski. High-quality video view\\ninterpolation using a layered representation. TOG, 2004. 2\\n115751\\n', metadata={'source': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'file_path': 'data/Lin et al. - 2021 - BARF Bundle-Adjusting Neural Radiance Fields.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'BARF: Bundle-Adjusting Neural Radiance Fields', 'author': 'Chen-Hsuan Lin;  Wei-Chiu Ma;  Antonio Torralba;  Simon Lucey', 'subject': 'IEEE International Conference on Computer Vision', 'keywords': '', 'creator': '', 'producer': 'pikepdf 3.1.0', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\nAvailable online 2 May 2023\\n2405-6316/© 2023 The Author(s). Published by Elsevier B.V. on behalf of European Society of Radiotherapy & Oncology. This is an open access article under the\\nCC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\\nOriginal Research Article \\nPatient-specific three-dimensional image reconstruction from a single X-ray \\nprojection using a convolutional neural network for on-line \\nradiotherapy applications \\nEstelle Loÿen *, Damien Dasnoy-Sumell, Benoit Macq \\nInstitute of Information and Communication Technologies, Electronics and Applied Mathematics (ICTEAM), UCLouvain, Place de l’Universit´e 1, 1348 Louvain-la-Neuve, \\nBelgium   \\nA R T I C L E  I N F O   \\nKeywords: \\n3D-CT reconstruction \\nConvolutional neural networks \\nReal-time markerless tumor tracking \\nFluoroscopy \\nA B S T R A C T   \\nBackground and purpose: Radiotherapy is commonly chosen to treat thoracic and abdominal cancers. However, \\nirradiating mobile tumors accurately is extremely complex due to the organs’ breathing-related movements. \\nDifferent methods have been studied and developed to treat mobile tumors properly. The combination of X-ray \\nprojection acquisition and implanted markers is used to locate the tumor in two dimensions (2D) but does not \\nprovide three-dimensional (3D) information. The aim of this work is to reconstruct a high-quality 3D computed \\ntomography (3D-CT) image based on a single X-ray projection to locate the tumor in 3D without the need for \\nimplanted markers. \\nMaterials and Methods: Nine patients treated for a lung or liver cancer in radiotherapy were studied. For each \\npatient, a data augmentation tool was used to create 500 new 3D-CT images from the planning four-dimensional \\ncomputed tomography (4D-CT). For each 3D-CT, the corresponding digitally reconstructed radiograph was \\ngenerated, and the 500 2D images were input into a convolutional neural network that then learned to recon-\\nstruct the 3D-CT. The dice score coefficient, normalized root mean squared error and difference between the \\nground-truth and the predicted 3D-CT images were computed and used as metrics. \\nResults: Metrics’ averages across all patients were 85.5% and 96.2% for the gross target volume, 0.04 and 0.45 \\nHounsfield unit (HU), respectively. \\nConclusions: The proposed method allows reconstruction of a 3D-CT image from a single digitally recon-\\nstructed radiograph that could be used in real-time for better tumor localization and improved treatment of \\nmobile tumors without the need for implanted markers.   \\n1. Introduction \\nRadiotherapy is one of the most widely used treatments in oncology \\nand is prescribed for more than half of all cancer patients, either alone or \\nin combination with surgery and chemotherapy [1]. In radiotherapy, \\nionizing radiation is used to kill cancer cells. A trade-off must be made \\nbetween delivering the prescribed dose to the target and not delivering \\nlarge doses to healthy tissues, which could lead to undesirable effects and \\ninduce secondary cancer [2]. Applying radiotherapy to lung and liver \\ncancers is even more challenging as the treatment must consider the \\nrespiratory motion. This requires specific strategies in the radiotherapy \\nworkflow to ensure adequate target coverage through successive treat-\\nment fractions. These strategies are generally classified in two categories. \\nThe first category consists in acquiring a four-dimensional computed \\ntomography (4D-CT) scan prior to the treatment and defining security \\nmargins. Safety margins ensure target coverage regardless of the \\nbreathing phase, but this method irradiates more the surrounding \\nhealthy organs [3]. The breathing motion in the treatment room may \\nalso differ significantly from the motion captured in the 4D-CT from \\ntime to time [4]. \\nThe second category encompasses breathing-synchronized methods \\nthat aim to minimize the contribution of the tumor’s motion in the \\ncomputation of the safety margins by monitoring the tumor’s position or \\nreducing/regularizing its motion amplitude during breathing. These \\nmethods gather abdominal compression [5], audio coaching [6], me-\\nchanically assisted ventilation [7] and respiratory gating [8]. Tumor \\n* Corresponding author. \\nE-mail address: estelle.loyen@uclouvain.be (E. Loÿen).  \\nContents lists available at ScienceDirect \\nPhysics and Imaging in Radiation Oncology \\njournal homepage: www.sciencedirect.com/journal/physics-and-imaging-in-radiation-oncology \\nhttps://doi.org/10.1016/j.phro.2023.100444 \\nReceived 11 November 2022; Received in revised form 6 April 2023; Accepted 25 April 2023   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 0, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\n2\\nmonitoring in these techniques is based on external surrogates of the \\ninternal motion to avoid the use of invasive procedures (the placement \\nof markers pinpoints the tumor position with greater accuracy but in-\\nvolves surgery before the treatment [9]). This approach requires a stable \\ncorrelation between the internal tumor motion and its external surro-\\ngate, which is usually not the case when changes occur in the patient’s \\nbreathing movement. \\nImage-guided radiation therapy (IGRT) incorporates imaging tech-\\nniques during each treatment session. By adding detailed images, it \\nensures that the radiation is narrowly focused on the target. A broad \\nrange of IGRT is now available [10]. X-ray projections are commonly \\nacquired to estimate the tumor’s position, but their use often requires \\nimplanted markers to identify the tumor volume correctly and make it \\nvisible on the X-ray projection [11]. Another disadvantage of this \\nmethod is that it does not provide 3D information. \\nAll these methods result in a small reduction in the safety margins, \\nwhile adapting the treatment in 3D and in real-time will lead to a big \\nreduction in the motion margins thanks to precise tracking of the 3D \\nanatomical structures. To achieve this, the real-time positions of the \\ntarget and surrounding organs must be known throughout treatment \\ndelivery. Most of the radiotherapy treatment rooms are equipped with \\n2D fluoroscopy to validate the patient positioning before treatment, we \\npropose to rely on this equipment to estimate the related 3D \\ninformation. \\nMany studies that reconstruct a 3D volume from a 2D X-ray projec-\\ntion have already been performed. Different fields of application in the \\nbiomedical sector have been explored: Henzler et al. investigated how to \\nreconstruct 3D volumes from 2D cranial x-rays by applying deep \\nlearning [12], while Liang et al. developed a new model architecture to \\nreconstruct a tooth in 3D from a single panoramic radiograph [13]. \\nMontaya et al. in [14], as well as Ying et al. in [15], demonstrated that it \\nwas possible to reconstruct a 3D-CT image from biplanar X-ray pro-\\njections using a neural network, and Shen et al. used a neural network to \\nreconstruct a 3D image from a single projection view [16]. \\nIn this context, the aim of the work described in this article was to use \\nthe 2D information available in the treatment rooms to obtain 3D in-\\nformation. To that end, we use a convolutional neural network that \\nreconstructs a high-quality 3D-CT image based on a single X-ray \\nprojection. This image, predicted in real-time, can then be used by a real- \\ntime segmentation method [17] in order to know the tumor and sur-\\nrounding organs’ positions at the moment of acquisition. This process \\nwould make it possible to locate the tumor and neighboring structures \\naccurately in 3D during the treatment without requiring implanted \\nmarkers. \\n2. Materials and methods \\nFig. 1 summarizes the proposed method’s workflow. The different \\nsteps of the process are detailed in the following sub-sections. \\n2.1. Dataset generation \\nThe data used in this work come from nine patients who were treated \\nfor lung or liver cancer at Cliniques universitaires Saint-Luc in Brussels \\nbetween 2010 and 2015. This retrospective study was approved by the \\nHospital Research Ethics Committee (B403201628906). Table 1 shows \\npatients information (tumor size and location, and its motion in the \\ndifferent sets). A planning 4D-CT composed of 10 breathing phases \\nevenly spread over the respiratory cycle was acquired for each patient \\nprior to treatment delivery. The dimensions of each 3D-CT image were \\n512 × 512 × 173, and the voxel size was 1 mm2 in plane with a slice \\nthickness of 2 mm. The Mid-Position (MidP)-CT image, defined as the \\nlocal mean position in the respiratory cycle, was computed using the \\naverage of all velocity fields obtained by non-rigid registration between \\nthe 4D-CT phases [18]. On the MidP-CT image, the gross target volume \\n(GTV) and surrounding organs at risk were delineated manually by an \\nexperienced radiation oncologist. \\nAs training a neural network requires a lot of data, it was necessary to \\ngenerate new 3D-CT images. To do so, we consider a polar coordinate \\nsystem (r, n) related to a breathing cycle, whose origin is the MidP-CT \\nimage and where n are the periodic phases. In this system, we know \\nthe deformation fields associated to the 10 breathing phases of the 4D- \\nCT which are F(1, N), with N ∈ {0, 0.1…, 0.9}. Then, to generate the \\nbreathing phase n at a normalized distance r of the MidP-CT, we \\ncompute the deformation field F(r, n) using a linear interpolation be-\\ntween the two closest discrete breathing phases plus a scaling: \\nFig. 1. Overview of the proposed method’s workflow.  \\nE. Loÿen et al.                                                                                                                                                                                                                                   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 1, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\n3\\nF(r, n) = [F(1, N) + (F(1, N + 0.1) − F(1, N) )⋅10⋅(n − N) ]⋅r\\n(1)  \\nwhere N⩽n⩽N + 0.1. Using this method, based on a previous work of \\nour team [19] and developed in [20], we can generate slightly different \\n3D-CT images, spread around the ten original phases of the 4D-CT, for \\nevery patient. The training set was composed of 500 images where n was \\na uniform random draw between 0 and 1, and r a random sample from a \\nnormal distribution N (1, 0.25) truncated between 0.4 and 1.1. A digi-\\ntally reconstructed radiograph (DRR) was generated from each of these \\nimages using the Beer–Lambert absorption-only model (implemented in \\nthe TomoPy Python library [21]) and a projection angle of 0◦ along the \\nanterior-posterior axis. The projection geometry was a 1440 × 1440 \\nimage with a pixel size of 0.296 × 0.296 mm2. The source-to-origin and \\nsource-to-detector distances were 1000 mm and 1500 mm. Each pa-\\ntient’s training dataset was made up of 500 pairs containing the created \\n3D-CT image and the associated DRR. An independent test set composed \\nof 100 3D-CT/DRR pairs was also created for each patient. For each \\nimage of the test set, the masks of the GTV, lungs and heart were also \\ngenerated by deforming the MidP-CT image’s 3D binary masks. The \\ndifference between the test and training sets comes from the normalized \\ndistance r used to generate the 3D-CT image. In the case of the training \\nset, r was a random sample from a normal distribution N (1, 0.25)\\ntruncated between 0.4 and 1.1, while r was a random sample from a \\nnormal distribution N (1, 0.5) truncated between 0.8 and 1.5 for the test \\nset. This means that deeper breathing situations were present in the test \\nset than in the training set. All breathing phases were used in both cases. \\n2.2. Patient-specific deep learning model for 3D-CT reconstruction \\nThe network used for the 3D-CT reconstruction process is a con-\\nvolutional neural network (CNN) that learns the mapping between a 2D \\nimage and a 3D volume. This network was proposed by Henzler et al. in \\n[12] and we have tuned the different hyper-parameters for our chal-\\nlenge. The overall structure of this network is an encoder-decoder with \\nskip connections. The goal of the encoder is to condense the information \\ncontained in the training data into a low-dimensional representation, \\nwhich the decoder then takes as input to predict the output [22]. The \\ninput of the network is a DRR of size 256 × 256, while the output con-\\nsists of a 128 × 128 × 128 3D-CT image. The details of the training \\ndataset, namely 3D-CT/DRR pairs, are explained in Section 2.1. The \\nnetwork training was patient-specific, a new network is trained inde-\\npendently for each patient. The same training strategy and hyper- \\nparameters were used for all patients. The Adam optimizer was used \\nto train the network with an initial learning rate of 10−3 and momentum \\nparameters β1 = 0.9 and β2 = 0.99. The model was trained for a total of \\n300 epochs using a mini-batch size of 16 on a NVIDIA RTX 6000, which \\nbrought the training time down to roughly 8 h. Then, it takes about 50μs \\nto predict the output from a new input. \\n2.3. Performance evaluation \\nIn order to evaluate the performance of the proposed method, 100 \\n3D-CT images independent of the training set were created for each \\npatient. These 3D-CT images are called the ground truth (GT) 3D-CT \\nimages in the rest of the paper. 100 DRRs were generated from these \\nimages to form the test set. The trained network was used on these ra-\\ndiographs to predict the corresponding 3D-CT images, called the pre-\\ndicted (P) 3D-CT images. The predicted 3D-CT images were compared \\nwith the ground truth 3D-CT images to evaluate the performance of the \\nmodel using several metrics. \\nDice similarity coefficient (DSC) is a common overlap-based metric \\nused to measure the performance of a segmentation algorithm, and is \\ndefined by: \\nDSC = 2|A ∩ B|\\n|A| + |B|⋅100 [%]\\n(2)  \\nwhere A and B are the sets containing the matrix indices of both binary \\nmasks A and B. In this work, the DSC was computed between a 3D binary \\nmask in the ground-truth 3D-CT image and the corresponding mask in \\nthe predicted 3D-CT image to evaluate the quality of the predicted 3D- \\nCT image in terms of anatomical structure positions. The 3D binary \\nmasks of a predicted 3D-CT image were obtained by computing the \\nMorphons non-rigid registration [23], then applying the resulting \\ndeformation fields to deform the masks on the predicted image. This was \\ndone between this predicted image and either the ground-truth 3D-CT \\nimage (GT-based), or the MidP-CT image (MidP-based). Using the \\nground-truth 3D-CT image for this part serves as a post-training quality \\nevaluation, to evaluate if a state-of-the-art registration algorithm sees a \\ndifference between the ground-truth and the predicted images. Using \\nthe MidP-CT image simulates how it could be used to evaluate the \\nquality of the predicted images after each treatment fraction as the \\nground-truth 3D-CT images are not available during a treatment. For \\nboth versions, the DSC was computed for the same 50 images of the 100 \\nitems constituting the test set, for each organ and each patient. In either \\ncase, this metric was an evaluation tool and not part of the real-time \\nprocess as the computation time of the Morphons is about 150 s. As a \\ncomplement to this analysis, the Euclidean distance was computed \\n(further details in Appendix A. Supplementary data). \\nNormalized root mean squared error (NRMSE) was computed \\nbetween two images A and B, and is defined by: \\nNRMSE =\\n̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅\\n∑\\nn\\na=1\\n(Aa−Ba)2\\nn\\n√\\nAmax − Amin\\n(3)  \\nwhere Xa is the voxel a in the image X. Amax and Amin stand for the \\nmaximum and minimum in image A, the ground-truth 3D-CT image. The \\nNRMSE was computed between the latter and the corresponding pre-\\ndicted 3D-CT image. This was repeated for all images in the test set. \\nDifference was computed between a ground-truth 3D-CT image and \\nthe corresponding predicted 3D-CT image, and the mean and median of \\nthe difference were studied, as well as quantifying the percentage of the \\nabsolute value of the difference below a certain threshold to evaluate the \\nproportion of the image that was correctly reconstructed. \\nTable 1 \\nPatient characteristics. MR4D−CT, MRTrainSet and MRTestSet stand for the motion \\nrange in 3D of the GTV’s centroid in the 4D-CT, training set and test set, \\nrespectively. The motion range is defined as the Euclidean distance between the \\ntwo most distant positions.  \\nPatient \\nID \\nTumor location \\nGTV \\nsize  \\n[cm3] \\nMR4D−CT \\n[mm] \\nMRTrainSet \\n[mm] \\nMRTestSet \\n[mm] \\nPatient 1 \\nRight upper lobe \\nof lung \\n137.1 \\n11.1 \\n17.2 \\n17.9 \\nPatient 2 \\nRight upper lobe \\nof lung \\n17.2 \\n9.9 \\n9.7 \\n12.6 \\nPatient 3 \\nRight middle lobe \\nof lung \\n153.8 \\n24.4 \\n32.4 \\n34.7 \\nPatient 4 \\nLeft upper lobe of \\nlung \\n13.8 \\n14.5 \\n15.2 \\n18.5 \\nPatient 5 \\nLeft upper lobe of \\nlung \\n315.1 \\n9.7 \\n10.1 \\n11.4 \\nPatient 6 \\nLeft upper lobe of \\nlung \\n67.2 \\n11.6 \\n15.2 \\n16.1 \\nPatient 7 \\nRight lobe of liver \\n28.6 \\n15.1 \\n18.7 \\n26.4 \\nPatient 8 \\nRight lobe of liver \\n80.4 \\n27.1 \\n29.9 \\n30.8 \\nPatient 9 \\nLeft lobe of liver \\n22.5 \\n24.1 \\n32.3 \\n34.8  \\nE. Loÿen et al.                                                                                                                                                                                                                                   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 2, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\n4\\n3. Results \\n3.1. Dice similarity coefficient \\nThe results of the DSC analysis for both GT-based and MidP-based \\nversions are summarized in Table 2. For the GT-based version, the mean, \\nthe median and the 95th percentile of the DSC vary respectively from \\n93.2% to 99.8%, from 93.2% to 99.9%, and from 95.1% to 99.9% for the \\nGTV; from 96.3% to 99.8%, from 96.5% to 99.9%, and from 96.8% to \\n99.9% for both lungs; from 93.5% to 99.8%, from 94.3% to 99.8%, and \\nfrom 95.1% to 99.9% for the heart. While, for the MidP-based version, \\nthe mean, the median and the 95th percentile of this metric vary \\nrespectively from 76.7% to 90.6%, from 77.6% to 90.8%, and from \\n82.7% to 93.4% for the GTV; from 90.9% to 97.3%, from 93.4% to \\n97.1%, and from 96.1% to 98.3% for both lungs; from 78.1% to 90.1%, \\nfrom 79.2% to 89.9%, and from 81.5% to 91.7% for the heart. \\nThe DSC results of the MidP-based version are lower than those of GT- \\nbased, but still over 75%. As the same 50 images were used for both, the \\ndifference might be due to the approximations in the deformations and \\nre-binarization of the masks, that probably have a higher impact with \\ndeformations over multiple voxels, but this was not quantified. \\n3.2. Normalized root mean squared error \\nThe results of the NRMSE analysis are displayed in Fig. 2. The mean \\nof this metric is lower for Patients 5, 2, 6 and 1 who have smaller mo-\\ntions in the test set (from 0.032 to 0.039) than the mean obtained for \\nPatients 7, 8, 3 and 9 (from 0.047 to 0.051) who have larger motions. \\nThis is also observed for the median and the 95th percentile, which range \\nrespectively from 0.032 to 0.038, and from 0.039 to 0.045 for the first \\nbatch of patients, while they are respectively between 0.045 and 0.052, \\nand between 0.051 and 0.059 for the second group of patients. This \\nanalysis also shows that the breathing phases have no impact on the \\nreconstruction process as there are uniformly distributed along the \\nNRMSE values range. \\n3.3. Difference \\nThe results of the difference analysis are summarized in Table 3. The \\nmean of the difference between a ground-truth 3D-CT image and the \\ncorresponding predicted 3D-CT image ranges from −1.32 Hounsfield \\nunit (HU) to 2.24 HU, with an average over all patients of 0.45 HU. The \\nmedian of this metric is between −0.26 HU and 1.93 HU, with an \\naverage over all patients of 0.24 HU. Depending on the patient, 25.1% to \\n39.8% of the image volume has an absolute value of the difference lower \\nthan 5 HU, 69.9% to 81.9% below 25 HU, and 88.6% to 94.6% less than \\n50 HU. In summary, the difference between the ground-truth and the \\npredicted images is very small, with about 91% of the image volume \\nhaving an absolute value of the difference smaller than 50 HU, which \\nrepresents 1.25% of the range of possible values, since the scale of a 3D- \\nCT image typically runs from −1000 HU for air to 3000 HU for dense \\nbone [24]. \\nA representative example (whose results are: DSCGT (GTV) = 98.5% \\n, DSCMidP (GTV) = 88.6%, NRMSE = 0.053, mean of the difference =\\n−1.73 HU and V<25HU = 80.3%) of the results obtained using the pro-\\nposed method can be seen in Fig. 3. For a human eye, the predicted 3D- \\nCT image looks pretty close in terms of anatomical structures. The zoom \\nshows that a red pixel (difference ≈ 200 HU) is commonly adjacent to a \\nblue pixel (difference ≈ −200 HU) or surrounded by two turquoise pixel \\n(difference ≈ −100 HU). This phenomenon is usually observed at tissue \\nborders. Looking at the histogram, one sees that there are few voxels \\nwith a significant difference and over 30% of the voxels have a differ-\\nence between −5 HU and 5 HU. \\n4. Discussion \\nIn this paper, it has been showed that the proposed CNN-based \\nmethodology (which requires a patient-specific training) allows to \\nreconstruct a high-quality 3D-CT image from a single digitally recon-\\nstructed radiograph. \\nTable 2 \\nResults of the DSC analysis for both GT-based and MidP-based versions. DSCGT and DSCMidP stand for the mean of the DSC over the 50 images taken from the test set for \\nthe GT-based version and MidP-based version, respectively. Patient 5’s lungs and heart were not delineated.  \\nPatient ID \\nGTV \\nLungR \\nLungL \\nHeart  \\nDSCGT \\n[%] \\nDSCMidP \\n[%] \\nDSCGT \\n[%] \\nDSCMidP \\n[%] \\nDSCGT \\n[%] \\nDSCMidP \\n[%] \\nDSCGT \\n[%] \\nDSCMidP \\n[%]  \\nPatient 1 \\n94.1 \\n89.4 \\n98.4 \\n94.9 \\n97.5 \\n93.3 \\n99.5 \\n83.1  \\nPatient 2 \\n93.2 \\n89.1 \\n99.2 \\n96.4 \\n97.4 \\n97.3 \\n99.8 \\n85.8  \\nPatient 3 \\n99.8 \\n81.3 \\n99.3 \\n96.7 \\n98.9 \\n95.6 \\n99.2 \\n80.8  \\nPatient 4 \\n92.5 \\n87.9 \\n98.8 \\n95.6 \\n98.7 \\n93.2 \\n98.9 \\n90.1  \\nPatient 5 \\n96.4 \\n90.4 \\nNA \\nNA \\nNA \\nNA \\nNA \\nNA  \\nPatient 6 \\n97.7 \\n90.6 \\n99.8 \\n95.6 \\n99.7 \\n93.4 \\n99.8 \\n89.9  \\nPatient 7 \\n93.3 \\n78.2 \\n97.2 \\n92.8 \\n96.3 \\n90.9 \\n93.5 \\n78.1  \\nPatient 8 \\n99.3 \\n86.3 \\n98.8 \\n94.6 \\n98.7 \\n95.1 \\n99.4 \\n83.8  \\nPatient 9 \\n99.2 \\n76.7 \\n99.4 \\n93.3 \\n99.1 \\n94.5 \\n96.3 \\n80.3   \\nFig. 2. Results of the NRMSE analysis. The NRMSE was computed between the \\nground-truth 3D-CT image and the corresponding predicted 3D-CT image for \\neach test set data. The color of a dot represents the breathing phase at which the \\nground-truth 3D-CT image was created. Patients are sorted by increasing mo-\\ntion range in the test set. \\nTable 3 \\nResults of the difference analysis. V<5HU, V<25HU and V<50HU stand for the per-\\ncentage of the 3D-CT image’s volume having an absolute value of the difference \\nbelow 5 HU, 25 HU and 50 HU.  \\nPatient ID \\nMean \\n[HU] \\nMedian \\n[HU] \\nV<5HU \\n[%] \\nV<25HU \\n[%] \\nV<50HU \\n[%] \\nPatient 1 \\n0.36 \\n−0.02 \\n25.4 \\n74.1 \\n91.1 \\nPatient 2 \\n0.31 \\n−0.13 \\n34.5 \\n80.1 \\n93.7 \\nPatient 3 \\n0.46 \\n−0.26 \\n31.8 \\n80.7 \\n94.6 \\nPatient 4 \\n0.51 \\n0.04 \\n39.8 \\n81.9 \\n94.2 \\nPatient 5 \\n0.65 \\n0.08 \\n29.9 \\n75.1 \\n91.5 \\nPatient 6 \\n0.53 \\n−0.16 \\n29.7 \\n76.8 \\n91.9 \\nPatient 7 \\n0.37 \\n0.56 \\n32.4 \\n75.9 \\n88.8 \\nPatient 8 \\n−1.32 \\n0.09 \\n27.1 \\n74.4 \\n89.9 \\nPatient 9 \\n2.24 \\n1.93 \\n25.1 \\n69.9 \\n88.6  \\nE. Loÿen et al.                                                                                                                                                                                                                                   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 3, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\n5\\nThe dice values computed between the masks of the predicted 3D-CT \\nimage and the corresponding ground-truth 3D-CT are all greater than \\n75%, which is reliable. If we compare our results of the MidP-based \\nversion (Table 2) for lungs and heart (94.6% and 83.9%) to previous \\nworks [25–27], whose goal was to segment organs at risk in lung cancer \\nutilizing deep learning algorithms, (best in [27]: 97.5% and 92.5%), \\nlungs have similar results to the literature and the heart has a higher \\ndifference. However, our results should be taken in hindsight, given that \\nthe masks in the predicted image are defined as the manually segmented \\nmasks on the MidP-CT image deformed using the deformation fields \\nobtained by the Morphons registration between both images. \\nThe mean of the difference between the ground truth image and the \\npredicted image is small for each patient, with an average value of 0.45 \\nHU over all patients. Comparing these results (Fig. 3) with those ob-\\ntained by [16] when they use only 1 view, the quality of our recon-\\nstructed image is similar to their own. Their method also performs less at \\ntissue borders. However, there is no scale or numerical value in their \\ndifference analysis, so it is not clear that the difference values are \\nsimilar. \\nOne limitation of this study is that the CNN was trained using \\ntraining sets composed of 3D-CT images created from deformations of a \\nplanning 4D-CT acquired prior to the treatment and paired DRRs \\ngenerated using the Beer–Lambert absorption-only model. This method \\nsupposes that inter-fraction variations such as tumor shrinking, tumor \\nbaseline shift and stomach and bladder fillings are not included in the \\ntraining set. A next step of this work is to evaluate whether the network \\nmust be retrained for each fraction or whether these variations are \\nnegligible in the reconstruction process. Another possibility to coun-\\nteract this limitation is to improve the data augmentation tool and \\nincorporate inter-fraction changes in the training set. \\nAn additional potential purpose of the predicted 3D-CT image would \\nbe to use it to compute the dose delivered during the treatment (either \\non-line or inter-fraction). To this end, the voxel value representing tissue \\ndensity is a crucial piece of information to have the dose delivered at the \\nright place. This paper shows that, for the human eye, the predicted 3D- \\nCT image is really close to the ground-truth 3D-CT image but the results \\nof the difference should be discussed further and it will be necessary to \\nassess whether the maximum of the difference is located on the beam’s \\npath or the difference, no matter how small, has too great an impact on \\nthe computed dose. Furthermore, in order to get a clinically usable dose, \\nthe standard resolution of a 3D-CT scan would be needed. Therefore, the \\npredicted 3D-CT image should be oversampled to get the desired \\nresolution. \\nIn conclusion, this study presents a method that allows reconstruc-\\ntion of a 3D-CT image from a single DRR. This method relies on a data \\naugmentation algorithm and on a patient-specific training of a CNN. \\nHowever, the study still needs to integrate inter-fractions changes and \\nadjust the image resolution to confirm the potential clinical use of the \\nmethod. \\nDeclaration of Competing Interest \\nThe authors declare that they have no known competing financial \\ninterests or personal relationships that could have appeared to influence \\nthe work reported in this paper. \\nAcknowledgments \\nEstelle Loÿen is a Televie grantee of the Fonds de la Recherche Sci-\\nentifique - F.N.R.S. Damien Dasnoy-Sumell is supported by the Walloon \\nRegion, SPWEER Win2Wal program project 2010149. \\nAppendix A. Supplementary data \\nSupplementary data associated with this article can be found, in the \\nonline version, at https://doi.org/10.1016/j.phro.2023.100444. \\nReferences \\n[1] Baskar R, Lee K, Yeo R, Yeoh K. Cancer and Radiation Therapy: Current Advances \\nand Future Directions. Int J Med Sci 2012;9:193–9. https://doi.org/10.7150/ \\nijms.3635. \\n[2] Warkentin B, Stavrev P, Stavreva N, Field C, Fallone B. A TCP-NTCP estimation \\nmodule using DVHs and known radiobiological models and parameter sets. J Appl \\nClin Med Phys 2004;5:50–63. https://doi.org/10.1120/jacmp.v5i1.1970. \\n[3] Rietzel E, Bert C. Respiratory motion management in particle therapy. Med Phys \\n2010;37:449–60. https://doi.org/10.1118/1.3250856. \\nFig. 3. Visualization of three slices of the ground-truth 3D-CT image of one patient compared with the corresponding slices of the predicted 3D-CT image, as well as \\nthe results of the difference analysis and a zoom of the boxed area. On the right of the color bar is the histogram of the difference concatenated for all patients and the \\n100 images of the nine test sets. \\nE. Loÿen et al.                                                                                                                                                                                                                                   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 4, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='Physics and Imaging in Radiation Oncology 26 (2023) 100444\\n6\\n[4] Dhont J, Vandemeulebroucke J, Burghelea M, Poels K, Depuydt T, Van Den \\nBegin R, et al. The long- and short-term variability of breathing induced tumor \\nmotion in lung and liver over the course of a radiotherapy treatment. Radiother \\nOncol 2018;126:339–46. https://doi.org/10.1016/j.radonc.2017.09.001. \\n[5] Piippo-Huotari O, Norrman E, Anderz´en-Carlsson A, Geijer H. New patient- \\ncontrolled abdominal compression method in radiography: radiation dose and \\nimage quality. Acta Radiol Open 2018;7:1–7. https://doi.org/10.1177/ \\n2058460118772863. \\n[6] Nakamura M, Narita Y, Matsuo Y, Narabayashi M, Nakata M, Sawada A, et al. \\nEffect of audio coaching on correlation of abdominal displacement with lung tumor \\nmotion. Int J Radiat Oncol Biol Phys 2009;75:558–63. https://doi.org/10.1016/j. \\nijrobp.2008.11.070. \\n[7] Van Ooteghem G, Dasnoy-Sumell D, Lee JA, Geets X. Mechanically-assisted and \\nnon-invasive ventilation for radiation therapy: A safe technique to regularize and \\nmodulate internal tumour motion. Radiother Oncol 2019;141:283–91. https://doi. \\norg/10.1016/j.radonc.2019.09.021. \\n[8] Muirhead R, Featherstone C, Duton A, Moore K, McNee S. The potential benefit of \\nrespiratory gated radiotherapy (RGRT) in non-small cell lung cancer. Radiother \\nOncol 2010;95:172–7. https://doi.org/10.1016/j.radonc.2010.02.002. \\n[9] Hirai R, Watanabe W, Sakata Y, Tanizawa A. Real-time linear fiducial marker \\ntracking in respiratory-gated radiotherapy for hepatocellular carcinoma. Int J \\nRadiat Oncol Biol Phys 2019;105:E750–1. https://doi.org/10.1016/j. \\nijrobp.2019.06.769. \\n[10] Ren XC, Liu YE, Li J, Lin Q. Progress in image-guided radiotherapy for the \\ntreatment of non-small cell lung cancer. World J Radiol 2019;11:46–54. https:// \\ndoi.org/10.4329/wjr.v11.i3.46. \\n[11] Soete G, Verellen D, Michielsen D, Vinh-Hung V, Van de Steene J, Van den Berge D, \\net al. Clinical use of stereoscopic X-ray positioning of patients treated with \\nconformal radiotherapy for prostate cancer. Int J Radiat Oncol Biol Phys 2002;54: \\n948–52. https://doi.org/10.1016/S0360-3016(02)03027-4. \\n[12] Henzler P, Rasche V, Ropinski T, Ritschel T. Single-image Tomography: 3D \\nVolumes from 2D Cranial X-Rays. arXiv 2017. https://doi.org/10.48550/ \\nARXIV.1710.04867. \\n[13] Liang Y, Song W, Yang J, Qiu L, Wang K, He L. X2Teeth: 3D Teeth Reconstruction \\nfrom a Single Panoramic Radiograph. arXiv 2021. https://doi.org/10.48550/ \\narXiv.2108.13004. \\n[14] Montoya JC, Zhang C, Li Y, Li K, Chen GH. Reconstruction of three-dimensional \\ntomographic patient models for radiation dose modulation in CT from two scout \\nviews using deep learning. Med Phys 2021;49:1–16. https://doi.org/10.1002/ \\nmp.15414. \\n[15] Ying X, Guo H, Ma K, Wu J, Weng Z, Zheng Y. X2CT-GAN: Reconstructing CT From \\nBiplanar X-Rays With Generative Adversarial Networks. In: 2019 IEEE/CVF \\nConference on Computer Vision and Pattern Recognition (CVPR); 2019. p. \\n10611–20. doi: 10.1109/CVPR.2019.01087. \\n[16] Shen L, Zhao W, Xing L. Patient-specific reconstruction of volumetric computed \\ntomography images from a single projection view via deep learning. Nat Biomed \\nEng 2019;3:880–8. https://doi.org/10.1038/s41551-019-0466-4. \\n[17] Zhou S, Xu X, Bai J, Bragin M. Combining multi-view ensemble and surrogate \\nlagrangian relaxation for real-time 3D biomedical image segmentation on the edge. \\nNeurocomputing 2022;512:466–81. https://doi.org/10.1016/j. \\nneucom.2022.09.039. \\n[18] Wolthaus J, Sonke J, van Herk M, Damen E. Reconstruction of a time-averaged \\nmidposition CT scan for radiotherapy planning of lung cancer patients using \\ndeformable registration. J Appl Clin Med Phys 2008;35:3998–4011. https://doi. \\norg/10.1118/1.2966347. \\n[19] Dasnoy-Sumell D, Aspeel A, Souris K, Macq B. Locally tuned deformation fields \\ncombination for 2D cine-MRI-based driving of 3D motion models. Phys Med 2022; \\n94:8–16. https://doi.org/10.1016/j.ejmp.2021.12.010. \\n[20] Wuyckens S, Dasnoy D, Janssens G, Hamaide V, Huet M, Loÿen E, et al. OpenTPS – \\nOpen-source treatment planning system for research in proton therapy. arXiv 2023. \\nhttps://doi.org/10.48550/arXiv.2303.00365. \\n[21] Gürsoy D, De Carlo F, Xiao X, Jacobsen C. TomoPy: a framework for the analysis of \\nsynchrotron tomographic data. J Synchrotron Radiat 2014;21:1188–93. https:// \\ndoi.org/10.1107/S1600577514013939. \\n[22] Minaee S, Boykov Y, Porikli F, Plaza A, Kehtarnavaz N, Terzopoulos D. Image \\nSegmentation Using Deep Learning: A Survey. IEEE Trans Pattern Anal Mach Intell \\n2022;44:3523–42. https://doi.org/10.1109/TPAMI.2021.3059968. \\n[23] Janssens G, Jacques L, Orban de Xivry J, Geets X, Macq B. Diffeomorphic \\nregistration of images with variable contrast enhancement. Int J Biomed Imaging \\n2011, 2011,:1–16. https://doi.org/10.1155/2011/891585. \\n[24] Bibb R, Eggbeer D, Paterson A. 2 - Medical imaging. In: Medical Modelling (Second \\nEdition) Woodhead Publishing; 2015. p. 7-34. https://doi.org/10.1016/B978-1- \\n78242-300-3.00002-0. \\n[25] Zhu J, Zhang J, Qiu B, Liu Y, Liu X, Chen L. Comparison of the automatic \\nsegmentation of multiple organs at risk in CT images of lung cancer between deep \\nconvolutional neural network-based and atlas-based techniques. Acta Oncol 2019; \\n58:257–64. https://doi.org/10.1080/0284186X.2018.1529421. \\n[26] Dong X, Lei Y, Wang T, Thomas M, Tang L, Curran WJ, et al. Automatic multiorgan \\nsegmentation in thorax CT images using U-net-GAN. Med Phys 2019;46:2157–68. \\nhttps://doi.org/10.1002/mp.13458. \\n[27] Feng X, Qing K, Tustison NJ, Meyer CH, Chen Q. Deep convolutional neural \\nnetwork for segmentation of thoracic organs-at-risk using cropped 3D images. Med \\nPhys 2019;46:2169–80. https://doi.org/10.1002/mp.13466. \\nE. Loÿen et al.                                                                                                                                                                                                                                   \\n', metadata={'source': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'file_path': 'data/Loyen et al. - 2023 - Patient-specific three-dimensional image reconstru.pdf', 'page': 5, 'total_pages': 6, 'format': 'PDF 1.7', 'title': 'Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications', 'author': 'Estelle Loÿen', 'subject': 'Physics and Imaging in Radiation Oncology, 26 (2023) 100444. doi:10.1016/j.phro.2023.100444', 'keywords': '3D-CT reconstruction,Convolutional neural networks,Real-time markerless tumor tracking,Fluoroscopy', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230506103622Z', 'modDate': 'D:20230506105901Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as\\nNeural Radiance Fields for View Synthesis\\nBen Mildenhall1⋆\\nPratul P. Srinivasan1⋆\\nMatthew Tancik1⋆\\nJonathan T. Barron2\\nRavi Ramamoorthi3\\nRen Ng1\\n1UC Berkeley\\n2Google Research\\n3UC San Diego\\nAbstract. We present a method that achieves state-of-the-art results\\nfor synthesizing novel views of complex scenes by optimizing an under-\\nlying continuous volumetric scene function using a sparse set of input\\nviews. Our algorithm represents a scene using a fully-connected (non-\\nconvolutional) deep network, whose input is a single continuous 5D coor-\\ndinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose\\noutput is the volume density and view-dependent emitted radiance at\\nthat spatial location. We synthesize views by querying 5D coordinates\\nalong camera rays and use classic volume rendering techniques to project\\nthe output colors and densities into an image. Because volume rendering\\nis naturally diﬀerentiable, the only input required to optimize our repre-\\nsentation is a set of images with known camera poses. We describe how to\\neﬀectively optimize neural radiance ﬁelds to render photorealistic novel\\nviews of scenes with complicated geometry and appearance, and demon-\\nstrate results that outperform prior work on neural rendering and view\\nsynthesis. View synthesis results are best viewed as videos, so we urge\\nreaders to view our supplementary video for convincing comparisons.\\nKeywords: scene representation, view synthesis, image-based render-\\ning, volume rendering, 3D deep learning\\n1\\nIntroduction\\nIn this work, we address the long-standing problem of view synthesis in a new\\nway by directly optimizing parameters of a continuous 5D scene representation\\nto minimize the error of rendering a set of captured images.\\nWe represent a static scene as a continuous 5D function that outputs the\\nradiance emitted in each direction (θ, φ) at each point (x, y, z) in space, and a\\ndensity at each point which acts like a diﬀerential opacity controlling how much\\nradiance is accumulated by a ray passing through (x, y, z). Our method optimizes\\na deep fully-connected neural network without any convolutional layers (often\\nreferred to as a multilayer perceptron or MLP) to represent this function by\\nregressing from a single 5D coordinate (x, y, z, θ, φ) to a single volume density\\nand view-dependent RGB color. To render this neural radiance ﬁeld (NeRF)\\n⋆ Authors contributed equally to this work.\\narXiv:2003.08934v2  [cs.CV]  3 Aug 2020\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 0, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='2\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nInput Images\\nOptimize NeRF\\nRender new views\\nFig. 1: We present a method that optimizes a continuous 5D neural radiance\\nﬁeld representation (volume density and view-dependent color at any continuous\\nlocation) of a scene from a set of input images. We use techniques from volume\\nrendering to accumulate samples of this scene representation along rays to render\\nthe scene from any viewpoint. Here, we visualize the set of 100 input views of the\\nsynthetic Drums scene randomly captured on a surrounding hemisphere, and we\\nshow two novel views rendered from our optimized NeRF representation.\\nfrom a particular viewpoint we: 1) march camera rays through the scene to\\ngenerate a sampled set of 3D points, 2) use those points and their corresponding\\n2D viewing directions as input to the neural network to produce an output\\nset of colors and densities, and 3) use classical volume rendering techniques to\\naccumulate those colors and densities into a 2D image. Because this process is\\nnaturally diﬀerentiable, we can use gradient descent to optimize this model by\\nminimizing the error between each observed image and the corresponding views\\nrendered from our representation. Minimizing this error across multiple views\\nencourages the network to predict a coherent model of the scene by assigning\\nhigh volume densities and accurate colors to the locations that contain the true\\nunderlying scene content. Figure 2 visualizes this overall pipeline.\\nWe ﬁnd that the basic implementation of optimizing a neural radiance ﬁeld\\nrepresentation for a complex scene does not converge to a suﬃciently high-\\nresolution representation and is ineﬃcient in the required number of samples per\\ncamera ray. We address these issues by transforming input 5D coordinates with\\na positional encoding that enables the MLP to represent higher frequency func-\\ntions, and we propose a hierarchical sampling procedure to reduce the number of\\nqueries required to adequately sample this high-frequency scene representation.\\nOur approach inherits the beneﬁts of volumetric representations: both can\\nrepresent complex real-world geometry and appearance and are well suited for\\ngradient-based optimization using projected images. Crucially, our method over-\\ncomes the prohibitive storage costs of discretized voxel grids when modeling\\ncomplex scenes at high-resolutions. In summary, our technical contributions are:\\n– An approach for representing continuous scenes with complex geometry and\\nmaterials as 5D neural radiance ﬁelds, parameterized as basic MLP networks.\\n– A diﬀerentiable rendering procedure based on classical volume rendering tech-\\nniques, which we use to optimize these representations from standard RGB\\nimages. This includes a hierarchical sampling strategy to allocate the MLP’s\\ncapacity towards space with visible scene content.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 1, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n3\\n– A positional encoding to map each input 5D coordinate into a higher dimen-\\nsional space, which enables us to successfully optimize neural radiance ﬁelds\\nto represent high-frequency scene content.\\nWe demonstrate that our resulting neural radiance ﬁeld method quantitatively\\nand qualitatively outperforms state-of-the-art view synthesis methods, including\\nworks that ﬁt neural 3D representations to scenes as well as works that train deep\\nconvolutional networks to predict sampled volumetric representations. As far as\\nwe know, this paper presents the ﬁrst continuous neural scene representation\\nthat is able to render high-resolution photorealistic novel views of real objects\\nand scenes from RGB images captured in natural settings.\\n2\\nRelated Work\\nA promising recent direction in computer vision is encoding objects and scenes\\nin the weights of an MLP that directly maps from a 3D spatial location to\\nan implicit representation of the shape, such as the signed distance [6] at that\\nlocation. However, these methods have so far been unable to reproduce realistic\\nscenes with complex geometry with the same ﬁdelity as techniques that represent\\nscenes using discrete representations such as triangle meshes or voxel grids. In\\nthis section, we review these two lines of work and contrast them with our\\napproach, which enhances the capabilities of neural scene representations to\\nproduce state-of-the-art results for rendering complex realistic scenes.\\nA similar approach of using MLPs to map from low-dimensional coordinates\\nto colors has also been used for representing other graphics functions such as im-\\nages [44], textured materials [12,31,36,37], and indirect illumination values [38].\\nNeural 3D shape representations Recent work has investigated the im-\\nplicit representation of continuous 3D shapes as level sets by optimizing deep\\nnetworks that map xyz coordinates to signed distance functions [15,32] or occu-\\npancy ﬁelds [11,27]. However, these models are limited by their requirement of\\naccess to ground truth 3D geometry, typically obtained from synthetic 3D shape\\ndatasets such as ShapeNet [3]. Subsequent work has relaxed this requirement of\\nground truth 3D shapes by formulating diﬀerentiable rendering functions that\\nallow neural implicit shape representations to be optimized using only 2D im-\\nages. Niemeyer et al. [29] represent surfaces as 3D occupancy ﬁelds and use a\\nnumerical method to ﬁnd the surface intersection for each ray, then calculate an\\nexact derivative using implicit diﬀerentiation. Each ray intersection location is\\nprovided as the input to a neural 3D texture ﬁeld that predicts a diﬀuse color for\\nthat point. Sitzmann et al. [42] use a less direct neural 3D representation that\\nsimply outputs a feature vector and RGB color at each continuous 3D coordinate,\\nand propose a diﬀerentiable rendering function consisting of a recurrent neural\\nnetwork that marches along each ray to decide where the surface is located.\\nThough these techniques can potentially represent complicated and high-\\nresolution geometry, they have so far been limited to simple shapes with low\\ngeometric complexity, resulting in oversmoothed renderings. We show that an al-\\nternate strategy of optimizing networks to encode 5D radiance ﬁelds (3D volumes\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 2, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='4\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nwith 2D view-dependent appearance) can represent higher-resolution geometry\\nand appearance to render photorealistic novel views of complex scenes.\\nView synthesis and image-based rendering Given a dense sampling of\\nviews, photorealistic novel views can be reconstructed by simple light ﬁeld sam-\\nple interpolation techniques [21,5,7]. For novel view synthesis with sparser view\\nsampling, the computer vision and graphics communities have made signiﬁcant\\nprogress by predicting traditional geometry and appearance representations from\\nobserved images. One popular class of approaches uses mesh-based representa-\\ntions of scenes with either diﬀuse [48] or view-dependent [2,8,49] appearance.\\nDiﬀerentiable rasterizers [4,10,23,25] or pathtracers [22,30] can directly optimize\\nmesh representations to reproduce a set of input images using gradient descent.\\nHowever, gradient-based mesh optimization based on image reprojection is often\\ndiﬃcult, likely because of local minima or poor conditioning of the loss land-\\nscape. Furthermore, this strategy requires a template mesh with ﬁxed topology\\nto be provided as an initialization before optimization [22], which is typically\\nunavailable for unconstrained real-world scenes.\\nAnother class of methods use volumetric representations to address the task\\nof high-quality photorealistic view synthesis from a set of input RGB images.\\nVolumetric approaches are able to realistically represent complex shapes and\\nmaterials, are well-suited for gradient-based optimization, and tend to produce\\nless visually distracting artifacts than mesh-based methods. Early volumetric\\napproaches used observed images to directly color voxel grids [19,40,45]. More\\nrecently, several methods [9,13,17,28,33,43,46,52] have used large datasets of mul-\\ntiple scenes to train deep networks that predict a sampled volumetric represen-\\ntation from a set of input images, and then use either alpha-compositing [34] or\\nlearned compositing along rays to render novel views at test time. Other works\\nhave optimized a combination of convolutional networks (CNNs) and sampled\\nvoxel grids for each speciﬁc scene, such that the CNN can compensate for dis-\\ncretization artifacts from low resolution voxel grids [41] or allow the predicted\\nvoxel grids to vary based on input time or animation controls [24]. While these\\nvolumetric techniques have achieved impressive results for novel view synthe-\\nsis, their ability to scale to higher resolution imagery is fundamentally limited\\nby poor time and space complexity due to their discrete sampling — rendering\\nhigher resolution images requires a ﬁner sampling of 3D space. We circumvent\\nthis problem by instead encoding a continuous volume within the parameters\\nof a deep fully-connected neural network, which not only produces signiﬁcantly\\nhigher quality renderings than prior volumetric approaches, but also requires\\njust a fraction of the storage cost of those sampled volumetric representations.\\n3\\nNeural Radiance Field Scene Representation\\nWe represent a continuous scene as a 5D vector-valued function whose input is\\na 3D location x = (x, y, z) and 2D viewing direction (θ, φ), and whose output\\nis an emitted color c = (r, g, b) and volume density σ. In practice, we express\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 3, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n5\\n(x,y,z,θ,ϕ)\\nFΘ\\n(RGBσ)\\n5D Input\\nPosition + Direction\\nOutput\\nColor + Density\\nVolume \\nRendering\\nRay 1\\nσ\\nσ\\nRendering\\nLoss\\ng.t.\\ng.t.\\n2\\n2\\n2\\n2\\nRay 2\\nRay 1\\nRay Distance\\n(b)\\n(a)\\n(c)\\n(d)\\nRay 2\\nFig. 2: An overview of our neural radiance ﬁeld scene representation and diﬀer-\\nentiable rendering procedure. We synthesize images by sampling 5D coordinates\\n(location and viewing direction) along camera rays (a), feeding those locations\\ninto an MLP to produce a color and volume density (b), and using volume ren-\\ndering techniques to composite these values into an image (c). This rendering\\nfunction is diﬀerentiable, so we can optimize our scene representation by mini-\\nmizing the residual between synthesized and ground truth observed images (d).\\ndirection as a 3D Cartesian unit vector d. We approximate this continuous 5D\\nscene representation with an MLP network FΘ : (x, d) → (c, σ) and optimize its\\nweights Θ to map from each input 5D coordinate to its corresponding volume\\ndensity and directional emitted color.\\nWe encourage the representation to be multiview consistent by restricting\\nthe network to predict the volume density σ as a function of only the location\\nx, while allowing the RGB color c to be predicted as a function of both location\\nand viewing direction. To accomplish this, the MLP FΘ ﬁrst processes the input\\n3D coordinate x with 8 fully-connected layers (using ReLU activations and 256\\nchannels per layer), and outputs σ and a 256-dimensional feature vector. This\\nfeature vector is then concatenated with the camera ray’s viewing direction and\\npassed to one additional fully-connected layer (using a ReLU activation and 128\\nchannels) that output the view-dependent RGB color.\\nSee Fig. 3 for an example of how our method uses the input viewing direction\\nto represent non-Lambertian eﬀects. As shown in Fig. 4, a model trained without\\nview dependence (only x as input) has diﬃculty representing specularities.\\n4\\nVolume Rendering with Radiance Fields\\nOur 5D neural radiance ﬁeld represents a scene as the volume density and di-\\nrectional emitted radiance at any point in space. We render the color of any ray\\npassing through the scene using principles from classical volume rendering [16].\\nThe volume density σ(x) can be interpreted as the diﬀerential probability of a\\nray terminating at an inﬁnitesimal particle at location x. The expected color\\nC(r) of camera ray r(t) = o + td with near and far bounds tn and tf is:\\nC(r) =\\n� tf\\ntn\\nT(t)σ(r(t))c(r(t), d)dt , where T(t) = exp\\n�\\n−\\n� t\\ntn\\nσ(r(s))ds\\n�\\n. (1)\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 4, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='6\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\n(a) View 1\\n(b) View 2\\n(c) Radiance Distributions\\nFig. 3: A visualization of view-dependent emitted radiance. Our neural radiance\\nﬁeld representation outputs RGB color as a 5D function of both spatial position\\nx and viewing direction d. Here, we visualize example directional color distri-\\nbutions for two spatial locations in our neural representation of the Ship scene.\\nIn (a) and (b), we show the appearance of two ﬁxed 3D points from two dif-\\nferent camera positions: one on the side of the ship (orange insets) and one on\\nthe surface of the water (blue insets). Our method predicts the changing spec-\\nular appearance of these two 3D points, and in (c) we show how this behavior\\ngeneralizes continuously across the whole hemisphere of viewing directions.\\nThe function T(t) denotes the accumulated transmittance along the ray from\\ntn to t, i.e., the probability that the ray travels from tn to t without hitting\\nany other particle. Rendering a view from our continuous neural radiance ﬁeld\\nrequires estimating this integral C(r) for a camera ray traced through each pixel\\nof the desired virtual camera.\\nWe numerically estimate this continuous integral using quadrature. Deter-\\nministic quadrature, which is typically used for rendering discretized voxel grids,\\nwould eﬀectively limit our representation’s resolution because the MLP would\\nonly be queried at a ﬁxed discrete set of locations. Instead, we use a stratiﬁed\\nsampling approach where we partition [tn, tf] into N evenly-spaced bins and\\nthen draw one sample uniformly at random from within each bin:\\nti ∼ U\\n�\\ntn + i − 1\\nN\\n(tf − tn), tn + i\\nN (tf − tn)\\n�\\n.\\n(2)\\nAlthough we use a discrete set of samples to estimate the integral, stratiﬁed\\nsampling enables us to represent a continuous scene representation because it\\nresults in the MLP being evaluated at continuous positions over the course of\\noptimization. We use these samples to estimate C(r) with the quadrature rule\\ndiscussed in the volume rendering review by Max [26]:\\nˆC(r) =\\nN\\n�\\ni=1\\nTi(1 − exp(−σiδi))ci , where Ti = exp\\n\\uf8eb\\n\\uf8ed−\\ni−1\\n�\\nj=1\\nσjδj\\n\\uf8f6\\n\\uf8f8 ,\\n(3)\\nwhere δi = ti+1 − ti is the distance between adjacent samples. This function\\nfor calculating ˆC(r) from the set of (ci, σi) values is trivially diﬀerentiable and\\nreduces to traditional alpha compositing with alpha values αi = 1 − exp(−σiδi).\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 5, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n7\\nGround Truth\\nComplete Model\\nNo View Dependence\\nNo Positional Encoding\\nFig. 4: Here we visualize how our full model beneﬁts from representing view-\\ndependent emitted radiance and from passing our input coordinates through\\na high-frequency positional encoding. Removing view dependence prevents the\\nmodel from recreating the specular reﬂection on the bulldozer tread. Removing\\nthe positional encoding drastically decreases the model’s ability to represent high\\nfrequency geometry and texture, resulting in an oversmoothed appearance.\\n5\\nOptimizing a Neural Radiance Field\\nIn the previous section we have described the core components necessary for\\nmodeling a scene as a neural radiance ﬁeld and rendering novel views from this\\nrepresentation. However, we observe that these components are not suﬃcient for\\nachieving state-of-the-art quality, as demonstrated in Section 6.4). We introduce\\ntwo improvements to enable representing high-resolution complex scenes. The\\nﬁrst is a positional encoding of the input coordinates that assists the MLP in\\nrepresenting high-frequency functions, and the second is a hierarchical sampling\\nprocedure that allows us to eﬃciently sample this high-frequency representation.\\n5.1\\nPositional encoding\\nDespite the fact that neural networks are universal function approximators [14],\\nwe found that having the network FΘ directly operate on xyzθφ input coordi-\\nnates results in renderings that perform poorly at representing high-frequency\\nvariation in color and geometry. This is consistent with recent work by Rahaman\\net al. [35], which shows that deep networks are biased towards learning lower fre-\\nquency functions. They additionally show that mapping the inputs to a higher\\ndimensional space using high frequency functions before passing them to the\\nnetwork enables better ﬁtting of data that contains high frequency variation.\\nWe leverage these ﬁndings in the context of neural scene representations, and\\nshow that reformulating FΘ as a composition of two functions FΘ = F ′\\nΘ ◦ γ, one\\nlearned and one not, signiﬁcantly improves performance (see Fig. 4 and Table 2).\\nHere γ is a mapping from R into a higher dimensional space R2L, and F ′\\nΘ is still\\nsimply a regular MLP. Formally, the encoding function we use is:\\nγ(p) =\\n�\\nsin\\n�\\n20πp\\n�\\n, cos\\n�\\n20πp\\n�\\n, · · · , sin\\n�\\n2L−1πp\\n�\\n, cos\\n�\\n2L−1πp\\n��\\n.\\n(4)\\nThis function γ(·) is applied separately to each of the three coordinate values\\nin x (which are normalized to lie in [−1, 1]) and to the three components of the\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 6, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='8\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nCartesian viewing direction unit vector d (which by construction lie in [−1, 1]).\\nIn our experiments, we set L = 10 for γ(x) and L = 4 for γ(d).\\nA similar mapping is used in the popular Transformer architecture [47], where\\nit is referred to as a positional encoding. However, Transformers use it for a\\ndiﬀerent goal of providing the discrete positions of tokens in a sequence as input\\nto an architecture that does not contain any notion of order. In contrast, we use\\nthese functions to map continuous input coordinates into a higher dimensional\\nspace to enable our MLP to more easily approximate a higher frequency function.\\nConcurrent work on a related problem of modeling 3D protein structure from\\nprojections [51] also utilizes a similar input coordinate mapping.\\n5.2\\nHierarchical volume sampling\\nOur rendering strategy of densely evaluating the neural radiance ﬁeld network\\nat N query points along each camera ray is ineﬃcient: free space and occluded\\nregions that do not contribute to the rendered image are still sampled repeat-\\nedly. We draw inspiration from early work in volume rendering [20] and propose\\na hierarchical representation that increases rendering eﬃciency by allocating\\nsamples proportionally to their expected eﬀect on the ﬁnal rendering.\\nInstead of just using a single network to represent the scene, we simultane-\\nously optimize two networks: one “coarse” and one “ﬁne”. We ﬁrst sample a set\\nof Nc locations using stratiﬁed sampling, and evaluate the “coarse” network at\\nthese locations as described in Eqns. 2 and 3. Given the output of this “coarse”\\nnetwork, we then produce a more informed sampling of points along each ray\\nwhere samples are biased towards the relevant parts of the volume. To do this,\\nwe ﬁrst rewrite the alpha composited color from the coarse network ˆCc(r) in\\nEqn. 3 as a weighted sum of all sampled colors ci along the ray:\\nˆCc(r) =\\nNc\\n�\\ni=1\\nwici ,\\nwi = Ti(1 − exp(−σiδi)) .\\n(5)\\nNormalizing these weights as ˆwi = wi/�Nc\\nj=1 wj produces a piecewise-constant\\nPDF along the ray. We sample a second set of Nf locations from this distribution\\nusing inverse transform sampling, evaluate our “ﬁne” network at the union of the\\nﬁrst and second set of samples, and compute the ﬁnal rendered color of the ray\\nˆCf(r) using Eqn. 3 but using all Nc+Nf samples. This procedure allocates more\\nsamples to regions we expect to contain visible content. This addresses a similar\\ngoal as importance sampling, but we use the sampled values as a nonuniform\\ndiscretization of the whole integration domain rather than treating each sample\\nas an independent probabilistic estimate of the entire integral.\\n5.3\\nImplementation details\\nWe optimize a separate neural continuous volume representation network for\\neach scene. This requires only a dataset of captured RGB images of the scene,\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 7, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n9\\nthe corresponding camera poses and intrinsic parameters, and scene bounds\\n(we use ground truth camera poses, intrinsics, and bounds for synthetic data,\\nand use the COLMAP structure-from-motion package [39] to estimate these\\nparameters for real data). At each optimization iteration, we randomly sample\\na batch of camera rays from the set of all pixels in the dataset, and then follow\\nthe hierarchical sampling described in Sec. 5.2 to query Nc samples from the\\ncoarse network and Nc + Nf samples from the ﬁne network. We then use the\\nvolume rendering procedure described in Sec. 4 to render the color of each ray\\nfrom both sets of samples. Our loss is simply the total squared error between\\nthe rendered and true pixel colors for both the coarse and ﬁne renderings:\\nL =\\n�\\nr∈R\\n���� ˆCc(r) − C(r)\\n���\\n2\\n2 +\\n��� ˆCf(r) − C(r)\\n���\\n2\\n2\\n�\\n(6)\\nwhere R is the set of rays in each batch, and C(r), ˆCc(r), and ˆCf(r) are the\\nground truth, coarse volume predicted, and ﬁne volume predicted RGB colors\\nfor ray r respectively. Note that even though the ﬁnal rendering comes from\\nˆCf(r), we also minimize the loss of ˆCc(r) so that the weight distribution from\\nthe coarse network can be used to allocate samples in the ﬁne network.\\nIn our experiments, we use a batch size of 4096 rays, each sampled at Nc = 64\\ncoordinates in the coarse volume and Nf = 128 additional coordinates in the\\nﬁne volume. We use the Adam optimizer [18] with a learning rate that begins at\\n5 × 10−4 and decays exponentially to 5 × 10−5 over the course of optimization\\n(other Adam hyperparameters are left at default values of β1 = 0.9, β2 = 0.999,\\nand ϵ = 10−7). The optimization for a single scene typically take around 100–\\n300k iterations to converge on a single NVIDIA V100 GPU (about 1–2 days).\\n6\\nResults\\nWe quantitatively (Tables 1) and qualitatively (Figs. 8 and 6) show that our\\nmethod outperforms prior work, and provide extensive ablation studies to vali-\\ndate our design choices (Table 2). We urge the reader to view our supplementary\\nvideo to better appreciate our method’s signiﬁcant improvement over baseline\\nmethods when rendering smooth paths of novel views.\\n6.1\\nDatasets\\nSynthetic renderings of objects We ﬁrst show experimental results on two\\ndatasets of synthetic renderings of objects (Table 1, “Diﬀuse Synthetic 360◦” and\\n“Realistic Synthetic 360◦”). The DeepVoxels [41] dataset contains four Lamber-\\ntian objects with simple geometry. Each object is rendered at 512 × 512 pixels\\nfrom viewpoints sampled on the upper hemisphere (479 as input and 1000 for\\ntesting). We additionally generate our own dataset containing pathtraced images\\nof eight objects that exhibit complicated geometry and realistic non-Lambertian\\nmaterials. Six are rendered from viewpoints sampled on the upper hemisphere,\\nand two are rendered from viewpoints sampled on a full sphere. We render 100\\nviews of each scene as input and 200 for testing, all at 800 × 800 pixels.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 8, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='10\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nDiﬀuse Synthetic 360◦ [41]\\nRealistic Synthetic 360◦\\nReal Forward-Facing [28]\\nMethod\\nPSNR↑\\nSSIM↑\\nLPIPS↓\\nPSNR↑\\nSSIM↑\\nLPIPS↓\\nPSNR↑\\nSSIM↑\\nLPIPS↓\\nSRN [42]\\n33.20\\n0.963\\n0.073\\n22.26\\n0.846\\n0.170\\n22.84\\n0.668\\n0.378\\nNV [24]\\n29.62\\n0.929\\n0.099\\n26.05\\n0.893\\n0.160\\n-\\n-\\n-\\nLLFF [28]\\n34.38\\n0.985\\n0.048\\n24.88\\n0.911\\n0.114\\n24.13\\n0.798\\n0.212\\nOurs\\n40.15\\n0.991\\n0.023\\n31.01\\n0.947\\n0.081\\n26.50\\n0.811\\n0.250\\nTable 1: Our method quantitatively outperforms prior work on datasets of\\nboth synthetic and real images. We report PSNR/SSIM (higher is better) and\\nLPIPS [50] (lower is better). The DeepVoxels [41] dataset consists of 4 diﬀuse ob-\\njects with simple geometry. Our realistic synthetic dataset consists of pathtraced\\nrenderings of 8 geometrically complex objects with complex non-Lambertian ma-\\nterials. The real dataset consists of handheld forward-facing captures of 8 real-\\nworld scenes (NV cannot be evaluated on this data because it only reconstructs\\nobjects inside a bounded volume). Though LLFF achieves slightly better LPIPS,\\nwe urge readers to view our supplementary video where our method achieves\\nbetter multiview consistency and produces fewer artifacts than all baselines.\\nReal images of complex scenes We show results on complex real-world\\nscenes captured with roughly forward-facing images (Table 1, “Real Forward-\\nFacing”). This dataset consists of 8 scenes captured with a handheld cellphone\\n(5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62\\nimages, and hold out 1/8 of these for the test set. All images are 1008×756 pixels.\\n6.2\\nComparisons\\nTo evaluate our model we compare against current top-performing techniques\\nfor view synthesis, detailed below. All methods use the same set of input views\\nto train a separate network for each scene except Local Light Field Fusion [28],\\nwhich trains a single 3D convolutional network on a large dataset, then uses the\\nsame trained network to process input images of new scenes at test time.\\nNeural Volumes (NV) [24] synthesizes novel views of objects that lie en-\\ntirely within a bounded volume in front of a distinct background (which must\\nbe separately captured without the object of interest). It optimizes a deep 3D\\nconvolutional network to predict a discretized RGBα voxel grid with 1283 sam-\\nples as well as a 3D warp grid with 323 samples. The algorithm renders novel\\nviews by marching camera rays through the warped voxel grid.\\nScene Representation Networks (SRN) [42] represent a continuous scene\\nas an opaque surface, implicitly deﬁned by a MLP that maps each (x, y, z) co-\\nordinate to a feature vector. They train a recurrent neural network to march\\nalong a ray through the scene representation by using the feature vector at any\\n3D coordinate to predict the next step size along the ray. The feature vector\\nfrom the ﬁnal step is decoded into a single color for that point on the surface.\\nNote that SRN is a better-performing followup to DeepVoxels [41] by the same\\nauthors, which is why we do not include comparisons to DeepVoxels.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 9, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n11\\nShip\\nLego\\nMicrophone\\nMaterials\\nGround Truth\\nNeRF (ours)\\nLLFF [28]\\nSRN [42]\\nNV [24]\\nFig. 5: Comparisons on test-set views for scenes from our new synthetic dataset\\ngenerated with a physically-based renderer. Our method is able to recover ﬁne\\ndetails in both geometry and appearance, such as Ship’s rigging, Lego’s gear\\nand treads, Microphone’s shiny stand and mesh grille, and Material’s non-\\nLambertian reﬂectance. LLFF exhibits banding artifacts on the Microphone\\nstand and Material’s object edges and ghosting artifacts in Ship’s mast and\\ninside the Lego object. SRN produces blurry and distorted renderings in every\\ncase. Neural Volumes cannot capture the details on the Microphone’s grille or\\nLego’s gears, and it completely fails to recover the geometry of Ship’s rigging.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 10, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='12\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nFern\\nT-Rex\\nOrchid\\nGround Truth\\nNeRF (ours)\\nLLFF [28]\\nSRN [42]\\nFig. 6: Comparisons on test-set views of real world scenes. LLFF is speciﬁcally\\ndesigned for this use case (forward-facing captures of real scenes). Our method\\nis able to represent ﬁne geometry more consistently across rendered views than\\nLLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex.\\nOur method also correctly reconstructs partially occluded regions that LLFF\\nstruggles to render cleanly, such as the yellow shelves behind the leaves in the\\nbottom Fern crop and green leaves in the background of the bottom Orchid crop.\\nBlending between multiples renderings can also cause repeated edges in LLFF,\\nas seen in the top Orchid crop. SRN captures the low-frequency geometry and\\ncolor variation in each scene but is unable to reproduce any ﬁne detail.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 11, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n13\\nLocal Light Field Fusion (LLFF) [28] LLFF is designed for producing pho-\\ntorealistic novel views for well-sampled forward facing scenes. It uses a trained 3D\\nconvolutional network to directly predict a discretized frustum-sampled RGBα\\ngrid (multiplane image or MPI [52]) for each input view, then renders novel\\nviews by alpha compositing and blending nearby MPIs into the novel viewpoint.\\n6.3\\nDiscussion\\nWe thoroughly outperform both baselines that also optimize a separate network\\nper scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively\\nand quantitatively superior renderings compared to LLFF (across all except one\\nmetric) while using only their input images as our entire training set.\\nThe SRN method produces heavily smoothed geometry and texture, and its\\nrepresentational power for view synthesis is limited by selecting only a single\\ndepth and color per camera ray. The NV baseline is able to capture reasonably\\ndetailed volumetric geometry and appearance, but its use of an underlying ex-\\nplicit 1283 voxel grid prevents it from scaling to represent ﬁne details at high\\nresolutions. LLFF speciﬁcally provides a “sampling guideline” to not exceed 64\\npixels of disparity between input views, so it frequently fails to estimate cor-\\nrect geometry in the synthetic datasets which contain up to 400-500 pixels of\\ndisparity between views. Additionally, LLFF blends between diﬀerent scene rep-\\nresentations for rendering diﬀerent views, resulting in perceptually-distracting\\ninconsistency as is apparent in our supplementary video.\\nThe biggest practical tradeoﬀs between these methods are time versus space.\\nAll compared single scene methods take at least 12 hours to train per scene. In\\ncontrast, LLFF can process a small input dataset in under 10 minutes. However,\\nLLFF produces a large 3D voxel grid for every input image, resulting in enor-\\nmous storage requirements (over 15GB for one “Realistic Synthetic” scene). Our\\nmethod requires only 5 MB for the network weights (a relative compression of\\n3000× compared to LLFF), which is even less memory than the input images\\nalone for a single scene from any of our datasets.\\n6.4\\nAblation studies\\nWe validate our algorithm’s design choices and parameters with an extensive\\nablation study in Table 2. We present results on our “Realistic Synthetic 360◦”\\nscenes. Row 9 shows our complete model as a point of reference. Row 1 shows\\na minimalist version of our model without positional encoding (PE), view-\\ndependence (VD), or hierarchical sampling (H). In rows 2–4 we remove these\\nthree components one at a time from the full model, observing that positional\\nencoding (row 2) and view-dependence (row 3) provide the largest quantitative\\nbeneﬁt followed by hierarchical sampling (row 4). Rows 5–6 show how our per-\\nformance decreases as the number of input images is reduced. Note that our\\nmethod’s performance using only 25 input images still exceeds NV, SRN, and\\nLLFF across all metrics when they are provided with 100 images (see supple-\\nmentary material). In rows 7–8 we validate our choice of the maximum frequency\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 12, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='14\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nInput\\n#Im.\\nL\\n( Nc , Nf )\\nPSNR↑\\nSSIM↑\\nLPIPS↓\\n1) No PE, VD, H\\nxyz\\n100\\n-\\n(256, - )\\n26.67\\n0.906\\n0.136\\n2) No Pos. Encoding\\nxyzθφ\\n100\\n-\\n(64, 128)\\n28.77\\n0.924\\n0.108\\n3) No View Dependence\\nxyz\\n100\\n10\\n(64, 128)\\n27.66\\n0.925\\n0.117\\n4) No Hierarchical\\nxyzθφ\\n100\\n10\\n(256, - )\\n30.06\\n0.938\\n0.109\\n5) Far Fewer Images\\nxyzθφ\\n25\\n10\\n(64, 128)\\n27.78\\n0.925\\n0.107\\n6) Fewer Images\\nxyzθφ\\n50\\n10\\n(64, 128)\\n29.79\\n0.940\\n0.096\\n7) Fewer Frequencies\\nxyzθφ\\n100\\n5\\n(64, 128)\\n30.59\\n0.944\\n0.088\\n8) More Frequencies\\nxyzθφ\\n100\\n15\\n(64, 128)\\n30.81\\n0.946\\n0.096\\n9) Complete Model\\nxyzθφ\\n100\\n10\\n(64, 128)\\n31.01\\n0.947\\n0.081\\nTable 2: An ablation study of our model. Metrics are averaged over the 8 scenes\\nfrom our realistic synthetic dataset. See Sec. 6.4 for detailed descriptions.\\nL used in our positional encoding for x (the maximum frequency used for d is\\nscaled proportionally). Only using 5 frequencies reduces performance, but in-\\ncreasing the number of frequencies from 10 to 15 does not improve performance.\\nWe believe the beneﬁt of increasing L is limited once 2L exceeds the maximum\\nfrequency present in the sampled input images (roughly 1024 in our data).\\n7\\nConclusion\\nOur work directly addresses deﬁciencies of prior work that uses MLPs to repre-\\nsent objects and scenes as continuous functions. We demonstrate that represent-\\ning scenes as 5D neural radiance ﬁelds (an MLP that outputs volume density and\\nview-dependent emitted radiance as a function of 3D location and 2D viewing\\ndirection) produces better renderings than the previously-dominant approach of\\ntraining deep convolutional networks to output discretized voxel representations.\\nAlthough we have proposed a hierarchical sampling strategy to make render-\\ning more sample-eﬃcient (for both training and testing), there is still much more\\nprogress to be made in investigating techniques to eﬃciently optimize and ren-\\nder neural radiance ﬁelds. Another direction for future work is interpretability:\\nsampled representations such as voxel grids and meshes admit reasoning about\\nthe expected quality of rendered views and failure modes, but it is unclear how\\nto analyze these issues when we encode scenes in the weights of a deep neural\\nnetwork. We believe that this work makes progress towards a graphics pipeline\\nbased on real world imagery, where complex scenes could be composed of neural\\nradiance ﬁelds optimized from images of actual objects and scenes.\\nAcknowledgements We thank Kevin Cao, Guowei Frank Yang, and Nithin\\nRaghavan for comments and discussions. RR acknowledges funding from ONR\\ngrants N000141712687 and N000142012529 and the Ronald L. Graham Chair.\\nBM is funded by a Hertz Foundation Fellowship, and MT is funded by an\\nNSF Graduate Fellowship. Google provided a generous donation of cloud com-\\npute credits through the BAIR Commons program. We thank the following\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 13, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n15\\nBlend Swap users for the models used in our realistic synthetic dataset: gregzaal\\n(ship), 1DInc (chair), bryanajones (drums), Herberhold (ﬁcus), erickfree (hot-\\ndog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic).\\nReferences\\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\\nJ., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\\nVi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015)\\n2. Buehler, C., Bosse, M., McMillan, L., Gortler, S., Cohen, M.: Unstructured lumi-\\ngraph rendering. In: SIGGRAPH (2001)\\n3. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\\nSavarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich\\n3d model repository. arXiv:1512.03012 (2015)\\n4. Chen, W., Gao, J., Ling, H., Smith, E.J., Lehtinen, J., Jacobson, A., Fidler, S.:\\nLearning to predict 3D objects with an interpolation-based diﬀerentiable renderer.\\nIn: NeurIPS (2019)\\n5. Cohen, M., Gortler, S.J., Szeliski, R., Grzeszczuk, R., Szeliski, R.: The lumigraph.\\nIn: SIGGRAPH (1996)\\n6. Curless, B., Levoy, M.: A volumetric method for building complex models from\\nrange images. In: SIGGRAPH (1996)\\n7. Davis, A., Levoy, M., Durand, F.: Unstructured light ﬁelds. In: Eurographics (2012)\\n8. Debevec, P., Taylor, C.J., Malik, J.: Modeling and rendering architecture from pho-\\ntographs: A hybrid geometry-and image-based approach. In: SIGGRAPH (1996)\\n9. Flynn, J., Broxton, M., Debevec, P., DuVall, M., Fyﬀe, G., Overbeck, R., Snavely,\\nN., Tucker, R.: DeepView: view synthesis with learned gradient descent. In: CVPR\\n(2019)\\n10. Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., , Freeman, W.T.: Un-\\nsupervised training for 3D morphable model regression. In: CVPR (2018)\\n11. Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit\\nfunctions for 3d shape. In: CVPR (2020)\\n12. Henzler, P., Mitra, N.J., Ritschel, T.: Learning a neural 3d texture space from 2d\\nexemplars. In: CVPR (2020)\\n13. Henzler, P., Rasche, V., Ropinski, T., Ritschel, T.: Single-image tomography: 3d\\nvolumes from 2d cranial x-rays. In: Eurographics (2018)\\n14. Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are uni-\\nversal approximators. Neural Networks (1989)\\n15. Jiang, C., Sud, A., Makadia, A., Huang, J., Nießner, M., Funkhouser, T.: Local\\nimplicit grid representations for 3d scenes. In: CVPR (2020)\\n16. Kajiya, J.T., Herzen, B.P.V.: Ray tracing volume densities. Computer Graphics\\n(SIGGRAPH) (1984)\\n17. Kar, A., H¨ane, C., Malik, J.: Learning a multi-view stereo machine. In: NeurIPS\\n(2017)\\n18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR\\n(2015)\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 14, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='16\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\n19. Kutulakos, K.N., Seitz, S.M.: A theory of shape by space carving. International\\nJournal of Computer Vision (2000)\\n20. Levoy, M.: Eﬃcient ray tracing of volume data. ACM Transactions on Graphics\\n(1990)\\n21. Levoy, M., Hanrahan, P.: Light ﬁeld rendering. In: SIGGRAPH (1996)\\n22. Li, T.M., Aittala, M., Durand, F., Lehtinen, J.: Diﬀerentiable monte carlo ray\\ntracing through edge sampling. ACM Transactions on Graphics (SIGGRAPH Asia)\\n(2018)\\n23. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A diﬀerentiable renderer for image-\\nbased 3D reasoning. In: ICCV (2019)\\n24. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.:\\nNeural volumes: Learning dynamic renderable volumes from images. ACM Trans-\\nactions on Graphics (SIGGRAPH) (2019)\\n25. Loper, M.M., Black, M.J.: OpenDR: An approximate diﬀerentiable renderer. In:\\nECCV (2014)\\n26. Max, N.: Optical models for direct volume rendering. IEEE Transactions on Visu-\\nalization and Computer Graphics (1995)\\n27. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy\\nnetworks: Learning 3D reconstruction in function space. In: CVPR (2019)\\n28. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi,\\nR., Ng, R., Kar, A.: Local light ﬁeld fusion: Practical view synthesis with prescrip-\\ntive sampling guidelines. ACM Transactions on Graphics (SIGGRAPH) (2019)\\n29. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Diﬀerentiable volumetric\\nrendering: Learning implicit 3D representations without 3D supervision. In: CVPR\\n(2019)\\n30. Nimier-David, M., Vicini, D., Zeltner, T., Jakob, W.: Mitsuba 2: A retargetable\\nforward and inverse renderer. ACM Transactions on Graphics (SIGGRAPH Asia)\\n(2019)\\n31. Oechsle, M., Mescheder, L., Niemeyer, M., Strauss, T., Geiger, A.: Texture ﬁelds:\\nLearning texture representations in function space. In: ICCV (2019)\\n32. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF: Learn-\\ning continuous signed distance functions for shape representation. In: CVPR (2019)\\n33. Penner, E., Zhang, L.: Soft 3D reconstruction for view synthesis. ACM Transactions\\non Graphics (SIGGRAPH Asia) (2017)\\n34. Porter, T., Duﬀ, T.: Compositing digital images. Computer Graphics (SIG-\\nGRAPH) (1984)\\n35. Rahaman, N., Baratin, A., Arpit, D., Dr¨axler, F., Lin, M., Hamprecht, F.A., Ben-\\ngio, Y., Courville, A.C.: On the spectral bias of neural networks. In: ICML (2018)\\n36. Rainer, G., Ghosh, A., Jakob, W., Weyrich, T.: Uniﬁed neural encoding of BTFs.\\nComputer Graphics Forum (Eurographics) (2020)\\n37. Rainer, G., Jakob, W., Ghosh, A., Weyrich, T.: Neural BTF compression and\\ninterpolation. Computer Graphics Forum (Eurographics) (2019)\\n38. Ren, P., Wang, J., Gong, M., Lin, S., Tong, X., Guo, B.: Global illumination with\\nradiance regression functions. ACM Transactions on Graphics (2013)\\n39. Sch¨onberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: CVPR (2016)\\n40. Seitz, S.M., Dyer, C.R.: Photorealistic scene reconstruction by voxel coloring. In-\\nternational Journal of Computer Vision (1999)\\n41. Sitzmann, V., Thies, J., Heide, F., Nießner, M., Wetzstein, G., Zollh¨ofer, M.: Deep-\\nvoxels: Learning persistent 3D feature embeddings. In: CVPR (2019)\\n42. Sitzmann, V., Zollhoefer, M., Wetzstein, G.: Scene representation networks: Con-\\ntinuous 3D-structure-aware neural scene representations. In: NeurIPS (2019)\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 15, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n17\\n43. Srinivasan, P.P., Tucker, R., Barron, J.T., Ramamoorthi, R., Ng, R., Snavely, N.:\\nPushing the boundaries of view extrapolation with multiplane images. In: CVPR\\n(2019)\\n44. Stanley, K.O.: Compositional pattern producing networks: A novel abstraction of\\ndevelopment. Genetic programming and evolvable machines (2007)\\n45. Szeliski, R., Golland, P.: Stereo matching with transparency and matting. In: ICCV\\n(1998)\\n46. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view\\nreconstruction via diﬀerentiable ray consistency. In: CVPR (2017)\\n47. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\\n�L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\\n48. Waechter, M., Moehrle, N., Goesele, M.: Let there be color! Large-scale texturing\\nof 3D reconstructions. In: ECCV (2014)\\n49. Wood, D.N., Azuma, D.I., Aldinger, K., Curless, B., Duchamp, T., Salesin, D.H.,\\nStuetzle, W.: Surface light ﬁelds for 3D photography. In: SIGGRAPH (2000)\\n50. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable\\neﬀectiveness of deep features as a perceptual metric. In: CVPR (2018)\\n51. Zhong, E.D., Bepler, T., Davis, J.H., Berger, B.: Reconstructing continuous distri-\\nbutions of 3D protein structure from cryo-EM images. In: ICLR (2020)\\n52. Zhou, T., Tucker, R., Flynn, J., Fyﬀe, G., Snavely, N.: Stereo magniﬁcation: Learn-\\ning view synthesis using multiplane images. ACM Transactions on Graphics (SIG-\\nGRAPH) (2018)\\nA\\nAdditional Implementation Details\\nNetwork Architecture Fig. 7 details our simple fully-connected architecture.\\nVolume Bounds Our method renders views by querying the neural radiance\\nﬁeld representation at continuous 5D coordinates along camera rays. For exper-\\niments with synthetic images, we scale the scene so that it lies within a cube of\\nside length 2 centered at the origin, and only query the representation within\\nthis bounding volume. Our dataset of real images contains content that can ex-\\nist anywhere between the closest point and inﬁnity, so we use normalized device\\ncoordinates to map the depth range of these points into [−1, 1]. This shifts all\\nthe ray origins to the near plane of the scene, maps the perspective rays of the\\ncamera to parallel rays in the transformed volume, and uses disparity (inverse\\ndepth) instead of metric depth, so all coordinates are now bounded.\\nTraining Details For real scene data, we regularize our network by adding\\nrandom Gaussian noise with zero mean and unit variance to the output σ values\\n(before passing them through the ReLU) during optimization, ﬁnding that this\\nslightly improves visual performance for rendering novel views. We implement\\nour model in Tensorﬂow [1].\\nRendering Details To render new views at test time, we sample 64 points per\\nray through the coarse network and 64 + 128 = 192 points per ray through the\\nﬁne network, for a total of 256 network queries per ray. Our realistic synthetic\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 16, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='18\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nRGB\\nγ(x)\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"C39OhB+IczRcjLNINXH2\\n9e9lt8M=\">AB2HicbZDNSgMxFIXv1L86Vq1rN8EiuCpTN+pOcOygmML7VAymTtaCYzJHeEMvQFXLhRf\\nDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0gMRa5y04+5RSU1hiRJYb8wyLNY\\nYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUmwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUic+8P\\nSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS6iSuigJtVh9lJaKUc4\\nWO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ90w4eAqjDKZzBXTgCm7hHroQgoAEXuDNm3iv3vuqp\\nq37uwEfsn7+Aap5IoM</latexit>\\n<latexit sha1_base64=\"hQhj+CULGdG3QS0LVa1K\\nRUGPisw=\">AB73icbVA9T8MwFHwpX6UCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/\\n4aNf4PTdoCWkyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGU57qaJY\\nRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQ2kyjOZ8XF0G16bW8BtEn8FWnCp2h+zUYJSQ\\nTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLSei8hOlhH1uleK/3n\\n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxdVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS\\n3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw=</latexit>\\n<latexit sha1_base64=\"hQhj+CULGdG3QS0LVa1K\\nRUGPisw=\">AB73icbVA9T8MwFHwpX6UCF1ZLCqkslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/\\n4aNf4PTdoCWkyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGU57qaJY\\nRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLARuDQ2kyjOZ8XF0G16bW8BtEn8FWnCp2h+zUYJSQ\\nTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLSei8hOlhH1uleK/3n\\n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZEZlghYmxdVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS\\n3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw=</latexit>\\n<latexit sha1_base64=\"Wrp6sfGRkT1YIiwCAWhs\\nM6HtC1M=\">AB+nicbVA9T8MwFHzhs5SvUEaWiAqpLFXKAmwVLIxFIrRSE1WO67RWbSeyHdQqyl9hYQDEy\\ni9h49/gtBmg5SRLp7v39M4XJowq7brf1tr6xubWdmWnuru3f3BoH9UeVZxKTDwcs1j2QqQIo4J4mpGeok\\niIeMdMPJbeF3n4hUNBYPepaQgKORoBHFSBtpYNf8EeIcNXyO9DiMsml+PrDrbtOdw1klrZLUoURnYH/5wxi\\nnAiNGVKq3ITHWRIaoZyat+qkiC8ASNSN9QgThRQTbPnjtnRhk6USzNE9qZq783MsSVmvHQTBYR1bJXiP9\\n5/VRHV0FGRZJqIvDiUJQyR8dOUYQzpJgzWaGICypyergMZIa1NX1ZTQWv7yKvEumtdN96t2/KNipwAq\\nfQgBZcQhvuoAMeYJjCM7zCm5VbL9a79bEYXbPKnWP4A+vzBz63lBw=</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmK\\nYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzCvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvf\\nhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0ksk\\nQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujxDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2O\\nciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9\\neP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlNVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4A\\nQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\nγ(x)\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"C39OhB+IczRcjLNINXH29e9lt8M=\">AB2HicbZDNSgMxFIXv1L86Vq1rN8Ei\\nuCpTN+pOcOygmML7VAymTtaCYzJHeEMvQFXLhRfDB3vo3pz0KtBwIf5yTk3hMXSloKgi+vtrW9s7tX3/cPGv7h0XGz8WTz0gMRa5y04+5RSU1hiRJYb8wyLNYS+e3i3y3jMaK3P9SLMCo4yPtUyl4OSs7qjZCtrBUm\\nwTOmtowVqj5ucwyUWZoSahuLWDTlBQVHFDUic+8PSYsHFlI9x4FDzDG1ULcecs3PnJCzNjTua2NL9+aLimbWzLHY3M04T+zdbmP9lg5LS6iSuigJtVh9lJaKUc4WO7NEGhSkZg64MNLNysSEGy7INeO7Djp/N96E8LJ9\\n0w4eAqjDKZzBXTgCm7hHroQgoAEXuDNm3iv3vuqpq37uwEfsn7+Aap5IoM</latexit>\\n<latexit sha1_base64=\"hQhj+CULGdG3QS0LVa1KRUGPisw=\">AB73icbVA9T8MwFHwpX6UCF1ZLCqk\\nslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLA\\nRuDQ2kyjOZ8XF0G16bW8BtEn8FWnCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZ\\nEZlghYmxdVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw=</latexit>\\n<latexit sha1_base64=\"hQhj+CULGdG3QS0LVa1KRUGPisw=\">AB73icbVA9T8MwFHwpX6UCF1ZLCqk\\nslQJC7AhsTAWidBKbVQ5rtNatZ3IdlCrKH+FhQEQ/4aNf4PTdoCWkyd7t7TO1+UcqaN5307la3tnd296n7toH54dOye1J90kilCA5LwRPUirClnkgaGU57qaJYRJx2o+ld6XefqdIskY9mntJQ4LFkMSPYWGnoNgZjLA\\nRuDQ2kyjOZ8XF0G16bW8BtEn8FWnCp2h+zUYJSQTVBrCsdZ930tNmGNlGOG0qA0yTVNMpnhM+5ZKLKgO80X2Ap1bZYTiRNknDVqovzdyLSei8hOlhH1uleK/3n9zMTXYc5kmhkqyfJQnHFkElQWgUZMUWL43BJMFLNZ\\nEZlghYmxdVsCf76lzdJcNm+aXsPHlThFM6gBT5cwS3cQwcCIDCDF3iDd6dwXp2PZVsVZ1VbA/7A+fwB4CiSuw=</latexit>\\n<latexit sha1_base64=\"Wrp6sfGRkT1YIiwCAWhsM6HtC1M=\">AB+nicbVA9T8MwFHzhs5SvUEaWiAqp\\nLFXKAmwVLIxFIrRSE1WO67RWbSeyHdQqyl9hYQDEyi9h49/gtBmg5SRLp7v39M4XJowq7brf1tr6xubWdmWnuru3f3BoH9UeVZxKTDwcs1j2QqQIo4J4mpGeokiIeMdMPJbeF3n4hUNBYPepaQgKORoBHFSBtpYNf8Ee\\nIcNXyO9DiMsml+PrDrbtOdw1klrZLUoURnYH/5wxinAiNGVKq3ITHWRIaoZyat+qkiC8ASNSN9QgThRQTbPnjtnRhk6USzNE9qZq783MsSVmvHQTBYR1bJXiP95/VRHV0FGRZJqIvDiUJQyR8dOUYQzpJgzWaGICyp\\nyergMZIa1NX1ZTQWv7yKvEumtdN96t2/KNipwAqfQgBZcQhvuoAMeYJjCM7zCm5VbL9a79bEYXbPKnWP4A+vzBz63lBw=</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\n<latexit sha1_base64=\"6F05rQ6IUhALFsWYUmKYX8h5zw=\">AB+nicbVBPS8MwHE3nvzn/1Xn0EhzC\\nvIxWBPU29OJxgnWDtYw0S7ewJC1JKhulX8WLBxWvfhJvfhvTrQfdfB4vPf78Xt5YcKo0o7zbVXW1jc2t6rbtZ3dvf0D+7D+qOJUYuLhmMWyFyJFGBXE01Qz0kskQTxkpBtObgu/+0SkorF40LOEByNBI0oRtpIA7vujx\\nDnqOlzpMdhlE3zs4HdcFrOHCVuCVpgBKdgf3lD2OciI0ZkipvuskOsiQ1BQzktf8VJE4Qkakb6hAnGigmyePYenRhnCKJbmCQ3n6u+NDHGlZjw0k0VEtewV4n9eP9XRVZBRkaSaCLw4FKUM6hgWRcAhlQRrNjMEYUlN\\nVojHSCKsTV01U4K7/OV4p23rlvO/UWjfVO2UQXH4AQ0gQsuQRvcgQ7wAZT8AxewZuVWy/Wu/WxGK1Y5c4R+APr8wc/95Qg</latexit>\\nγ(d)\\n<latexit sha1_base64=\"Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=\">AB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4He78fv5cXpowq7Tjf1srq2vrGZm2rvr\\n2zu7dvHzQeVJTDycsET2Q6QIo4J4mpG+qkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kpiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RFibuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlA\\nw=</latexit>\\n<latexit sha1_base64=\"Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=\">AB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4He78fv5cXpowq7Tjf1srq2vrGZm2rvr\\n2zu7dvHzQeVJTDycsET2Q6QIo4J4mpG+qkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kpiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RFibuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlA\\nw=</latexit>\\n<latexit sha1_base64=\"Z+u0Yue3pS9TMvqDyVRY2E+Un5Q=\">AB+nicbVBPS8MwHE39O+e/Oo9egkOYl9GKoN6GXjxOsG6wlpGm6RaWpCVJxVH6Vbx4UPHqJ/HmtzHdetDNB4He78fv5cXpowq7Tjf1srq2vrGZm2rvr\\n2zu7dvHzQeVJTDycsET2Q6QIo4J4mpG+qkiIeM9MLJTen3HolUNBH3epqSgKORoDHFSBtpaDf8EeIctXyO9DiM86g4HdpNp+3MAJeJW5EmqNAd2l9+lOCME6ExQ0oNXCfVQY6kpiRou5niqQIT9CIDAwViBMV5LPsBTwxSgTjRJonNJypvzdyxJWa8tBMlhHVoleK/3mDTMeXQU5Fmki8PxQnDGoE1gWASMqCdZsagjCkpqsEI+RFibuqmBHfxy8vEO2tftZ2782bnumqjBo7AMWgBF1yADrgFXeABDJ7AM3gFb1ZhvVjv1sd8dMWqdg7BH1ifPyGTlA\\nw=</latexit>\\nσ\\n<latexit sha1_base64=\"PHtNjW6na207435B/B6JIWe5ANM=\">AB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJI\\nljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpurvilDNjf/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWE0m5\\nmxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJ5ouSjCOr0PR1GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</la\\ntexit>\\n<latexit sha1_base64=\"PHtNjW6na207435B/B6JIWe5ANM=\">AB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJI\\nljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpurvilDNjf/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWE0m5\\nmxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJ5ouSjCOr0PR1GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</la\\ntexit>\\n<latexit sha1_base64=\"PHtNjW6na207435B/B6JIWe5ANM=\">AB7HicbVDLSgNBEOz1GeMr6tHLYBA8hV0R1FvQi8cIbhJI\\nljA7mU3GzGOZmRXCkn/w4kHFqx/kzb9xkuxBEwsaiqpurvilDNjf/bW1ldW9/YLG2Vt3d29/YrB4dNozJNaEgUV7odY0M5kzS0zHLaTjXFIua0FY9up37riWrDlHyw45RGAg8kSxjB1knNrmEDgXuVql/zZ0DLJChIFQo0epWvbl+RTFBpCcfGdAI/tVGOtWE0m5\\ntexit>mxmaYjLCA9pxVGJBTZTPrp2gU6f0UaK0K2nRTP09kWNhzFjErlNgOzSL3lT8z+tkNrmKcibTzFJ5ouSjCOr0PR1GeaEsvHjmCimbsVkSHWmFgXUNmFECy+vEzC89p1zb+/qNZvijRKcAwncAYBXEId7qABIRB4hGd4hTdPeS/eu/cxb13xipkj+APv8wcIeY72</la\\n+\\n+\\n60\\n256\\n256\\n256\\n256\\n256\\n256\\n256\\n256\\n60\\n24\\n256\\n128\\nFig. 7: A visualization of our fully-connected network architecture. Input vectors\\nare shown in green, intermediate hidden layers are shown in blue, output vectors\\nare shown in red, and the number inside each block signiﬁes the vector’s dimen-\\nsion. All layers are standard fully-connected layers, black arrows indicate layers\\nwith ReLU activations, orange arrows indicate layers with no activation, dashed\\nblack arrows indicate layers with sigmoid activation, and “+” denotes vector\\nconcatenation. The positional encoding of the input location (γ(x)) is passed\\nthrough 8 fully-connected ReLU layers, each with 256 channels. We follow the\\nDeepSDF [32] architecture and include a skip connection that concatenates this\\ninput to the ﬁfth layer’s activation. An additional layer outputs the volume den-\\nsity σ (which is rectiﬁed using a ReLU to ensure that the output volume density\\nis nonnegative) and a 256-dimensional feature vector. This feature vector is con-\\ncatenated with the positional encoding of the input viewing direction (γ(d)),\\nand is processed by an additional fully-connected ReLU layer with 128 channels.\\nA ﬁnal layer (with a sigmoid activation) outputs the emitted RGB radiance at\\nposition x, as viewed by a ray with direction d.\\ndataset requires 640k rays per image, and our real scenes require 762k rays per\\nimage, resulting in between 150 and 200 million network queries per rendered\\nimage. On an NVIDIA V100, this takes approximately 30 seconds per frame.\\nB\\nAdditional Baseline Method Details\\nNeural Volumes (NV) [24] We use the NV code open-sourced by the authors\\nat https://github.com/facebookresearch/neuralvolumes and follow their\\nprocedure for training on a single scene without time dependence.\\nScene Representation Networks (SRN) [42] We use the SRN code open-\\nsourced by the authors at https://github.com/vsitzmann/scene-representation-ne\\nand follow their procedure for training on a single scene.\\nLocal Light Field Fusion (LLFF) [28] We use the pretrained LLFF model\\nopen-sourced by the authors at https://github.com/Fyusion/LLFF.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 17, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n19\\nQuantitative Comparisons The SRN implementation published by the au-\\nthors requires a signiﬁcant amount of GPU memory, and is limited to an image\\nresolution of 512 × 512 pixels even when parallelized across 4 NVIDIA V100\\nGPUs. We compute quantitative metrics for SRN at 512 × 512 pixels for our\\nsynthetic datasets and 504 × 376 pixels for the real datasets, in comparison to\\n800 × 800 and 1008 × 752 respectively for the other methods that can be run at\\nhigher resolutions.\\nC\\nNDC ray space derivation\\nWe reconstruct real scenes with “forward facing” captures in the normalized\\ndevice coordinate (NDC) space that is commonly used as part of the triangle\\nrasterization pipeline. This space is convenient because it preserves parallel lines\\nwhile converting the z axis (camera axis) to be linear in disparity.\\nHere we derive the transformation which is applied to rays to map them from\\ncamera space to NDC space. The standard 3D perspective projection matrix for\\nhomogeneous coordinates is:\\nM =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nn\\nr 0\\n0\\n0\\n0 n\\nt\\n0\\n0\\n0 0 −(f+n)\\nf−n\\n−2fn\\nf−n\\n0 0\\n−1\\n0\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(7)\\nwhere n, f are the near and far clipping planes and r and t are the right and top\\nbounds of the scene at the near clipping plane. (Note that this is in the convention\\nwhere the camera is looking in the −z direction.) To project a homogeneous point\\n(x, y, z, 1)⊤, we left-multiply by M and then divide by the fourth coordinate:\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nn\\nr 0\\n0\\n0\\n0 n\\nt\\n0\\n0\\n0 0 −(f+n)\\nf−n\\n−2fn\\nf−n\\n0 0\\n−1\\n0\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nx\\ny\\nz\\n1\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nn\\nr x\\nn\\nt y\\n−(f+n)\\nf−n z − −2fn\\nf−n\\n−z\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(8)\\nproject →\\n\\uf8eb\\n\\uf8ec\\n\\uf8ed\\nn\\nr\\nx\\nn −z\\nt\\ny\\n−z\\n(f+n)\\nf−n − 2fn\\nf−n\\n1\\n−z\\n\\uf8f6\\n\\uf8f7\\n\\uf8f8\\n(9)\\nThe projected point is now in normalized device coordinate (NDC) space, where\\nthe original viewing frustum has been mapped to the cube [−1, 1]3.\\nOur goal is to take a ray o+td and calculate a ray origin o′ and direction d′\\nin NDC space such that for every t, there exists a new t′ for which π(o + td) =\\no′ + t′d′ (where π is projection using the above matrix). In other words, the\\nprojection of the original ray and the NDC space ray trace out the same points\\n(but not necessarily at the same rate).\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 18, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='20\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nLet us rewrite the projected point from Eqn. 9 as (axx/z, ayy/z, az +bz/z)⊤.\\nThe components of the new origin o′ and direction d′ must satisfy:\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax ox+tdx\\noz+tdz\\nay\\noy+tdy\\noz+tdz\\naz +\\nbz\\noz+tdz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 =\\n\\uf8eb\\n\\uf8ed\\no′\\nx + t′d′\\nx\\no′\\ny + t′d′\\ny\\no′\\nz + t′d′\\nz\\n\\uf8f6\\n\\uf8f8 .\\n(10)\\nTo eliminate a degree of freedom, we decide that t′ = 0 and t = 0 should map\\nto the same point. Substituting t = 0 and t′ = 0 Eqn. 10 directly gives our NDC\\nspace origin o′:\\no′ =\\n\\uf8eb\\n\\uf8ed\\no′\\nx\\no′\\ny\\no′\\nz\\n\\uf8f6\\n\\uf8f8 =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax ox\\noz\\nay\\noy\\noz\\naz + bz\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 = π(o) .\\n(11)\\nThis is exactly the projection π(o) of the original ray’s origin. By substituting\\nthis back into Eqn. 10 for arbitrary t, we can determine the values of t′ and d′:\\n\\uf8eb\\n\\uf8ed\\nt′d′\\nx\\nt′d′\\ny\\nt′d′\\nz\\n\\uf8f6\\n\\uf8f8 =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax ox+tdx\\noz+tdz − ax ox\\noz\\nay\\noy+tdy\\noz+tdz − ay\\noy\\noz\\naz +\\nbz\\noz+tdz − az − bz\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(12)\\n=\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax\\noz(ox+tdx)−ox(oz+tdz)\\n(oz+tdz)oz\\nay\\noz(oy+tdy)−oy(oz+tdz)\\n(oz+tdz)oz\\nbz\\noz−(oz+tdz)\\n(oz+tdz)oz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(13)\\n=\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax\\ntdz\\noz+tdz\\n�\\ndx\\ndz − ox\\noz\\n�\\nay\\ntdz\\noz+tdz\\n�\\ndy\\ndz − oy\\noz\\n�\\n−bz\\ntdz\\noz+tdz\\n1\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(14)\\nFactoring out a common expression that depends only on t gives us:\\nt′ =\\ntdz\\noz + tdz\\n= 1 −\\noz\\noz + tdz\\n(15)\\nd′ =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nax\\n�\\ndx\\ndz − ox\\noz\\n�\\nay\\n�\\ndy\\ndz − oy\\noz\\n�\\n−bz 1\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 .\\n(16)\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 19, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n21\\nNote that, as desired, t′ = 0 when t = 0. Additionally, we see that t′ → 1 as\\nt → ∞. Going back to the original projection matrix, our constants are:\\nax = −n\\nr\\n(17)\\nay = −n\\nt\\n(18)\\naz = f + n\\nf − n\\n(19)\\nbz = 2fn\\nf − n\\n(20)\\nUsing the standard pinhole camera model, we can reparameterize as:\\nax = −fcam\\nW/2\\n(21)\\nay = −fcam\\nH/2\\n(22)\\nwhere W and H are the width and height of the image in pixels and fcam is the\\nfocal length of the camera.\\nIn our real forward facing captures, we assume that the far scene bound is\\ninﬁnity (this costs us very little since NDC uses the z dimension to represent\\ninverse depth, i.e., disparity). In this limit the z constants simplify to:\\naz = 1\\n(23)\\nbz = 2n .\\n(24)\\nCombining everything together:\\no′ =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n− fcam\\nW/2\\nox\\noz\\n− fcam\\nH/2\\noy\\noz\\n1 + 2n\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n(25)\\nd′ =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n− fcam\\nW/2\\n�\\ndx\\ndz − ox\\noz\\n�\\n− fcam\\nH/2\\n�\\ndy\\ndz − oy\\noz\\n�\\n−2n 1\\noz\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8 .\\n(26)\\nOne ﬁnal detail in our implementation: we shift o to the ray’s intersection with\\nthe near plane at z = −n (before this NDC conversion) by taking on = o + tnd\\nfor tn = −(n+oz)/dz. Once we convert to the NDC ray, this allows us to simply\\nsample t′ linearly from 0 to 1 in order to get a linear sampling in disparity from\\nn to ∞ in the original space.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 20, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='22\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nPedestal\\nCube\\nGround Truth\\nNeRF (ours)\\nLLFF [28]\\nSRN [42]\\nNV [24]\\nFig. 8: Comparisons on test-set views for scenes from the DeepVoxels [41] syn-\\nthetic dataset. The objects in this dataset have simple geometry and perfectly\\ndiﬀuse reﬂectance. Because of the large number of input images (479 views)\\nand simplicity of the rendered objects, both our method and LLFF [28] perform\\nnearly perfectly on this data. LLFF still occasionally presents artifacts when in-\\nterpolating between its 3D volumes, as in the top inset for each object. SRN [42]\\nand NV [24] do not have the representational power to render ﬁne details.\\nD\\nAdditional Results\\nPer-scene breakdown Tables 3, 4, 5, and 6 include a breakdown of the quanti-\\ntative results presented in the main paper into per-scene metrics. The per-scene\\nbreakdown is consistent with the aggregate quantitative metrics presented in\\nthe paper, where our method quantitatively outperforms all baselines. Although\\nLLFF achieves slightly better LPIPS metrics, we urge readers to view our sup-\\nplementary video where our method achieves better multiview consistency and\\nproduces fewer artifacts than all baselines.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 21, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n23\\nPSNR↑\\nSSIM↑\\nLPIPS↓\\nChair\\nPedestal\\nCube\\nVase\\nChair\\nPedestal\\nCube\\nVase\\nChair\\nPedestal\\nCube\\nVase\\nDeepVoxels [41]\\n33.45\\n32.35\\n28.42\\n27.99\\n0.99\\n0.97\\n0.97\\n0.96\\n−\\n−\\n−\\n−\\nSRN [42]\\n36.67\\n35.91\\n28.74\\n31.46\\n0.982\\n0.957\\n0.944\\n0.969\\n0.093\\n0.081\\n0.074\\n0.044\\nNV [24]\\n35.15\\n36.47\\n26.48\\n20.39\\n0.980\\n0.963\\n0.916\\n0.857\\n0.096\\n0.069\\n0.113\\n0.117\\nLLFF [28]\\n36.11\\n35.87\\n32.58\\n32.97\\n0.992\\n0.983\\n0.983\\n0.983\\n0.051\\n0.039\\n0.064\\n0.039\\nOurs\\n42.65\\n41.44\\n39.19\\n37.32\\n0.991\\n0.986\\n0.996\\n0.992\\n0.047\\n0.024\\n0.006\\n0.017\\nTable 3: Per-scene quantitative results from the DeepVoxels [41] dataset. The\\n“scenes” in this dataset are all diﬀuse objects with simple geometry, rendered\\nfrom texture-mapped meshes captured by a 3D scanner. The metrics for the\\nDeepVoxels method are taken directly from their paper, which does not report\\nLPIPS and only reports two signiﬁcant ﬁgures for SSIM.\\nPSNR↑\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\nSRN [42]\\n26.96\\n17.18\\n20.73\\n26.81\\n20.85\\n18.09\\n26.85\\n20.60\\nNV [24]\\n28.33\\n22.58\\n24.79\\n30.71\\n26.08\\n24.22\\n27.78\\n23.93\\nLLFF [28]\\n28.72\\n21.13\\n21.79\\n31.41\\n24.54\\n20.72\\n27.48\\n23.22\\nOurs\\n33.00\\n25.01\\n30.13\\n36.18\\n32.54\\n29.62\\n32.91\\n28.65\\nSSIM↑\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\nSRN [42]\\n0.910\\n0.766\\n0.849\\n0.923\\n0.809\\n0.808\\n0.947\\n0.757\\nNV [24]\\n0.916\\n0.873\\n0.910\\n0.944\\n0.880\\n0.888\\n0.946\\n0.784\\nLLFF [28]\\n0.948\\n0.890\\n0.896\\n0.965\\n0.911\\n0.890\\n0.964\\n0.823\\nOurs\\n0.967\\n0.925\\n0.964\\n0.974\\n0.961\\n0.949\\n0.980\\n0.856\\nLPIPS↓\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\nSRN [42]\\n0.106\\n0.267\\n0.149\\n0.100\\n0.200\\n0.174\\n0.063\\n0.299\\nNV [24]\\n0.109\\n0.214\\n0.162\\n0.109\\n0.175\\n0.130\\n0.107\\n0.276\\nLLFF [28]\\n0.064\\n0.126\\n0.130\\n0.061\\n0.110\\n0.117\\n0.084\\n0.218\\nOurs\\n0.046\\n0.091\\n0.044\\n0.121\\n0.050\\n0.063\\n0.028\\n0.206\\nTable 4: Per-scene quantitative results from our realistic synthetic dataset. The\\n“scenes” in this dataset are all objects with more complex gometry and non-\\nLambertian materials, rendered using Blender’s Cycles pathtracer.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 22, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='24\\nB. Mildenhall, P. P. Srinivasan, M. Tancik et al.\\nPSNR↑\\nRoom\\nFern\\nLeaves\\nFortress\\nOrchids\\nFlower\\nT-Rex\\nHorns\\nSRN [42]\\n27.29\\n21.37\\n18.24\\n26.63\\n17.37\\n24.63\\n22.87\\n24.33\\nLLFF [28]\\n28.42\\n22.85\\n19.52\\n29.40\\n18.52\\n25.46\\n24.15\\n24.70\\nOurs\\n32.70\\n25.17\\n20.92\\n31.16\\n20.36\\n27.40\\n26.80\\n27.45\\nSSIM↑\\nRoom\\nFern\\nLeaves\\nFortress\\nOrchids\\nFlower\\nT-Rex\\nHorns\\nSRN [42]\\n0.883\\n0.611\\n0.520\\n0.641\\n0.449\\n0.738\\n0.761\\n0.742\\nLLFF [28]\\n0.932\\n0.753\\n0.697\\n0.872\\n0.588\\n0.844\\n0.857\\n0.840\\nOurs\\n0.948\\n0.792\\n0.690\\n0.881\\n0.641\\n0.827\\n0.880\\n0.828\\nLPIPS↓\\nRoom\\nFern\\nLeaves\\nFortress\\nOrchids\\nFlower\\nT-Rex\\nHorns\\nSRN [42]\\n0.240\\n0.459\\n0.440\\n0.453\\n0.467\\n0.288\\n0.298\\n0.376\\nLLFF [28]\\n0.155\\n0.247\\n0.216\\n0.173\\n0.313\\n0.174\\n0.222\\n0.193\\nOurs\\n0.178\\n0.280\\n0.316\\n0.171\\n0.321\\n0.219\\n0.249\\n0.268\\nTable 5: Per-scene quantitative results from our real image dataset. The scenes\\nin this dataset are all captured with a forward-facing handheld cellphone.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 23, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\\n25\\nPSNR↑\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\n1)\\nNo PE, VD, H\\n28.44\\n23.11\\n25.17\\n32.24\\n26.38\\n24.69\\n28.16\\n25.12\\n2)\\nNo Pos. Encoding\\n30.33\\n24.54\\n29.32\\n33.16\\n27.75\\n27.79\\n30.76\\n26.55\\n3)\\nNo View Dependence\\n30.06\\n23.41\\n25.91\\n32.65\\n29.93\\n24.96\\n28.62\\n25.72\\n4)\\nNo Hierarchical\\n31.32\\n24.55\\n29.25\\n35.24\\n31.42\\n29.22\\n31.74\\n27.73\\n5)\\nFar Fewer Images\\n30.92\\n22.62\\n24.39\\n32.77\\n27.97\\n26.55\\n30.47\\n26.57\\n6)\\nFewer Images\\n32.19\\n23.70\\n27.45\\n34.91\\n31.53\\n28.54\\n32.33\\n27.67\\n7)\\nFewer Frequencies\\n32.19\\n25.29\\n30.73\\n36.06\\n30.77\\n29.77\\n31.66\\n28.26\\n8)\\nMore Frequencies\\n32.87\\n24.65\\n29.92\\n35.78\\n32.50\\n29.54\\n32.86\\n28.34\\n9)\\nComplete Model\\n33.00\\n25.01\\n30.13\\n36.18\\n32.54\\n29.62\\n32.91\\n28.65\\nSSIM↑\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\n1)\\nNo PE, VD, H\\n0.919\\n0.896\\n0.926\\n0.955\\n0.882\\n0.905\\n0.955\\n0.810\\n2)\\nNo Pos. Encoding\\n0.938\\n0.918\\n0.953\\n0.956\\n0.903\\n0.933\\n0.968\\n0.824\\n3)\\nNo View Dependence\\n0.948\\n0.906\\n0.938\\n0.961\\n0.947\\n0.912\\n0.962\\n0.828\\n4)\\nNo Hierarchical\\n0.951\\n0.914\\n0.956\\n0.969\\n0.951\\n0.944\\n0.973\\n0.844\\n5)\\nFar Fewer Images\\n0.956\\n0.895\\n0.922\\n0.966\\n0.930\\n0.925\\n0.972\\n0.832\\n6)\\nFewer Images\\n0.963\\n0.911\\n0.948\\n0.971\\n0.957\\n0.941\\n0.979\\n0.847\\n7)\\nFewer Frequencies\\n0.959\\n0.928\\n0.965\\n0.972\\n0.947\\n0.952\\n0.973\\n0.853\\n8)\\nMore Frequencies\\n0.967\\n0.921\\n0.962\\n0.973\\n0.961\\n0.948\\n0.980\\n0.853\\n9)\\nComplete Model\\n0.967\\n0.925\\n0.964\\n0.974\\n0.961\\n0.949\\n0.980\\n0.856\\nLPIPS↓\\nChair\\nDrums\\nFicus\\nHotdog\\nLego\\nMaterials\\nMic\\nShip\\n1)\\nNo PE, VD, H\\n0.095\\n0.168\\n0.084\\n0.104\\n0.178\\n0.111\\n0.084\\n0.261\\n2)\\nNo Pos. Encoding\\n0.076\\n0.104\\n0.050\\n0.124\\n0.128\\n0.079\\n0.041\\n0.261\\n3)\\nNo View Dependence\\n0.075\\n0.148\\n0.113\\n0.112\\n0.088\\n0.102\\n0.073\\n0.220\\n4)\\nNo Hierarchical\\n0.065\\n0.177\\n0.056\\n0.130\\n0.072\\n0.080\\n0.039\\n0.249\\n5)\\nFar Fewer Images\\n0.058\\n0.173\\n0.082\\n0.123\\n0.081\\n0.079\\n0.035\\n0.229\\n6)\\nFewer Images\\n0.051\\n0.166\\n0.057\\n0.121\\n0.055\\n0.068\\n0.029\\n0.223\\n7)\\nFewer Frequencies\\n0.055\\n0.143\\n0.038\\n0.087\\n0.071\\n0.060\\n0.029\\n0.219\\n8)\\nMore Frequencies\\n0.047\\n0.158\\n0.045\\n0.116\\n0.050\\n0.064\\n0.027\\n0.261\\n9)\\nComplete Model\\n0.046\\n0.091\\n0.044\\n0.121\\n0.050\\n0.063\\n0.028\\n0.206\\nTable 6: Per-scene quantitative results from our ablation study. The scenes used\\nhere are the same as in Table 4.\\n', metadata={'source': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'file_path': 'data/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf', 'page': 24, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20200805000751Z', 'modDate': 'D:20200805000751Z', 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\nTHOMAS MÜLLER, NVIDIA, Switzerland\\nALEX EVANS, NVIDIA, United Kingdom\\nCHRISTOPH SCHIED, NVIDIA, USA\\nALEXANDER KELLER, NVIDIA, Germany\\nTrained for 1 second\\n15 seconds\\n1 second\\n15 seconds\\n60 seconds\\nreference\\nGigapixel image\\nSDF\\nNRC\\nNeRF\\nFig. 1. We demonstrate instant training of neural graphics primitives on a single GPU for multiple tasks. In Gigapixel image we represent a gigapixel image by\\na neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. Neural radiance caching (NRC) [Müller\\net al. 2021] employs a neural network that is trained in real-time to cache costly lighting calculations. Lastly, NeRF [Mildenhall et al. 2020] uses 2D images\\nand their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. In all tasks, our encoding and its\\nefficient implementation provide clear benefits: rapid training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation\\nand hyperparameters across all tasks and only vary the hash table size which trades off quality and performance. Tokyo gigapixel photograph ©Trevor\\nDobson (CC BY-NC-ND 2.0), Lego bulldozer 3D model ©Håvard Dalen (CC BY-NC 2.0)\\nNeural graphics primitives, parameterized by fully connected neural net-\\nworks, can be costly to train and evaluate. We reduce this cost with a versatile\\nnew input encoding that permits the use of a smaller network without sac-\\nrificing quality, thus significantly reducing the number of floating point\\nand memory access operations: a small neural network is augmented by a\\nAuthors’ addresses: Thomas Müller, NVIDIA, Zürich, Switzerland, tmueller@nvidia.\\ncom; Alex Evans, NVIDIA, London, United Kingdom, alexe@nvidia.com; Christoph\\nSchied, NVIDIA, Seattle, USA, cschied@nvidia.com; Alexander Keller, NVIDIA, Berlin,\\nGermany, akeller@nvidia.com.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\n© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\n0730-0301/2022/7-ART102 $15.00\\nhttps://doi.org/10.1145/3528223.3530127\\nmultiresolution hash table of trainable feature vectors whose values are op-\\ntimized through stochastic gradient descent. The multiresolution structure\\nallows the network to disambiguate hash collisions, making for a simple\\narchitecture that is trivial to parallelize on modern GPUs. We leverage this\\nparallelism by implementing the whole system using fully-fused CUDA ker-\\nnels with a focus on minimizing wasted bandwidth and compute operations.\\nWe achieve a combined speedup of several orders of magnitude, enabling\\ntraining of high-quality neural graphics primitives in a matter of seconds,\\nand rendering in tens of milliseconds at a resolution of 1920×1080.\\nCCS Concepts: • Computing methodologies → Massively parallel algo-\\nrithms; Vector / streaming algorithms; Neural networks.\\nAdditional Key Words and Phrases: Image Synthesis, Neural Networks, En-\\ncodings, Hashing, GPUs, Parallel Computation, Function Approximation.\\nACM Reference Format:\\nThomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. 2022.\\nInstant Neural Graphics Primitives with a Multiresolution Hash Encoding.\\nACM Trans. Graph. 41, 4, Article 102 (July 2022), 15 pages. https://doi.org/10.\\n1145/3528223.3530127\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:2\\n•\\nMüller et al.\\n1\\nINTRODUCTION\\nComputer graphics primitives are fundamentally represented by\\nmathematical functions that parameterize appearance. The quality\\nand performance characteristics of the mathematical representation\\nare crucial for visual fidelity: we desire representations that remain\\nfast and compact while capturing high-frequency, local detail. Func-\\ntions represented by multi-layer perceptrons (MLPs), used as neural\\ngraphics primitives, have been shown to match these criteria (to\\nvarying degree), for example as representations of shape [Martel\\net al. 2021; Park et al. 2019] and radiance fields [Liu et al. 2020;\\nMildenhall et al. 2020; Müller et al. 2020, 2021].\\nThe important commonality of the these approaches is an encod-\\ning that maps neural network inputs to a higher-dimensional space,\\nwhich is key for extracting high approximation quality from com-\\npact models. Most successful among these encodings are trainable,\\ntask-specific data structures [Liu et al. 2020; Takikawa et al. 2021]\\nthat take on a large portion of the learning task. This enables the use\\nof smaller, more efficient MLPs. However, such data structures rely\\non heuristics and structural modifications (such as pruning, split-\\nting, or merging) that may complicate the training process, limit\\nthe method to a specific task, or limit performance on GPUs where\\ncontrol flow and pointer chasing is expensive.\\nWe address these concerns with our multiresolution hash encod-\\ning, which is adaptive and efficient, independent of the task. It is\\nconfigured by just two values—the number of parameters 𝐴��� and the\\ndesired finest resolution 𝐴���max—yielding state-of-the-art quality on\\na variety of tasks (Figure 1) after a few seconds of training.\\nKey to both the task-independent adaptivity and efficiency is a\\nmultiresolution hierarchy of hash tables:\\n• Adaptivity: we map a cascade of grids to corresponding fixed-\\nsize arrays of feature vectors. At coarse resolutions, there is a 1:1\\nmapping from grid points to array entries. At fine resolutions, the\\narray is treated as a hash table and indexed using a spatial hash\\nfunction, where multiple grid points alias each array entry. Such\\nhash collisions cause the colliding training gradients to average,\\nmeaning that the largest gradients—those most relevant to the\\nloss function—will dominate. The hash tables thus automatically\\nprioritize the sparse areas with the most important fine scale detail.\\nUnlike prior work, no structural updates to the data structure are\\nneeded at any point during training.\\n• Efficiency: our hash table lookups are O(1) and do not require\\ncontrol flow. This maps well to modern GPUs, avoiding execution\\ndivergence and serial pointer-chasing inherent in tree traversals.\\nThe hash tables for all resolutions may be queried in parallel.\\nWe validate our multiresolution hash encoding in four representa-\\ntive tasks (see Figure 1):\\n(1) Gigapixel image: the MLP learns the mapping from 2D coordi-\\nnates to RGB colors of a high-resolution image.\\n(2) Neural signed distance functions (SDF): the MLP learns the\\nmapping from 3D coordinates to the distance to a surface.\\n(3) Neural radiance caching (NRC): the MLP learns the 5D light\\nfield of a given scene from a Monte Carlo path tracer.\\n(4) Neural radiance and density fields (NeRF): the MLP learns\\nthe 3D density and 5D light field of a given scene from image\\nobservations and corresponding perspective transforms.\\nThe source code and data pertaining to this paper can be found\\nat https://nvlabs.github.io/instant-ngp.\\nIn the following, we first review prior neural network encodings\\n(Section 2), then we describe our encoding (Section 3) and its imple-\\nmentation (Section 4), followed lastly by our experiments (Section 5)\\nand discussion thereof (Section 6).\\n2\\nBACKGROUND AND RELATED WORK\\nEarly examples of encoding the inputs of a machine learning model\\ninto a higher-dimensional space include the one-hot encoding [Har-\\nris and Harris 2013] and the kernel trick [Theodoridis 2008] by which\\ncomplex arrangements of data can be made linearly separable.\\nFor neural networks, input encodings have proven useful in the at-\\ntention components of recurrent architectures [Gehring et al. 2017]\\nand, subsequently, transformers [Vaswani et al. 2017], where they\\nhelp the neural network to identify the location it is currently pro-\\ncessing. Vaswani et al. [2017] encode scalar positions 𝐴��� ∈ R as a\\nmultiresolution sequence of 𝐴��� ∈ N sine and cosine functions\\nenc(𝐴���) = � sin(20𝐴���), sin(21𝐴���), . . . , sin(2𝐴���−1𝐴���),\\ncos(20𝐴���), cos(21𝐴���), . . . , cos(2𝐴���−1𝐴���) � .\\n(1)\\nThis has been adopted in computer graphics to encode the spatio-\\ndirectionally varying light field and volume density in the NeRF\\nalgorithm [Mildenhall et al. 2020]. The five dimensions of this light\\nfield are independently encoded using the above formula; this was\\nlater extended to randomly oriented parallel wavefronts [Tancik\\net al. 2020] and level-of-detail filtering [Barron et al. 2021a]. We will\\nrefer to this family of encodings as frequency encodings. Notably,\\nfrequency encodings followed by a linear transformation have been\\nused in other computer graphics tasks, such as approximating the\\nvisibility function [Annen et al. 2007; Jansen and Bavoil 2010].\\nMüller et al. [2019; 2020] suggested a continuous variant of the\\none-hot encoding based on rasterizing a kernel, the one-blob en-\\ncoding, which can achieve more accurate results than frequency\\nencodings in bounded domains at the cost of being single-scale.\\nParametric encodings. Recently, state-of-the-art results have been\\nachieved by parametric encodings which blur the line between clas-\\nsical data structures and neural approaches. The idea is to arrange\\nadditional trainable parameters (beyond weights and biases) in an\\nauxiliary data structure, such as a grid [Chabra et al. 2020; Jiang\\net al. 2020; Liu et al. 2020; Mehta et al. 2021; Peng et al. 2020a; Sun\\net al. 2021; Tang et al. 2018; Yu et al. 2021a] or a tree [Takikawa et al.\\n2021], and to look-up and (optionally) interpolate these parameters\\ndepending on the input vector x ∈ R𝐴���. This arrangement trades a\\nlarger memory footprint for a smaller computational cost: whereas\\nfor each gradient propagated backwards through the network, every\\nweight in the fully connected MLP network must be updated, for\\nthe trainable input encoding parameters (“feature vectors”), only\\na very small number are affected. For example, with a trilinearly\\ninterpolated 3D grid of feature vectors, only 8 such grid points need\\nto be updated for each sample back-propagated to the encoding. In\\nthis way, although the total number of parameters is much higher\\nfor a parametric encoding than a fixed input encoding, the number\\nof FLOPs and memory accesses required for the update during train-\\ning is not increased significantly. By reducing the size of the MLP,\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 1, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:3\\n(a) No encoding\\n(b) Frequency\\n[Mildenhall et al. 2020]\\n(c) Dense grid\\nSingle resolution\\n(d) Dense grid\\nMulti resolution\\n(e) Hash table (ours)\\n𝐴��� = 214\\n(f) Hash table (ours)\\n𝐴��� = 219\\n411 k + 0 parameters\\n438 k + 0\\n10 k + 33.6 M\\n10 k + 16.3 M\\n10 k + 494 k\\n10 k + 12.6 M\\n11:28 (mm:ss) / PSNR 18.56\\n12:45 / PSNR 22.90\\n1:09 / PSNR 22.35\\n1:26 / PSNR 23.62\\n1:48 / PSNR 22.61\\n1:47 / PSNR 24.58\\nFig. 2. A demonstration of the reconstruction quality of different encodings and parametric data structures for storing trainable feature embeddings. Each\\nconfiguration was trained for 11 000 steps using our fast NeRF implementation (Section 5.4), varying only the input encoding and MLP size. The number of\\ntrainable parameters (MLP weights + encoding parameters), training time and reconstruction accuracy (PSNR) are shown below each image. Our encoding (e)\\nwith a similar total number of trainable parameters as the frequency encoding configuration (b) trains over 8× faster, due to the sparsity of updates to the\\nparameters and smaller MLP. Increasing the number of parameters (f) further improves reconstruction accuracy without significantly increasing training time.\\nsuch parametric models can typically be trained to convergence\\nmuch faster without sacrificing approximation quality.\\nAnother parametric approach uses a tree subdivision of the do-\\nmain R𝐴���, wherein a large auxiliary coordinate encoder neural net-\\nwork (ACORN) [Martel et al. 2021] is trained to output dense feature\\ngrids in the leaf node around x. These dense feature grids, which\\nhave on the order of 10 000 entries, are then linearly interpolated, as\\nin Liu et al. [2020]. This approach tends to yield a larger degree of\\nadaptivity compared with the previous parametric encodings, albeit\\nat greater computational cost which can only be amortized when\\nsufficiently many inputs x fall into each leaf node.\\nSparse parametric encodings. While existing parametric encod-\\nings tend to yield much greater accuracy than their non-parametric\\npredecessors, they also come with downsides in efficiency and versa-\\ntility. Dense grids of trainable features consume much more memory\\nthan the neural network weights. To illustrate the trade-offs and to\\nmotivate our method, Figure 2 shows the effect on reconstruction\\nquality of a neural radiance field for several different encodings.\\nWithout any input encoding at all (a), the network is only able to\\nlearn a fairly smooth function of position, resulting in a poor ap-\\nproximation of the light field. The frequency encoding (b) allows\\nthe same moderately sized network (8 hidden layers, each 256 wide)\\nto represent the scene much more accurately. The middle image (c)\\npairs a smaller network with a dense grid of 1283 trilinearly inter-\\npolated, 16-dimensional feature vectors, for a total of 33.6 million\\ntrainable parameters. The large number of trainable parameters can\\nbe efficiently updated, as each sample only affects 8 grid points.\\nHowever, the dense grid is wasteful in two ways. First, it allocates\\nas many features to areas of empty space as it does to those areas\\nnear the surface. The number of parameters grows as O(𝐴��� 3), while\\nthe visible surface of interest has surface area that grows only as\\nO(𝐴��� 2). In this example, the grid has resolution 1283, but only 53 807\\n(2.57%) of its cells touch the visible surface.\\nSecond, natural scenes exhibit smoothness, motivating the use\\nof a multi-resolution decomposition [Chibane et al. 2020; Hadadan\\net al. 2021]. Figure 2 (d) shows the result of using an encoding in\\nwhich interpolated features are stored in eight co-located grids with\\nresolutions from 163 to 1733, each containing 2-dimensional feature\\nvectors. These are concatenated to form a 16-dimensional (same as\\n(c)) input to the network. Despite having less than half the number\\nof parameters as (c), the reconstruction quality is similar.\\nIf the surface of interest is known a priori, a data structure such\\nas an octree [Takikawa et al. 2021] or sparse grid [Chabra et al.\\n2020; Chibane et al. 2020; Hadadan et al. 2021; Jiang et al. 2020; Liu\\net al. 2020; Peng et al. 2020a] can be used to cull away the unused\\nfeatures in the dense grid. However, in the NeRF setting, surfaces\\nonly emerge during training. NSVF [Liu et al. 2020] and several\\nconcurrent works [Sun et al. 2021; Yu et al. 2021a] adopt a multi-\\nstage, coarse to fine strategy in which regions of the feature grid are\\nprogressively refined and culled away as necessary. While effective,\\nthis leads to a more complex training process in which the sparse\\ndata structure must be periodically updated.\\nOur method—Figure 2 (e,f)—combines both ideas to reduce waste.\\nWe store the trainable feature vectors in a compact spatial hash table,\\nwhose size is a hyper-parameter 𝐴��� which can be tuned to trade the\\nnumber of parameters for reconstruction quality. It neither relies on\\nprogressive pruning during training nor on a priori knowledge of the\\ngeometry of the scene. Analogous to the multi-resolution grid in (d),\\nwe use multiple separate hash tables indexed at different resolutions,\\nwhose interpolated outputs are concatenated before being passed\\nthrough the MLP. The reconstruction quality is comparable to the\\ndense grid encoding, despite having 20× fewer parameters.\\nUnlike prior work that used spatial hashing [Teschner et al. 2003]\\nfor 3D reconstruction [Nießner et al. 2013], we do not explicitly han-\\ndle collisions of the hash functions by typical means like probing,\\nbucketing, or chaining. Instead, we rely on the neural network to\\nlearn to disambiguate hash collisions itself, avoiding control flow\\ndivergence, reducing implementation complexity and improving\\nperformance. Another performance benefit is the predictable mem-\\nory layout of the hash tables that is independent of the data that is\\nrepresented. While good caching behavior is often hard to achieve\\nwith tree-like data structures, our hash tables can be fine-tuned for\\nlow-level architectural details such as cache size.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 2, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:4\\n•\\nMüller et al.\\nx\\ny\\n𝐴���(y; Φ)\\n𝐴���\\n𝐴���\\n𝐴��� · 𝐴���\\n𝐴���\\n𝛹���\\n𝐴��� = 2, 𝐴��� = 1.5\\n1/𝐴���1\\n1/𝐴���0\\n𝐴��� = 0\\n𝐴��� = 1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n2\\n3\\n6\\n0\\n4\\n1\\n7\\n(1) Hashing of voxel vertices\\n(2) Lookup\\n(3) Linear interpolation\\n(4) Concatenation\\n(5) Neural network\\nFig. 3. Illustration of the multiresolution hash encoding in 2D. (1) for a given input coordinate x, we find the surrounding voxels at 𝐴��� resolution levels and\\nassign indices to their corners by hashing their integer coordinates. (2) for all resulting corner indices, we look up the corresponding 𝐴���-dimensional feature\\nvectors from the hash tables 𝛹���𝐴��� and (3) linearly interpolate them according to the relative position of x within the respective 𝐴���-th voxel. (4) we concatenate the\\nresult of each level, as well as auxiliary inputs 𝛹��� ∈ R𝐴���, producing the encoded MLP input 𝑦 ∈ R𝐴���𝐴���+𝐴���, which (5) is evaluated last. To train the encoding, loss\\ngradients are backpropagated through the MLP (5), the concatenation (4), the linear interpolation (3), and then accumulated in the looked-up feature vectors.\\nTable 1. Hash encoding parameters and their ranges in this paper. Only the\\nhash table size 𝐴��� and max. resolution 𝐴���max need to be tuned to the task.\\nParameter\\nSymbol\\nValue\\nNumber of levels\\n𝐴���\\n16\\nMax. entries per level (hash table size)\\n𝐴���\\n214 to 224\\nNumber of feature dimensions per entry\\n𝐴���\\n2\\nCoarsest resolution\\n𝐴���min\\n16\\nFinest resolution\\n𝐴���max\\n512 to 524288\\n3\\nMULTIRESOLUTION HASH ENCODING\\nGiven a fully connected neural network𝐴���(y; Φ), we are interested in\\nan encoding of its inputs y = enc(x;𝛹���) that improves the approxima-\\ntion quality and training speed across a wide range of applications\\nwithout incurring a notable performance overhead. Our neural net-\\nwork not only has trainable weight parameters Φ, but also trainable\\nencoding parameters 𝛹���. These are arranged into 𝐴��� levels, each con-\\ntaining up to𝐴��� feature vectors with dimensionality 𝐴���. Typical values\\nfor these hyperparameters are shown in Table 1. Figure 3 illustrates\\nthe steps performed in our multiresolution hash encoding. Each\\nlevel (two of which are shown as red and blue in the figure) is inde-\\npendent and conceptually stores feature vectors at the vertices of a\\ngrid, the resolution of which is chosen to be a geometric progression\\nbetween the coarsest and finest resolutions [𝐴���min, 𝐴���max]:\\n𝐴���𝐴��� :=\\n�\\n𝐴���min · 𝐴���𝐴��� �\\n,\\n(2)\\n𝐴��� := exp\\n� ln 𝐴���max − ln 𝐴���min\\n𝐴��� − 1\\n�\\n.\\n(3)\\n𝐴���max is chosen to match the finest detail in the training data. Due\\nto the large number of levels 𝐴���, the growth factor is usually small.\\nOur use cases have 𝐴��� ∈ [1.26, 2].\\nConsider a single level 𝐴���. The input coordinate x ∈ R𝐴��� is scaled\\nby that level’s grid resolution before rounding down and up ⌊x𝐴���⌋ :=\\n⌊x · 𝐴���𝐴���⌋, ⌈x𝐴���⌉ := ⌈x · 𝐴���𝐴���⌉.\\n⌊x𝐴���⌋ and ⌈x𝐴���⌉ span a voxel with 2𝐴��� integer vertices in Z𝐴���. We map\\neach corner to an entry in the level’s respective feature vector array,\\nwhich has fixed size of at most𝐴���. For coarse levels where a dense grid\\nrequires fewer than 𝐴��� parameters, i.e. (𝐴���𝐴��� + 1)𝐴��� ≤ 𝐴���, this mapping\\nis 1:1. At finer levels, we use a hash function ℎ : Z𝐴��� → Z𝐴��� to index\\ninto the array, effectively treating it as a hash table, although there is\\nno explicit collision handling. We rely instead on the gradient-based\\noptimization to store appropriate sparse detail in the array, and the\\nsubsequent neural network 𝐴���(y; Φ) for collision resolution. The\\nnumber of trainable encoding parameters 𝛹��� is therefore O(𝐴���) and\\nbounded by 𝐴��� · 𝐴��� · 𝐴��� which in our case is always 𝐴��� · 16 · 2 (Table 1).\\nWe use a spatial hash function [Teschner et al. 2003] of the form\\nℎ(x) =\\n� 𝐴���\\n�\\n𝐴���=1\\n𝐴���𝐴���𝜋𝐴���\\n�\\nmod 𝐴��� ,\\n(4)\\nwhere ⊕ denotes the bit-wise XOR operation and 𝜋𝐴��� are unique,\\nlarge prime numbers. Effectively, this formula XORs the results\\nof a per-dimension linear congruential (pseudo-random) permuta-\\ntion [Lehmer 1951], decorrelating the effect of the dimensions on\\nthe hashed value. Notably, to achieve (pseudo-)independence, only\\n𝐴��� − 1 of the 𝐴��� dimensions must be permuted, so we choose 𝜋1 := 1\\nfor better cache coherence, 𝜋2 = 2 654 435 761, and 𝜋3 = 805 459 861.\\nLastly, the feature vectors at each corner are 𝐴���-linearly interpo-\\nlated according to the relative position of x within its hypercube,\\ni.e. the interpolation weight is w𝐴��� := x𝐴��� − ⌊x𝐴���⌋.\\nRecall that this process takes place independently for each of the\\n𝐴��� levels. The interpolated feature vectors of each level, as well as\\nauxiliary inputs 𝛹��� ∈ R𝐴��� (such as the encoded view direction and\\ntextures in neural radiance caching), are concatenated to produce\\ny ∈ R𝐴���𝐴���+𝐴���, which is the encoded input enc(x;𝛹���) to the MLP𝐴���(y; Φ).\\nPerformance vs. quality. Choosing the hash table size𝐴��� provides a\\ntrade-off between performance, memory and quality. Higher values\\nof 𝐴��� result in higher quality and lower performance. The memory\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 3, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:5\\n0\\n100\\n200\\n30\\n40\\n50\\n𝐴��� = 216\\n𝐴��� = 219\\n𝐴��� = 224\\nTraining time (seconds)\\nPSNR (dB)\\nGigapixel image\\nPluto\\nMars\\nTokyo\\n0\\n50\\n100\\n150\\n20\\n25\\n30\\n𝐴��� = 214\\n𝐴��� = 219 𝐴��� = 221\\nTraining time (seconds)\\nMAPE (dB)\\nSDF\\nClockwork\\nLizard\\nBearded Man\\n0\\n100\\n200\\n300\\n30\\n35\\n𝐴��� = 214\\n𝐴��� = 219\\n𝐴��� = 221\\nTraining time (seconds)\\nPSNR (dB)\\nNeRF\\nLego\\nShip\\nFig. 4. The main curves plot test error over training time for varying hash table size 𝐴��� which determines the number of trainable encoding parameters.\\nIncreasing 𝐴��� improves reconstruction, at the cost of higher memory usage and slower training and inference. A performance cliff is visible at 𝐴��� > 219 where\\nthe cache of our RTX 3090 GPU becomes oversubscribed (particularly visible for SDF and NeRF). The plot also shows model convergence over time leading up\\nto the final state. This highlights how high quality results are already obtained after only a few seconds. Jumps in the convergence (most visible towards the\\nend of SDF training) are caused by learning rate decay. For NeRF and Gigapixel image, training finishes after 31 000 steps and for SDF after 11 000 steps.\\n200\\n300\\n400\\n25\\n30\\n35\\n40\\n𝐴��� = 2\\n𝐴��� = 4\\n𝐴��� = 8\\n𝐴��� = 16\\n𝐴��� = 32\\nTraining time (seconds)\\nPSNR (dB)\\nGigapixel image: Tokyo\\nF=1\\nF=2\\nF=4\\nF=8\\n60\\n80\\n100\\n20\\n21\\n22\\n𝐴��� = 4\\n𝐴��� = 8\\n𝐴��� = 16\\n𝐴��� = 32\\nTraining time (seconds)\\nMAPE (dB)\\nSigned Distance Function: Cow\\nF=1\\nF=2\\nF=4\\nF=8\\n200\\n300\\n400\\n500\\n33\\n34\\n35\\n36\\n𝐴��� = 4\\n𝐴��� = 8\\n𝐴��� = 16\\n𝐴��� = 32\\nTraining time (seconds)\\nPSNR (dB)\\nNeural Radiance Field: Lego\\nF=1\\nF=2\\nF=4\\nF=8\\nFig. 5. Test error over training time for fixed values of feature dimensionality 𝐴��� as the number of hash table levels 𝐴��� is varied. To maintain a roughly equal\\ntrainable parameter count, the hash table size 𝐴��� is set according to 𝐴��� · 𝐴��� · 𝐴��� = 224 for SDF and NeRF, whereas gigapixel image uses 228. Since (𝐴��� = 2, 𝐴��� = 16)\\nis near the best-case performance and quality (top-left) for all applications, we use this configuration in all results. 𝐴��� = 1 is slow on our RTX 3090 GPU since\\natomic half-precision accumulation is only efficient for 2D vectors but not for scalars. For NeRF and Gigapixel image, training finishes after 31 000 steps\\nwhereas SDF completes at 11 000 steps.\\nfootprint is linear in 𝐴���, whereas quality and performance tend to\\nscale sub-linearly. We analyze the impact of𝐴��� in Figure 4, where we\\nreport test error vs. training time for a wide range of 𝐴���-values for\\nthree neural graphics primitives. We recommend practitioners to use\\n𝐴��� to tweak the encoding to their desired performance characteristics.\\nThe hyperparameters 𝐴��� (number of levels) and 𝐴��� (number of fea-\\nture dimensions) also trade off quality and performance, which we\\nanalyze for an approximately constant number of trainable encoding\\nparameters 𝛹��� in Figure 5. In this analysis, we found (𝐴��� = 2, 𝐴��� = 16)\\nto be a favorable Pareto optimum in all our applications, so we use\\nthese values in all other results and recommend them as the default.\\nImplicit hash collision resolution. It may appear counter-intuitive\\nthat this encoding is able to reconstruct scenes faithfully in the\\npresence of hash collisions. Key to its success is that the different\\nresolution levels have different strengths that complement each\\nother. The coarser levels, and thus the encoding as a whole, are\\ninjective—that is, they suffer from no collisions at all. However, they\\ncan only represent a low-resolution version of the scene, since they\\noffer features which are linearly interpolated from a widely spaced\\ngrid of points. Conversely, fine levels can capture small features due\\nto their fine grid resolution, but suffer from many collisions—that is,\\ndisparate points which hash to the same table entry. Nearby inputs\\nwith equal integer coordinates ⌊x𝐴���⌋ are not considered a collision;\\na collision occurs when different integer coordinates hash to the\\nsame index. Luckily, such collisions are pseudo-randomly scattered\\nacross space, and statistically unlikely to occur simultaneously at\\nevery level for a given pair of points.\\nWhen training samples collide in this way, their gradients average.\\nConsider that the importance to the final reconstruction of such\\nsamples is rarely equal. For example, a point on a visible surface\\nof a radiance field will contribute strongly to the reconstructed\\nimage (having high visibility and high density, both multiplicatively\\naffecting the magnitude of gradients) causing large changes to its\\ntable entries, while a point in empty space that happens to refer to\\nthe same entry will have a much smaller weight. As a result, the\\ngradients of the more important samples dominate the collision\\naverage and the aliased table entry will naturally be optimized in\\nsuch a way that it reflects the needs of the higher-weighted point.\\nThe multiresolution aspect of the hash encoding covers the full\\nrange from a coarse resolution 𝐴���min that is guaranteed to be collision-\\nfree to the finest resolution 𝐴���max that the task requires. Thereby, it\\nguarantees that all scales at which meaningful learning could take\\nplace are included, regardless of sparsity. Geometric scaling allows\\ncovering these scales with only O� log (𝐴���max/𝐴���min)� many levels,\\nwhich allows picking a conservatively large value for 𝐴���max.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 4, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:6\\n•\\nMüller et al.\\nOnline adaptivity. Note that if the distribution of inputs x changes\\nover time during training, for example if they become concentrated\\nin a small region, then finer grid levels will experience fewer colli-\\nsions and a more accurate function can be learned. In other words,\\nthe multiresolution hash encoding automatically adapts to the train-\\ning data distribution, inheriting the benefits of tree-based encod-\\nings [Takikawa et al. 2021] without task-specific data structure\\nmaintenance that might cause discrete jumps during training. One\\nof our applications, neural radiance caching in Section 5.3, con-\\ntinually adapts to animated viewpoints and 3D content, greatly\\nbenefitting from this feature.\\n𝐴���-linear interpolation. Interpolating the queried hash table entries\\nensures that the encoding enc(x;𝛹���), and by the chain rule its com-\\nposition with the neural network 𝐴���(enc(x;𝛹���); Φ), are continuous.\\nWithout interpolation, grid-aligned discontinuities would be present\\nin the network output, which would result in an undesirable blocky\\nappearance. One may desire higher-order smoothness, for exam-\\nple when approximating partial differential equations. A concrete\\nexample from computer graphics are signed distance functions, in\\nwhich case the gradient 𝜋���𝐴���(enc(x;𝛹���); Φ)/𝜋���x, i.e. the surface normal,\\nwould ideally also be continuous. If higher-order smoothness must\\nbe guaranteed, we describe a low-cost approach in Appendix A,\\nwhich we however do not employ in any of our results due to a\\nsmall decrease in reconstruction quality.\\n4\\nIMPLEMENTATION\\nTo demonstrate the speed of the multiresolution hash encoding, we\\nimplemented it in CUDA and integrated it with the fast fully-fused\\nMLPs of the tiny-cuda-nn framework [Müller 2021].1 We release the\\nsource code of the multiresolution hash encoding as an update to\\nMüller [2021] and the source code pertaining to the neural graphics\\nprimitives at https://github.com/nvlabs/instant-ngp.\\nPerformance considerations. In order to optimize inference and\\nbackpropagation performance, we store hash table entries at half\\nprecision (2 bytes per entry). We additionally maintain a master\\ncopy of the parameters in full precision for stable mixed-precision\\nparameter updates, following Micikevicius et al. [2018].\\nTo optimally use the GPU’s caches, we evaluate the hash tables\\nlevel by level: when processing a batch of input positions, we sched-\\nule the computation to look up the first level of the multiresolution\\nhash encoding for all inputs, followed by the second level for all\\ninputs, and so on. Thus, only a small number of consecutive hash\\ntables have to reside in caches at any given time, depending on how\\nmuch parallelism is available on the GPU. Importantly, this struc-\\nture of computation automatically makes good use of the available\\ncaches and parallelism for a wide range of hash table sizes 𝐴���.\\nOn our hardware, the performance of the encoding remains\\nroughly constant as long as the hash table size stays below 𝐴��� ≤ 219.\\nBeyond this threshold, performance starts to drop significantly; see\\nFigure 4. This is explained by the 6 MB L2 cache of our NVIDIA\\nRTX 3090 GPU, which becomes too small for individual levels when\\n2 · 𝐴��� · 𝐴��� > 6 · 220, with 2 being the size of a half-precision entry.\\n1We observe speed-ups on the order of 10× compared to a naïve Python implementation.\\nWe therefore also release PyTorch bindings around our hash encoding and fully fused\\nMLPs to permit their use in existing projects with little overhead.\\nThe optimal number of feature dimensions 𝐴��� per lookup depends\\non the GPU architecture. On one hand, a small number favors cache\\nlocality in the previously mentioned streaming approach, but on\\nthe other hand, a large 𝐴��� favors memory coherence by allowing for\\n𝐴���-wide vector load instructions. 𝐴��� = 2 gave us the best cost-quality\\ntrade-off (see Figure 5) and we use it in all experiments.\\nArchitecture. In all tasks, except for NeRF which we will describe\\nlater, we use an MLP with two hidden layers that have a width\\nof 64 neurons, rectified linear unit (ReLU) activation functions on\\ntheir hidden layers, and a linear output layer. The maximum resolu-\\ntion 𝐴���max is set to 2048 × scene size for NeRF and signed distance\\nfunctions, to half of the gigapixel image width, and 219 in radiance\\ncaching (large value to support close-by objects in expansive scenes).\\nInitialization. We initialize neural network weights according\\nto Glorot and Bengio [2010] to provide a reasonable scaling of ac-\\ntivations and their gradients throughout the layers of the neural\\nnetwork. We initialize the hash table entries using the uniform dis-\\ntribution U(−10−4, 10−4) to provide a small amount of randomness\\nwhile encouraging initial predictions close to zero. We also tried a\\nvariety of different distributions, including zero-initialization, which\\nall resulted in a very slightly worse initial convergence speed. The\\nhash table appears to be robust to the initialization scheme.\\nTraining. We jointly train the neural network weights and the\\nhash table entries by applying Adam [Kingma and Ba 2014], where\\nwe set 𝛹���1 = 0.9, 𝛹���2 = 0.99,𝜋��� = 10−15, The choice of 𝛹���1 and 𝛹���2 makes\\nonly a small difference, but the small value of 𝜋��� = 10−15 can signifi-\\ncantly accelerate the convergence of the hash table entries when\\ntheir gradients are sparse and weak. To prevent divergence after long\\ntraining periods, we apply a weak L2 regularization (factor 10−6) to\\nthe neural network weights, but not to the hash table entries.\\nWhen fitting gigapixel images or NeRFs, we use the L2 loss. For\\nsigned distance functions, we use the mean absolute percentage\\nerror (MAPE), defined as |prediction − target|\\n|target| + 0.01\\n, and for neural radiance\\ncaching we use a luminance-relative L2 loss [Müller et al. 2021].\\nWe observed fastest convergence with a learning rate of 10−4 for\\nsigned distance functions and 10−2 otherwise, as well a a batch size\\nof 214 for neural radiance caching and 218 otherwise.\\nLastly, we skip Adam steps for hash table entries whose gradient is\\nexactly 0. This saves ∼10% performance when gradients are sparse,\\nwhich is a common occurrence with 𝐴��� ≫ BatchSize. Even though\\nthis heuristic violates some of the assumptions behind Adam, we\\nobserve no degradation in convergence.\\nNon-spatial input dimensions 𝛹��� ∈ R𝐴���. The multiresolution hash\\nencoding targets spatial coordinates with relatively low dimension-\\nality. All our experiments operate either in 2D or 3D. However, it\\nis frequently useful to input auxiliary dimensions 𝛹��� ∈ R𝐴��� to the\\nneural network, such as the view direction and material parameters\\nwhen learning a light field. In such cases, the auxiliary dimensions\\ncan be encoded with established techniques whose cost does not\\nscale superlinearly with dimensionality; we use the one-blob encod-\\ning [Müller et al. 2019] in neural radiance caching [Müller et al. 2021]\\nand the spherical harmonics basis in NeRF, similar to concurrent\\nwork [Verbin et al. 2021; Yu et al. 2021a].\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 5, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:7\\nHash table size: 𝐴��� = 222\\n𝐴��� = 222\\n𝐴��� = 212\\n𝐴��� = 217\\n𝐴��� = 222\\nReference\\nFig. 6. Approximating an RGB image of resolution 20 000 × 23 466 (469 M RGB pixels) with our multiresolution hash encoding. With hash table sizes 𝐴��� of 212,\\n217, and 222 the models shown have 117 k, 2.7 M, and 47.5 M trainable parameters respectively. With only 3.4% of the degrees of freedom of the input, the last\\nmodel achieves a reconstruction PSNR of 29.8 dB. “Girl With a Pearl Earring” renovation ©Koorosh Orooj (CC BY-SA 4.0)\\n5\\nEXPERIMENTS\\nTo highlight the versatility and high quality of the encoding, we com-\\npare it with previous encodings in four distinct computer graphics\\nprimitives that benefit from encoding spatial coordinates.\\n5.1\\nGigapixel Image Approximation\\nLearning the 2D to RGB mapping of image coordinates to colors\\nhas become a popular benchmark for testing a model’s ability to\\nrepresent high-frequency detail [Martel et al. 2021; Müller et al.\\n2019; Sitzmann et al. 2020; Tancik et al. 2020]. Recent breakthroughs\\nin adaptive coordinate networks (ACORN) [Martel et al. 2021] have\\nshown impressive results when fitting very large images—up to a bil-\\nlion pixels—with high fidelity at even the smallest scales. We target\\nour multiresolution hash encoding at the same task and converge\\nto high-fidelity images in seconds to minutes (Figure 4).\\nFor comparison, on the Tokyo panorama from Figure 1, ACORN\\nachieves a PSNR of 38.59 dB after 36.9 h of training. With a similar\\nnumber of parameters (𝐴��� = 224), our method achieves the same\\nPSNR after 2.5 minutes of training, peaking at 41.9 dB after 4 min.\\nFigure 6 showcases the level of detail contained in our model for a\\nvariety of hash table sizes 𝐴��� on another image.\\nIt is difficult to directly compare the performance of our encoding\\nto ACORN; a factor of ∼10 stems from our use of fully fused CUDA\\nkernels, provided by the tiny-cuda-nn framework [Müller 2021].\\nThe input encoding allows for the use of a much smaller MLP than\\nwith ACORN, which accounts for much of the remaining 10×–100×\\nspeedup. That said, we believe that the biggest value-add of the\\nmultiresolution hash encoding is its simplicity. ACORN relies on an\\nadaptive subdivision of the scene as part of a learning curriculum,\\nnone of which is necessary with our encoding.\\n5.2\\nSigned Distance Functions\\nSigned distance functions (SDFs), in which a 3D shape is repre-\\nsented as the zero level-set of a function of position x, are used in\\nmany applications including simulation, path planning, 3D mod-\\neling, and video games. DeepSDF [Park et al. 2019] uses a large\\nMLP to represent one or more SDFs at a time. In contrast, when just\\na single SDF needs to be fit, a spatially learned encoding, such as\\nours can be employed and the MLP shrunk significantly. This is the\\napplication we investigate in this section. As baseline, we compare\\nwith NGLOD [Takikawa et al. 2021], which achieves state-of-the-art\\nresults in both quality and speed by prefixing its small MLP with a\\nlookup from an octree of trainable feature vectors. Lookups along\\nthe hierarchy of this octree act similarly to our multiresolution cas-\\ncade of grids: they are a collision-free analog to our technique, with\\na fixed growth factor 𝐴��� = 2. To allow meaningful comparisons in\\nterms of both performance and quality, we implemented an opti-\\nmized version of NGLOD in our framework, details of which we\\ndescribe in Appendix B. Details pertaining to real-time training of\\nSDFs are described in Appendix C.\\nIn Figure 7, we compare NGLOD with our multiresolution hash\\nencoding at roughly equal parameter count. We also show a straight-\\nforward application of the frequency encoding [Mildenhall et al.\\n2020] to provide a baseline, details of which are found in Appen-\\ndix D. By using a data structure tailored to the reference shape,\\nNGLOD achieves the highest visual reconstruction quality. How-\\never, even without such a dedicated data structure, our encoding\\napproaches a similar fidelity to NGLOD in terms of the intersection-\\nover-union metric (IoU2) with similar performance and memory cost.\\nFurthermore, the SDF is defined ev-\\nerywhere within the training volume,\\nas opposed to NGLOD, which is only\\ndefined within the octree (i.e. close\\nto the surface). This permits the use\\nof certain SDF rendering techniques\\nsuch as approximate soft shadows\\nfrom a small number of off-surface\\ndistance samples [Evans 2006], as\\nshown in the adjacent figure.\\nTo emphasize differences between the compared methods, we\\nvisualize the SDF using a shading model. The resulting colors are\\nsensitive to even slight changes in the surface normal, which empha-\\nsizes small fluctuations in the prediction more strongly than in other\\ngraphics primitives where color is predicted directly. This sensitivity\\nreveals undesired microstructure in our hash encoding on the scale\\n2IoU is the ratio of volumes of the interiors of the intersection and union of the pair\\nof shapes being compared. IoU is always ≤ 1 with a perfect fit corresponding to = 1.\\nWe measure IoU by comparing the signs of the SDFs at 128 million points uniformly\\ndistributed within the bounding box of the scene.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 6, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:8\\n•\\nMüller et al.\\nHash (ours)\\nNGLOD\\nHash (ours)\\nFrequency\\nFrequency\\nHash (ours)\\nNGLOD\\nHash (ours)\\n22.3 M (params)\\n12.2 M\\n124.9 k\\n124.9 k\\n12.2 M\\n16.0 M\\n1:56 (mm:ss)\\n1:14\\n1:32\\n2:10\\n1:54\\n1:49\\n0.9777 (IoU)\\n0.9812\\n0.8432\\n0.9898\\n0.9997\\n0.9998\\n11.1 M (params)\\n12.2 M\\n124.9 k\\n124.9 k\\n12.2 M\\n24.2 M\\n1:37 (mm:ss)\\n1:19\\n1:35\\n1:21\\n1:04\\n1:50\\n0.9911 (IoU)\\n0.9872\\n0.8470\\n0.7575\\n0.9691\\n0.9749\\nFig. 7. Neural signed distance functions trained for 11 000 steps. The frequency encoding [Mildenhall et al. 2020] struggles to capture the sharp details on\\nthese intricate models. NGLOD [Takikawa et al. 2021] achieves the highest visual quality, at the cost of only training the SDF inside the cells of a close-fitting\\noctree. Our hash encoding exhibits similar numeric quality in terms of intersection over union (IoU) and can be evaluated anywhere in the scene. However, it\\nalso exhibits visually undesirable surface roughness that we attribute to randomly distributed hash collisions. Bearded Man ©Oliver Laric (CC BY-NC-SA 2.0)\\nFeature buffers\\n𝐴����enc(𝐴���;𝛹���); Φ�\\nPredicted color\\nOnline\\nsupervised\\ntraining\\nReal-time sparse path tracer\\nFig. 8. Summary of the neural radiance caching application [Müller et al. 2021]. The MLP 𝐴����enc(𝐴���;𝛹���); Φ� is tasked with predicting photorealistic pixel colors\\nfrom feature buffers independently for each pixel. The feature buffers contain, among other variables, the world-space position x, which we propose to encode\\nwith our method. Neural radiance caching is a challenging application, because it is supervised online during real-time rendering. The training data are a sparse\\nset of light paths that are continually spawned from the camera view. As such, the neural network and encoding do not learn a general mapping from features\\nto color, but rather they continually overfit to the current shape and lighting. To support animated content, training has a budget of one millisecond per frame.\\nMultiresolution hash encoding (Ours), 𝐴��� = 15, 133 FPS\\nTriangle wave encoding [Müller et al. 2021], 147 FPS\\nFar view\\nMedium view\\nClose-by view\\nFar view\\nMedium view\\nClose-by view\\nFig. 9. Neural radiance caching [Müller et al. 2021] gains much improved quality from the multiresolution hash encoding with only a mild performance\\npenalty: 133 versus 147 frames per second at a resolution of 1920×1080px. To demonstrate the online adaptivity of the multiple hash resolutions vs. the prior\\ntriangle wave encoding, we show screenshots from a smooth camera motion that starts with a far-away view of the scene (left) and zooms onto a close-by\\nview of an intricate shadow (right). Throughout the camera motion, which takes just a few seconds, the neural radiance cache continually learns from sparse\\ncamera paths, enabling the cache to learn (“overfit”) intricate detail at the scale of the content that the camera is momentarily observing.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:9\\n200\\n300\\n400\\n500\\n600\\n700\\n35.5\\n36\\n36.5\\n𝐴���neurons = 16\\n𝐴���neurons = 32\\n𝐴���neurons = 64\\n𝐴���neurons = 128\\n𝐴���neurons = 256\\nTraining time (seconds)\\nPSNR (dB)\\nNeural Radiance Field: Lego\\n𝐴���layers = 1\\n𝐴���layers = 2\\n𝐴���layers = 3\\nFig. 10. The effect of the MLP size on test error vs. training time (31 000\\ntraining steps) on the Lego scene. Other scenes behave almost identically.\\nEach curve represents a different MLP depth, where the color MLP has\\n𝐴���layers hidden layers and the density MLP has 1 hidden layer; we do not\\nobserve an improvement with deeper density MLPs. The curves sweep the\\nnumber of neurons the hidden layers of the density and color MLPs from 16\\nto 256. Informed by this analysis, we choose 𝐴���layers = 2 and 𝐴���neurons = 64.\\nof the finest grid resolution, which is absent in NGLOD and does\\nnot disappear with longer training times. Since NGLOD is essen-\\ntially a collision-free analog to our hash encoding, we attribute this\\nartifact to hash collisions. Upon close inspection, similar microstruc-\\nture can be seen in other neural graphics primitives, although with\\nsignificantly lower magnitude.\\n5.3\\nNeural Radiance Caching\\nIn neural radiance caching [Müller et al. 2021], the task of the MLP\\nis to predict photorealistic pixel colors from feature buffers; see Fig-\\nure 8. The MLP is run independently for each pixel (i.e. the model is\\nnot convolutional), so the feature buffers can be treated as per-pixel\\nfeature vectors that contain the 3D coordinate x as well as addi-\\ntional features. We can therefore directly apply our multiresolution\\nhash encoding to x while treating all additional features as auxiliary\\nencoded dimensions 𝛹��� to be concatenated with the encoded position,\\nusing the same encoding as Müller et al. [2021]. We integrated our\\nwork into Müller et al.’s implementation of neural radiance caching\\nand therefore refer to their paper for implementation details.\\nFor photorealistic rendering, the neural radiance cache is typ-\\nically queried only for indirect path contributions, which masks\\nits reconstruction error behind the first reflection. In contrast, we\\nwould like to emphasize the neural radiance cache’s error, and thus\\nthe improvement that can be obtained by using our multiresolution\\nhash encoding, so we directly visualize the neural radiance cache at\\nthe first path vertex.\\nFigure 9 shows that—compared to the triangle wave encoding of\\nMüller et al. [2021]—our encoding results in sharper reconstruction\\nwhile incurring only a mild performance overhead of 0.7 ms that\\nreduces the frame rate from 147 to 133 FPS at a resolution of 1920 ×\\n1080px. Notably, the neural radiance cache is trained online—during\\nrendering—from a path tracer that runs in the background, which\\nmeans that the 0.7 ms overhead includes both training and runtime\\ncosts of our encoding.\\n5.4\\nNeural Radiance and Density Fields (NeRF)\\nIn the NeRF setting, a volumetric shape is represented in terms of a\\nspatial (3D) density function and a spatiodirectional (5D) emission\\nOurs (MLP)\\nLinear\\nMLP\\nReference\\nFig. 11. Feeding the result of our encoding through a linear transformation\\n(no neural network) versus an MLP when learning a NeRF. The models\\nwere trained for 1 min. The MLP allows for resolving specular details and\\nreduces the amount of background noise caused by hash collisions. Due to\\nthe small size and efficient implementation of the MLP, it is only 15% more\\nexpensive—well worth the significantly improved quality.\\nfunction, which we represent by a similar neural network architec-\\nture as Mildenhall et al. [2020]. We train the model in the same ways\\nas Mildenhall et al.: by backpropagating through a differentiable ray\\nmarcher driven by 2D RGB images from known camera poses.\\nModel Architecture. Unlike the other three applications, our NeRF\\nmodel consists of two concatenated MLPs: a density MLP followed\\nby a color MLP [Mildenhall et al. 2020]. The density MLP maps\\nthe hash encoded position y = enc(x;𝛹���) to 16 output values, the\\nfirst of which we treat as log-space density. The color MLP adds\\nview-dependent color variation. Its input is the concatenation of\\n• the 16 output values of the density MLP, and\\n• the view direction projected onto the first 16 coefficients of the\\nspherical harmonics basis (i.e. up to degree 4). This is a natural\\nfrequency encoding over unit vectors.\\nIts output is an RGB color triplet, for which we use either a sigmoid\\nactivation when the training data has low dynamic-range (sRGB) or\\nan exponential activation when it has high dynamic range (linear\\nHDR). We prefer HDR training data due to the closer resemblance\\nto physical light transport. This brings numerous advantages as has\\nalso been noted in concurrent work [Mildenhall et al. 2021].\\nInformed by the analysis in Figure 10, our results were generated\\nwith a 1-hidden-layer density MLP and a 2-hidden-layer color MLP,\\nboth 64 neurons wide.\\nAccelerated ray marching. When marching along rays for both\\ntraining and rendering, we would like to place samples such that\\nthey contribute somewhat uniformly to the image, minimizing\\nwasted computation. Thus, we concentrate samples near surfaces by\\nmaintaining an occupancy grid that coarsely marks empty vs. non-\\nempty space. In large scenes, we additionally cascade the occupancy\\ngrid and distribute samples exponentially rather than uniformly\\nalong the ray. Appendix E describes these procedures in detail.\\nAt HD resolutions, synthetic and even real-world scenes can be\\ntrained in seconds and rendered at 60 FPS, without the need of\\ncaching of the MLP outputs [Garbin et al. 2021; Wizadwongsa et al.\\n2021; Yu et al. 2021b]. This high performance makes it tractable to\\nadd effects such as anti-aliasing, motion blur and depth of field by\\nbrute-force tracing of multiple rays per pixel, as shown in Figure 12.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:10\\n•\\nMüller et al.\\nTable 2. Peak signal to noise ratio (PSNR) of our NeRF implementation with multiresolution hash encoding (“Ours: Hash”) vs. NeRF [Mildenhall et al. 2020],\\nmip-NeRF [Barron et al. 2021a], and NSVF [Liu et al. 2020], which require ∼hours to train (values taken from the respective papers). To demonstrate the\\ncomparatively rapid training of our method, we list its results after training for 1 s to 5 min. For each scene, we mark the methods with least error using gold ,\\nsilver\\n, and bronze\\nmedals. To analyze the degree to which our speedup originates from our optimized implementation vs. from our hash encoding, we also\\nreport PSNR for a nearly identical version of our implementation, in which the hash encoding has been replaced by the frequency encoding and the MLP\\ncorrespondingly enlarged to match Mildenhall et al. [2020] (“Ours: Frequency”; details in Appendix D). It approaches NeRF’s quality after training for just\\n∼5 min, yet is outperformed by our full method after training for 5 s–15 s, amounting to a 20–60× improvement that can be attributed to the hash encoding.\\nMic\\nFicus\\nChair\\nHotdog\\nMaterials\\nDrums\\nShip\\nLego\\navg.\\nOurs: Hash (1 s)\\n26.09\\n21.30\\n21.55\\n21.63\\n22.07\\n17.76\\n20.38\\n18.83\\n21.202\\nOurs: Hash (5 s)\\n32.60\\n30.35\\n30.77\\n33.42\\n26.60\\n23.84\\n26.38\\n30.13\\n29.261\\nOurs: Hash (15 s)\\n34.76\\n32.26\\n32.95\\n35.56\\n28.25\\n25.23\\n28.56\\n33.68\\n31.407\\nOurs: Hash (1 min)\\n35.92\\n33.05\\n34.34\\n36.78\\n29.33\\n25.82\\n30.20\\n35.63\\n32.635\\nOurs: Hash (5 min)\\n36.22\\n33.51\\n35.00\\n37.40\\n29.78\\n26.02\\n31.10\\n36.39\\n33.176\\nmip-NeRF (∼hours)\\n36.51\\n33.29\\n35.14\\n37.48\\n30.71\\n25.48\\n30.41\\n35.70\\n33.090\\nNSVF (∼hours)\\n34.27\\n31.23\\n33.19\\n37.14\\n32.68\\n25.18\\n27.93\\n32.29\\n31.739\\nNeRF (∼hours)\\n32.91\\n30.13\\n33.00\\n36.18\\n29.62\\n25.01\\n28.65\\n32.54\\n31.005\\nOurs: Frequency (5 min)\\n31.89\\n28.74\\n31.02\\n34.86\\n28.93\\n24.18\\n28.06\\n32.77\\n30.056\\nOurs: Frequency (1 min)\\n26.62\\n24.72\\n28.51\\n32.61\\n26.36\\n21.33\\n24.32\\n28.88\\n26.669\\nFig. 12. NeRF reconstruction of a modular synthesizer and large natural\\n360 scene. The left image took 5 seconds to accumulate 128 samples at 1080p\\non a single RTX 3090 GPU, allowing for brute force defocus effects. The\\nright image was taken from an interactive session running at 10 frames per\\nsecond on the same GPU.\\nComparison with direct voxel lookups. Figure 11 shows an ablation\\nwhere we replace the entire neural network with a single linear\\nmatrix multiplication, in the spirit of (although not identical to)\\nconcurrent direct voxel-based NeRF [Sun et al. 2021; Yu et al. 2021a].\\nWhile the linear layer is capable of reproducing view-dependent\\neffects, the quality is significantly compromised as compared to the\\nMLP, which is better able to capture specular effects and to resolve\\nhash collisions across the interpolated multiresolution hash tables\\n(which manifest as high-frequency artifacts). Fortunately, the MLP\\nis only 15% more expensive than the linear layer, thanks to its small\\nsize and efficient implementation.\\nComparison with high-quality offline NeRF. In Table 2, we compare\\nthe peak signal to noise ratio (PSNR) our NeRF implementation\\nwith multiresolution hash encoding (“Ours: Hash”) with that of\\nNeRF [Mildenhall et al. 2020], mip-NeRF [Barron et al. 2021a], and\\nNSVF [Liu et al. 2020], which all require on the order of hours to\\ntrain. In contrast, we list results of our method after training for\\n1 s to 5 min. Our PSNR is competitive with NeRF and NSVF after\\njust 15 s of training, and competitive with mip-NeRF after 1 min to\\n5 min of training.\\nOn one hand, our method performs best on scenes with high\\ngeometric detail, such as Ficus, Drums, Ship and Lego, achieving the\\nbest PSNR of all methods. On the other hand, mip-NeRF and NSVF\\noutperform our method on scenes with complex, view-dependent\\nreflections, such as Materials; we attribute this to the much smaller\\nMLP that we necessarily employ to obtain our speedup of several\\norders of magnitude over these competing implementations.\\nNext, we analyze the degree to which our speedup originates\\nfrom our efficient implementation versus from our encoding. To\\nthis end, we additionally report PSNR for a nearly identical ver-\\nsion of our implementation: we replace the hash encoding by the\\nfrequency encoding and enlarge the MLP to approximately match\\nthe architecture of Mildenhall et al. [2020] (“Ours: Frequency”); see\\nAppendix D for details. This version of our algorithm approaches\\nNeRF’s quality after training for just ∼5 min, yet is outperformed by\\nour full method after training for a much shorter duration (5 s–15 s),\\namounting to a 20–60× improvement caused by the hash encoding\\nand smaller MLP.\\nFor “Ours: Hash”, the cost of each training step is roughly constant\\nat ∼6 ms per step. This amounts to 50 k steps after 5 min at which\\npoint the model is well converged. We decay the learning rate after\\n20 k steps by a factor of 0.33, which we repeat every further 10 k\\nsteps. In contrast, the larger MLP used in “Ours: Frequency” requires\\n∼30 ms per training step, meaning that the PSNR listed after 5 min\\ncorresponds to about 10 k steps. It could thus keep improving slightly\\nif trained for extended periods of time, as in the offline NeRF variants\\nthat are often trained for several 100 k steps.\\nWhile we isolated the performance and convergence impact of\\nour hash encoding and its small MLP, we believe an additional study\\nis required to quantify the impact of advanced ray marching schemes\\n(such as ours, coarse-fine [Mildenhall et al. 2020], or DONeRF [Neff\\net al. 2021]) independently from the encoding and network archi-\\ntecture. We report additional information in Section E.3 to aid in\\nsuch an analysis.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 9, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:11\\n(a) Offline rendered reference\\n(b) Hash (ours), trained for 10 s\\n(c) Path tracer\\nRendered in 32 ms (2 samples per pixel)\\nRendered in 32 ms (16 samples per pixel)\\nFig. 13. Preliminary results of training a NeRF cloud model (b) from real-time path tracing data. Within 32 ms, a 1024×1024 image of our model convincingly\\napproximates the offline rendered ground truth (a). Our model exhibits less noise than a GPU path tracer that ran for an equal amount of time (c). The cloud\\ndata is ©Walt Disney Animation Studios (CC BY-SA 3.0)\\n6\\nDISCUSSION AND FUTURE WORK\\nConcatenation vs. reduction. At the end of the encoding, we con-\\ncatenate rather than reduce (for example, by summing) the 𝐴���-di-\\nmensional feature vectors obtained from each resolution. We prefer\\nconcatenation for two reasons. First, it allows for independent, fully\\nparallel processing of each resolution. Second, a reduction of the\\ndimensionality of the encoded result y from 𝐴���𝐴��� to 𝐴��� may be too\\nsmall to encode useful information. While 𝐴��� could be increased\\nproportionally, it would make the encoding much more expensive.\\nHowever, we recognize that there may be applications in which\\nreduction is favorable, such as when the neural network is signifi-\\ncantly more expensive than the encoding, in which case the added\\ncomputational cost of increasing 𝐴��� could be insignificant. We thus\\nargue for concatenation by default and not as a hard-and-fast rule. In\\nour applications, concatenation, coupled with 𝐴��� = 2 always yielded\\nby far the best results.\\nChoice of hash function. A good hash function is efficient to com-\\npute, leads to coherent look-ups, and uniformly covers the feature\\nvector array regardless of the structure of query points. We chose\\nour hash function for its good mixture of these properties and also\\nexperimented with three others:\\n(1) The PCG32 [O’Neill 2014] RNG, which has superior statisti-\\ncal properties. Unfortunately, it did not yield a higher-quality\\nreconstruction, making its higher cost not worthwhile.\\n(2) Ordering the least significant bits of Z𝐴��� by a space-filling curve\\nand only hashing the higher bits. This leads to better look-up\\ncoherence at the cost of worse reconstruction quality. However,\\nthe speed-up is only marginally better than setting 𝜋1 := 1 as\\ndone in our hash, and is thus not worth the reduced quality.\\n(3) Even better coherence can be achieved by treating the hash\\nfunction as a tiling of space into dense grids. Like (2), the speed-\\nup is small in practice with significant detriment to quality.\\nAlternatively to hand-crafted hash functions, it is conceivable to\\noptimize the hash function in future work, turning the method into\\na dictionary-learning approach. Two possible avenues are (1) de-\\nveloping a continuous formulation of indexing that is amenable to\\nanalytic differentiation or (2) applying an evolutionary optimization\\nalgorithm that can efficiently explore the discrete function space.\\nMicrostructure due to hash collisions. The salient artifact of our\\nencoding is a small amount of “grainy” microstructure, most visible\\non the learned signed distance functions (Figure 1 and Figure 7).\\nThe graininess is a result of hash collisions that the MLP is unable\\nto fully compensate for. We believe that the key to achieving state-\\nof-the-art quality on SDFs with our encoding will be to find a way\\nto overcome this microstructure, for example by filtering hash table\\nlookups or by imposing an additional smoothness prior on the loss.\\nGenerative setting. Parametric input encodings, when used in\\na generative setting, typically arrange their features in a dense\\ngrid which can then be populated by a separate generator network,\\ntypically a CNN such as StyleGAN [Chan et al. 2021; DeVries et al.\\n2021; Peng et al. 2020b]. Our hash encoding adds an additional layer\\nof complexity, as the features are not arranged in a regular pattern\\nthrough the input domain; that is, the features are not bijective with\\na regular grid of points. We leave it to future work to determine\\nhow best to overcome this difficulty.\\nOther applications. We are interested in applying the multireso-\\nlution hash encoding to other low-dimensional tasks that require\\naccurate, high-frequency fits. The frequency encoding originated\\nfrom the attention mechanism of transformer networks [Vaswani\\net al. 2017]. We hope that parametric encodings such as ours can\\nlead to a meaningful improvement in general, attention-based tasks.\\nHeterogenous volumetric density fields, such as cloud and smoke\\nstored in a VDB [Museth 2013, 2021] data structure, often include\\nempty space on the outside, a solid core on the inside, and sparse\\ndetail on the volumetric surface. This makes them a good fit for\\nour encoding. In the code released alongside this paper, we have\\nincluded a preliminary implementation that fits a radiance and\\ndensity field directly from the noisy output of a volumetric path\\ntracer. The initial results are promising, as shown in Figure 13, and\\nwe intend to pursue this direction further in future work.\\n7\\nCONCLUSION\\nMany graphics problems rely on task specific data structures to\\nexploit the sparsity or smoothness of the problem at hand. Our\\nmulti-resolution hash encoding provides a practical learning-based\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 10, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:12\\n•\\nMüller et al.\\nalternative that automatically focuses on relevant detail, indepen-\\ndent of the task. Its low overhead allows it to be used even in time-\\nconstrained settings like online training and inference. In the context\\nof neural network input encodings, it is a drop-in replacement, for\\nexample speeding up NeRF by several orders of magnitude and\\nmatching the performance of concurrent non-neural 3D reconstruc-\\ntion techniques.\\nSlow computational processes in any setting, from lightmap bak-\\ning to the training of neural networks, can lead to frustrating work-\\nflows due to long iteration times [Enderton and Wexler 2011]. We\\nhave demonstrated that single-GPU training times measured in\\nseconds are within reach for many graphics applications, allowing\\nneural approaches to be applied where previously they may have\\nbeen discounted.\\nACKNOWLEDGMENTS\\nWe are grateful to Andrew Tao, Andrew Webb, Anjul Patney, David\\nLuebke, Fabrice Rousselle, Jacob Munkberg, James Lucas, Jonathan\\nGranskog, Jonathan Tremblay, Koki Nagano, Marco Salvi, Nikolaus\\nBinder, and Towaki Takikawa for profound discussions, proofread-\\ning, feedback, and early testing. We also thank Arman Toorians and\\nSaurabh Jain for the factory robot dataset in Figure 12 (right).\\nREFERENCES\\nThomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz.\\n2007. Convolution Shadow Maps. In Rendering Techniques, Jan Kautz and Sumanta\\nPattanaik (Eds.). The Eurographics Association.\\nhttps://doi.org/10.2312/EGWR/\\nEGSR07/051-060\\nJonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-\\nBrualla, and Pratul P. Srinivasan. 2021a. Mip-NeRF: A Multiscale Representation for\\nAnti-Aliasing Neural Radiance Fields. arXiv (2021). https://jonbarron.info/mipnerf/\\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hed-\\nman. 2021b.\\nMip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields.\\narXiv:2111.12077 (Nov. 2021).\\nRohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Love-\\ngrove, and Richard Newcombe. 2020. Deep Local Shapes: Learning Local SDF Priors\\nfor Detailed 3D Reconstruction. In Computer Vision – ECCV 2020, Andrea Vedaldi,\\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International\\nPublishing, Cham, 608–625.\\nEric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De\\nMello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\\nKarras, and Gordon Wetzstein. 2021. Efficient Geometry-aware 3D Generative\\nAdversarial Networks. arXiv:2112.07945 (2021). arXiv:2112.07945 [cs.CV]\\nJulian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. 2020. Implicit Functions in\\nFeature Space for 3D Shape Reconstruction and Completion. In IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR). IEEE.\\nTerrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and\\nJoshua M. Susskind. 2021. Unconstrained Scene Generation with Locally Condi-\\ntioned Radiance Fields. arXiv (2021).\\nEric Enderton and Daniel Wexler. 2011. The Workflow Scale. In Computer Graphics\\nInternational Workshop on VFX, Computer Animation, and Stereo Movies.\\nAlex Evans. 2006. Fast Approximations for Global Illumination on Dynamic Scenes. In\\nACM SIGGRAPH 2006 Courses (Boston, Massachusetts) (SIGGRAPH ’06). Association\\nfor Computing Machinery, New York, NY, USA, 153–171. https://doi.org/10.1145/\\n1185657.1185834\\nStephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and\\nJulien Valentin. 2021.\\nFastNeRF: High-Fidelity Neural Rendering at 200FPS.\\narXiv:2103.10380 (March 2021).\\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\\n2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th\\nInternational Conference on Machine Learning - Volume 70 (Sydney, NSW, Australia)\\n(ICML’17). JMLR.org, 1243—-1252.\\nXavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep\\nFeedforward Neural Networks. In Proc. 13th International Conference on Artificial\\nIntelligence and Statistics (Sardinia, Italy, May 13–15). JMLR.org, 249–256.\\nSaeed Hadadan, Shuhong Chen, and Matthias Zwicker. 2021. Neural radiosity. ACM\\nTransactions on Graphics 40, 6 (Dec. 2021), 1—-11. https://doi.org/10.1145/3478513.\\n3480569\\nDavid Money Harris and Sarah L. Harris. 2013. 3.4.2 - State Encodings. In Digital\\nDesign and Computer Architecture (second ed.). Morgan Kaufmann, Boston, 129–131.\\nhttps://doi.org/10.1016/B978-0-12-394424-5.00002-1\\nJon Jansen and Louis Bavoil. 2010. Fourier Opacity Mapping. In Proceedings of the 2010\\nACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (Washington,\\nD.C.) (I3D ’10). Association for Computing Machinery, New York, NY, USA, 165—-172.\\nhttps://doi.org/10.1145/1730804.1730831\\nChiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner,\\nand Thomas Funkhouser. 2020. Local Implicit Grid Representations for 3D Scenes.\\nIn Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).\\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.\\narXiv:1412.6980 (June 2014).\\nDerrick H. Lehmer. 1951. Mathematical Methods in Large-scale Computing Units. In\\nProceedings of the Second Symposium on Large Scale Digital Computing Machinery.\\nHarvard University Press, Cambridge, United Kingdom, 141–146.\\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika\\nAittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration without\\nClean Data. arXiv:1803.04189 (March 2018).\\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.\\nNeural Sparse Voxel Fields. NeurIPS (2020). https://lingjie0206.github.io/papers/\\nNSVF/\\nJulien N.P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro,\\nand Gordon Wetzstein. 2021. ACORN: Adaptive Coordinate Networks for Neural\\nRepresentation. ACM Trans. Graph. (SIGGRAPH) (2021).\\nIshit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and\\nManmohan Chandraker. 2021. Modulated Periodic Activations for Generalizable\\nLocal Functional Representations. In IEEE International Conference on Computer\\nVision. IEEE.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David\\nGarcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and\\nHao Wu. 2018. Mixed Precision Training. arXiv:1710.03740 (Oct. 2018).\\nBen Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul Srinivasan, and\\nJonathan T. Barron. 2021. NeRF in the Dark: High Dynamic Range View Synthesis\\nfrom Noisy Raw Images. arXiv:2111.13679 (Nov. 2021).\\nBen Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari,\\nRavi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion:\\nPractical View Synthesis with Prescriptive Sampling Guidelines. ACM Trans. Graph.\\n38, 4, Article 29 (July 2019), 14 pages. https://doi.org/10.1145/3306346.3322980\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\\nfor View Synthesis. In ECCV.\\nThomas\\nMüller.\\n2021.\\nTiny\\nCUDA\\nNeural\\nNetwork\\nFramework.\\nhttps://github.com/nvlabs/tiny-cuda-nn.\\nThomas Müller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novák.\\n2019. Neural Importance Sampling. ACM Trans. Graph. 38, 5, Article 145 (Oct. 2019),\\n19 pages. https://doi.org/10.1145/3341156\\nThomas Müller, Fabrice Rousselle, Alexander Keller, and Jan Novák. 2020. Neural\\nControl Variates. ACM Trans. Graph. 39, 6, Article 243 (Nov. 2020), 19 pages. https:\\n//doi.org/10.1145/3414685.3417804\\nThomas Müller, Fabrice Rousselle, Jan Novák, and Alexander Keller. 2021. Real-time\\nNeural Radiance Caching for Path Tracing. ACM Trans. Graph. 40, 4, Article 36 (Aug.\\n2021), 16 pages. https://doi.org/10.1145/3450626.3459812\\nKen Museth. 2013. VDB: High-Resolution Sparse Volumes with Dynamic Topology.\\nACM Trans. Graph. 32, 3, Article 27 (July 2013), 22 pages. https://doi.org/10.1145/\\n2487228.2487235\\nKen Museth. 2021. NanoVDB: A GPU-Friendly and Portable VDB Data Structure For\\nReal-Time Rendering And Simulation. In ACM SIGGRAPH 2021 Talks (Virtual Event,\\nUSA) (SIGGRAPH ’21). Association for Computing Machinery, New York, NY, USA,\\nArticle 1, 2 pages. https://doi.org/10.1145/3450623.3464653\\nThomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller,\\nChakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger.\\n2021.\\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance\\nFields using Depth Oracle Networks.\\nComputer Graphics Forum 40, 4 (2021).\\nhttps://doi.org/10.1111/cgf.14340\\nMatthias Nießner, Michael Zollhöfer, Shahram Izadi, and Marc Stamminger. 2013. Real-\\nTime 3D Reconstruction at Scale Using Voxel Hashing. ACM Trans. Graph. 32, 6,\\nArticle 169 (nov 2013), 11 pages. https://doi.org/10.1145/2508363.2508374\\nFakir S. Nooruddin and Greg Turk. 2003. Simplification and Repair of Polygonal Models\\nUsing Volumetric Techniques. IEEE Transactions on Visualization and Computer\\nGraphics 9, 2 (apr 2003), 191––205. https://doi.org/10.1109/TVCG.2003.1196006\\nMelissa E. O’Neill. 2014. PCG: A Family of Simple Fast Space-Efficient Statistically Good\\nAlgorithms for Random Number Generation. Technical Report HMC-CS-2014-0905.\\nHarvey Mudd College, Claremont, CA.\\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-\\ngrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 11, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:13\\nRepresentation. arXiv:1901.05103 (Jan. 2019).\\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.\\n2020a. Convolutional Occupancy Networks. In European Conference on Computer\\nVision (ECCV).\\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.\\n2020b. Convolutional Occupancy Networks. (2020). arXiv:2003.04618 [cs.CV]\\nMatt Pharr, Wenzel Jakob, and Greg Humphreys. 2016. Physically Based Rendering:\\nFrom Theory to Implementation (3rd ed.) (3rd ed.). Morgan Kaufmann Publishers\\nInc., San Francisco, CA, USA. 1266 pages.\\nVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and\\nGordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation\\nFunctions. In Proc. NeurIPS.\\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2021. Direct Voxel Grid Optimization:\\nSuper-fast Convergence for Radiance Fields Reconstruction. arXiv:2111.11215 (Nov.\\n2021).\\nTowaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek\\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural\\nGeometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (2021).\\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin\\nRaghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng.\\n2020. Fourier Features Let Networks Learn High Frequency Functions in Low\\nDimensional Domains. NeurIPS (2020). https://bmild.github.io/fourfeat/index.html\\nDanhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan\\nTaylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, Shahram Izadi,\\nand Andrea Tagliasacchi. 2018. Real-Time Compression and Streaming of 4D Per-\\nformances. ACM Trans. Graph. 37, 6, Article 256 (dec 2018), 11 pages.\\nhttps:\\n//doi.org/10.1145/3272127.3275096\\nMatthias Teschner, Bruno Heidelberger, Matthias Müller, Danat Pomeranets, and\\nMarkus Gross. 2003. Optimized Spatial Hashing for Collision Detection of De-\\nformable Objects. In Proceedings of VMV’03, Munich, Germany. 47–54.\\nSergios Theodoridis. 2008. Pattern Recognition. Elsevier.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\\nGomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\\nAttention Is All You Need.\\narXiv:1706.03762 (June 2017).\\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and\\nPratul P. Srinivasan. 2021. Ref-NeRF: Structured View-Dependent Appearance for\\nNeural Radiance Fields. arXiv:2112.03907 (Dec. 2021).\\nSuttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn\\nSuwajanakorn. 2021. NeX: Real-time View Synthesis with Neural Basis Expansion.\\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\\nAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and\\nAngjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks.\\narXiv:2112.05131 (Dec. 2021).\\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021b.\\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.\\nA\\nSMOOTH INTERPOLATION\\nOne may desire smoother interpolation than the 𝐴���-linear interpola-\\ntion that our multiresolution hash encoding uses by default.\\nIn this case, the obvious solution would be using a 𝐴���-quadratic or\\n𝐴���-cubic interpolation, both of which are however very expensive\\ndue to requiring the lookup of 3𝐴��� and 4𝐴��� instead of 2𝐴��� vertices,\\nrespectively. As a low-cost alternative, we recommend applying the\\nsmoothstep function,\\n𝐴���1(𝐴���) = 𝐴���2(3 − 2𝐴���) ,\\n(5)\\nto the 𝐴���-linear interpolation weights. Crucially, the derivative of the\\nsmoothstep,\\n𝐴���′\\n1(𝐴���) = 6𝐴���(1 − 𝐴���) ,\\n(6)\\nvanishes at 0 and at 1, causing the discontinuity in the derivatives\\nof the encoding to vanish by the chain rule. The encoding thus\\nbecomes 𝐴���1-smooth.\\nHowever, by this trick, we have merely traded discontinuities for\\nzero-points in the individual levels which are not necessarily more\\ndesirable. So, we offset each level by half of its voxel size 1/(2𝐴���𝐴���),\\nwhich prevents the zero derivatives from aligning across all levels.\\nThe encoding is thus able to learn smooth, non-zero derivatives for\\nall spatial locations x.\\nFor higher-order smoothness, higher-order smoothstep functions\\n𝐴���𝐴��� can be used at small additional cost. In practice, the computational\\ncost of the 1st order smoothstep function 𝐴���1 is hidden by memory\\nbottlenecks, making it essentially free. However, the reconstruction\\nquality tends to decrease as higher-order interpolation is used. This\\nis why we do not use it by default. Future research is needed to\\nexplain the loss of quality.\\nB\\nIMPLEMENTATION DETAILS OF NGLOD\\nWe designed our implementation of NGLOD [Takikawa et al. 2021]\\nsuch that it closely resembles that of our hash encoding, only dif-\\nfering in the underlying data structure; i.e. using the vertices of\\nan octree around ground-truth triangle mesh to store collision-free\\nfeature vectors, rather than relying on hash tables. This results\\nin a notable difference to the original NGLOD: the looked-up fea-\\nture vectors are concatenated rather than summed, which in our\\nimplementation serendipitously resulted in higher reconstruction\\nquality compared to the summation of an equal number of trainable\\nparameters.\\nThe octree implies a fixed growth factor 𝐴��� = 2, which leads to\\na smaller number of levels than our hash encoding. We obtained\\nthe most favorable performance vs. quality trade-off at a roughly\\nequal number of trainable parameters as our method, through the\\nfollowing configuration:\\n(1) the number of feature dimensions per entry is 𝐴��� = 8,\\n(2) the number of levels is 𝐴��� = 10, and\\n(3) look-ups start at level 𝐴��� = 4.\\nThe last point is important for two reasons: first, it matches the\\ncoarsest resolution of our hash tables 24 = 16 = 𝐴���min, and second, it\\nprevents a performance bottleneck that would arise when all threads\\nof the GPU atomically accumulate gradients in few, coarse entries.\\nWe experimentally verified that this does not lead to reduced quality,\\ncompared to looking up the entire hierarchy.\\nC\\nREAL-TIME SDF TRAINING DATA GENERATION\\nIn order to not bottleneck our SDF training, we must be able to\\ngenerate a large number of ground truth signed distances to high-\\nresolution meshes very quickly (∼millions per second).\\nC.1\\nEfficient Sampling of 3D Training Positions\\nSimilar to prior work [Takikawa et al. 2021], we distribute some\\n(1/8th) of our training positions uniformly in the unit cube, some\\n(4/8ths) uniformly on the surface of the mesh, and the remainder\\n(3/8ths) perturbed from the surface of the mesh.\\nThe uniform samples in the unit cube are trivial to generate using\\nany pseudorandom number generator; we use a GPU implementa-\\ntion of PCG32 [O’Neill 2014].\\nTo generate the uniform samples on the surface of the mesh, we\\ncompute the area of each triangle in a preprocessing step, normalize\\nthe areas to represent a probability distribution, and store the corre-\\nsponding cumulative distribution function (CDF) in an array. Then,\\nfor each sample, we select a triangle proportional to its area by the\\ninversion method—a binary search of a uniform random number\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='102:14\\n•\\nMüller et al.\\nover the CDF array—and sample a uniformly random position on\\nthat triangle by standard sample warping [Pharr et al. 2016].\\nLastly, for those surface samples that must be perturbed, we add\\na random 3D vector, each dimension independently drawn from\\na logistic distribution (similar shape to a Gaussian, but cheaper to\\ncompute) with standard deviation 𝐴���/1024, where 𝐴��� is the bounding\\nradius of the mesh.\\nOctree sampling for NGLOD. When training our implementation\\nof Takikawa et al. [2021], we must be careful to rarely generate\\ntraining positions outside of octree leaf nodes. To this end, we\\nreplace the uniform unit cube sampling routine with one that creates\\nuniform 3D positions in the leaf nodes of the octree by first rejection\\nsampling a uniformly random leaf node from the array of all nodes\\nand then generating a uniform random position within the node’s\\nvoxel. Fortunately, the standard deviation 𝐴���/1024 of our logistic\\nperturbation is small enough to almost never leave the octree, so\\nwe do not need to modify the surface sampling routine.\\nC.2\\nEfficient Signed Distances to the Triangle Mesh\\nFor each sampled 3D position x, we must compute the signed dis-\\ntance to the triangle mesh. To this end, we first construct a triangle\\nbounding volume hierarchy (BVH) with which we perform efficient\\nunsigned distance queries; O� log 𝐴���triangles\\n� on average.\\nNext, we sign these distances by tracing 32 “stab rays” [Nooruddin\\nand Turk 2003], which we distribute uniformly over the sphere using\\na Fibonacci lattice that is pseudorandomly and independently offset\\nfor every training position. If any of these rays reaches infinity, the\\ncorresponding position x is deemed “outside” of the object and the\\ndistance is marked positive. Otherwise, it is marked negative.3\\nFor maximum efficiency, we use NVIDIA ray tracing hardware\\nthrough the OptiX 7 framework, which is over an order of magnitude\\nfaster than using the previously mentioned triangle BVH for ray-\\nshape intersections on our RTX 3090 GPU.\\nD\\nBASELINE MLPS WITH FREQUENCY ENCODING\\nIn our signed distance function (SDF), neural radiance caching\\n(NRC), and neural radiance and density fields (NeRF) experiments,\\nwe use an MLP prefixed by a frequency encoding as baseline. The\\nrespective architectures are equal to those in the main text, except\\nthat the MLPs are larger and that the hash encoding is replaced by\\nsine and cosine waves (SDF and NeRF) or triangle waves (NRC).\\nThe following table lists the number of hidden layers, neurons per\\nhidden layer, frequency cascades (each scaled by a factor of 2 as per\\nVaswani et al. [2017]), and adjusted learning rates.\\nPrimitive\\nHidden layers\\nNeurons\\nFrequencies\\nLearning rate\\nSDF\\n8\\n128\\n10\\n3 · 10−4\\nNRC\\n3\\n64\\n10\\n10−2\\nNeRF\\n7 / 1\\n256 / 256\\n16 / 4\\n10−3\\nFor NeRF, the first listed number corresponds to the density MLP\\n3If the mesh is watertight, it is cheaper to sign the distance based on the normal(s) of\\nthe closest triangle(s) from the previous step. We also implemented this procedure, but\\ndisable it by default due to its incompatibility with typical meshes in the wild.\\nand the second number to the color MLP. For SDFs, we make two ad-\\nditional changes: (1) we optimize against the relative L2 loss [Lehti-\\nnen et al. 2018] instead of the MAPE described in the main text, and\\n(2) we perturb training samples with a standard deviation of 𝐴���/128\\nas opposed to the value of 𝐴���/1024 from Appendix C.1. Both changes\\nsmooth the loss landscape, resulting in a better reconstruction with\\nthe above configuration.\\nNotably, even though the above configurations have fewer param-\\neters and are slower than our configurations with hash encoding,\\nthey represent favorable performance vs. quality trade-offs. An equal\\nparameter count comparison would make pure MLPs too expensive\\ndue to their scaling with O(𝐴���2) as opposed to the sub-linear scaling\\nof trainable encodings. On the other hand, an equal throughput com-\\nparison would require prohibitively small MLPs, thus underselling\\nthe reconstruction quality that pure MLPs are capable of.\\nWe also experimented with Fourier features [Tancik et al. 2020]\\nbut did not obtain better results compared to the axis-aligned fre-\\nquency encodings mentioned previously.\\nE\\nACCELERATED NERF RAY MARCHING\\nThe performance of ray marching algorithms such as NeRF strongly\\ndepends on the marching scheme. We utilize three techniques with\\nimperceivable error to optimize our implementation:\\n(1) exponential stepping for large scenes,\\n(2) skipping of empty space and occluded regions, and\\n(3) compaction of samples into dense buffers for efficient execution.\\nE.1\\nRay Marching Step Size and Stopping\\nIn synthetic NeRF scenes, which we bound to the unit cube [0, 1]3,\\nwe use a fixed ray marching step size equal to Δ𝐴��� :=\\n√\\n3/1024;\\n√\\n3\\nrepresents the diagonal of the unit cube.\\nIn all other scenes, based on the intercept theorem4, we set the\\nstep size proportional to the distance 𝐴��� along the ray Δ𝐴��� := 𝐴���/256,\\nclamped to the interval\\n�√\\n3/1024,𝐴��� ·\\n√\\n3/1024\\n�, where 𝐴��� is size of\\nthe largest axis of the scene’s bounding box. This choice of step\\nsize exhibits exponential growth in 𝐴���, which means that the compu-\\ntation cost grows only logarithmically in scene diameter, with no\\nperceivable loss of quality.\\nLastly, we stop ray marching and set the remaining contribution to\\nzero as soon as the transmittance of the ray drops below a threshold;\\nin our case 𝜋��� = 10−4.\\nRelated work. Mildenhall et al. [2019] already identified a non-\\nlinear step size as benefitial: they recommend sampling uniformly\\nin the disparity-space of the average camera frame, which is more\\naggressive than our exponential stepping, requiring on one hand\\nonly a constant number of steps, but on the other hand can lead to a\\nloss of fidelity compared to exponential stepping [Neff et al. 2021].\\nIn addition to non-linear stepping, some prior methods propose to\\nwarp the 3D domain of the scene towards the origin, thereby improv-\\ning the numerical properties of their input encodings [Barron et al.\\n2021b; Mildenhall et al. 2020; Neff et al. 2021]. This causes rays to\\ncurve, which leads to a worse reconstruction in our implementation.\\n4The appearance of objects stays the same as long as their size and distance from the\\nobserver remain proportional.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\\n•\\n102:15\\nIn contrast, we linearly map input coordinates into the unit cube\\nbefore feeding them to our hash encoding, relying on its exponential\\nmultiresolution growth to reach a proportionally scaled maximum\\nresolution 𝐴���max with a constant number of levels (variable 𝐴��� as in\\nEquation (3)) or logarithmically many levels 𝐴��� (constant 𝐴���).\\nE.2\\nOccupancy Grids\\nTo skip ray marching steps in empty space, we maintain a cascade\\nof 𝐴��� multiscale occupancy grids, where 𝐴��� = 1 for all synthetic NeRF\\nscenes (single grid) and 𝐴��� ∈ [1, 5] for larger real-world scenes (up to\\n5 grids, depending on scene size). Each grid has a resolution of 1283,\\nspanning a geometrically growing domain [−2𝐴���−1 +0.5, 2𝐴���−1 +0.5]3\\nthat is centered around (0.5, 0.5, 0.5).\\nEach grid cell stores occupancy as a single bit. The cells are laid\\nout in Morton (z-curve) order to facilitate memory-coherent traver-\\nsal by a digital differential analyzer (DDA). During ray marching,\\nwhenever a sample is to be placed according to the step size from\\nthe previous section, the sample is skipped if its grid cell’s bit is low.\\nWhich one of the 𝐴��� grids is queried is determined by both the\\nsample position x and the step size Δ𝐴���: among the grids covering x,\\nthe finest one with cell side-length larger than Δ𝐴��� is queried.\\nUpdating the occupancy grids. To continually update the occu-\\npancy grids while training, we maintain a second set of grids that\\nhave the same layout, except that they store full-precision floating\\npoint density values rather than single bits.\\nWe update the grids after every 16 training iterations by perform-\\ning the following steps. We\\n(1) decay the density value in each grid cell by a factor of 0.95,\\n(2) randomly sample 𝐴��� candidate cells, and set their value to the\\nmaximum of their current value and the density component of\\nthe NeRF model at a random location within the cell, and\\n(3) update the occupancy bits by thresholding each cell’s density\\nwith 𝐴��� = 0.01 · 1024/\\n√\\n3, which corresponds to thresholding the\\nopacity of a minimal ray marching step by 1 − exp(−0.01) ≈ 0.01.\\nThe sampling strategy of the 𝐴��� candidate cells depends on the\\ntraining progress since the occupancy grid does not store reliable\\ninformation in early iterations. During the first 256 training steps,\\nwe sample 𝐴��� = 𝐴��� · 1283 cells uniformly without repetition. For sub-\\nsequent training steps we set 𝐴��� = 𝐴��� · 1283/2 which we partition\\ninto two sets. The first 𝐴���/2 cells are sampled uniformly among\\nall cells. Rejection sampling is used for the remaining samples to\\nrestrict selection to cells that are currently occupied.\\nRelated work. The idea of constraining the MLP evaluation to\\noccupied cells has already been exploited in prior work on trainable,\\ncell-based encodings [Liu et al. 2020; Sun et al. 2021; Yu et al. 2021a,b].\\nIn contrast to these papers, our occupancy grid is independent from\\nthe learned encoding, allowing us to represent it more compactly\\nas a bitfield (and thereby at a resolution that is decoupled from that\\nof the encoding) and to utilize it when comparing against other\\nmethods that do not have a trained spatial encoding, e.g. “Ours:\\nFrequency” in Table 2.\\nEmpty space can also be skipped by importance sampling the\\ndepth distribution, such as by resampling the result of a coarse\\nTable 3. Batch size, number of rays per batch, and number of samples per\\nray for our full method (“Ours: Hash”), our implementation of frequency\\nencoding NeRF (“Ours: Freq.”) and mip-NeRF. Since the values correspond-\\ning to our method vary by scene, we report minimum and maximum values\\nover the synthetic scenes from Table 2.\\nMethod\\nBatch size\\n=\\nSamples per ray\\n×\\nRays per batch\\nOurs: Hash\\n256 Ki\\n3.1 to 25.7\\n10 Ki to 85 Ki\\nOurs: Freq.\\n256 Ki\\n2.5 to 9\\n29 Ki to 105 Ki\\nmip-NeRF\\n1 Mi\\n128 coarse + 128 fine\\n4 Ki\\nprediction [Mildenhall et al. 2020] or through neural importance\\nsampling [Müller et al. 2019] as done in DONeRF [Neff et al. 2021].\\nE.3\\nNumber of Rays Versus Batch Size\\nThe batch size has a significant effect on the quality and speed of\\nNeRF convergence. We found that training from a larger number\\nof rays, i.e. incorporating more viewpoint variation into the batch,\\nconverged to lower error in fewer steps. In our implementation\\nwhere the number of samples per ray is variable due to occupancy,\\nwe therefore include as many rays as possible in batches of fixed size\\nrather than building variable-size batches from a fixed ray count.\\nIn Table 3, we list ranges of the resulting number of rays per\\nbatch and corresponding samples per ray. We use a batch size of\\n256 Ki, which resulted in the fastest wall-clock convergence in our\\nexperiments. This is 4× smaller than the batch size chosen in mip-\\nNeRF, likely due to the larger number of samples each of their\\nrays requires. However, due to the myriad other differences across\\nimplementations, a more detailed study must be carried out to draw\\na definitive conclusion.\\nLastly, we note that the occupancy grid in our frequency-encoding\\nbaseline (“Ours: Freq.”; Appendix D) produces even fewer samples\\nthan when used alongside our hash encoding. This can be explained\\nby the slightly more detailed reconstruction of the hash encoding:\\nwhen the extra detail is finer than the occupancy grid resolution,\\nits surrounding empty space can not be effectively culled away and\\nmust be traversed by extra steps.\\nACM Trans. Graph., Vol. 41, No. 4, Article 102. Publication date: July 2022.\\n', metadata={'source': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'file_path': 'data/Muller et al. - 2022 - Instant neural graphics primitives with a multires.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Instant neural graphics primitives with a multiresolution hash encoding', 'author': 'Thomas Müller', 'subject': 'ACM Trans. Graph. 2022.41:1-15', 'keywords': 'Image Synthesis, Neural Networks, Encodings, Hashing, GPUs, Parallel Computation, Function Approximation. ', 'creator': 'LaTeX with acmart 2021/09/24 v1.80 Typesetting articles for the Association for Computing Machinery and hyperref 2021-02-27 v7.00k Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) kpathsea version 6.3.3; modified using iText 4.2.0 by 1T3XT', 'creationDate': \"D:20220626064329+02'00'\", 'modDate': \"D:20230814093520-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='CCX-RAYNET: A CLASS CONDITIONED CONVOLUTIONAL NEURAL NETWORK FOR\\nBIPLANAR X-RAYS TO CT VOLUME\\nMd Aminur Rab Ratul, Kun Yuan, WonSook Lee\\nSchool of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada\\nABSTRACT\\nDespite the advancement of the deep neural network, the 3D\\nCT reconstruction from its correspondence 2D X-ray is still a\\nchallenging task in computer vision. To tackle this issue here,\\nwe proposed a new class-conditioned network, namely CCX-\\nrayNet, which is proﬁcient in recapturing the shapes and tex-\\ntures with prior semantic information in the resulting CT vol-\\nume. Firstly, we propose a Deep Feature Transform (DFT)\\nmodule to modulate the 2D feature maps of semantic segmen-\\ntation spatially by generating the afﬁne transformation pa-\\nrameters. Secondly, by bridging 2D and 3D features (Depth-\\nAware Connection), we heighten the feature representation of\\nthe X-ray image. Particularly, we approximate a 3D atten-\\ntion mask to be employed on the enlarged 3D feature map,\\nwhere the contextual association is emphasized. Furthermore,\\nin the biplanar view model, we incorporate the Adaptive Fea-\\nture Fusion (AFF) module to relieve the registration problem\\nthat occurs with unrestrained input data by using the simi-\\nlarity matrix. As far as we are aware, this is the ﬁrst study\\nto utilize prior semantic knowledge in the 3D CT reconstruc-\\ntion. Both qualitative and quantitative analyses manifest that\\nour proposed CCX-rayNet outperforms the baseline method.\\nIndex Terms— CT Reconstruction, GAN, X-ray, Medi-\\ncal Imaging, CNN, LS-GAN, 3D Patch Discriminator\\n1. INTRODUCTION\\nX-ray is a popular medical imaging technique where all tis-\\nsues and organs are projected on a 2D plane. Contrarily, in\\nComputed Tomography (CT), physicians can inspect the in-\\nner body’s anatomies in the 3D space to properly understand\\na patient’s injuries than the X-ray. However, the high proba-\\nbility of cancer to patients and surgeons due to excessive ra-\\ndiation and less accessibility because of cost inspired us to\\nbuild a 3D CT reconstruction model from X-ray. Besides,\\nreconstructed CT volume can help physicians measure the di-\\nmension of major organs and diagnose ill-positioned organs.\\nLately, a new GAN [1] based method, called X2CT-GAN [2]\\nuse orthogonal views of X-rays to learn anatomies of the hu-\\nman body. However, this method suffers from several issues:\\nlost depth knowledge in the 2D X-ray feature map, and during\\nthe X-ray capturing registration problem occurred because the\\nmethod foists limitations on the patient’s movement, which\\ncauses artifacts. Moreover, the deep learning based CT recon-\\nstruction methods mainly suffering from three main issues:\\nshape distortion, loss of depth information, and registration\\nproblem. In this work, we attempt to solve these problems by\\nemploying three new modules in the CCX-rayNet.\\nUnlike several previous methods where hundreds of X-\\nrays are required, for CCX-rayNet, we need at most two X-\\nrays to provide a corresponding CT. Here, the contributions of\\nour work are three folds: (1) to the best of our knowledge, we\\nare the ﬁrst to apply the semantic prior constraint during 3D\\nCT reconstruction to provide the network distinct anatomical\\nstructures and textures information about different organs as\\ndifferent organs with different semantics should treat differ-\\nently. Inspired by [3], we proposed DFT modules to modulate\\nthe 2D feature maps of semantic segmentation spatially. This\\nprocedure provides the structure and textures of different or-\\ngans in our system to tackle the shape distortion problem. (2)\\nWe proposed Depth Aware Connection (DAC) to lessen the\\ndepth information loss and to remove insigniﬁcant features\\nwhen bridging 2D and 3D features. (3) The proposed AFF\\nmodule used weighted sum instead of the average sum to fuse\\nthe multiple views of features. The AFF module vastly relies\\non the attention mechanism that helps the network amplify the\\nmost productive features and restrain the unregistered ones.\\n2. METHOD AND MATERIALS\\nWe have two parallel encoder-decoder networks for frontal\\nand lateral X-rays, and a fusion network (in the middle) in-\\ncludes 3D basic blocks to fuse the information. We follow\\nthe same structure as X2CT [2] for the reconstruction and uti-\\nlize UNet [4] to achieve the frontal prior segmentation infor-\\nmation and feed this in the frontal encoder-decoder network.\\nWe precisely plug-in the DFT and DAC in encoder-decoder\\nnetworks and attached AFF in the fusion network. We ap-\\nply a 3D patch discriminator [5] with CCX-rayNet to produce\\nCCX-rayGAN to improve the visualization of CT volumes.\\n2.1. Deep Feature Transform (DFT)\\nAs reﬂected in Fig. 1, to conserve the shape information and\\ntopology, we fed the segmentation map of the frontal view X-\\n2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)\\nApril 13-16, 2021, Nice, France\\n978-1-6654-1246-9/21/$31.00 ©2021 IEEE\\n1655\\n2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) | 978-1-6654-1246-9/20/$31.00 ©2021 IEEE | DOI: 10.1109/ISBI48211.2021.9433870\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'file_path': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.6', 'title': 'Ccx-Raynet: A Class Conditioned Convolutional Neural Network for Biplanar X-Rays to Ct Volume', 'author': 'Md. Aminur Ratul, Kun Yuan, WonSook Lee', 'subject': '2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), April 13-16, 2021, Nice, France', 'keywords': 'Machine learning; X-ray imaging; Computed tomography (CT)', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64)', 'creationDate': \"D:20210515142714-07'00'\", 'modDate': \"D:20210724165149+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 1. The generator part of biplanar CCX-rayNet contains two encoder-decoder networks. It includes the proposed DFT,\\nDAC, and AFF modules. The input of the system is posterior-anterior and lateral views of X-ray, and the segmentation map.\\nray into the condition network to originate the conditions. To\\ntransform the 2D feature, the DFT module uses the mapping\\nfunction M to provide modulation parameter pair (α, β).\\n(α, β) = M(c)\\n(1)\\nWhere c indicates the input conditions. The main feature\\nmap will be shifted and scaled after obtaining (α, β).\\nFout = DFT(Fin) = Fin ⊙ α + β\\n(2)\\nFin has the similar dimension of α and β.\\nWith speciﬁc\\nsemantic prior information, the element-wise addition (+)\\nand element-wise product ⊙ use to perform an element-wise\\ntransformation in DFT module.\\nAs illustrated in Fig.\\n1,\\nconditions have been set as shared intermediate values and\\ntransmitted to every 2D encoder so that the DFT can acquire\\nfew parameters. End-to-end training use to optimize M.\\n2.2. Depth-Aware Connection (DAC)\\nDepth information loss in the perspective projection is a com-\\nmon problem in some encoder-decoder based 3D reconstruc-\\ntion method, and their connection is not constructed for the\\nmedical ﬁeld. We build a skip connection, namely DAC, be-\\ntween 2D and 3D features maps to tackle the depth infor-\\nmation loss. DAC follows the principle of X-ray generation,\\nwhere it reduces the information lost in the compressed axis.\\nEach CT slice holds different details, but expanding opera-\\ntion considers that each slice has the same features and, at the\\nsame time, ignores valuable depth information of inter slices.\\nThus, we create a depth-aware connection (DAC) which ac-\\ncentuate speciﬁc areas from X-ray for distinct CT slices.\\nAt ﬁrst, the intermediate 3D feature map has been ex-\\npanded from a 2D feature map by duplicating this 2D one\\nalong the third axis. Then, employ the 3D feature map into a\\nbasic CNN block impose it with a shape of (B, C, D, H, W).\\nNext, from the 2D feature map, originate an attention ma-\\ntrix with the shape of (B, D, H, W). Lastly, we enhance the\\nattention matrix C times and perform the element-wise mul-\\ntiplication with the 3D feature map. The action sums up as:\\nG = conv 2D(I)\\n(3)\\nA = sigmoid(E(Conv 2D(I)))\\n(4)\\nˆG = conv 3D(G)\\n(5)\\nDAC(I) = A ⊙ ˆG\\n(6)\\nI, G, and ˆG denote the input 2D feature map, intermedi-\\nate 3D feature map, and 3D feature map before applying the\\nattention map, respectively. Moreover, expanding operation\\nis signiﬁed as E, and the attention matrix is produced from I.\\n2.3. Adaptive Feature Fusion (AFF)\\nWe use a synthesized X-ray dataset to train the model, which\\nis unfeasible in the real world. In the clinical X-ray acquiring\\nprocess, biplanar X-rays offset with each other in a little mar-\\ngin. Besides, we have to consider the computational burden\\n1656\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'file_path': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'page': 1, 'total_pages': 5, 'format': 'PDF 1.6', 'title': 'Ccx-Raynet: A Class Conditioned Convolutional Neural Network for Biplanar X-Rays to Ct Volume', 'author': 'Md. Aminur Ratul, Kun Yuan, WonSook Lee', 'subject': '2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), April 13-16, 2021, Nice, France', 'keywords': 'Machine learning; X-ray imaging; Computed tomography (CT)', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64)', 'creationDate': \"D:20210515142714-07'00'\", 'modDate': \"D:20210724165149+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='in the fusing part. Hence, in the proposed AFF module, we\\ninclude an attention mechanism and similarity matrix to fuse\\nthe feature ﬂexibly from orthogonal views.\\nSIM = I1 ⊙ I2\\n(7)\\nHere, we compute a similarity matrix SIM for the dis-\\ntance of two 3D feature maps in the fusing part. I1 and I2\\nare orthogonal feature maps. The similarity matrix is a typ-\\nical blueprint of the non-local module and is utilized when\\ncorresponding feature voxels are consistent. We apply the dot\\ndistance for distance measurement due to computational is-\\nsues. Besides, two weighting matrices ﬂexibly enhance one\\nof the feature point and compress the other one’s contribution.\\nW1 = conv 3D(SIM)\\n(8)\\nW2 = conv 3D(SIM)\\n(9)\\nThe conv 3D does not share the weights, and the weighted\\nsum is implemented to fuse the features by multiplying the 3D\\nfeature maps with the corresponding weighting matrices.\\nAFF(I1, I2) = W1 ⊙ I1 + W2 ⊙ I2\\n(10)\\n2.4. Dataset Preparation\\nFig. 2. Posterior-anterior chest X-rays from the dataset (ﬁrst\\nrow); predicted segmentation map from UNets (second row).\\nFor training, we require an X-ray and CT paired dataset.\\nYing et al. [2] provide synthetic X-rays from real CT scans\\nby employing digitally reconstructed radiograph (DRR) tech-\\nnology followed by CycleGAN [6]. The 1018 chest CT scans\\nare gathered from the LIDC-IDRI dataset [7]. For training\\nand testing, we use 916 and 102 CT scans, respectively.\\nTo deliver semantic information to the main network, we\\ntrain two binary UNets[4] for lungs and heart segmentation.\\n704 posterior-anterior chest X-ray and the ground truth value\\nof Montgomery and Shenzhen chest X-ray dataset [8] have\\nbeen used to produce the lung segmentation maps.\\nNext,\\nJSRT [9] and SCR [10] databases utilized to produce the heart\\nsegmentation map. After training and testing on our dataset,\\nwe acquired multi-categorical segmentation maps (in Fig.2)\\nand then resized to 128×128 for the reconstruction task.\\n3. EXPERIMENT\\nTo show the efﬁcacy of our proposed CCX-rayNet in this sec-\\ntion, we provide the model’s qualitative and quantitive analy-\\nses. We also display the network settings and ablation study.\\n3.1. Network Settings\\nWe train this network for 100 epochs with Adam optimizer,\\nwhere the initial learning rate is 2e−3 and decay the learning\\nrate after 50 epochs with 30 percent. For training, we apply\\ninstance normalization instead of batch normalization.\\nWe follow two different approaches to train our model:\\n(1) the projection pixel-wise L1 loss and voxel-wise loss\\nare incorporated, signiﬁed as CCX-rayNet.\\n(2) the GAN-\\nbased training approach is called CCX-rayGAN, where the\\nbackpropagation steps of the generator and discriminator im-\\nitate the same procedure as in LS-GAN [11]. We resized the\\nfrontal and lateral view X-rays, and frontal view segmentation\\nmap to 128×128 for training. In the frontal view segmenta-\\ntion map, we acquire two categories, such as the heart and\\nlungs, but the ‘background’ category is utilized to encompass\\nregions that do not include in the categories above. Finally,\\nthe output dimension of these models is 128 × 128 × 128.\\n3.2. Qualitative Analysis\\nFig. 3. Lateral view (ﬁrst row) and axial view (second row)\\nfrom a CT volume yielded by CCX-rayGAN+B and X2CT-\\nGAN+B. CCX-rayGAN+B generates precise anatomical re-\\nconstructions than X2CT-GAN+B.\\nCT volume reconstruction from X-rays is comparatively a\\nnew proposed approach, and our CCX-rayGAN+B can vastly\\nenhance the perceptual quality of the reconstruction result. In\\n1657\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'file_path': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'page': 2, 'total_pages': 5, 'format': 'PDF 1.6', 'title': 'Ccx-Raynet: A Class Conditioned Convolutional Neural Network for Biplanar X-Rays to Ct Volume', 'author': 'Md. Aminur Ratul, Kun Yuan, WonSook Lee', 'subject': '2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), April 13-16, 2021, Nice, France', 'keywords': 'Machine learning; X-ray imaging; Computed tomography (CT)', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64)', 'creationDate': \"D:20210515142714-07'00'\", 'modDate': \"D:20210724165149+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 4. 3D CT volumes are reconstructed from CCX-rayGAN\\nand X2CT-GAN. ’+B’ denotes biplanar X-ray inputs. The\\nﬁrst row indicates the posterior-anterior (PA) view of the CT,\\nand the second row signiﬁes the bone structure reconstruction.\\nFig. 3 and Fig. 4, we compare the visual quality of our GAN-\\nbased biplanar network and baseline X2CT-GAN+B method.\\nThrough the lateral and axial view of Fig.3, we can exhibit\\nthat CCX-rayGAN+B generates high-quality reconstruction\\noutcomes with intricate anatomical parts compared to X2CT-\\nGAN+B. Speciﬁcally, our method yields sharper boundaries\\nand internal textures of the organs compared to the state-of-\\nthe-art-method. Next, in Fig. 4, we can inspect that the CCX-\\nrayGAN+B can reconstruct the outline shape of the organs\\n(e.g., lungs) and small complex anatomies such as small ves-\\nsels in the lungs posterior-anterior (PA) view (ﬁrst row). Be-\\nsides, our network can reconstruct the chest (ribs) bone struc-\\nture and backbone (second row) close to the ground truth. We\\ncan produce superior results in both internal anatomies and\\nbone structure than the baseline method X2CT-GAN+B.\\n3.3. Quantitative Analysis\\nWe use the Peak signal-to-noise ratio (PSNR) and Structural\\nSimilarity Index (SSIM) to evaluate the quantitative outcome\\nof our methods. As shown in Table 1, the proposed CCX-\\nrayNet+B produces superior PSNR and SSIM values than the\\nbaseline X2CT-CNN+B. CCX-rayNet+B yields better PSNR\\nand SSIM values, whereas CCX-rayGAN+B generates re-\\nmarkably better visual output (according to Fig. 3, 4) with\\nsubtle anatomical details, as it is typical characteristics of a\\nGAN based network to sacriﬁces MSE based loss to obtain\\nbetter visual results. Thus, we can raise a logical trade-off\\nby arguing that it is valuable for physicians to acquire high-\\nquality images to perceive small complex anatomies. In this\\npaper, we only showcase our biplanar X-ray method, though\\nwe also train the single view methods (CCX-rayNet+S and\\nCCX-rayGAN+S) where the input is the frontal view of\\nX-ray and the frontal view segmentation map. We almost\\nTable 1. Quantitative results from CCX-rayNet, X2CT [2]\\nand 2DCNN [12]. CCX-rayNet is the proposed generator net-\\nwork, and ’CCX-rayGAN’ is our GAN based method. Here,\\n’+S’ denotes the single-view X-ray input, and ’+B’ indicates\\nthe biplanar X-ray input. The best results highlighted in bold\\nModel\\nPSNR (dB)\\nSSIM\\n2DCNN [12]\\n23.10\\n0.461\\nX2CT-GAN+S [2]\\n22.30\\n0.525\\nX2CT-CNN+S [2]\\n23.12\\n0.587\\nCCX-rayGAN+S (ours)\\n22.48\\n0.536\\nCCX-rayNet+S (ours)\\n24.73\\n0.597\\nX2CT-GAN+B [2]\\n26.19\\n0.656\\nX2CT-CNN+B [2]\\n27.29\\n0.721\\nCCX-rayGAN+B (ours)\\n26.93\\n0.694\\nCCX-rayNet+B (ours)\\n28.18\\n0.752\\nTable 2. Several combinations of proposed biplanar CCX-\\nrayNet and the best outcome marked in bold\\nDFT\\n✓\\n✓\\n✓\\nDAC\\n✓\\n✓\\nAFF\\n✓\\nPSNR\\n27.29\\n27.68\\n27.49\\n28.18\\nachieve 3.4 dB more PSNR value in our biplanar method\\n(CCX-rayNet+B) than the single view one (CCX-rayNet+S).\\n3.4. Ablation Study\\nFor the CCX-rayNet+B, we assess the impacts of our pro-\\nposed modules, and the result displayed in Table 2. Without\\nthree modules, the network’s PSNR value is 27.29 dB; wheres\\nthe combination of DFT, DAC, and AFF modules improve\\nPSNR value to 28.18 dB. Additionally, the DFT and DAC\\nmodules also enhance the PSNR value than the basic model.\\n4. CONCLUSION\\nThis paper represents a class-conditioned network called\\nCCX-rayNet, to reconstruct 3D CT volume from synthetic\\nchest X-rays. We proposed three modules (DFT, DAC, AFF)\\nto recapturing textures and shapes faithful to semantic classes\\nin the practical-world scenario. In order to make the dataset,\\nwe use two binary UNet architectures to generate the seg-\\nmentation maps. Our proposed biplanar method with prior\\nsemantic information restore density information, anatomical\\nstructure, and shape in the reconstructed 3D volumes with\\nhigh visual quality than baseline models. Our ablation study\\nproclaims the competence of our proposed modules.\\n1658\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'file_path': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.6', 'title': 'Ccx-Raynet: A Class Conditioned Convolutional Neural Network for Biplanar X-Rays to Ct Volume', 'author': 'Md. Aminur Ratul, Kun Yuan, WonSook Lee', 'subject': '2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), April 13-16, 2021, Nice, France', 'keywords': 'Machine learning; X-ray imaging; Computed tomography (CT)', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64)', 'creationDate': \"D:20210515142714-07'00'\", 'modDate': \"D:20210724165149+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='5. COMPLIANCE WITH ETHICAL STANDARDS\\nThis is a numerical simulation study for which no ethical ap-\\nproval was required.\\n6. CONFLICTS OF INTEREST\\nNo funding was received for conducting this study. The au-\\nthors have no relevant ﬁnancial or non-ﬁnancial interests to\\ndisclose.\\n7. REFERENCES\\n[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\\n“Generative adversarial nets,” in Advances in neural in-\\nformation processing systems, 2014, pp. 2672–2680.\\n[2] X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, and Y. Zheng,\\n“X2ct-gan: Reconstructing ct from biplanar x-rays with\\ngenerative adversarial networks,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), June 2019.\\n[3] X. Wang, K. Yu, C. Dong, and C. Change Loy, “Recov-\\nering realistic texture in image super-resolution by deep\\nspatial feature transform,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition,\\n2018, pp. 606–615.\\n[4] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convo-\\nlutional networks for biomedical image segmentation,”\\nin International Conference on Medical image com-\\nputing and computer-assisted intervention.\\nSpringer,\\n2015, pp. 234–241.\\n[5] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-\\nto-image translation with conditional adversarial net-\\nworks,” in Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, 2017, pp. 1125–\\n1134.\\n[6] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired\\nimage-to-image translation using cycle-consistent ad-\\nversarial networks,” in Proceedings of the IEEE interna-\\ntional conference on computer vision, 2017, pp. 2223–\\n2232.\\n[7] S. G. Armato III, G. McLennan, M. F. McNitt-Gray,\\nC. R. Meyer, D. Yankelevitz, D. R. Aberle, C. I. Hen-\\nschke, E. A. Hoffman, E. A. Kazerooni, H. MacMahon\\net al., “Lung image database consortium: developing a\\nresource for the medical imaging research community,”\\nRadiology, vol. 232, no. 3, pp. 739–748, 2004.\\n[8] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. W´ang, P.-\\nX. Lu, and G. Thoma, “Two public chest x-ray datasets\\nfor computer-aided screening of pulmonary diseases,”\\nQuantitative imaging in medicine and surgery, vol. 4,\\nno. 6, p. 475, 2014.\\n[9] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto,\\nT. Kobayashi, K.-i. Komatsu, M. Matsui, H. Fujita,\\nY. Kodera, and K. Doi, “Development of a digital image\\ndatabase for chest radiographs with and without a lung\\nnodule: receiver operating characteristic analysis of ra-\\ndiologists’ detection of pulmonary nodules,” American\\nJournal of Roentgenology, vol. 174, no. 1, pp. 71–74,\\n2000.\\n[10] B. van Ginneken, M. Stegmann, and M. Loog, “Seg-\\nmentation of anatomical structures in chest radiographs\\nusing supervised methods: a comparative study on a\\npublic database,” Medical Image Analysis, vol. 10, no. 1,\\npp. 19–40, 2006.\\n[11] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and\\nS. Paul Smolley, “Least squares generative adversar-\\nial networks,” in Proceedings of the IEEE international\\nconference on computer vision, 2017, pp. 2794–2802.\\n[12] P. Henzler, V. Rasche, T. Ropinski, and T. Ritschel,\\n“Single-image tomography:\\n3d volumes from 2d x-\\nrays,” arXiv preprint arXiv:1710.04867, 2017.\\n1659\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:28:32 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'file_path': 'data/Ratul et al. - 2021 - CCX-rayNet A Class Conditioned Convolutional Neur.pdf', 'page': 4, 'total_pages': 5, 'format': 'PDF 1.6', 'title': 'Ccx-Raynet: A Class Conditioned Convolutional Neural Network for Biplanar X-Rays to Ct Volume', 'author': 'Md. Aminur Ratul, Kun Yuan, WonSook Lee', 'subject': '2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), April 13-16, 2021, Nice, France', 'keywords': 'Machine learning; X-ray imaging; Computed tomography (CT)', 'creator': 'The Engineering in Medicine and Biology Conference Management System', 'producer': 'PDFlib+PDI 8.0.1p8 (Perl 5.10.0/Linux-x86_64)', 'creationDate': \"D:20210515142714-07'00'\", 'modDate': \"D:20210724165149+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='PROCEEDINGS OF SPIE\\nSPIEDigitalLibrary.org/conference-proceedings-of-spie\\nHarnessing the power of deep\\nlearning for volumetric CT imaging\\nwith single or limited number of\\nprojections\\nLiyue Shen, Wei Zhao, Lei Xing\\nLiyue Shen, Wei Zhao, Lei Xing, \"Harnessing the power of deep learning for\\nvolumetric CT imaging with single or limited number of projections,\" Proc.\\nSPIE 10948, Medical Imaging 2019: Physics of Medical Imaging, 1094826 (1\\nMarch 2019); doi: 10.1117/12.2513032\\nEvent: SPIE Medical Imaging, 2019, San Diego, California, United States\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023  Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Harnessing the power of deep learning for volumetric CT\\nimaging with single or limited number of projections\\nLiyue Shena, Wei Zhaob, and Lei Xinga,b\\naDepartment of Electrical Engineering, Stanford University, Stanford, USA\\nbDepartment of Radiation Oncology, Stanford University, Stanford, USA\\nABSTRACT\\nTomographic imaging using a penetrating wave, such as X-ray, light and microwave, is a fundamental approach\\nto generate cross-sectional views of internal anatomy in a living subject or interrogate material composition\\nof an object and plays an important role in modern science. To obtain an image free of aliasing artifacts, a\\nsuﬃciently dense angular sampling that satisﬁes the Shannon-Nyquist criterion is required. In the past two\\ndecades, image reconstruction strategy with sparse sampling has been investigated extensively using approaches\\nsuch as compressed-sensing. This type of approach is, however, ad hoc in nature as it encourages certain form\\nof images. Recent advancement in deep learning provides an enabling tool to transform the way that an image\\nis constructed.\\nAlong this line, Zhu et al1 presented a data-driven supervised learning framework to relate\\nthe sensor and image domain data and applied the method to magnetic resonance imaging (MRI). Here we\\ninvestigate a deep learning strategy of tomographic X-ray imaging in the limit of a single-view projection data\\ninput. For the ﬁrst time, we introduce the concept of dimension transformation in image feature domain to\\nfacilitate volumetric imaging by using a single or multiple 2D projections. The mechanism here is fundamentally\\ndiﬀerent from the traditional approaches in that the image formation is driven by prior knowledge casted in\\nthe deep learning model. This work pushes the boundary of tomographic imaging to the single-view limit and\\nopens new opportunities for numerous practical applications, such as image guided interventions and security\\ninspections. It may also revolutionize the hardware design of future tomographic imaging systems.\\nKeywords: image reconstruction, deep learning, convolutional neural network, sparse data sampling\\n1. INTRODUCTION\\nThe ability of tomographic imaging to take a deep and quantitative look into a patient or an object with\\nhigh spatial resolution holds signiﬁcant value in scientiﬁc explorations and medical practice. Traditionally, a\\ntomographic image is obtained by mathematical inversion of the encoding function of the imaging wave for a given\\nset of measured data from diﬀerent angular positions. A prerequisite of artifacts-free inversion is the satisfaction\\nof classical Shannon-Nyquist theorem in angular data sampling, which imposes a practically achievable limit\\nin imaging time and object irradiation. To mitigate the problem, image reconstruction strategy with sparse or\\nlimited sampling has been investigated extensively using techniques such as compressed-sensing,2–5 and maximum\\na posteriori.6,7 This type of approach introduces a regularization term to the ﬁdelity function to encourage some\\nad hoc or presumed characteristics in the resultant image.8–17 The resultant sparsity without compromising the\\nimage quality is generally limited and does not meet the unmet demand for real-time imaging with substantially\\nreduced subject irradiation. Indeed, while continuous eﬀort has been made in imaging with reduced angular\\nmeasurements over the years, tomographic imaging with ultra-sparse sampling has yet to be realized. In this\\nwork, we push the sparsity to the limit of a single projection and demonstrate what seemingly unlikely scenario\\nof a single-view tomographic imaging is readily achievable by leveraging from the state-of-the-art deep learning\\ntechnique and seamless integration of prior knowledge in the deep learning-based image reconstruction process.\\nDeep neural network has recently attracted much attention for its unprecedented ability to learn complex\\nrelationships and incorporate existing knowledge into the inference model through feature extraction and rep-\\nresentation learning.18–20 The method has found widespread applications across disciplines, such as computer\\nFurther author information: (Send correspondence to Lei Xing)\\nLei Xing: E-mail: lei@stanford.edu, Telephone: 1 650 498 7896\\nMedical Imaging 2019: Physics of Medical Imaging, edited by Taly Gilat Schmidt, Guang-Hong Chen, Hilde Bosmans, \\nProc. of SPIE Vol. 10948, 1094826 · © 2019 SPIE · CCC code: 1605-7422/19/$18 · doi: 10.1117/12.2513032\\nProc. of SPIE Vol. 10948  1094826-1\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Training Dataset\\nValidation Dataset\\nGenerated Dataset\\nTesting Dataset\\n3D CT Images\\n2D Projections\\nModel \\nTraining\\nModel \\nValidation\\nModel \\nEvaluation\\nSimulate\\nData \\nPreprocessing\\nSplit\\nFinal \\nModel\\nFigure 1: Experiments workﬂow.\\nThe ﬁgure demonstrates the pipeline for reconstruction experiments.\\nFirstly, we\\ngenerate our dataset using clinical patient’ 3D CT images and corresponding 2D projections. After data preprocessing,\\nthe whole dataset is randomly splitted as three subsets for training, validation and testing. Then in the training process,\\nwe learn our model by iteratively updating network parameters. Finally, the trained model is evaluated on testing subset\\nwith various metrics.\\nvision,21–23 autonomous driving,24 natural language processing.25 The success of deep learning in these ﬁelds\\nhas spurred tremendous activities in applying the technique for and biomedicine and medical image analysis.\\nThe strong performance of deep learning in various tasks such as skin cancer classiﬁcation,26 retinopathy diagno-\\nsis,27–29 organ segmentation,30 and image-guided radiotherapy31,32 have been demonstrated. Here we introduce\\nan eﬃcient deep-learning-based method to solve the inverse problem with ultra-sparse X-ray projection data.\\nSpecially, we propose a residual deep neural network with an encoder-decoder framework for reconstruction of\\nthree-dimension (3D) CT images with few-view of two-dimension (2D) projections. We show that our algorithm\\nis able to reconstruct high-quality CT images with only a single or a few of projections by maximizing the prior\\nknowledge learned from training data as well as the feature representations from each projection. The pipeline\\nof our approach is shown in Figure 1.\\n2. METHODS\\nIn this section, we study the problem of 3D image reconstruction using 2D projections or views. Mathematically,\\nthe input of our neural network is a sequence of 2D projections denoted as {X1, X2, . . . , XN}, where Xi ∈ Rm×n\\nfor all {1 ≤ i ≤ N} and N is the number of available projections which are used for reconstruction task. Output\\nimages is the predicted 3D image Yp ∈ Ru×v×w most close to the ground truth 3D image Yt. In this study, the\\ninput 2D images are in the size of Xi ∈ R128×128 while the output images are in the size of Xi ∈ R90×128×128.\\n2.1 Model architecture.\\nThe neural network aims at learning an inverse mapping function from 2D projections to 3D images. As shown\\nin Figure 2, we propose a deep neural network consisting of an encoder-decoder framework. With the use of a\\nsequence of 2D projections as input, encoder network extracts a semantic feature representation in high-dimension\\nfeature space and learns to transform from 2D raw data domain to a feature domain. Using the learned feature\\nrepresentation as input, decoder network is trained to learn how to generate 3D images by transforming from\\nthe feature domain to 3D image domain. Following the pattern of encoder-decoder framework, the network is\\ntrained to reconstruct or generate volumetric images from 2D projections by using the feature representation as\\na connection bridge. The essence behind such a design is that both the 2D projections and the 3D images should\\nProc. of SPIE Vol. 10948  1094826-2\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='2D View\\n(A) Representation Network\\n(C) Generation Network\\n3D Volumetric \\nImages\\nInput\\nOutput\\n2D conv 4x4, BN, ReLU\\n2D conv 3x3, BN, ReLU\\n3D deconv 1x1, ReLU\\n3D conv 1x1, ReLU\\n2D conv 1x1, ReLU\\nFeature Reshape\\n1\\n256\\n256\\n512\\n512\\n1024\\n1024\\n2048\\n2048\\n4096\\n4096\\n1024 2048\\n512\\n256\\n128\\n64\\n512\\n256\\n128\\n64\\n3D conv 4x4, BN, ReLU\\n3D conv 3x3, BN, ReLU\\n(B) Transform\\n      Module \\n4096\\n4096\\n2048\\n2048\\nFigure 2: Architecture of the proposed deep learning network. The input of the model is a single or multiple 2D projec-\\ntion(s). The representation network learns feature representation of physical structure from the input. The extracted 2D\\nfeature vector is reshaped and transferred by the transform module to 3D representation cube for subsequent reconstruc-\\ntion. The generation network utilizes representative features extracted in the former stages to generate the corresponding\\n3D volumetric images.\\nshare the same semantic feature representation in the feature domain, because they are image expressions of the\\nsame object in diﬀerent spatial dimensions.\\nMotivated by outstanding performance of deep residual network,33 we introduce a similar pattern of residual\\nlearning in our encoding representation network with a residual convolution block as in Fig. 2. The key to\\ndeep residual learning is the identity mapping in each residual block which enables faster training process and\\navoids gradient vanish. The proposed residual representation network is built up by concatenating multi-level\\nresidual blocks. In the encoder network, we use the convolution residual block which consists of a pattern of\\n“2D convolution layer (with kernel size 4 and stride 2) → 2D batch normalization layer34 → rectiﬁed linear unit\\n(ReLU) activation35 → 2D convolution layer (with kernel size 3 and stride 1) → 2D batch normalization layer\\n→ ReLU activation”. Moreover, before applying the ﬁnal ReLU layer, an extra shortcut path is established\\nto add up the output of the ﬁrst convolution layer to get the ﬁnal output tensor. In a sense, we enforces the\\nsecond convolution layer to learn the residual feature representations. In order to bridge the representation\\nand generation networks, a transform module is introduced following the representation network. This module\\nconsists of a 2D convolution layer (with kernel size 1 and stride 1), feature shaping, and a symmetric dual 3D\\ndeconvolution layer (with kernel size 1 and stride 1). The kernel-1 convolution/deconvolution operation helps\\nto construct the transformation relationship between 2D/3D feature representations respectively. Furthermore,\\nreshaping the 2D representative vector into 3D tensor facilitates the feature transformation across dimensionality.\\nThe role of the generation network is to provide 3D volumetric images with ﬁne physical structures based on\\nthe learned features from the representation network. This network is constructed by stacking together multi-\\nstage deconvolution blocks, which consists of a symmetric-dual structure as convolution block, except that 2D\\nconvolution layers are replaced by 3D deconvolution layers instead. Speciﬁcally, each deconvolution block consists\\nof a ﬂow of 3D convolution layer (with kernel size 4 and stride 2) → 3D batch normalization layer → ReLU layer\\n→ 3D convolution layer (with kernel size 3 and stride 1) → 3D batch normalization layer → ReLU layer. Before\\noutputting the predicted 3D volume, an additional 3D convolution layer (with kernel size 1 and stride 1) and 2D\\nconvolution layer (with kernel size 1 and stride 1) are performed to ensure the right size of the output 3D image.\\nProc. of SPIE Vol. 10948  1094826-3\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Phase 1\\nPhase 10\\nPhase 9\\nPhase 8\\nPhase 7\\nPhase 6\\nPhase 5\\nPhase 4\\nPhase 3\\nPhase 2\\nPhase 1\\nPhase 10\\nPhase 9\\nPhase 8\\nPhase 7\\nPhase 6\\nPhase 5\\nPhase 4\\nPhase 3\\nPhase 2\\na\\nb\\nFigure 3: phase-resolved 4D-CT dataset (a) and the corresponding synthetic 2D projection (b) using the Varian\\nTrueBeam on-board imager geometry.\\n3. EXPERIMENTS\\n3.1 Dataset.\\nThe dataset is collected and generated from a 4D simulation CT of a patient who received volumetric modulated\\narc therapy (VMAT). The 4D-CT data was acuqired using a CT subsystem of a PET-CT system (Biograph 128,\\nSiemens) with trigger delay from 0% to 90%, as shown in Figure 3 a. Each of CT dataset has a volume size\\n512 × 512 × 92. The phase 1-6 datasets were ﬁrst extracted for model traing and the left 4 phases datasets were\\nused for model testing. In order to increase the training sample size, the ﬁrst 6 phase datasets were registered with\\neach other to generate ﬁve motion vector ﬁelds (MVF). The MVF is a 4D matrix with size of 512 × 512 × 92 × 3,\\nwhere the three 512 × 512 × 92 in the 4D matrix contains displacements along the x-axis, y-axis, and z-axis,\\nrespectively. We then randomly select two MVFs to generate a new MVF′ as follows:\\nMV F\\n′ = rand · MV Fi + (1 − rand) · MV Fj,\\n(1)\\nwhere MV Fi,j are two MVFs from ﬁve MVFs set, and rand is a uniformed distributed random number in the\\ninterval (0, 1). With this method, a set of 30 MVFs is generated and applied to the ﬁrst 6 phase datasets to\\ngenerate 180 CT datasets. Each of the CT dataset are then rotated between −5◦ and 5◦ with 2.5◦ interval to\\nfurther enlarger the sample size. With the augmentation, a total of 900 CT datasets is obtained from model\\ntraining. Using the same augmentation approach, a total of 600 CT datasets is obtained for testing.\\nTo simulate 2D project, we project each 3D CT data in the direction of 100 diﬀerent view points which are\\nevenly distributed around a circle. In other words, 180 degrees are splited as 50 intervals uniformly. To be\\nrealistic, the projection geometry is consistent with the amounted on-board imager of TrueBeam system (Varian\\nMedical System, Palo Alto, CA). Speciﬁcally, the source-to-detector distance is 1500 mm, and the source-to-\\nisocenter distance is 1000 mm. The dimension of project image is 320 × 200 (width × height) with a pixel size\\nof 2 mm. For illustration, Figure 3 b shows the project of the 10 phases of the 4D-CT.\\nTo fasten model training, each 2D projection and 3D dataset sample pair is resize to 128 × 128 and 128 ×\\n128 × 90, respectively. In addition, following standard data pre-processing, we adopt the scaling normalization\\nfor both input 2D projections and output 3D images, where pixel-wise or voxel-wise intensities are normalized\\ninto interval [0, 1]. Moreover, we calculate the statistics including mean and variance respectively for each 2D\\nview among all training data. Then in both training and testing process, input 2D projections are normalized\\nProc. of SPIE Vol. 10948  1094826-4\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Number of 2D Projections\\nMAE\\nRMSE\\nSSIM\\nPSNR\\n1\\n0.018\\n0.177\\n0.929\\n30.523\\n2\\n0.015\\n0.140\\n0.945\\n32.554\\n5\\n0.016\\n0.155\\n0.942\\n31.823\\n10\\n0.018\\n0.165\\n0.939\\n31.355\\nTable 1: Table 1. Quantitative reconstruction results. Diﬀerent numbers of 2D projections are utilized for prediction.\\nWe evaluate on testing dataset and display average values of four metrics including mean absolute error (MAE), root\\nmean squared error (RMSE), structural similarity (SSIM), and peak signal noise ratio (PSNR).\\nusing corresponding mean and variance, which is usually used to make the data distribution closer to normal\\ndistribution in statistics.\\n3.2 Training details\\nFor the training objective, we deﬁne the cost function based on the mean squared error (L2 norm loss) between\\nthe predicted results and the ground truth. For comparison purpose, we use the same training strategy and\\nhyper-parameters for all experiments. We implement the network with PyTorch36 deep learning framework. In\\ntraining process, we use a mean squared error (MSE) loss function to compute voxel-wise mean-squared-error\\nbetween the ground truth and predicted 3D images as deﬁned below. By using a random initialization for network\\nparameters, the Adam optimizer37 is utilized to minimize loss objective and update network parameters through\\nback-propagation with iterative epochs. Speciﬁcally, we use the learning rate of 0.00002 and the mini-batch size\\nof 1 because of the memory limitation. As shown in 4, the training loss objective is minimized iteratively. And\\nat the end of each epoch, the trained model is evaluated on an independent validation data set. This strategy\\nis commonly used to monitor the model performance and avoid overﬁtting the training samples. In addition,\\nlearning rate is scheduled decaying according to the validation loss. Speciﬁcally, if the validation loss remains\\nunchanged for 10 epochs, learning rate will be reduced by a factor 2. Finally, the best checkpoint model with the\\nsmallest validation loss is selected as ﬁnal model in the experiments. We train the network using one NVIDIA\\nTITAN V100 graphics processing unit (GPU) for 100 epochs.\\n3.3 Results\\nReconstruction experiments were conducted using the proposed method and 4D-CT data as described above.\\nTo evaluate the performance of our network, we deployed the trained model on a separate testing dataset and\\nanalyzed reconstruction results using both qualitative and quantitative evaluation metrics. Specially, in order to\\ninvestigate reconstruction performance with diﬀerent number of 2D projections, we have conducted experiments\\nwith 1, 2, 5, and 10 projections as input for comparison purpose. The multiple view angles are distributed evenly\\naround a 180-degree semicircle. For instance, for 2-views, the two orthogonal directions are 0 degree (AP) and\\n90 degrees (lateral). Here we stack the 2D projections from diﬀerent view angles as diﬀerent channels of the\\ninput data and modify the ﬁrst convolution layer to ﬁt the input data size.\\nWith the same model training procedure and hyper-parameter, we obtain the qualitative reconstructed CT\\nimages for 1, 2, 5, and 10 views in Figs. 5-7. In the ﬁgures, we display our reconstruction results for one example\\nchosen from testing set. Specially, we demonstrate the 3D CT image from three view points: axial (Fig. 5),\\ncoronal (Fig. 6), and sagittal (Fig. 7). For visualization purpose, in each ﬁgure, each column shows one slice\\nimage selected from the predicted 3D images together with the ground truth. The diﬀerence image between the\\npredicted image and ground truth image is also shown. It is seen that the prediction images are very similar to\\nthe target images, which shows that our model performs well for 3D CT reconstruction even with only a single\\nprojection.\\nFor quantitative evaluation, the metrics of mean absolute error (MAE), root mean squared error (RMSE),\\nstructural similarity (SSIM) are calculated to measure the prediction error between estimated images and ground\\ntruth images. In addition, we also compute the peak signal noise ratio (PSNR) to show the reconstructed image\\nProc. of SPIE Vol. 10948  1094826-5\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(A) 1 View\\n(B) 2 Views\\n(C) 5 Views\\n(D) 10 Views\\nFigure 4: Training loss curves of the image reconstruction in the study of case 1. Blue and orange curves denote training\\nand validation loss respectively. (A)-(D) Image reconstructed using 1, 2, 5, and 10 views, respectively.\\nProc. of SPIE Vol. 10948  1094826-6\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='quality. By computing the average values of various evaluation metrics for all the 100 examples in testing set,\\nthe quantitative evaluation of the results are presented in Table 1.\\nInterestingly, we observed that a single\\n2D projection provides suﬃcient data to produce a high-quality reconstruction similar to the reconstructions\\nperformed with multiple projection images, when comparing the quantitative evaluation metrics.\\nFrom above results, we conclude that the proposed deep learning reconstruction framework is capable of\\nproviding reasonable 3D images by using only a single or a few view projections.\\n4. CONCLUSION\\nWe have presented a novel deep learning framework for volumetric imaging with ultra-sparse data sampling.\\nIt is both intriguing and important that the proposed strategy is capable of holistically extracting the feature\\ncharacteristics embedded in a single or a few 2D projection data and transform them into the corresponding\\n3D image with high ﬁdelity. Although this work is focused on the most commonly used X-ray imaging, the\\nconcept and implementation should be easily extendable to other imaging modalities with ultra-sparse sampling.\\nPractically, the single-view imaging may present a revolutionary solution to various practical applications, ranging\\nfrom image guidance in interventions, cellular imaging, objection inspection, to greatly simpliﬁed imaging system\\ndesign. A few medical applications along these directions are underway.\\nREFERENCES\\n[1] Zhu, B., Liu, J. Z., Cauley, S. F., Rosen, B. R., and Rosen, M. S., “Image reconstruction by domain-transform\\nmanifold learning,” Nature 555(7697), 487 (2018).\\n[2] Candes, E. J., Romberg, J. K., and Tao, T., “Stable signal recovery from incomplete and inaccurate mea-\\nsurements,” Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute\\nof Mathematical Sciences 59(8), 1207–1223 (2006).\\n[3] Lustig, M., Donoho, D., and Pauly, J. M., “Sparse mri: The application of compressed sensing for rapid mr\\nimaging,” Magnetic Resonance in Medicine 58(6), 1182–1195 (2007).\\n[4] Chen, G.-H., Tang, J., and Leng, S., “Prior image constrained compressed sensing (piccs): a method to accu-\\nrately reconstruct dynamic ct images from highly undersampled projection data sets,” Medical physics 35(2),\\n660–663 (2008).\\n[5] Choi, K., Wang, J., Zhu, L., Suh, T.-S., Boyd, S., and Xing, L., “Compressed sensing based cone-beam\\ncomputed tomography reconstruction with a ﬁrst-order method a,” Medical physics 37(9), 5113–5125 (2010).\\n[6] Fessler, J. A. and Rogers, W. L., “Spatial resolution properties of penalized-likelihood image reconstruction:\\nspace-invariant tomographs,” IEEE Transactions on Image processing 5(9), 1346–1358 (1996).\\n[7] Ji, S., Xue, Y., Carin, L., et al., “Bayesian compressive sensing,” IEEE Transactions on Signal Process-\\ning 56(6), 2346 (2008).\\n[8] Engl, H. W., Hanke, M., and Neubauer, A., [Regularization of inverse problems], vol. 375, Springer Science\\n& Business Media (1996).\\n[9] Stayman, J. W. and Fessler, J. A., “Regularization for uniform spatial resolution properties in penalized-\\nlikelihood image reconstruction,” (2000).\\n[10] Jiang, M. and Wang, G., “Convergence studies on iterative algorithms for image reconstruction,” IEEE\\nTransactions on Medical Imaging 22(5), 569–579 (2003).\\n[11] Wang, J., Li, T., Lu, H., and Liang, Z., “Penalized weighted least-squares approach to sinogram noise\\nreduction and image reconstruction for low-dose x-ray computed tomography,” IEEE transactions on medical\\nimaging 25(10), 1272–1283 (2006).\\n[12] Sidky, E. Y. and Pan, X., “Image reconstruction in circular cone-beam computed tomography by con-\\nstrained, total-variation minimization,” Physics in Medicine & Biology 53(17), 4777 (2008).\\n[13] Jia, X., Dong, B., Lou, Y., and Jiang, S. B., “Gpu-based iterative cone-beam ct reconstruction using tight\\nframe regularization,” Physics in Medicine & Biology 56(13), 3787 (2011).\\n[14] Gao, H., Lin, H., Ahn, C. B., and Nalcioglu, O., “Prism: A divide-and-conquer low-rank and sparse\\ndecomposition model for dynamic mri,” UCLA CAM Report , 11–26 (2011).\\nProc. of SPIE Vol. 10948  1094826-7\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[15] Gao, H., Rapacchi, S., Wang, D., Moriarty, J., Meehan, C., Sayre, J., Laub, G., Finn, P., and Hu, P.,\\n“Compressed sensing using prior rank, intensity and sparsity model (prism): applications in cardiac cine\\nmri,” in [Proceedings of the 20th Annual Meeting of ISMRM, Melbourne, Australia], 2242 (2012).\\n[16] Xu, Q., Yu, H., Mou, X., Zhang, L., Hsieh, J., and Wang, G., “Low-dose x-ray ct reconstruction via\\ndictionary learning,” IEEE transactions on medical imaging 31(9), 1682–1697 (2012).\\n[17] Chen, G.-H. and Li, Y., “Synchronized multiartifact reduction with tomographic reconstruction (smart-\\nrecon): A statistical model based iterative image reconstruction method to eliminate limited-view artifacts\\nand to mitigate the temporal-average artifacts in time-resolved ct,” Medical physics 42(8), 4698–4707 (2015).\\n[18] Eslami, S. A., Rezende, D. J., Besse, F., Viola, F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A.,\\nDanihelka, I., Gregor, K., et al., “Neural scene representation and rendering,” Science 360(6394), 1204–1210\\n(2018).\\n[19] LeCun, Y., Bengio, Y., and Hinton, G., “Deep learning,” nature 521(7553), 436 (2015).\\n[20] Schmidhuber, J., “Deep learning in neural networks: An overview,” Neural networks 61, 85–117 (2015).\\n[21] Krizhevsky, A., Sutskever, I., and Hinton, G. E., “Imagenet classiﬁcation with deep convolutional neural\\nnetworks,” in [Advances in neural information processing systems], 1097–1105 (2012).\\n[22] Simonyan, K. and Zisserman, A., “Very deep convolutional networks for large-scale image recognition,”\\narXiv preprint arXiv:1409.1556 (2014).\\n[23] Shen, L., Yeung, S., Hoﬀman, J., Mori, G., and Fei-Fei, L., “Scaling human-object interaction recogni-\\ntion through zero-shot learning,” in [2018 IEEE Winter Conference on Applications of Computer Vision\\n(WACV)], 1568–1576, IEEE (2018).\\n[24] Chen, C., Seﬀ, A., Kornhauser, A., and Xiao, J., “Deepdriving: Learning aﬀordance for direct perception\\nin autonomous driving,” in [Proceedings of the IEEE International Conference on Computer Vision], 2722–\\n2730 (2015).\\n[25] Collobert, R. and Weston, J., “A uniﬁed architecture for natural language processing: Deep neural networks\\nwith multitask learning,” in [Proceedings of the 25th international conference on Machine learning], 160–167,\\nACM (2008).\\n[26] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun, S., “Dermatologist-\\nlevel classiﬁcation of skin cancer with deep neural networks,” Nature 542(7639), 115–118 (2017).\\n[27] Poplin, R., Varadarajan, A. V., Blumer, K., Liu, Y., McConnell, M. V., Corrado, G. S., Peng, L., and\\nWebster, D. R., “Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning,”\\nNature Biomedical Engineering 2(3), 158 (2018).\\n[28] Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner,\\nK., Madams, T., Cuadros, J., et al., “Development and validation of a deep learning algorithm for detection\\nof diabetic retinopathy in retinal fundus photographs,” Jama 316(22), 2402–2410 (2016).\\n[29] Ting, D. S. W., Cheung, C. Y.-L., Lim, G., Tan, G. S. W., Quang, N. D., Gan, A., Hamzah, H., Garcia-\\nFranco, R., San Yeo, I. Y., Lee, S. Y., et al., “Development and validation of a deep learning system\\nfor diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with\\ndiabetes,” Jama 318(22), 2211–2223 (2017).\\n[30] Ibragimov, B. and Xing, L., “Segmentation of organs-at-risks in head and neck ct images using convolutional\\nneural networks,” Medical physics 44(2), 547–557 (2017).\\n[31] Zhao, W., Han, B., Yang, Y., Buyyounouski, M., Hancock, S. L., Bagshaw, H., and Xing, L., “Incorporating\\ndeep layer image information into image guided radiation therapy,” in [American Association of Physicists\\nin Medicine Annual Meeting], (2018).\\n[32] Zhao, W., Han, B., Yang, Y., Buyyounouski, M., Hancock, S. L., Bagshaw, H., and Xing, L., “Visualizing\\nthe invisible in prostate radiation therapy: markerless prostate target localization via a deep learning model\\nand monoscopic kv projection x-ray image,” in [American Society for Radiation Oncology (ASTRO) Annual\\nMeeting], (2018).\\n[33] He, K., Zhang, X., Ren, S., and Sun, J., “Deep residual learning for image recognition,” in [CVPR], 770–778,\\nIEEE Computer Society (2016).\\n[34] Ioﬀe, S. and Szegedy, C., “Batch normalization: Accelerating deep network training by reducing internal\\ncovariate shift,” arXiv preprint arXiv:1502.03167 (2015).\\nProc. of SPIE Vol. 10948  1094826-8\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='A\\n1-view \\nPred\\nDiff\\nPred\\nPred\\nDiff\\nPred\\nDiff\\nTruth\\nB\\n2-view \\nC\\n5-view \\nD\\n10-view \\nDiff\\nFigure 5: Axial images of test example with diﬀerent number of 2D projections. Both predicted images (Pred) and diﬀer-\\nence images (Diﬀ) between the prediction and the corresponding ground truth are shown. (A)-(D) Image reconstructed\\nusing 1, 2, 5, and 10 views, respectively.\\n[35] Nair, V. and Hinton, G. E., “Rectiﬁed linear units improve restricted boltzmann machines,” in [Proceedings\\nof the 27th international conference on machine learning (ICML-10)], 807–814 (2010).\\n[36] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L.,\\nand Lerer, A., “Automatic diﬀerentiation in pytorch,” (2017).\\n[37] Kingma, D. P. and Ba, J., “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980\\n(2014).\\nProc. of SPIE Vol. 10948  1094826-9\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='D\\n10-view \\nC\\n5-view \\nB\\n2-view \\nA\\n1-view \\nPred\\nDiff\\nTruth\\nPred\\nDiff\\nPred\\nDiff\\nPred\\nDiff\\nFigure 6:\\nCoronal images of test example with diﬀerent number of 2D projections.\\nBoth predicted images (Pred)\\nand diﬀerence images (Diﬀ) between the prediction and the corresponding ground truth are shown.\\n(A)-(D) Image\\nreconstructed using 1, 2, 5, and 10 views, respectively.\\nPred\\nDiff\\nTruth\\nD\\n10-view \\nPred\\nDiff\\nPred\\nDiff\\nPred\\nDiff\\nC\\n5-view \\nB\\n2-view \\nA\\n1-view \\nFigure 7:\\nSagittal images of test example with diﬀerent number of 2D projections.\\nBoth predicted images (Pred)\\nand diﬀerence images (Diﬀ) between the prediction and the corresponding ground truth are shown.\\n(A)-(D) Image\\nreconstructed using 1, 2, 5, and 10 views, respectively.\\nProc. of SPIE Vol. 10948  1094826-10\\nDownloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 14 Aug 2023\\nTerms of Use: https://www.spiedigitallibrary.org/terms-of-use\\n', metadata={'source': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'file_path': 'data/Shen et al. - 2019 - Harnessing the power of deep learning for volumetr.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'SPIE', 'producer': 'PDFlib+PDI 9.0.2 (.NET/Win64)', 'creationDate': \"D:20230814081824-07'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Articles\\nhttps://doi.org/10.1038/s41551-019-0466-4\\n1Department of Radiation Oncology, Stanford University, Stanford, CA, USA. 2Department of Electrical Engineering, Stanford University, Stanford, CA, USA. \\n3These authors contributed equally: Liyue Shen, Wei Zhao. *e-mail: lei@stanford.edu\\nT\\nhe ability of computed tomography (CT) to take a deep and \\nquantitative image of a patient or an object with high spatial \\nresolution is highly valuable in scientific research and medi-\\ncal practice. Traditionally, a tomographic image is obtained via the \\nmathematical inversion of the encoding function of the imaging \\nwave for a given set of measured data from different angular posi-\\ntions (Fig. 1a,b). A prerequisite for artefact-free inversion is the sat-\\nisfaction of the classical Shannon–Nyquist theorem in angular-data \\nsampling, which imposes a practically achievable limit in imaging \\ntime and object irradiation. To mitigate the problem, image recon-\\nstruction with sparse sampling has been investigated extensively \\nusing techniques such as compressed sensing1–6 and maximum \\na\\xa0posteriori7,8. These types of approaches introduce a regularization \\nterm to the inversion to encourage some ad hoc or presumed char-\\nacteristics in the resultant image9–13. If imaging quality cannot be \\ncompromised, the resultant sparsity is generally limited and does \\nnot address the unmet demand for real-time imaging with substan-\\ntially reduced subject irradiation (Fig. 1c). Indeed, while continuous \\nefforts have been made to reduce the number of angular measure-\\nments in medical imaging, tomographic imaging with ultra-sparse \\nsampling has not yet been realized.\\nIn this study, we push sparse sampling to the limit of a single \\nprojection view and demonstrate single-view tomographic imag-\\ning with a patient-specific prior by leveraging deep learning and \\nthe seamless integration of prior knowledge in the data-driven \\nimage-reconstruction process. The harnessing of prior knowl-\\nedge by machine-learning techniques in different data domains \\nfor improved imaging is an emerging topic of research. Some \\nrecent studies14–19 have also investigated machine-learning-based \\nimage reconstruction. Whereas the data-driven approach repre-\\nsents a potentially general strategy for image reconstruction, here \\nsingle-view CT imaging is achieved via a patient-specific prior. \\nPractically, it is actually advantageous to work with the patient-\\nspecific prior: for many image-guided interventional applications, \\nthe approach would enable scenarios most relevant to the specific \\npatient under treatment.\\nDeep neural networks have attracted much attention for their \\nability to learn complex relationships and to incorporate existing \\nknowledge into the inference model through feature extraction \\nand representation learning20–22. The method has found wide-\\nspread applications across disciplines, such as computer vision23–25, \\nautonomous driving26, natural language processing27 and bio-\\nmedicine15,28–36. Here we design a hierarchical neural network for \\nX-ray CT imaging with ultra-sparse projection views, and develop \\na structured training process for deep learning to generate three-\\ndimensional (3D) CT images from two-dimensional (2D) X-ray \\nprojections. Our approach introduces a feature-space transforma-\\ntion between a 2D projection and a 3D volumetric CT image within \\na representation–generation (encoder–decoder) framework. By \\nusing the transformation module, we transfer the representations \\nlearned from the 2D projection to a representative tensor for 3D \\nvolume reconstruction in the subsequent generation network. \\nThrough the model-training process, the transformation module \\nlearns the underlying relationship between feature representations \\nacross dimensionality, making it possible to generate a volumetric \\nCT image from a 2D projection. It should be emphasized that an \\nX-ray projection is not a purely 2D cross-sectional image, as higher \\ndimensional information is already encoded during the projec-\\ntion process (see schematic in Fig. 1a), with the encoding func-\\ntion determined by the physics of interactions between the X-ray \\nand media. Generally, a single projection alone is not sufficient \\nfor capturing the anatomical information in the projection direc-\\ntion for the subsequent volumetric image reconstruction. What \\nenables our deep-learning model for patient-specific volumetric \\nimage reconstruction is that anatomical relations (including the \\ninformation in the direction of the projection view) are encoded \\nduring the model-training process via the use of augmented data-\\nsets containing different 2D–3D data pairs of body positions and \\nanatomical distributions. The deep-learning transformation deci-\\nphers the hidden information in the projection data and predicts a \\nvolumetric image with the help of prior knowledge gained during \\nmodel training (Fig. 1d).\\nPatient-specific reconstruction of volumetric \\ncomputed tomography images from a single \\nprojection view via deep learning\\nLiyue Shen1,2,3, Wei Zhao\\u200a \\u200a1,3 and Lei Xing\\u200a \\u200a1,2*\\nTomographic imaging using penetrating waves generates cross-sectional views of the internal anatomy of a living subject. For \\nartefact-free volumetric imaging, projection views from a large number of angular positions are required. Here we show that \\na deep-learning model trained to map projection radiographs of a patient to the corresponding 3D anatomy can subsequently \\ngenerate volumetric tomographic X-ray images of the patient from a single projection view. We demonstrate the feasibility \\nof the approach with upper-abdomen, lung, and head-and-neck computed tomography scans from three patients. Volumetric \\nreconstruction via deep learning could be useful in image-guided interventional procedures such as radiation therapy and needle \\nbiopsy, and might help simplify the hardware of tomographic imaging systems.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n880\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nresults\\nFigure 2 shows the detailed structure of our deep-learning frame-\\nwork. The input to the neural network is single or multiple 2D \\nprojection images from different view angles. The output of the \\nnetwork is the corresponding volumetric CT image. During the \\nmodel-training process, the neural network learns the mapping \\nfunction from the 2D projection(s) to the volumetric image. \\nSpecifically, our deep-learning architecture consists of three  \\nmain parts: a representation network, a transformation module  \\nand a generation network. The representation network extracts \\nc\\nNumber of projections\\nRepresentation\\nnetwork\\nGeneration\\nnetwork\\nTransformation\\nmodule\\n2D image\\n3D image\\n3D feature\\nrepresentation\\n2D feature\\nrepresentation\\nLevel of prior\\nknowledge \\n0\\n100%\\nDeep learning\\nreconstruction\\nRegularized\\nreconstruction\\nConventional\\nreconstruction\\nb\\na\\nd\\nX-ray source\\nPatient\\nDetector\\nFig. 1 | 3d image reconstruction with ultra-sparse projection-view data. a, A geometric view of an X-ray source, a patient and a detector in a CT system. \\nb, X-ray projection views of a patient from three different angles. c, Different image-reconstruction schemes in the context of prior knowledge and \\nprojection sampling. d, Volumetric image reconstruction using deep learning with one or multiple 2D projection images.\\n2D projection\\nRepresentation network\\nGeneration network\\n3D volumetric\\nimage\\nInput\\na\\nb\\nd\\nc\\ne\\nOutput\\n2D conv 4 × 4, BN, ReLU\\n2D conv 3 × 3, BN, ReLU\\n3D deconv 1 × 1 × 1, ReLU\\n3D conv 1 × 1 × 1, ReLU\\n2D conv 1 × 1, ReLU\\nFeature reshape\\n1\\n256 256\\n512 512\\n1,024 1,024\\n2,048 2,048\\n4,096\\n4,096\\n1,024 2,048\\n512\\n256\\n128\\n64\\n512\\n256\\n128\\n64\\n3D deconv 4 × 4 × 4, BN, ReLU\\n3D deconv 3 × 3 × 3, BN, ReLU\\nTransformation\\nmodule\\n4,096\\n4,096\\n2,048\\n2,048\\n1\\nFig. 2 | architecture of the deep-learning network. a, The input of the model is a single projection view or multiple 2D projection views. b, The \\nrepresentation network learns the feature representation of the imaged object from the input. c, The extracted 2D features are reshaped and transferred \\nby the transformation module to a 3D representation, for subsequent reconstruction. d, The generation network uses representation features extracted in \\nthe former stages to generate the corresponding volumetric images. e, The output of the model is the correponding volumetric image. Conv, convolution \\nlayer; deconv, deconvolution layer (the numbers indicate the specific kernel size used); BN, batch normalization; ReLU, rectified linear unit; +, feature-map \\naddition with residual path; the numbers underneath each layer denote the number of feature maps for each layer.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n881\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nembedding features and learns a semantic representation of the \\nactual 3D scene from the input 2D projection(s). The transforma-\\ntion module bridges the representation and generation networks \\nthrough convolution and deconvolution operations and relates \\nthe 2D and 3D feature representations. The role of the generation \\nnetwork is to provide volumetric images with subtle structures on \\nthe basis of the learned features from the representation network.  \\nIn constructing the model, we assume that one or more 2D projec-\\ntions and the corresponding 3D image possess the same seman-\\ntic representation, as they represent the same object or scene.  \\nIn other words, the representation in feature space remains invari-\\nant in the transformation of a 2D projection into a 3D image. To a \\nlarge extent, the task of 3D image reconstruction here is to train the \\nencoder (that is, the representation network) and decoder (that is, \\nthe generation network) to reliably learn the relationship between \\nthe feature space and the image space. Details about the network \\narchitecture are included in Methods.\\nTraining a deep-learning model requires a large amount of anno-\\ntated data—this is often a bottleneck37. Instead of actually measur-\\ning a large number of paired X-ray projections and CT images for \\nsupervised training, we digitally produce projection images from a \\nCT image of a patient by using the geometry consistent with a clini-\\ncal on-board cone-beam CT system for radiation therapy (Fig. 1a). \\nFor imaging in the thoracic or upper-abdominal region, where four-\\ndimensional (4D) CT is often acquired to resolve organ motion \\ncaused by involuntary respiration, each 4D phase (that is, phase-\\nresolved) CT is selected to form a 3D CT dataset. In reality, a 3D CT \\nimage captures only one out of numerous possible scenarios of the \\npatient’s internal anatomy. To consider various clinical situations in \\nthe modelling, a series of translations, rotations and organ defor-\\nmations are introduced to the 3D CT to mimic different imaging \\nsituations. For each of the transformations, the corresponding 2D \\nprojection image or digitally reconstructed radiograph (DRR) for \\none or more specified angles is produced. In this way, a dataset of \\nDRR-CT pairs is generated for the training and testing of the deep-\\nlearning model. In practice, the dataset produced by using the CT of \\na given patient can be used to train a patient-specific deep-learning \\nmodel for subsequent volumetric imaging of the same patient. The \\nmodel can be used for interventional procedures such as radiation \\ntherapy and image-guided biopsy, in which pre-operational CT can \\nbe used to train the deep-learning model. One may, of course, con-\\nstruct a training dataset composed of an ensemble of patients with \\nthe above-mentioned DRR-CT pairs. This would lead to a more \\ngenerally applicable model, but the fundamental principle would \\nbe the same. For simplicity, we will focus on the development of a \\npatient-specific 2D–3D image mapping model.\\nWe evaluate the approach by using different disease sites: an \\nupper-abdominal case, a lung case and a head-and-neck case. We \\nuse the anterior–posterior 2D projection as input (Fig. 2). In all \\nexperiments, the same network architecture and training strategy \\nare used. The loss curves (Fig. 3) indicate that the model is trained \\nto fit the training data well and can also be generalized to work on \\nthe data not included in the training datasets. The details of dataset \\ngeneration and the training process are described in Methods.\\nTo evaluate the feasibility of the approach, we deploy the trained \\nnetwork on an independent testing dataset. Figure 4a shows our \\nreconstruction results with a single anterior–posterior-view input \\nfor the abdominal CT and lung CT cases, together with the ground-\\ntruth CT images and the difference images between the obtained \\nimages and the ground truth. The deep-learning-derived images \\nresemble the target images, indicating the potential of the model \\nfor volumetric imaging. We also reconstruct volumetric images \\nAbdominal CT\\n5\\na\\nb\\nTraining loss\\nValidation loss\\nTraining loss\\nValidation loss\\nTraining loss\\nValidation loss\\nTraining loss\\nValidation loss\\n4\\n3\\nMSE loss (×10–2)\\n2\\n1\\n0\\n5\\n4\\n3\\nMSE loss (×10–3)\\n2\\n1\\n0\\n5\\n4\\n3\\nMSE loss (×10–3)\\n2\\n1\\n0\\n5\\n4\\n3\\nMSE loss (×10–2)\\n2\\n1\\n0\\n0\\n1\\n2\\n3\\nTraining iterations (×104)\\n4\\n5\\n0\\n1\\n2\\n3\\nTraining iterations (×104)\\n4\\n5\\n0\\n1\\n2\\n3\\nTraining iterations (×104)\\n4\\n5\\n0\\n1\\n2\\n3\\nTraining iterations (×104)\\n4\\n5\\nLung CT\\nFig. 3 | training-loss and validation-loss curves for the abdominal ct and lung ct cases. a,b, Graph of mean-squared error (MSE) loss for training data \\n(in blue) and validation data (in orange) against the number of training iterations for abdominal CT (a) and lung CT (b), during the image reconstruction \\nwith a single projection view. Details are shown in the zoom-in figures on the right.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n882\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nwith a single lateral view as input for the abdominal case, with \\nsimilar results (see the Experiments section of the Supplementary \\nInformation). Furthermore, we use multiple quantitative evalua-\\ntion metrics to measure the results. Table 1 summarizes the average \\nvalues of the evaluation metrics. The qualitative and quantitative \\nresults demonstrate that our model is capable of achieving 3D image \\nreconstruction even with only a single 2D projection. The results \\n(Fig. 5) also confirm the validity of our approach.\\nIn addition, we conduct experiments with two, five or ten \\nprojection views as inputs. The multiple-view angles are distrib-\\nuted evenly around a 180° semicircle (for instance, for two views,  \\nthe two orthogonal directions are 0° (anterior–posterior) and 90° \\n(lateral)). We stack the 2D projections from different view angles as \\nthe input data and modify the first convolution layer to fit the input \\nchannel size. With the same model-training procedure and hyper-\\nparameters, we obtain the CT images for 2, 5 and 10 views for both \\nthe abdominal CT and lung CT cases (Fig. 4b–d). The quantitative \\nevaluation of the results for these cases is summarized in Table 1. \\nThe training-loss curves and the corresponding coronal and sagit-\\ntal views of the images are shown in Supplementary Figs. 1, 2 and  \\nFigs. 3–6, respectively. By comparing the quantitative evaluation \\nmetrics, it is clear that a single 2D projection is capable of produc-\\ning a reconstructed image similar to that obtained with multiple \\nprojections. In a sense, the network structure is optimized for a \\nsingle-view input. Generally, there should be a relation between \\nthe number of projections and the architecture of the hierarchical \\nnetwork in deep-learning-based reconstruction. More projections \\nshould either lead to better performance or yield room to simplify \\nthe network structure because the learned representation is gener-\\nally enhanced with the additional projection information.\\ndiscussion\\nTo better understand the deep-learning model, we analyse  \\nthe semantic representations learned from the model. Generally \\na\\n1-view \\nPredicted\\nDifference\\nPredicted\\nDifference\\nPredicted\\nDifference\\nPredicted\\nDifference\\nTruth\\nb\\n2-view \\nc\\n5-view \\nd\\n10-view \\nAbdominal CT\\nLung CT\\nFig. 4 | examples from the abdominal ct and lung ct cases. a–d, Images reconstructed by using 1 (a), 2 (b), 5 (c) and 10 (d) projection views. Predicted \\nand difference images between predicted and ground truth are shown. The corresponding coronal and sagittal views of the images for both experiments \\nare presented in Supplementary Figs. 3–6. For the abdominal CT case, 720, 180 and 600 images are used for training, validation and testing, respectively. \\nFor the lung CT case, 2,400, 600 and 200 images are used for training, validation and testing, respectively.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n883\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nspeaking, successful generation of volumetric images is possible \\nonly if the model is able to learn the semantic representation of the \\n3D structure from the input projections. Thus, for the same volume, \\nthe representations obtained via learning from different angular \\nprojections should be similar, since they describe the same under-\\nlying 3D scene. In Fig. 6a, we visualize the feature maps extracted \\nTable 1 | reconstruction results for the abdominal ct and lung ct cases\\nNumber of 2d \\nprojections\\nabdominal ct\\nLung ct\\nmae\\nrmSe\\nSSim\\nPSNr\\nmae\\nrmSe\\nSSim\\nPSNr\\n1\\n0.018\\n0.177\\n0.929\\n30.523\\n0.025\\n0.385\\n0.838\\n27.157\\n2\\n0.015\\n0.140\\n0.945\\n32.554\\n0.024\\n0.399\\n0.837\\n26.985\\n5\\n0.016\\n0.155\\n0.942\\n31.823\\n0.028\\n0.452\\n0.831\\n26.247\\n10\\n0.018\\n0.165\\n0.939\\n31.355\\n0.027\\n0.429\\n0.817\\n26.636\\nMAE, mean absolute error; RMSE, root mean squared error; SSIM, structural similarity; PSNR, peak signal noise ratio.\\nTransverse\\nSagittal\\nCoronal\\nTraining samples\\nTransverse\\nSagittal\\nCoronal\\nDifference\\nDifference\\nDifference\\nPredicted images and differences from ground truth\\nTesting samples and differences from training samples\\na\\nb\\nFig. 5 | examples from the head-and-neck ct case. a, 3D CT images of a head-and-neck case used for the training of the deep-learning model. b, Left: \\ntesting samples and the corresponding difference images (with respect to the training samples) in the transverse, sagittal and coronal planes. Right: \\npredicted images and the corresponding difference images (with respect to the ground truth) in the transverse, sagittal and coronal planes. For this case, \\n2,000, 500 and 200 images are used for training, validation and testing, respectively.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n884\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nfrom the transformation module for two testing samples. For visu-\\nalization purposes, only 5 randomly chosen channels among the \\n4,096 feature maps are shown, each with a size of 4 × 4 pixels. The \\nfeature maps learned from different numbers of 2D projections \\nare displayed separately in different columns. The results show \\nthat, when different 2D views are given, the model extracts similar \\nsemantic representations of the underlying 3D scene. Furthermore, \\nFig. 6b shows the visualization of t-distributed stochastic neighbour \\nembedding (t-SNE) for the feature maps of 15 testing samples. The \\nt-SNE technique is commonly used to visualize high-dimensional \\ndata by embedding each sample as a point in a 2D space38. The four \\npoints in a cluster of the same colour represent the learned features \\nfrom one-, two-, five- and ten-view reconstructions. The figure \\nshows clustering behaviour for feature maps from the same sample,  \\nindicating that the model learns a similar representation from  \\ndifferent 2D projections.\\nWe also measure the similarity of the embedding representations \\nby calculating the Euclidean distance between two feature maps. In \\nthis way, we compute a similarity score, ranging from 0 to 1, where \\nhigh similarity (a score approaching 1) indicates that the distance \\nbetween two feature maps is close to zero. We plot a correlation \\nmatrix (Fig. 6c) among 50 randomly selected testing samples, with \\ntheir feature representations extracted from one-view and two-view \\nreconstruction models. The highest values stand out in the diagonal \\nof the correlation matrix whereas other off-diagonal values remain \\nrelatively low. This illustrates that the two sets of feature represen-\\ntations learned from one-view and two-view projections for the \\nsame 3D scene are more similar or closer in Euclidean distance \\nspace compared with the feature representations learned from other  \\ndifferent 3D scenes. This provides additional evidence supporting \\nthe capability of the model to learn a semantic representation of the \\n3D scene with a single projection.\\nRobustness against possible irregular breathing patterns is \\nimportant for future clinical implementation of the approach. The \\nrobustness of deep networks against various perturbations is an \\nintense area of research in artificial intelligence39–46. As summarized \\nin ref. 43, possible solutions come in three categories: (1) the modi-\\nfication of network architectures (for example, adding more layers, \\nchanging the loss function and modifying the activation functions); \\n(2) the use of external models as a network add-on to detect out-of-\\ndistribution data (for example, using an external detector to rectify \\nthe irregular data); and (3) the modification of the training-data dis-\\ntribution or the training strategy (for example, adding regulariza-\\ntion, data augmentation or leveraging adversarial training). In (1), \\nthe efforts are focused on refining the learning models. In (2), irreg-\\nular motions might be regarded as out-of-distribution data, where \\nsome potential techniques, such as a detector subnetwork41 or the \\nconfidence-based method42,44, might be useful for detecting irreg-\\nular input. Among the various methods, the modification of the \\ntraining-data distribution is arguably the most straightforward way \\na\\nTest sample 1\\nTest sample 2\\nb\\n1-view\\n2-view\\n5-view 10-view\\n1-view\\n2-view\\n5-view 10-view\\nc\\n0\\n10\\n20\\n30\\n40\\n0\\n10\\n20\\n30\\n40\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTesting-sample number\\nTesting-sample number\\nFig. 6 | analysis of feature maps. a, Visualization of the feature maps, learned from different 2D projections, for two testing samples. The different colours \\nin the figure indicate different intensity values in the feature maps (a lighter colour indicates a higher intensity value). b, t-SNE visualization of the feature \\nrepresentations of 15 testing examples with the input of different 2D views. A total of 15 clusters (4 points of the same colour) are shown. The four points \\nin a cluster represent the learned features from 1-, 2-, 5- and 10-view reconstruction models. Each cluster denotes the embedded representations for each \\nof the 15 randomly chosen testing samples. c, Correlation matrix of representation vectors in 1-view and 2-view reconstructions from 50 randomly chosen \\ntesting samples out of 600.\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n885\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\nto proceed. The rationale is that if the irregularities can be incorpo-\\nrated effectively into the training dataset and the training strategy \\ncan be adjusted accordingly, the robustness of the trained model \\nwould be enhanced. To a certain extent, this has been elaborated \\nin the example in Supplementary Fig. 7, where it is demonstrated \\nthat, because of the inclusion of augmented training datasets with \\nrotational transformations, the deep-learning approach is much \\nmore robust against a small rotation of the imaging subject than a \\nconventional principal component analysis (PCA)-based method. \\nQuantitative results of the study for the testing sample presented in \\nSupplementary Fig. 7 are shown in Supplementary Table 1.\\nOutlook. We have described a deep-learning approach for volumet-\\nric imaging with ultra-sparse data sampling and a patient-specific \\nprior. The data-driven strategy is capable of holistically extract-\\ning the feature characteristics embedded in a single projection or \\nin a few 2D projections, and of transforming them into the cor-\\nresponding 3D image through model learning. The image-feature \\nspace transformation plays an essential role in the ultra-sparse \\nimage reconstruction. At the training stage, the method incorpo-\\nrates diverse forms of a\\xa0priori knowledge into the reconstruction. \\nThe manifold-mapping function is learned from the training data-\\nsets, rather than relying on any ad hoc form of motion trajectory. \\nAlthough we have used X-ray imaging and patient-specific data, the \\nconcept and implementation of the approach could be extended to \\nother imaging modalities or to other data domains with ultra-sparse \\nsampling. Practically, single-view imaging represents a potential \\nsolution for many image-guided interventional procedures and may \\nhelp to simplify the hardware of tomographic imaging systems.\\nmethods\\nProblem formulation. We formulate the problem of 3D image reconstruction \\nfrom 2D projection(s) into a deep-learning framework. Given a sequence of 2D \\nprojections denoted as fX1; X2; \\ue001 \\ue001 \\ue001 ; XNg\\nI\\n, where Xi 2 RH2D ´ W2D\\nI\\n for all 1 ≤ i ≤ N \\nand N is the number of given 2D projections, the goal is to generate a volumetric \\n3D image Y describing the corresponding 3D physical scene. With the sequence of \\n2D projections as input, the deep-learning model outputs the predicted 3D volume \\ndenoted as Ypred 2 RC3D ´ H3D ´ W3D\\nI\\n, while Ytruth 2 RC3D ´ H3D ´ W3D\\nI\\n is the ground-\\ntruth 3D image as the reconstruction target. Note that network prediction Ypred \\nis of the same size as ground-truth image Ytruth, where each entry is a voxel-wise \\nintensity value. Thus, the problem is formulated as finding a mapping function \\nF transforming 2D projections to volumetric images. To tackle this problem, a \\ndeep-learning model is trained to find the mapping function F, which uses 2D \\nprojections fX1; X2; \\ue001 \\ue001 \\ue001 ; XNg\\nI\\n as input and predicts the corresponding 3D image \\nYpred, as expressed in Equation (1).\\nF X1; X2; \\ue001 \\ue001 \\ue001 ; XN\\nð\\nÞ ¼ Ypred\\nð1Þ\\nIn order to use a sequence of 2D projections as model input, we stack all the 2D \\nprojections together as a single 3D tensor. In other words, a set of 2D projections \\nfX1; X2; \\ue001 \\ue001 \\ue001 ; XNg\\nI\\n Xi 2 RH2D ´ W2D\\n�\\n�\\nI\\n are stacked as a 3D volume Z 2 RN ´ H2D ´ W2D\\nI\\n, \\nwhere N is the number of 2D projections. In what follows, we introduce the model \\narchitecture of the deep neural network in detail.\\nEncoder–decoder framework. The deep neural network is formulated into an \\nencoder–decoder framework (Fig. 2). In the auto-encoder model47, the encoder \\nconverts high-dimensional data into embedded representations whereas the \\ndecoder reconstructs high-dimensional input. In our task, instead of decoding \\nto get the input, we developed a modified decoder to generate the corresponding \\nvolumetric images based on the codes converted by the encoder. More precisely, \\nwith a sequence of 2D projections as input, the encoder network learns the \\nfeature representation by extracting semantic information from 2D projections, \\nsuch as organ position and size. In this way, the encoder network learns a \\ntransformation function h1 from the 2D image domain to the feature domain. A \\ntransformation module then follows to learn the manifold-mapping function h2 \\nin the feature domain to transform the feature representation across dimensions. \\nUsing the learned feature representation as the input, the decoder network is \\ntrained to generate the 3D volume. In other words, the decoder network learns a \\ntransformation function h3 from the feature domain to the 3D image domain. In \\nthis way, we fit the target mapping function F by decomposition: F ¼ h1 \\ue00e h2 \\ue00e h3\\nI\\n. The rationale behind our network design is that both 2D projections and 3D \\nimages should share the same semantic feature representation in the feature \\ndomain, as they represent the image expressions of the same object or physical \\nscene. Accordingly, the representation in feature space should remain invariant. \\nIn a sense, if the model can learn the transformation function between the feature \\nspace and the 2D and/or 3D image space, it is possible to reconstruct 3D images \\nfrom 2D projections. Therefore, following this encoder–decoder framework,  \\nour model is able to learn how to generate 3D images from 2D projections by  \\nusing the learned representations in high-dimensional feature space.\\nRepresentation network. Superb performance has been achieved by deep residual \\nnetworks (such as ResNet)48 in many tasks. A key step in residual learning is the \\nidentity mapping that facilitates the training process and avoids gradient vanish \\nin back-propagation48, which encourages residual learning of the hierarchical \\nrepresentation at each stage and eases the training of the deep network. Motivated \\nby this feature, we introduce a residual-learning scheme in the representation \\nnetwork (Fig. 2), in which the 2D convolution residual block is used to assist the \\ndeep model to learn semantic representations from 2D projections. More details \\nabout the residual-learning scheme are presented in Supplementary Information, \\nAblative study and discussion, and the results are summarized in Supplementary \\nTable 2. Specifically, each 2D convolution residual block consists of a pattern of \\n‘2D convolution layer (with kernel size 4 and stride 2) → 2D batch normalization \\nlayer → ReLU layer → 2D convolution layer (with kernel size 3 and stride 1) → 2D \\nbatch normalization layer → ReLU layer’. The first layer performs 2D convolution \\noperations using a 4 × 4 kernel with sliding stride 2 × 2, which down-samples \\nthe spatial size of the feature map by a factor 2. In addition, to keep the sparsity \\nof high-dimensional feature representation, we correspondingly double the \\nchannel number of the feature maps by increasing the number of convolutional \\nfilters. A distribution normalization layer among the training mini-batch (batch \\nnormalization)49 then follows before feeding the feature maps through the ReLU \\nlayer50. Next, the second 2D convolution layer and 2D batch normalization layer \\nare done by a kernel size of 3 × 3 and sliding stride 1 × 1, which keeps the spatial \\nshape of the feature maps. Moreover, before applying the second ReLU layer, an \\nextra shortcut path is established to add up the output of the first convolution \\nlayer to obtain the final output. By setting up the shortcut path of identity \\nmapping, the second convolution layer is encouraged to learn the residual feature \\nrepresentations. To extract hierarchical semantic features from 2D projections, \\nwe constructed the representation network by concatenating five 2D convolution \\nresidual blocks with different number of convolutional filters. A detailed discussion \\nof the network depth is available in Supplementary Information, Ablative study and \\ndiscussion, with some results illustrated in Supplementary Fig. 8. To be concise, \\nwe use the notation k × m × n to denote k channels of feature maps in a spatial \\nsize of m × n. In the generation network, the size of input images is denoted as \\nN × 128 × 128, where N is the number of 2D projections. The change of feature-\\nmap size through the network i s N  × 128 × 128 → 256 × 64 × 64 → 512 × 32 × 32 \\n → 1024 × 16 × 16 → 2048 × 8 × 8 → 4096 × 4 × 4, where each ‘→’ represents going \\nthrough a 2D convolution residual block as described above, except that batch \\nnormalization and ReLU activation are removed in the first convolution layer. \\nThus, the output of the representation network is a feature representation  \\nextracted from 2D projections with a size of 4096 × 4 × 4.\\nTransformation module. To bridge the representation and generation networks, \\na transformation module is deployed after learning the representations. As shown \\nin Fig. 2, by taking the convolution operations with a kernel size of 1 × 1 and \\nReLU activation, the 2D convolution layer learns a transformation across all 2D \\nfeature maps. Then, we reshape embedded representations from 4096 × 4 × 4 \\nto 2048 × 2 × 4 × 4. In this way, we transform the feature representation across \\ndimensions for subsequent 3D volume generation. Next, a 3D deconvolution layer \\nwith a kernel size of 1 × 1 × 1 and sliding stride of 1 × 1 × 1 learns a transformation \\namong all 3D feature cubes while keeping the feature size unchanged. This \\ntransformation module bridges the 2D and 3D feature spaces. Moreover, as \\ndescribed in previous work51, we also remove the batch normalization in the \\ntransformation module to help the knowledge transfer through this module.\\nGeneration network. The generation network is built upon the 3D deconvolution \\nblock, which consists of a pattern of ‘3D deconvolution layer (with kernel size 4 and \\nstride 2) → 3D batch normalization layer → ReLU layer → 3D deconvolution layer \\n(with kernel size 3 and stride 1) → 3D batch normalization layer → ReLU layer’. \\nNote that the ‘deconvolution’ layer actually means the operation of ‘transformed \\nconvolution’ or fractional stride convolution which performs up-sampling \\noperation. The first deconvolution layer up-samples a feature map by a factor \\n2 with a 4 × 4 × 4 kernel and sliding stride 2 × 2 × 2. To transform from a high-\\ndimensional feature domain to a 3D image domain, we accordingly reduce the \\nnumber of feature maps by decreasing the number of deconvolutional filters. The \\nsecond deconvolution layer with a 3 × 3 × 3 kernel and sliding stride 1 × 1 × 1 keeps \\nthe spatial shape of feature maps. A 3D batch normalization layer and a ReLU layer \\nfollow after each deconvolution layer. Hierarchically, the 3D generation network \\nconsists of five concatenated deconvolution blocks. Following the same convention \\nas in the representation network, we use a notation of k × m × n × p to denote k \\nchannels of 3D feature maps with a spatial size of m × n × p. With a representation \\ninput of 2,048 × 2 × 4 × 4, the data flow of the feature maps i s: 2 ,0 48 ×\\xad 2 ×\\xad 4 ×\\xad 4 →\\xad 1 ,0-\\n24 × 4 × 8 × 8 → 512 × 8 × 16 × 16 → 256 × 16 × 32 × 32 → 128 × 32 × 64 × 64 → 64 × 64-\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n886\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\n × 128 × 128, where each ‘→’ denotes a 3D residual block. At the end of the \\ngeneration network, an output transformation module is constructed with a 3D \\nconvolution layer and a 2D convolution layer with a kernel size of 1, which outputs \\n3D images fitting the shape of the reconstructed images. Finally, the generation \\nnetwork outputs the predicted 3D images of size C3D × 128 × 128, where C3D is \\nthe size of the target volumetric images along the z-axis. Note that in the output \\ntransformation module, the batch normalization layer is removed, and that there  \\nis no ReLU layer after the final convolution layer.\\nMaterials. The approach is evaluated by using three cases of different disease \\nsites. In the first study, a ten-phase upper-abdominal 4D CT scan of a patient for \\nradiation therapy treatment planning is selected. To proceed, the first six phases \\nare used to generate the CT-DRR pairs for model training and validation with \\nthe procedure described above. We use the anterior–posterior 2D projection as \\ninput (Fig. 2). With translation, rotation and deformation introduced to the CT \\nvolume, we obtain a total of 720 DRRs representing different scenarios of the \\npatient anatomy for model training and 180 DRRs for validation. To ensure that \\nthe testing data are not seen in the model-training process, we generate 600 testing \\nDRR samples independently from the remaining 4 phases of the 4D CT. The 4D \\nCT images are acquired with 120 kV, 80 mA on a positron emission tomography–\\nCT simulator (Biograph mCT 128, Siemens Medical Solutions) together with a \\nVarian Real-time Position Management system (Varian Medical Systems). The \\n2D projection data are obtained by projecting each of the 3D CT data points in \\nthe geometry of the on-board imager of the TrueBeam system (Varian Medical \\nSystem). In the second experiment, a lung cancer patient is chosen with two \\nindependent treatment-planning 4D CT scans acquired at two different times \\nwith the same imaging parameter settings as above. Using the data-augmentation \\nstrategy as described above, the first 4D CT is used to generate training (2,400 \\nsamples) and validation (600 samples) datasets, whereas the second 4D CT was \\nused to generate a testing dataset (200 samples). For each of the images in the \\ntraining and testing datasets, the corresponding 2D projections are produced \\nby projecting the 3D CT volume in the geometry of the on-board imager of the \\nTrueBeam system. To build a reliable model, the training and testing datasets \\nmight come from the same data distribution, but the datasets are independently \\nsampled. The data acquisition and processing for the head-and-neck case are \\ndescribed in Supplementary Information.\\nImage pre-processing. Data are pre-processed before feeding them into the \\nnetwork. First, we resize all data samples to the same size. For example, all the \\n2D projection images are resized to 128 × 128. The volumetric images of the \\nabdominal CT and the lung CT are resized to 46 × 128 × 128 and 168 × 128 × 128 \\nrespectively, due to their different depths in the z axis. Each data sample is a pair \\nof 2D projected view(s) and the corresponding 3D CT. Similar to other deep-\\nlearning-based imaging studies15, down-sampling is introduced purely because \\nof the memory limitation and for the purpose of computational efficiency. The \\nformulation and algorithm are scalable to full-size images (512 × 512), because the \\ncomponent layers used in our model are also scalable to images of different sizes. \\nAt the current resolution of 128 × 128 (which is the same as that used in the deep-\\nlearning-based MRI reconstruction15), small motions of less than 3 mm may not \\nbe described accurately. However, we should emphasize that this resolution does \\nnot represent a fundamental limit of the deep-learning-based approach and can \\nbe improved as computational technology advances. In practice, methods such as \\ndeep-learning-based super-resolution are being actively pursued, which may be \\nemployed to improve the spatial resolution of the approach. Additionally, following \\nthe standard protocol of data pre-processing, we conduct scaling normalization for \\nboth the 2D projections and the 3D volumetric images, where pixel-wise or voxel-\\nwise intensities are normalized to the interval [0,1]. Moreover, we normalize the \\nstatistical distribution of the pixel-wise intensity values in the input 2D projections \\nto be closer to a standard Gaussian distribution N 0; 1\\nð\\nÞ\\nI\\n. Specifically, we calculate \\nthe statistical mean and standard derivation among all the training data. When a \\nnew sample is inputted, we subtract the mean value from the input image(s) and \\ndivide the image(s) by the standard derivation to get the input 2D image(s).\\nTraining details. With input images X containing a stacked sequence of 2D \\nprojections, we train the deep network to predict the volumetric images Ypred, \\nwhich is expected to be as close as possible to the ground-truth images Ytruth. We \\ndefine the cost function as the MSE between the prediction Ypred and the ground \\ntruthYtruth, and the model was optimized by stochastic gradient descent iteratively. \\nFor comparison, we used the same training strategy and hyper-parameters for \\nall experiments. We implemented the network by using the PyTorch52 library, \\nand used the Adam optimizer53 to minimize the loss function and to update \\nthe network parameters iteratively through back-propagation. A learning rate \\nof 0.00002 and a mini-batch size of 1 are used because of memory limitations. \\nAt the end of each training epoch, the model is evaluated on the validation \\nset. This strategy is commonly used to monitor the model performance and \\navoid overfitting the training data. In addition, the learning rate is scheduled to \\ndecay according to the validation loss. Specifically, if the validation loss remains \\nunchanged for 10 epochs, the learning rate is reduced by a factor 2. Finally, the \\nbest checkpoint model with the smallest validation loss is saved as the final model \\nin the experiments. We trained the network using one Nvidia Tesla V100 graphics \\nprocessing unit for 100 epochs (duration typically around 20 h for the abdominal \\nCT case). During testing, the typical inference time for 3D reconstruction of one \\ntesting sample is around 0.5 s.\\nEvaluation. To evaluate the performance of the approach, we deploy the trained \\nmodel on a testing dataset, and analyse the reconstruction results using both \\nqualitative and quantitative evaluation metrics. We use four different metrics to \\nmeasure the quality of predicted 3D images: MAE, RMSE, SSIM54 and PSNR.  \\nWe compute the average values across all testing samples, and they are shown \\nin Table 1. MAE/MSE is the L1-norm/L2-norm error between Ypred and Ytruth. As \\nusual, we take the square root of MSE to get RMSE. In practice, MAE and RMSE \\nare commonly used to estimate the difference between the prediction and ground-\\ntruth images. SSIM score is calculated with a windowing approach in an image, \\nand is used for measuring the overall similarity between two images. In general, a \\nlower value of MAE and RMSE or a higher SSIM score indicates a better prediction \\ncloser to the ground-truth images. PSNR is defined as the ratio between the \\nmaximum signal power and the noise power that affects the image quality.  \\nPSNR is widely used to measure the quality of image reconstruction.\\nComparison study. To better benchmark the proposed method against the \\nexisting techniques, we conduct a comparative study with the published PCA-\\nbased method55–57 and elaborate the difference and advantages of our proposed \\napproach. The comparison is done for a special situation of 4D CT reconstruction \\n(abdominal CT), where the anatomical motion may be characterized by principal \\ncomponents. We find that the PCA and deep-learning-based methods produce \\nsimilar results in an ideal case when there is no inter-scan variation in patient \\npositioning (since the results are very similar, the resultant images are not shown). \\nHowever, the deep-learning model outperforms the PCA method in more realistic \\nscenarios when the patient position deviates slightly from that of the reference  \\nscan (see Supplementary Information for details).\\nReporting Summary. Further information on research design is available in the \\nNature Research Reporting Summary linked to this article.\\ndata availability\\nThe authors declare that the main data supporting the results in this study are \\navailable within the paper and its Supplementary Information. The raw datasets \\nfrom Stanford Hospital are protected because of patient privacy yet can be made \\navailable upon request provided that approval is obtained after an Institutional \\nReview Board procedure at Stanford.\\ncode availability\\nThe source code of the deep-learning algorithm is available for research uses at \\nhttps://github.com/liyues/PatRecon.\\nReceived: 29 November 2018; Accepted: 19 September 2019;  \\nPublished online: 28 October 2019\\nreferences\\n 1. Candes, E. J., Romberg, J. K. & Tao, T. Stable signal recovery from  \\nincomplete and inaccurate measurements. Commun. Pure Appl. Math. 59, \\n1207–1223 (2006).\\n 2. Lustig, M., Donoho, D. & Pauly, J. M. Sparse MRI: the application of \\ncompressed sensing for rapid MR imaging. Magn. Reson. Med. 58,  \\n1182–1195 (2007).\\n 3. Sidky, E. Y. & Pan, X. Image reconstruction in circular cone-beam computed \\ntomography by constrained, total-variation minimization. Phys. Med. Biol. 53, \\n4777–4807 (2008).\\n 4. Chen, G. H., Tang, J. & Leng, S. Prior image constrained compressed sensing \\n(PICCS): a method to accurately reconstruct dynamic CT images from highly \\nundersampled projection data sets. Med. Phys. 35, 660–663 (2008).\\n 5. Yu, H. & Wang, G. Compressed sensing based interior tomography.  \\nPhys. Med. Biol. 54, 2791–2805 (2009).\\n 6. Choi, K., Wang, J., Zhu, L., Suh, TS., Boyd, S. & Xing, L. Compressed sensing \\nbased cone-beam computed tomography reconstruction with a first-order \\nmethod. Med. Phys. 37, 5113–5125 (2010).\\n 7. Fessler, J. A. & Rogers, W. L. Spatial resolution properties of penalized-\\nlikelihood image reconstruction: space-invariant tomographs. IEEE Trans. \\nImage Process 5, 1346–1358 (1996).\\n 8. Ji, S., Xue, Y. & Carin, L. Bayesian compressive sensing. IEEE Trans. Signal \\nProcess. 56, 2346–2356 (2008).\\n 9. Engl, H. W., Hanke, M. & Neubauer, A. Regularization of inverse problems, \\nVol. 375 (Springer Science & Business Media, 1996).\\n 10. Stayman, J. W. & Fessler, J. A. Regularization for uniform spatial resolution \\nproperties in penalized-likelihood image reconstruction. IEEE Trans. Med. \\nImaging 19, 601–615 (2000).\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n887\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='Articles\\nNaTure BiomeDical eNgiNeeriNg\\n 11. Jiang, M. & Wang, G. Convergence studies on iterative algorithms for image \\nreconstruction. IEEE Trans. Med. Imaging 22, 569–579 (2003).\\n 12. Wang, J., Li, T., Lu, H. & Liang, Z. Penalized weighted least-squares approach \\nto sinogram noise reduction and image reconstruction for low-dose X-ray \\ncomputed tomography. IEEE Trans. Med. Imaging 25, 1272–1283 (2006).\\n 13. Xu, Q. et\\xa0al. Low-dose X-ray CT reconstruction via dictionary learning.  \\nIEEE Trans. Med. Imaging 31, 1682–1697 (2012).\\n 14. Preiswerk, F. et\\xa0al. Hybrid MRI-Ultrasound acquisitions, and scannerless \\nreal-time imaging. Magn. Reson. Med. 78, 897–908 (2017).\\n 15. Zhu, B., Liu, J. Z., Cauley, S. F., Rosen, B. R. & Rosen, M. S. Image \\nreconstruction by domain-transform manifold learning. Nature 555,  \\n487–492 (2018).\\n 16. Henzler, P., Rasche, V., Ropinski, T. & Ritschel, T. Single-image tomography: \\n3D volumes from 2D cranial X-rays. Computer Graph. Forum 37,  \\n377–388 (2018).\\n 17. Montoya, J. C., Zhang, C., Li, K. & Chen, G. Volumetric scout CT images \\nreconstructed from conventional two-view radiograph localizers using deep \\nlearning. In Proc. SPIE Medical Imaging 2019: Physics of Medical Imaging  \\n(eds Schmidt, T. G. et\\xa0al) 1094825 (SPIE, 2019).\\n 18. Nomura, Y., Xu, Q., Shirato, H., Shimizu, S. & Xing, L. Projection-domain \\nscatter correction for cone beam computed tomography using a residual \\nconvolutional neural network. Med. Phys. 46, 3142–3155 (2019).\\n 19. Wu, Y. et\\xa0al. Incorporating prior knowledge via volumetric deep residual \\nnetwork to optimize the reconstruction of sparsely sampled MRI.  \\nMagn. Reson. Imaging https://doi.org/10.1016/j.mri.2019.03.012 (2019).\\n 20. Eslami, S. A. et\\xa0al. Neural scene representation and rendering. Science 360, \\n1204–1210 (2018).\\n 21. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\\n 22. Schmidhuber, J. Deep learning in neural networks: an overview. Neural Netw. \\n61, 85–117 (2015).\\n 23. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \\nconvolutional neural networks. In Proc 25th Conf. on Advances in Neural \\nInformation Processing Systems (eds Pereira, F. et\\xa0al.) 1097–1105 (NIPS, 2012).\\n 24. Simonyan, K. & Zisserman, A. Very deep convolutional networks for \\nlarge-scale image recognition. In Proc. 3rd International Conference on \\nLearning Representations (ICLR, 2015).\\n 25. Shen, L., Yeung, S., Hoffman, J., Mori, G. & Fei-Fei, L. Scaling Human-Object \\nInteraction Recognition through Zero-Shot Learning. In 2018 Winter \\nConference on Applications of Computer Vision 1568–1576 (IEEE, 2018).\\n 26. Chen, C., Seff, A., Kornhauser, A. & Xiao, J. Deepdriving: Learning \\naffordance for direct perception in autonomous driving. In 2015 International \\nConference on Computer Vision 2722–2730 (IEEE, 2015).\\n 27. Collobert, R. & Weston, J. A unified architecture for natural language \\nprocessing: Deep neural networks with multitask learning. In Proc. 25th \\nInternational Conference on Machine Learning (eds Cohen, W. et\\xa0al.) 160–167 \\n(ACM, 2008).\\n 28. Ibragimov, B., Toesca, D., Chang, D., Koong, A. & Xing, L. Development of \\ndeep neural network for individualized hepatobiliary toxicity prediction after \\nliver SBRT. Med. Phys. 45, 4763–4774 (2018).\\n 29. Poplin, R. et\\xa0al. Prediction of cardiovascular risk factors from retinal fundus \\nphotographs via deep learning. Nat. Biomed. Eng. 2, 158–164 (2018).\\n 30. Esteva, A. et\\xa0al. Dermatologist-level classification of skin cancer with deep \\nneural networks. Nature 542, 115–118 (2017).\\n 31. Gulshan, V. et\\xa0al. Development and validation of a deep learning algorithm \\nfor detection of diabetic retinopathy in retinal fundus photographs. JAMA \\n316, 2402–2410 (2016).\\n 32. Ting, D. S. W. et\\xa0al. Development and validation of a deep learning system \\nfor diabetic retinopathy and related eye diseases using retinal images from \\nmultiethnic populations with diabetes. JAMA 318, 2211–2223 (2017).\\n 33. Liu, F. et\\xa0al. Deep convolutional neural network and 3D deformable approach \\nfor tissue segmentation in musculoskeletal magnetic resonance imaging. \\nMagn. Reson. Med. 79, 2379–2391 (2018).\\n 34. Zhao, W. et\\xa0al. Incorporating imaging information from deep neural network \\nlayers into image guided radiation therapy (IGRT). Radiother. Oncol. 140, \\n167–174 (2019).\\n 35. Liu, F., Feng, L. & Kijowski, R. MANTIS: Model-Augmented neural network \\nwith incoherent k-space sampling for efficient mr parameter mapping. Magn. \\nReson. Med. 82, 174–188 (2019).\\n 36. Zhao, W. et\\xa0al. Markerless pancreatic tumor target localization enabled by \\ndeep learning. Int. J. Radiat. Oncol. Biol. Phys. 105, 432–439 (2019).\\n 37. Hoo-Chang, S. et\\xa0al. Deep convolutional neural networks for computer-aided \\ndetection: CNN architectures, dataset characteristics and transfer learning. \\nIEEE Trans. Med. Imaging 35, 1285–1298 (2016).\\n 38. van\\xa0der Maaten, L. & Hinton, G. Visualizing data using t-SNE.  \\nJ. Mach. Learn. Res. 9, 2579–2605 (2008).\\n 39. Papernot, N., McDaniel, P. & Goodfellow, I. Transferability in machine \\nlearning: from phenomena to black-box attacks using adversarial samples. \\nPreprint at https://arxiv.org/abs/1605.07277 (2016).\\n 40. Eykholt, K. et\\xa0al. Robust physical-world attacks on deep learning visual \\nclassification. In Proc. IEEE Conference on Computer Vision and Pattern \\nRecognition 1625–1634 (IEEE, 2018).\\n 41. Metzen, J. H., Genewein, T., Fischer, V. & Bischoff, B. On detecting \\nadversarial perturbations. In Proc. 5th International Conference on Learning \\nRepresentations (ICLR, 2017).\\n 42. Lee, K., Lee, H., Lee, K. & Shin, J. Training confidence-calibrated classifiers \\nfor detecting out-of-distribution samples. In Proc. 6th International \\nConference on Learning Representations (ICLR, 2018).\\n 43. Akhtar, N. & Mian, A. Threat of adversarial attacks on deep  \\nlearning in computer vision: a survey. IEEE Access 6,  \\n14410–14430 (2018).\\n 44. Lee, K., Lee, K., Lee, H. & Shin, J. A simple unified framework for detecting \\nout-of-distribution samples and adversarial attacks. In Proc. 31st Conference \\non Advances in Neural Information Processing Systems (eds Bengjo, S. et\\xa0al.) \\n7167–7177 (NIPS, 2018).\\n 45. Su, J., Vargas, D. V. & Sakurai, K. One pixel attack for fooling deep neural \\nnetworks. IEEE Trans. Evol. Comput. 23, 828–841 (2019).\\n 46. Yuan, X., He, P., Zhu, Q. & Li, X. Adversarial examples: attacks and  \\ndefenses for deep learning. IEEE Trans. Neural Netw. Learn. Syst. 30, \\n2805–2824 (2019).\\n 47. Hinton, G. E. & Salakhutdinov, R. R. Reducing the dimensionality of data \\nwith neural networks. Science 313, 504–507 (2006).\\n 48. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image \\nrecognition. In Proc. IEEE Conference on Computer Vision and Pattern \\nRecognition 770–778 (IEEE, 2016).\\n 49. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network \\ntraining by reducing internal covariate shift. In Proc. 32nd International \\nConference on Machine Learning, Vol. 37, 448–456 (JMLR, 2015).\\n 50. Nair, V. & Hinton, G. E. Rectified linear units improve restricted boltzmann \\nmachines. In Proc. 27th International Conference on Machine Learning \\n807–814 (ICML, 2010).\\n 51. Isola, P., Zhu, J.-Y., Zhou, T. & Efros, A. Image-to-image translation with \\nconditional adversarial networks. In Proc. IEEE Conference on Computer \\nVision and Pattern Recognition 1125–1134 (IEEE, 2017).\\n 52. Paszke, A. et\\xa0al. Automatic differentiation in pytorch. In Proc. 30th Conference \\non Advances in Neural Information Processing Systems Autodiff. Workshop \\n(NIPS, 2017).\\n 53. Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization.  \\nIn Proc. 3rd International Conference on Learning Representations  \\n(ICLR, 2015).\\n 54. Wang, Z., Bovik, A. C., Sheikh, H. R. & Simoncelli, E. P. Image quality \\nassessment: from error visibility to structural similarity. IEEE Trans. Image \\nProcess 13, 600–612 (2004).\\n 55. Li, R. et\\xa0al. Real-time volumetric image reconstruction and 3D tumor \\nlocalization based on a single x-ray projection image for lung cancer \\nradiotherapy. Med. Phys. 37, 2822–2826 (2010).\\n 56. Li, R. et\\xa0al. 3D tumor localization through real-time volumetric x-ray imaging \\nfor lung cancer radiotherapy. Med. Phys. 38, 2783–2794 (2011).\\n 57. Xu, Y. et\\xa0al. A method for volumetric imaging in radiotherapy using single \\nx-ray projection. Med. Phys. 42, 2498–2509 (2015).\\nacknowledgements\\nThis research is partially supported by the National Institutes of Health (R01CA176553 \\nand R01EB016777). The contents of this article are solely the responsibility of the authors \\nand do not necessarily represent the official NIH views.\\nauthor contributions\\nL.X. proposed the original notion of single-view reconstruction for tomographic imaging \\nand supervised the research, L.S. designed and implemented the algorithm. W.Z. \\ndesigned the experiments and implemented the data generation process. L.S. and W.Z. \\ncarried out experimental work. L.X., L.S. and W.Z. wrote the manuscript. All the authors \\nreviewed the manuscript.\\ncompeting interests\\nThe authors declare no competing interests.\\nadditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/\\ns41551-019-0466-4.\\nCorrespondence and requests for materials should be addressed to L.X.\\nReprints and permissions information is available at www.nature.com/reprints.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \\npublished maps and institutional affiliations.\\n© The Author(s), under exclusive licence to Springer Nature Limited 2019\\nNature BiomedicaL eNgiNeeriNg | VOL 3 | NOVEMBER 2019 | 880–888 | www.nature.com/natbiomedeng\\n888\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content=\"1\\nnature research  |  reporting summary\\nOctober 2018\\nCorresponding author(s):\\nLei Xing\\nLast updated by author(s): Sep 19, 2019\\nReporting Summary\\nNature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \\nin reporting. For further information on Nature Research policies, see Authors & Referees and the Editorial Policy Checklist.\\nStatistics\\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\\nn/a Confirmed\\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\\nThe statistical test(s) used AND whether they are one- or two-sided \\nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\\nA description of all covariates tested\\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \\nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \\nGive P values as exact values whenever suitable.\\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\\nOur web collection on statistics for biologists contains articles on many of the points above.\\nSoftware and code\\nPolicy information about availability of computer code\\nData collection\\nAll patient data were acquired with 120 kV on a Positron emission tomography–computed tomography (PET-CT) simulator (Biograph mCT \\n128, Siemens Medical Solutions, Erlangen, Germany).\\nData analysis\\nPython (version 3.5.5), PyTorch (version 0.4.1), and custom algorithms.\\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. \\nWe strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\\nData\\nPolicy information about availability of data\\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \\n- Accession codes, unique identifiers, or web links for publicly available datasets \\n- A list of figures that have associated raw data \\n- A description of any restrictions on data availability\\nThe authors declare that the main data supporting the results in this study are available within the paper and its Supplementary Information. The raw datasets from \\nStanford Hospital are protected because of patient privacy yet can be made available upon request provided that approval is obtained after an Institutional Review \\nBoard procedure at Stanford. \\n\", metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content='2\\nnature research  |  reporting summary\\nOctober 2018\\nField-specific reporting\\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\\nLife sciences\\nBehavioural & social sciences\\n Ecological, evolutionary & environmental sciences\\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\\nLife sciences study design\\nAll studies must disclose on these points even when the disclosure is negative.\\nSample size\\nFor abdominal CT, 720, 180, and 600 images are used for training, validation, and testing. For lung CT, 2400, 600, and 200 images are used for \\ntraining, validation, and testing. For head and neck CT, 2000, 500, and 200 images are used for training, validation, and testing.\\nData exclusions\\nNo data were excluded from the analyses. \\nReplication\\nAll experiments were conducted in a manner that produced clean replicates. The algorithm performance should have similar results if the \\nmodel is trained following the training strategy described in the paper.\\nRandomization\\nThe training and validation datasets were randomly selected.\\nBlinding\\nThe test datasets were different from the training and testing datasets.\\nReporting for specific materials, systems and methods\\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \\nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \\nMaterials & experimental systems\\nn/a Involved in the study\\nAntibodies\\nEukaryotic cell lines\\nPalaeontology\\nAnimals and other organisms\\nHuman research participants\\nClinical data\\nMethods\\nn/a Involved in the study\\nChIP-seq\\nFlow cytometry\\nMRI-based neuroimaging\\nClinical data\\nPolicy information about clinical studies\\nAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.\\nClinical trial registration\\nThe study is not a clinical trial.\\nStudy protocol\\nThe study is not a clinical trial.\\nData collection\\nAll patient data were acquired with 120 kV on a Positron emission tomography–computed tomography (PET-CT) simulator \\n(Biograph mCT 128, Siemens Medical Solutions, Erlangen, Germany).\\nOutcomes\\nThe study is not a clinical trial.\\n', metadata={'source': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'file_path': 'data/Shen et al. - 2019 - Patient-specific reconstruction of volumetric comp.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning', 'author': 'Liyue Shen', 'subject': 'Nature Biomedical Engineering, doi:10.1038/s41551-019-0466-4', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20191102115659+05'30'\", 'modDate': \"D:20191102115747+05'30'\", 'trapped': ''}),\n",
       " Document(page_content=\"Novel-view X-ray Projection Synthesis through Geometry-\\nintegrated Deep Learning\\nLiyue Shena, Lequan Yub, Wei Zhaob, John Paulya, Lei Xinga,b\\naDepartment of Electrical Engineering, Stanford University, Stanford, CA, USA\\nbDepartment of Radiation Oncology, Stanford University, Stanford, CA, USA\\nAbstract\\nX-ray imaging is a widely used approach to view the internal structure of a subject for clinical \\ndiagnosis, image-guided interventions and decision-making. The X-ray projections acquired \\nat different view angles provide complementary information of patient’s anatomy and are \\nrequired for stereoscopic or volumetric imaging of the subject. In reality, obtaining multiple-\\nview projections inevitably increases radiation dose and complicates clinical workflow. Here \\nwe investigate a strategy of obtaining the X-ray projection image at a novel view angle from \\na given projection image at a specific view angle to alleviate the need for actual projection \\nmeasurement. Specifically, a Deep Learning-based Geometry-Integrated Projection Synthesis \\n(DL-GIPS) framework is proposed for the generation of novel-view X-ray projections. The \\nproposed deep learning model extracts geometry and texture features from a source-view \\nprojection, and then conducts geometry transformation on the geometry features to accommodate \\nthe change of view angle. At the final stage, the X-ray projection in the target view is synthesized \\nfrom the transformed geometry and the shared texture features via an image generator. The \\nfeasibility and potential impact of the proposed DL-GIPS model are demonstrated using lung \\nimaging cases. The proposed strategy can be generalized to a general case of multiple projections \\nsynthesis from multiple input views and potentially provides a new paradigm for various \\nstereoscopic and volumetric imaging with substantially reduced efforts in data acquisition.\\nKeywords\\nprojection view synthesis; X-ray imaging; geometry-integrated deep learning\\nAuthor statement\\nLiyue Shen: Conceptualization, Methodology, Software, Investigation, Writing - Original Draft, Formal analysis, Visualization\\nLequan Yu: Writing - Review & Editing, Software\\nWei Zhao: Data Curation\\nJohn Pauly: Supervision, Funding acquisition\\nLei Xing: Writing - Review & Editing, Supervision, Funding acquisition, Resources\\nPublisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our \\ncustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review \\nof the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered \\nwhich could affect the content, and all legal disclaimers that apply to the journal pertain.\\nDeclaration of interests\\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to \\ninfluence the work reported in this paper.\\nHHS Public Access\\nAuthor manuscript\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nPublished in final edited form as:\\nMed Image Anal. 2022 April ; 77: 102372. doi:10.1016/j.media.2022.102372.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n\", metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 0, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='1. Main Text\\nMedical imaging such as X-ray imaging, computed tomography (CT), magnetic resonance \\nimaging (MRI), and positron emission tomography (PET) presents a significant approach to \\nview the internal structure of a patient for diagnosis, image-guided interventions and many \\nother clinical decision-making procedures. Every year in the U.S., over hundred millions of \\nmedical imaging examinations are performed for patient care. In 2006, about 377 million \\ndiagnostic and interventional radiologic examinations were performed in the U.S. (Mettler \\net al., 2009). Among them, X-ray imaging and X-ray CT are widely used modalities in \\nvarious applications (Xing et al., 2020; Winder et al., 2021). In X-ray imaging, an incident \\nX-ray beam goes through the patient body, and produces a projection image of the internal \\nanatomic structure of the body on the image plane as illustrated in Fig. 1. For image \\nguidance of interventional procedures, projection images from different view angles are \\noften needed to localize or recognize a structure accurately in 3D. X-ray projection data \\nacquired at many different angles around the patient are also required in tomographic CT \\nimaging.\\nConsidering the practical needs for multi-view X-ray projections at different view \\nangles and the general overhead associated with the data acquisition, it is desirable to \\ndevelop alternative ways to obtain the multi-view projections with minimal cost such \\nas computational image synthesis. Such a technique can not only reduce the cost to \\nacquire multi-view X-ray projections, but also open new possibilities for some significant \\nrelevant challenges such as sparse-view tomographic imaging. For example, in sparse-view \\ntomographic CT image reconstruction, synthesized X-ray projections at novel view angles \\ncould potentially help to reconstruct better CT images while reducing the needs of X-ray \\nprojection acquisition, thus, reducing the radiation dose during the imaging protocol (Shen \\net al., 2021). However, there is no previous work specifically discussing this interesting \\nresearch topic. Toward this goal, in this work, we investigate an effective strategy of \\nsynthesizing novel-view X-ray projections by using geometry-integrated deep learning.\\nRecent advances in deep learning have led to impressive progress in many application fields \\n(Xing et al., 2020), including image reconstruction (Zhu et al., 2018; Mardani et al., 2018; \\nShen et al., 2019) and image recognition (Krizhevsky et al., 2012; He et al., 2016; Shen et \\nal., 2018). Moreover, in computer vision research, deep learning also brings new possibility \\nfor view synthesis task for natural images, i.e., to synthesize the novel-view images from \\nthe images at the given view angles (Eslami et al., 2018; Sitzmann et al., 2019; Mildenhall \\net al., 2020). Although some progress has been made in solving the view synthesis problem \\nthrough volume rendering with deep learning methodology (Lombardi et al., 2019; Wiles \\net al., 2020), these methods cannot be directly transferred to novel-view X-ray projection \\nsynthesis because of the difference in image-forming physics between photographic and \\ntomographic imaging. For photographic imaging, the natural lights hit the surface of objects \\nand reflect back to the camera image plane to form an image of the object shape and \\nthe background in the RGB format. In tomographic imaging, the penetrating waves such \\nX-ray passes through the object and project the internal structures onto image plane by \\nray integration. Therefore, a new formulation for the novel-view X-ray projection synthesis \\nproblem is required.\\nShen et al.\\nPage 2\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 1, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Understanding the underlying geometry changes in the physical world is important to \\nsolve this X-ray projection synthesis problem. In this case, each pixel value in the X-ray \\nprojection does not represent the RGB value at a certain point like that in the natural image, \\nbut indicates the integration along the ray line in the projection direction. Therefore, to \\nsynthesize an X-ray projection at a new view angle, it is necessary to consider the geometric \\nchanges in the physical world, which relates the projections at different angles. Specifically, \\nwe observe that the underlying subject is rotated when viewed from different angles, \\nthus, the geometry features should also be transformed according to the rotation angle \\nof view point when synthesizing a new-view projection. In reality, since the projections \\nfrom different views depict the same imaging subject, they should share some common \\ncharacteristics such as the subject texture. Therefore, in synthesizing a projection, we \\nassume that the projections at different view angles share the common texture features \\nwhile keeping the view-specific geometry features. In this way, we integrate the geometric \\nrelationship from physical world in the deep learning to construct a robust and interpretable \\nprojection synthesis model.\\nIn this work, we introduce a Deep Learning-based Geometry-Integrated Projection Synthesis \\n(DL-GIPS) framework for generating novel-view X-ray projections. Specifically, given the \\nX-ray projections at certain angle(s), the model learns to extract the geometric and texture \\nfeatures from input projection(s) simultaneously, which are then transformed and combined \\nto form the X-ray projection image at the target angle(s). The texture feature extracts the \\nappearance characteristics such as the subject texture from the input projection to help to \\nsynthesize the target projection, whereas the geometry feature captures the subject geometric \\nstructure, such as the spatial distribution, contour, shape, size of bones, organs, soft tissues, \\nand other body parts. To incorporate the geometry information into the view synthesis \\nprocedure, the extracted geometry features are transformed according to the source and \\ntarget view angle changes based on the 3D cone-beam projection geometry, which relates to \\nthe subject’s geometry changes. Such a combination of geometry priors and deep learning \\nalso makes the model more robust and reliable. More details will be introduced in the \\nsubsequent sections.\\nOverall, the main contributions of this paper are three folds:\\n•\\nWe for the first time investigate the novel-view projection synthesis problem for \\nX-ray imaging. The approach can also be generalized to a more general synthesis \\nfrom multi-views to multi-views projections.\\n•\\nWe propose a deep learning-based geometry-integrated projection synthesis \\nmodel (DL-GIPS) to generate novel-view X-ray projections through feature \\ndisentanglement and geometry transformation.\\n•\\nWe validate the feasibility of the proposed approach by experimenting on the \\none-to-one and multi-to-multi X-ray projection synthesis using lung imaging \\ncases across various patients.\\nThe remainders of this paper are organized as follows. We introduce the related works in \\nSection 2 and elaborate our framework in Section 3. The experimental setting and results are \\nShen et al.\\nPage 3\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 2, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='presented in Section 4 and Section 5, respectively. We further discuss the key issues of our \\nmethod in Section 6 and draw the conclusions in Section 7.\\n2 Related Work\\n2.1 View Synthesis\\nThe view synthesis problem in computer graphics has been researched for many years \\n(Eslami et al., 2018; Sitzmann et al., 2019; Mildenhall et al., 2020; Lombardi et al., 2019; \\nWiles et al., 2020). With the development of deep learning, the recent works are focused on \\nsolving this problem through volume rendering (Lombardi et al., 2019; Wiles et al., 2020), \\nneural scene representation learning (Eslami et al., 2018; Sitzmann et al., 2019), neural \\nradiance field (Mildenhall et al., 2020). These works employ deep neural networks to learn \\nthe representation of the underlying object or scene and try to find the intersection points \\nof the lights and the object outer surface to generate the RGB values of the corresponding \\npixels onto the image plane. However, because of the physical difference of X-ray imaging \\nscanning, it is difficult, if not possible, to apply these methods to X-ray projection forming. \\nIn this work, we propose an X-ray projection synthesis method with integration of X-ray \\nimaging physics.\\n2.2 Image Translation\\nIn recent years, there is a line of research for image-to-image translation (Zhu et al., 2017; \\nHuang et al., 2018; Choi et al., 2018; Lee et al., 2019; Shen et al., 2020; Lyu et al., 2021), \\nwhere a deep learning model is trained to translate or synthesize images across multiple \\ndomains or modalities, such as multi-contrast MRI (Lee et al., 2019; Shen et al., 2020), \\nMRI to CT (Zhang et al., 2018) or facial images with different expressions (Huang et al., \\n2018; Choi et al., 2018; Shen et al., 2020). Some of the image translation methods generate \\nnew-domain images by disentangling the image semantics as content features and style \\nfeatures (Huang et al., 2018; Shen et al., 2020). In these methods, it is assumed that the \\ncontent features are shared across domains, while style features are domain-specific. For the \\nnovel-view projection synthesis problem, it is possible to treat projection images at different \\nview angles as different image domains (Shen et al., 2021). In this way, the problem is \\ntransferred to an image-to-image translation problem. However, such assumption completely \\nignores the specific viewpoint change and the corresponding geometric relationship between \\nprojections from different views.\\n2.3 Geometry-Integrated Deep Learning\\nAlthough deep learning has achieved impressive results in many image reconstruction \\nand recognition tasks, in recent years, some concerns have been raised with regard to its \\nrobustness, generalization, and interpretability (Hutson, 2018; Heaven, 2019; Finlayson et \\nal., 2019; Tatarchenko et al., 2019). To build a practically useful deep learning model for \\nmedical imaging, it is desirable to incorporate the geometry and/or physical prior into the \\nlearning process. In practice, how to integrate seamlessly the geometry into a deep learning \\nmodel presents a daunting challenge. Previous works introduce feasible geometry-integrated \\ndeep learning models for 3D tomographic image reconstruction (Liu et al., 2017, Shen et al., \\n2021), object surface reconstruction (Yariv et al., 2020) and scene representations (Bear et \\nShen et al.\\nPage 4\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 3, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='al., 2020). In our projection synthesis here, the geometry priors are introduced naturally into \\nthe deep learning model by relating the geometries between different view angles.\\n3 Method\\n3.1 Approach Overview\\nAs shown in Fig. 2, the input to the framework is the source-view projection image, \\nand the output of the model is the synthesized target-view projection image. In order to \\ngenerate the projection from the input view angle to the target view angle, two aspects of \\ninformation are required. First, an important knowledge embedded in the input projection \\nis the anatomic geometry structure of the scanned subject including the distribution and \\nlocation of various organs and body parts. It is important to understand this patient-specific \\ngeometry information to generate the projection at a new view angle. Due to the view angle \\nchange from the source to the target views, the underlying subject should also be rotated \\nand transformed accordingly, as shown in Fig. 1. Particularly, the geometry transformation \\nshould comply with the physical model of the X-ray imaging. Correctly conducting the \\ngeometric transformation from the source view to the target view helps the model to capture \\nthe change of the projected subject structure.\\nBeyond the geometry information, the texture characteristics are also critical to generate \\nhigh quality target-view projection, and ensure the consistency of the texture and appearance \\ncharacteristics between the source-view and target-view projections. We assume that the \\nsource-view and target-view images are X-ray projections scanned under the same imaging \\nsetting, which should share some common image characteristics such as the subject texture \\nand image appearance. Therefore, the texture features captured from source-view projection \\nare combined with the transformed geometry features to synthesize target-view projection \\nsimultaneously.\\nBased on the above assumptions, we propose a DL-GIPS framework for novel-view \\ngeneration. As illustrated in Fig. 2, the proposed framework consists of three modules: \\nthe texture and geometry feature extraction encoders, the projection transformation based \\non geometry priors and view angles, and the image generator to synthesize projections. \\nSpecifically, the two image encoders extract the texture features and the geometry features \\nfrom the input image, respectively. Then, the geometry features are transformed through \\nthe physical model of backward projection and forward projection. Finally, the transformed \\ngeometry features at the target view are combined with the shared texture features to \\nsynthesize the target-view projection via an image generator. The details of each module are \\ndescribed as follows.\\n3.2 DL-GIPS Net\\n3.2.1 Feature encoder—The feature encoders are built up by stacked downsampling \\nconvolutional layers and residual convolutional blocks to learn the semantic representations \\nfrom the input projection. As aforementioned, two separate encoders are constructed to \\nlearn the geometry and texture features from the input projection respectively, denoted as \\nShen et al.\\nPage 5\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 4, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='geometry feature encoder εg, and texture feature εt. Given the source-view image as Iscr, the \\nfeature encoding process is denoted as follows:\\nfsrc\\ng\\n= εg Isrc\\n(1)\\nft = εt Isrc\\n(2)\\nTo be specific, the encoder firstly contains a convolutional layer with kernel size 7 and \\nchannel dimension of 16. Then, the feature maps are downsampled by two 2D convolutional \\nlayer with kernel size 4 and stride 2, where the channel dimension is doubled in each layer. \\nEach convolutional layer is followed by instance normalization (Ulyanov et al., 2016) and \\nReLU activation function. Next, four residual blocks are built up to further process the \\nlearned features. Each residual block contains two convolutional layers with kernel size 3 \\nand the same channel dimension. A skip connection is added up from the output of the \\nfirst convolutional layer to the second convolutional layer. Thus, the feature maps keep the \\nsame spatial size and channel dimension when getting through all the residual blocks. The \\ngeometry feature encoder and texture feature encoder have a similar architecture, except that \\nthere is an additional convolutional layer at the end of geometry feature encoder to reduce \\nthe geometry feature channel dimension to 32 for the subsequent geometry transformation \\nmodule.\\n3.2.2 Geometry transformation—After the geometry features capture the geometric \\ndistribution of different parts from input projection, it is necessary for the model to \\nunderstand the change between the source and target view angles. For projections at \\ndifferent view angles, the bones and organs projected on the image plane are differently \\ndistributed accordingly, which is related to the projection geometry from the physical model \\nof X-ray imaging system. Thus, these geometry information and view angles should be \\nincorporated together for the new view synthesis. To achieve that, as shown in Fig. 3, \\nwe firstly backward project the source-view geometry features extracted from the input \\nprojection backward to the 3D volume, which represents the semantic features of the \\nunderlying subject in 3D space. The backward projector is performed according to the 3D \\ncone-beam geometry of the X-ray imaging scanner and the source view angle. As shown in \\nthe blue lines in Fig. 3, the pixel values in the image are put back to the intersected voxels \\nalong the corresponding X-ray line directions. Then, the 3D feature volume is forward \\nprojected to the target image plane according to the cone-beam geometry and target view \\nangle. As demonstrated in the purple lines in Fig. 3, the projected pixel values are computed \\nby integrating the crossed voxels along the ray direction. In this way, the geometry features \\nextracted from source view projection is transformed to the corresponding features in the \\ntarget view.\\nHowever, when the input projection(s) is very sparse (e.g., a single source view), the \\nback-projected 3D feature volume is also very sparse with a lot of zero-padding voxels, \\nwhich is not informative. Thus, we build up a 3D feature refinement model to inpaint the 3D \\nfeature volume after the back-projection operation. It is designed to learn a more complete \\nShen et al.\\nPage 6\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 5, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='3D representation to improve the target view synthesis. With the 3D feature refinement \\nmodel denoted as ℳ, the whole projection transformation process is denoted as follows:\\nFsrc\\ng , Ftgt\\ng = Pf ∘ ℳ ∘ Pb fsrc\\ng\\n(3)\\nwhere we denote the transformed features after forward and backward projection as Fsrc\\ng , \\nFtgt\\ng . Pf and Pg represent the forward projection and backward projection operators. Note \\nthat during the forward projection, the 3D feature volume not only projects onto the target \\nview, but also projects back to the source view to keep consistency. Therefore, the projection \\ntransformation module outputs the geometry features for target view Ftgt\\ng  as well as source \\nview Fsrc\\ng .\\nNote that the backward projection and forward projection are deterministic operations \\nwithout any learnable variables. All the transformation parameters are set up based on the \\ngeometry priors including the physical model from X-ray imaging scanner, and the source \\nand target view angles. The 3D feature refinement network is built up by two 3D residual \\nconvolutional blocks. Each residual block contains two 3D convolutional layers with kernel \\nsize 3, followed by instance normalization and ReLU activation function. An additive path \\nconnects the outputs of the first and the second layer to enforce residual learning. Since \\nthe backward and forward projectors are differentiable, the geometry transformations are \\nwrapped up together with network modules for end-to-end optimization.\\n3.2.3 Image generator—Combining the transformed geometry features and extracted \\ntexture features, image generator learns to synthesize the projection in the target view \\nfrom the semantic representations of both sides. The model structure of image generator \\nis the mirror of feature encoder, which is constructed by residual convolutional blocks \\nand upsampling convolutional layers. The geometry features and texture features are \\nconcatenated together in the channel dimension. Denoting the generator as G, the model \\noutputs the target-view projection as well as the source-view projection based on the \\ndifferent geometry features:\\nI′src = G Fsrc\\ng , ft\\n(4)\\nI′tgt = G Ftgt\\ng , ft\\n(5)\\nSpecifically, the generator firstly contains four residual convolutional blocks, where \\neach block consists of two convolutional layers of kernel size 3 followed by instance \\nnormalization and ReLU activation. Similarly, the skip connection builds up in each residual \\nblock. Then, the upsampling block contains the an upsampling layer with scale factor 2 and \\na 2D convolutional layer with kernel size of 5. The upsampling layer interpolates the feature \\nmaps to increase the spatial size, while the convolutional layer reduces the feature channel \\ndimension to the half. Finally, another 2D convolutional layer with kernel size 7 and tangent \\nShen et al.\\nPage 7\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 6, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='hyperbolic activation function transforms the feature map to the expected output channel \\ndimension for the synthesized projection.\\n3.2.4 Image discriminator—In order to generate realistic projections in the target view, \\nwe build up the image discriminator deployed to the generated images for adversarial \\ntraining. To be specific, the discriminator is used to distinguish between the real \\nprojections and the synthesized projections. Together training the image generator and image \\ndiscriminator in an adversarial manner, the generated projections are expected to be in the \\nsame distribution as the real projections, which cannot be distinguished by the discriminator. \\nWe denote the discriminator as D.\\nIn our method, we build up a multi-scale image discriminator (Huang et al., 2018; Shen \\net al., 2020). Specifically, we use the 2D average pooling layer to downsample the image \\nto three different scales by 2 times. Separate classifier networks are constructed for input \\nimages at different scales. The classifier contains four 2D convolutional layers with kernel \\nsize 4 and stride 2, which further downsample the spatial size of feature maps. The final \\nlayer is the 2D convolution with kernel size 1. During training, the outputs from image \\ndiscriminator are used to compute the adversarial loss based on the method of least squares \\ngenerative adversarial networks (Mao et al., 2017).\\n3.3 Training Loss\\nIn order to train the whole model reliably, we design a training strategy that contains the \\nimage and feature consistency loss, image reconstruction loss, and adversarial loss.\\n3.3. 1 Consistency loss—In the whole framework, the encoder extracts geometry and \\ntexture features from image while the generator combines the features to generate images. \\nThese two processes are actually inverse operations, which should also keep consistency \\nfor the input source-view projection. That is, if we recombine the extracted features from \\nthe source-view projection, the generator can recover the same projection. The formula is \\ndenoted as:\\nℒcyc\\nimg = E\\nG εg Isrc , εt Isrc\\n− Isrc 1\\n(6)\\nwhere the expectation is sampled based on all the training samples. This constraint guarantee \\nthat the feature encoding and image generation keep consistency and can recover the input \\nimage.\\nFurthermore, in order to guarantee that the geometry features represent the correct semantic \\ninformation for corresponding views, we add additional constraint on the consistency of \\ngeometry features. Suppose the generator output the synthesized projections I′src, I′tgt \\nat source view and target view respectively. The geometry features extracted from the \\ngenerated projections should have the same representation as the previous transformed \\ngeometry features, from which the synthesized projections are derived, as shown in Fig. 2. \\nThis also further guarantee the geometry feature encoding and image generation conduct the \\ninverse operations. Thus, the geometry feature consistency loss is denoted as follows:\\nShen et al.\\nPage 8\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 7, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='ℒcyc\\nf\\n= E\\nεg I′tgt − Ftgt\\ng\\n1 + εg I′src − Fsrc\\ng\\n1\\n(7)\\nFor convenience of notation, we denote the aforementioned image and feature consistency \\nconstraints as a total consistency loss. That is,\\nℒcyc = ℒcyc\\nimg + ℒcyc\\nf\\n(8)\\n3.3.2 Reconstruction loss—After applying the geometry transformation to the \\nencoded features, the transformed geometry features are supposed to generate the projection \\nin the novel view. We add the L1-norm loss between the synthesized projection and the \\nground truth projection to strengthen the anatomical-structure related generation. Moreover, \\nthe transformed geometry features in the source view should also be able to recover the \\nsource view projection through the image generator. Thus, the reconstruction loss includes \\nthe constraints on both source-view and target-view projections output from the image \\ngenerator. Thus, the total image reconstruction loss is as follows:\\nℒrec = E\\nG Ftgt\\ng , ft − Itgt 1 + G Fsrc\\ng , ft − Isrc 1\\n(9)\\nThis constraint makes sure the backward and forward projection operators conduct the \\ncorrect geometry transformation as expected. Compared with Eq. (6), the difference between \\nthe consistency loss and reconstruction loss on source-view projection is from the different \\ngeometry feature. In Eq. (6), the source-view geometry feature is directly extracted from the \\ninput source-view projection through feature encoder. In Eq. (9), the source-view geometry \\nfeature is outputted after the geometry transformation including the 3D feature refinement \\nnetwork. In other words, we assume the geometry feature of source view should keep \\nconsistent in these two positions. The geometry transformation module helps to derive the \\ntarget-view geometry feature while it should also keep the correctness of the source-view \\ngeometry feature.\\n3.3.3 Adversarial Loss—The simultaneous adversarial training of the image generator \\nand discriminator enforces the distribution of the generated projections to be close to that of \\nthe real projections. In this way, the generated projections are more realistic. The adversarial \\nloss is as follows:\\nℒadv = E D G Ftgt\\ng , ft\\n− a\\n2 + E D Itgt − b 2\\n(10)\\nHere, we adopt the objective loss function introduced by least squares generative adversarial \\nnetworks (Mao et al., 2017), where a and b are the labels for synthesized and real images. \\nThe real projections and generated projections are the inputs to train the discriminator model \\nto distinguish between the real or fake class. For the projections at different view angles, \\nwe use different discriminators for adversarial training. Thus, the loss in Eq. (10) can be \\nShen et al.\\nPage 9\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 8, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='also applied to the source-view projections with the corresponding real and synthesized \\nprojections.\\nThe feature encoders, image generators, and image discriminators are jointly trained to \\noptimize the total loss as follows:\\nmin\\nεg, εt, G, ℳ\\nmax\\nD\\nℒ εg, εt, G, ℳ, D\\n= λcycℒcyc + λrecℒrec + λadvℒadv\\n(11)\\nwhere λcyc, λrec, λadv are the hyper-parameters to balance the different parts of the total \\nloss. The whole framework is trained end-to-end to optimize the total loss objective.\\n4. Experiments\\nTo validate the feasibility of the proposed DL-GIPS model for view synthesis of X-ray \\nprojection images, we conduct experiments on lung X-ray CT images for novel-view \\nprojection synthesis. In the following sections, we will describe the dataset, experimental \\nsetting and training details.\\n4.1 Dataset\\nThe experiments were conducted on a public dataset of The Lung Image Database \\nConsortium and Image Database Resource Initiative (LIDC-IDRI) (Armato et al., 2011; \\nArmato et al., 2015; Clark et al., 2013). This dataset contains 1018 thoracic 3D CT images \\nfrom different patients. We regarded each CT image as an independent data sample for \\nmodel training. In data pre-processing, all the CT images were resampled with the same \\nresolution of 1 mm in the z-axis, and were resized to the same image size of 128 × 128 in the \\nxy-plane.\\nIn order to obtain the X-ray projections at different angles, we projected the 3D CT image to \\nget the digitally reconstructed radiographs (DRRs) in different view angles. The cone-beam \\ngeometry of the projection operation was defined according to the clinical on-board cone-\\nbeam CT system for radiation therapy. Each 2D X-ray projection was of the size 180 × \\n300. Following the image processing of model training, the intensity values of the all the \\n2D X-ray projection images were normalized to the data range of [0, 1]. In experiments, we \\nrandomly selected 80% of the dataset for training and validation (815 samples) while 20% of \\nthe data were held out for testing (203 samples).\\n4.2 Experimental Setting\\nTo validate the proposed model under different cases of view synthesis, we designed two \\nexperimental setting: one-to-one view synthesis and multi-to-multi view synthesis. In the \\nexperiments of one-to-one view synthesis, we focus on two typical projections: the anterior-\\nposterior (AP) projection (0 degree) and the lateral (LT) projection (90 degree), which are \\ncommonly used in clinical practice. We experimented on two situations: to generate AP \\nprojection from LT projection, or to generate LT projection from AP projection. Given \\nonly a single projection, it is very hard to generate another novel view because the given \\nShen et al.\\nPage 10\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 9, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='information from the source view is very limited. We want to explore if the data-driven \\nlearning and the geometry priors can provide more additional information to make it \\npossible.\\nIn the experiments of multi-to-multi view synthesis, the model was trained to generate the \\nprojections at 30 and 60 degrees from the AP (0 degree) and LT (90 degree) projections. \\nThis is a more general case with multiple input sources views and more than one target \\nviews. Such multi-to-multi projection generation may further contribute to the application of \\nsparse-view 3D CT image reconstruction for the real-time applications.\\n4.3 Training Details\\nThe deep neural networks in the framework were implemented with PyTorch (Paszke et al., \\n2017). The backward projection and forward projection operators were implemented based \\non the Operator Discretization Library (ODL) in Python (Adler et al., 2017), which were \\nwrapped up as the differentiable PyTorch module. Thus, the whole framework was built up \\nby PyTorch layers and was trained in an end-to-end fashion. Specifically, the whole model \\nwas trained by optimizing the total loss in Eq. (11), with the loss weights λcyc, λadv as 1 \\nand λrec as 10. We trained the model using the Adam optimizer (Kingma and Ba, 2015) with \\nthe beta parameters as (0.5, 0.999), the learning rate of 0.0001, and batch size 1. The model \\nwas trained for 100000 iterations in total. We will release our code publicly upon the paper \\nacceptance.\\n5. Results\\nIn the following section, we demonstrate the qualitative and quantitative results respectively \\nfor different experimental settings: one-to-one projection synthesis and multi-to-multi \\nprojection synthesis. Also, we compare the results of the proposed method with the previous \\nmethods for image translation. Finally, we conduct ablative studies to further investigate the \\nimportance of each component in the proposed DL-GIPS model.\\n5.1 Compared Models\\nUNet (Ronneberger et al., 2015): Since there is no previous method particularly designed for \\nX-ray projection synthesis problem, we firstly compare the proposed DL-GIPS model with \\nthe baseline UNet model (Ronneberger et al., 2015), which has been widely used for image \\nsegmentation and restoration in both natural image and medical image applications. In the \\nUNet structure, the skip connections are built up between the multi-scale features of the \\nencoder and the decoder, but there is not feature disentanglement. To apply the UNet model \\nfor X-ray projection synthesis, the given source projections are stacked as multi-channel \\nimages for model inputs while the model outputs the stacked multi-view images altogether. \\nThe same image reconstruction loss (i.e., L1-norm loss) is applied to measure the difference \\nbetween the outputs and ground truth images. The same training strategy is used to train the \\nUNet model.\\nReMIC (Shen et al., 2020): We also compare the DL-GIPS model with more advanced \\napproach with feature disentanglement. ReMIC model (Shen et al., 2020) is a recent work \\nproposed for multi-modality image translation such as multi-contrast MRI images. The \\nShen et al.\\nPage 11\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 10, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='ReMIC model generates the images across domains by feature disentanglement of content \\nand style codes. Specifically, it assumes that multi-modality images share the same content \\nfeatures while also keep the domain-specific style features. Thus, the across-domain image \\nis generated by sampling the style feature from the prior distribution. To apply the ReMIC \\nmodel for the X-ray projection synthesis problem, we assume the style features are sampled \\nfrom different prior distributions for different view angles. Then during the image generation \\nprocess, the sampled style features are used to change the parameters of adaptive instance \\nnormalization layers in the image generator by using the same approach as described in \\n(Shen et al., 2020). For comparison purpose, we use the same model structures for feature \\nencoder and image generator in both ReMIC and DL-GIPS methods. The same training \\nstrategy is used for ReMIC model as well. Note that in the optimization of ReMIC model, \\nthe training losses also contain reconstruction loss, adversarial loss and feature consistency \\nloss (Shen et al., 2020). Thus, the comparison between the ReMIC and DL-GIPS models is \\nmainly focused on different feature learning approaches.\\n5.2 One-to-One Projection Synthesis\\nThe qualitative results of synthesized projections are demonstrated in Fig. 4 and Fig. 5. \\nFig. 4 shows the results of synthesized LT projections from AP projections for five testing \\nsamples, while Fig. 5 shows the corresponding results of synthesized AP projections from \\nLT projections. Each row shows the results of one testing sample. The columns present \\nthe input projection, the output projection from UNet model, the output projection from \\nReMIC model, the output image from DL-GIPS model, and the ground truth image. The \\ncorresponding quantitative results averaged across all the testing data are reported in Table I \\nfor both “AP → LT” and “LT → AP”. The evaluation metrics include mean absolute error \\n(MAE), normalized root mean squared error (RMSE), structural similarity (SSIM) (Wang et \\nal., 2004) and peak signal noise ratio (PSNR).\\nFrom both qualitative and quantitative results, we can see that the proposed DL-GIPS model \\nobtains more accurate synthesized projections especially about the illustration of the human \\nbody and organs in terms of the shape, contour and size. For example, the synthesized \\nprojections obtained by DL-GIPS model get more accurate human body contours, as pointed \\nout by the red arrows in Fig. 4. Besides, as shown in the Fig. 5, the projection images \\nsynthesized by the DL-GIPS model obtain a better shape estimation of the heart and \\nliver denoted by the red arrows. These advantages result from the proposed geometry \\ntransformation and feature disentanglement in the DL-GIPS model. In the ReMIC model, \\nalthough the learned features extracted from images are also disentangled as the shared \\ncontent feature and the view-specific style feature (Shen et al., 2020), there is no explicit \\nor implicit geometry priors to guide the feature transformation across different view angles \\nin the feature disentanglement. Therefore, in term of human body structure and internal \\nanatomy, the proposed DL-GIPS model is able to generate more accurate results than the \\nReMIC model. Moreover, the synthesized images of DL-GIPS model contain more accurate \\ndetails in the bone and lung region compare with the UNet model, which results from the \\nadversarial loss in the model training. But we also notice that the adversarial loss may also \\nintroduce inaccuracy in some cases such as the unclear liver region in the testing sample 4.\\nShen et al.\\nPage 12\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 11, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='5.3 Multi-to-Multi Projection Synthesis\\nIn the experiments of multi-to-multi projection synthesis, the model is further developed to \\nsimultaneously generate multiple projections from the given multiple source views. To be \\nspecific, the model aims to synthesize the projections at the view angle of 30-degree and \\n60-degree when given the AP and LT projections at 0-degree and 90-degree. In Fig. 6, we \\nshow the results of five testing samples in rows respectively. The columns display the input \\nprojections, UNet results, ReMIC results, DL-GIPS results and the ground truth projections \\nat two different output angles, respectively. We also compute the quantitative evaluation \\nmetrics averaged across all the testing samples and report in Table I as “Multi→Multi”.\\nFirst of all, these results show that the proposed DL-GIPS method is a general projection \\nsynthesis approach that can be generalized to accept multiple source-view projections and \\npredict multiple target-view projections. Furthermore, we see that more input projections \\ncan provide more information for the underlying subject and synthesize more accurate \\nnovel-view projections compared with the ground truth, especially for the structure of \\nbones, organs, and soft tissues. Thus, this indicates the source-view projections can always \\nbe increased adaptively to obtain more precise synthesized projections according to the \\ndifferent requirements in specific practical applications.\\nIn comparison methods, we also observe the increased source-view projections and the \\nincreased target-view supervisions also help the UNet and ReMIC models to generate \\nbetter synthesized images than the results in one-to-one projection synthesis. Due to this, \\nUNet model can provide more reasonable structures of human body and organs in the \\nsynthesized projections despite without precise details, which even gets better quantitative \\nresults than ReMIC model in Table I. This is also because the ReMIC model synthesizes \\nsome inaccurate details due to the adversarial training loss. Similar phenomenon was also \\nfound in previous works (Ying et al., 2019) that the adversarial loss brings the trade-off \\nbetween the qualitative image qualities and the quantitative evaluation scores using the \\nmetrics like MAE, SSIM, PSNR.\\nIn the proposed DL-GIPS model, the geometry priors and integrated transformation relief \\nsuch disadvantages, which not only gets correct anatomic structures with high SSIM score, \\nbut also synthesizes precise fine details with high PSNR score, as reported in Table I. \\nBesides, as shown in Fig. 6 the synthesized projections from DL-GIPS model obtain more \\naccurate anatomy structures pointed out by the red arrows.\\n5.4 Feature Visualization\\nFinally, we visualize the feature maps to better understand the projection synthesis \\nprocedure. Fig. 7 shows the geometry features extracted from AP and LT views and also \\ndemonstrates the texture features as introduced in the pipeline of Fig. 2. It is observed that \\nthe geometry features highlight the shape, contour and boundary of the chest including \\norgans, bones and thoracic wall. Furthermore, the geometry features from AP and LT view \\nangles are correlated through the geometric transformation. Unlike the geometry features, \\nthe texture features focus more on the appearance textures and the fine details of the \\nchest with different contrasts. In this way, by combining the texture features and geometry \\nShen et al.\\nPage 13\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 12, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='features, the DL-GIPS model can synthesize the corresponding projections at novel-view \\nangles.\\n5.5 Ablative Study\\nIn order to further analyze the proposed DL-GIPS model, we conduct the ablative study \\nexperiments for studying the necessity of different losses introduced in the total loss \\nobjective. Firstly, we train the DL-GIPS model under the same experimental settings while \\nremoving the consistency loss. In Table II, we report the results of ablative study with \\naveraged quantitative metrics across all the testing samples, which includes MAE, RMSE, \\nSSIM and PSNR. According to the results, the consistency loss can improve the synthesized \\nimages especially in one-to-one projection synthesis, as the feature consistency loss makes \\nthe semantic information transferred more smoothly. In the experiments of multi-to-multi \\nprojection synthesis, more given input information makes the task easier, and thus, the \\nconsistency loss does not make an obvious improvement in this case.\\nThen, we conduct ablative study by removing the adversarial loss. As the results shown \\nin Fig. 8, the synthesized projections without adversarial loss are obviously blurry. Thus, \\nadding the adversarial loss helps the model to obtain more fine details and achieve better \\nvisualized image quality, which is important to the radiologists in the practical applications.\\nIn addition, since both ReMIC and DL-GIPS models use the same backbone model \\nstructures for feature encoder and image generator, with the same training losses, the \\ncomparison results between ReMIC and DL-GIPS models as shown in Figs. 4–6 and \\nTable I demonstrate the superiority of geometric feature transformation in view synthesis. \\nThis indicates that the geometry priors introduced in DL-GIPS model structure help to \\nreconstruct more reliable object structures.\\nBeyond the above, we also conduct the ablation study of the model structure on the \\nlearned feature dimensions. In the proposed DL-GIPS model structure, a key module is \\nthe geometry feature and texture feature encoder and decoder. Thus, we investigate how the \\nlearned feature dimensions would influence the final performance of view synthesis. The \\nablation experiments are conducted in the AP → LT task. The results for variant feature \\ndimensions in the model structure are shown in Table III. For fixed geometry (texture) \\nfeature dimension, increasing texture (geometry) features lead to better performance of view \\nsynthesis, as this transfers more useful information through feature encoding and decoding \\nfor generating novel-view projection images. This also indicates that both geometry \\nfeatures and texture features are necessary representation learning to synthesize novel-view \\nprojections.\\n6. Discussion\\nIn this work, we tackle the problem of novel-view synthesis for X-ray projections and \\npropose a deep learning-based DL-GIPS network. It is shown that the proposed model \\nis able to generate the X-ray projection at the target-view angle with the given source-\\nview projection. The synthesized X-ray projections can be utilized for numerous practical \\napplications, such as gaining comprehensive perspectives of the patient anatomy (Ge et al., \\nShen et al.\\nPage 14\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 13, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='2019), tumor target localization and patient setup in image guided radiation therapy (Zhao \\net al., 2021), to ultra-sparse CT image reconstruction (Shen et al., 2021), and so forth. \\nSpecifically, for clinical usage, when the 3D tomographic image is not available for various \\nreasons (such as limited accessible angles for imaging and restriction in imaging dose, e.g. \\npediatric and/or pregnant patients), synthetic projections at different view angles could help \\nto better visualize the patient’s anatomy for tumor target localization and patient setup in \\nimage guided radiation therapy. In another example, in MRI-guided radiation therapy, for \\nreal-time guidance, current MRI-LINAC acquires only 2D images instead of 3D images. The \\ncurrent framework could provide an effective way to provide complementary multi-view 2D \\nimages in this case.\\nThe proposed DL-GIPS model adopts the geometry priors of X-ray imaging model to \\ntransform the geometry features and relate the source and target view angles. Such a \\ngeometric transformation is derived from the X-ray imaging system and properly integrated \\nwith the deep learning networks in the proposed approach. Beyond the proposed approach, \\nthere may be more advanced methods to leverage the geometry priors especially in medical \\nimaging. For example, the other image modalities of the same patients such as Magnetic \\nResonance Image (MRI) can also provide the prior information of anatomic structure for the \\nsame patient, which may further contribute to synthesizing the fine details in the novel-view \\nX-ray projections.\\nComputational efficiency of the current method is still not ideal. The incorporation of \\ngeometry transformation increases the time consumption as compared with the standard \\ndeep learning model training process. In the experiment setting of one-to-one projection \\nsynthesis, the inference time of one data sample for different methods are around: 0.04 s, \\n0.05 s, 0.56 s for UNet, ReMIC, and DL-GIPS models respectively. How to speed up this \\nprocess by, for examples, CUDA acceleration, parallel computing and more efficient model \\ndesign, represents an interesting direction of future research.\\nIn real CT data, there may be some specific issues that need further attention. For instance, \\ndata truncation is a common issue for most deep learning-based approaches for CT imaging \\nand has raised a lot of attention in recent research. For example, in the recent work (Huang \\net al., 2021), the authors proposed a method for truncation correction in CT by extrapolating \\nthe projections. This problem is orthogonal to the view synthesis problem studied in our \\nwork and can naturally become another useful future research direction following our work.\\n7. Conclusion\\nThis work investigates a strategy of the synthesis of novel-view X-ray projections and \\npresents a robust model combining the deep learning and geometry transformation. It is \\nshown that the generated X-ray projections reveal the internal anatomy distribution from \\nnew viewpoints, which may be utilized for numerous practical applications, such as gaining \\ncomplementary perspectives of the patient anatomy, tumor target localization and patient \\nsetup in image guided radiation therapy, while reducing the overhead associated with actual \\nprojection measurements, such as reducing the extra radiation dose and speeding up the \\nimaging process. Finally, the proposed projection synthesis has the potential to significantly \\nShen et al.\\nPage 15\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 14, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='simply the clinical workflow and provide a new paradigm for various stereoscopic and \\nvolumetric imaging procedures with substantially reduced efforts in data acquisition.\\nAcknowledgements\\nThe authors acknowledge the funding supports from Stanford Bio-X Bowes Graduate Student Fellowship and \\nNIH/NCI (R01CA227713, R01CA256890, and R01CA223667). The authors acknowledge the National Cancer \\nInstitute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free \\npublicly available LIDC/IDRI Database used in this study.\\nReferences\\n[1]. Adler J, Kohr H, Oktem O, 2017. Operator Discretization Library (ODL).\\n[2]. Armato S III, McLennan G, Bidaut L, McNitt-Gray M, Meyer C, Reeves A, Zhao B, Aberle D, \\nHenschke C, Hoffman E, Kazerooni E, MacMahon H, van Beek E, Yankelevitz D, Biancardi A, \\nBland P, Brown M, Engelmann R, Laderach G, Max D, Pais R, Qing D, Roberts R, Smith A, \\nStarkey A, Batra P, Caligiuri P, Farooqi A, Gladish G, Jude C, Munden R, Petkovska I, Quint L, \\nSchwartz L, Sundaram B, Dodd L, Fenimore C, Gur D, Petrick N, Freymann J, Kirby J, Hughes \\nB, Casteele A, Gupte S, Sallam M, Heath M, Kuhn M, Dharaiya E, Burns R, Fryd D, Salganicoff \\nM, Anand V, Shreter U, Vastagh S, Croft BY, Clarke L, 2011. The Lung Image Database \\nConsortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference \\ndatabase of lung nodules on CT scans. Medical Physics, 38: 915–931. [PubMed: 21452728] \\n[3]. Armato S III, McLennan G, Bidaut L, McNitt-Gray M, Meyer C, Reeves A, Zhao B, Aberle D, \\nHenschke C, Hoffman E, Kazerooni E, MacMahon H, van Beek E, Yankelevitz D, Biancardi A, \\nBland P, Brown M, Engelmann R, Laderach G, Max D, Pais R, Qing D, Roberts R, Smith A, \\nStarkey A, Batra P, Caligiuri P, Farooqi A, Gladish G, Jude C, Munden R, Petkovska I, Quint L, \\nSchwartz L, Sundaram B, Dodd L, Fenimore C, Gur D, Petrick N, Freymann J, Kirby J, Hughes \\nB, Casteele A, Gupte S, Sallam M, Heath M, Kuhn M, Dharaiya E, Burns R, Fryd D, Salganicoff \\nM, Anand V, Shreter U, Vastagh S, Croft BY, Clarke L, 2015. Data from LIDC-IDRI. The Cancer \\nImaging Archive\\n[4]. Bear DM, Fan C, Mrowca D, Li Y, Alter S, Nayebi A, Schwartz J, Fei-Fei L, Wu J, Tenenbaum JB \\nand Yamins DL, 2020. Learning physical graph representations from visual scenes. Advances in \\nNeural Information Processing Systems, 33, 2020.\\n[5]. Choi Y, Choi M, Kim M, Ha J-W, Kim S, Choo J, 2018. Stargan: Unified generative adversarial \\nnetworks for multi-domain image-to-image translation In Proceedings of the IEEE Conference \\non Computer Vision and Pattern Recognition, pp. 8789–8797.\\n[6]. Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle \\nM, Tarbox L, Prior F, 2013. The Cancer Imaging Archive (TCIA): Maintaining and operating \\na public information repository, Journal of Digital Imaging, 26, 6, pp 1045–1057. [PubMed: \\n23884657] \\n[7]. Eslami SA, Rezende DJ, Besse F, Viola F, Morcos AS, Garnelo M, Ruderman A, Rusu AA, \\nDanihelka I, Gregor K, Reichert DP, 2018. Neural scene representation and rendering. Science, \\n360(6394), 1204–1210. [PubMed: 29903970] \\n[8]. Finlayson SG, Bowers JD, Ito J, Zittrain JL, Beam AL, Kohane IS, 2019. Adversarial attacks on \\nmedical machine learning. Science, 363(6433):1287–1289. [PubMed: 30898923] \\n[9]. Ge R, Yang G, Chen Y, Luo L, Feng C, Zhang H, Li S, 2019. PV-LVNet: Direct left ventricle \\nmultitype indices estimation from 2D echocardiograms of paired apical views with deep neural \\nnetworks. Medical image analysis, 58, p.101554. [PubMed: 31546227] \\n[10]. He K, Zhang X, Ren S, Sun J, 2016. Deep residual learning for image recognition In Proceedings \\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–778.\\n[11]. Heaven D, 2019. Why deep-learning AIs are so easy to fool. Nature 574, 163–166. [PubMed: \\n31597977] \\n[12]. Huang X, Liu M-Y, Belongie S, Kautz J, 2018. Multimodal unsupervised image-to-image \\ntranslation In Proceedings of the European Conference on Computer Vision, pp. 172–189.\\nShen et al.\\nPage 16\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 15, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='[13]. Huang Y, Preuhs A, Manhart M, Lauritsch G and Maier A, 2021. Data Extrapolation from \\nLearned Prior Images for Truncation Correction in Computed Tomography. IEEE Transactions \\non Medical Imaging\\n[14]. Hutson M, 2018. Has artificial intelligence become alchemy. Science 360, 4782013478.\\n[15]. Kingma DP, Ba J, 2015. Adam: A method for stochastic optimization In Proceedings of the \\nInternational Conference on Learning Representations (ICLR).\\n[16]. Krizhevsky A, Sutskever I, Hinton GE, 2012. Imagenet classification with deep convolutional \\nneural networks. Advances in Neural Information Processing Systems, 1097–1105.\\n[17]. Lee D, Kim J, Moon W-J, Ye JC, 2019. Collagan: Collaborative gan for missing image data \\nimputation In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, \\npp. 2487–2496.\\n[18]. Liu J, Ma J, Zhang Y, Chen Y, Yang J, Shu H, Luo L, Coatrieux G, Yang W, Feng Q, Chen \\nW, 2017. Discriminative feature representation to improve projection data inconsistency for low \\ndose CT imaging. IEEE transactions on medical imaging, 36(12), pp.2499–2509. [PubMed: \\n28816658] \\n[19]. Lombardi S, Simon T, Saragih J, Schwartz G, Lehrmann A, Sheikh Y, 2019. Neural volumes: \\nlearning dynamic renderable volumes from images. ACM Trans. Graph. 38(4), 65.\\n[20]. Lyu T, Zhao W, Zhu Y, Wu Z, Zhang Y, Chen Y, Luo L, Li S, Xing L, 2021. Estimating \\ndual-energy CT imaging from single-energy CT data with material decomposition convolutional \\nneural network. Medical image analysis, 70, p.102001. [PubMed: 33640721] \\n[21]. Mao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley S, 2017. Least squares generative \\nadversarial networks In Proceedings of the IEEE international conference on computer vision \\n(pp. 2794–2802).\\n[22]. Mardani M, Gong E, Cheng JY, Vasanawala SS, Zaharchuk G, Xing L, Pauly JM, 2018. Deep \\ngenerative adversarial neural networks for compressive sensing MRI. IEEE Trans. on Medical \\nImaging, 38(1), 167–179. [PubMed: 30040634] \\n[23]. Mettler FA Jr, Bhargavan M, Faulkner K, Gilley DB, Gray JE, Ibbott GS, Lipoti JA, Mahesh M, \\nMcCrohan JL, Stabin MG, Thomadsen BR, 2009. Radiologic and nuclear medicine studies in \\nthe United States and worldwide: frequency, radiation dose, and comparison with other radiation \\nsources—1950–2007. Radiology, 253(2), pp.520–531. [PubMed: 19789227] \\n[24]. Mildenhall B, Srinivasan PP, Tancik M, Barron JT, Ramamoorthi R, Ng R, 2020. Nerf: \\nRepresenting scenes as neural radiance fields for view synthesis In Proceedings of the European \\nConference on Computer Vision (ECCV).\\n[25]. Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, \\nLerer A, 2017. Automatic differentiation in pytorch In Proc. 30th Conference on Advances in \\nNeural Information Processing Systems Autodiff Workshop.\\n[26]. Ronneberger O, Fischer P, Brox T, 2015. U-net: Convolutional networks for biomedical image \\nsegmentation In International Conference on Medical Image Computing and Computer-Assisted \\nIntervention (MICCAI), 234–241.\\n[27]. Shen L, Shpanskaya K, Lee E, McKenna E, Maleki M, Lu Q, Halabi S, Pauly J, Yeom K, 2018. \\nDeep learning with attention to predict gestational age of the fetal brain In Machine Learning for \\nHealth Workshop of Advances in Neural Information Processing Systems.\\n[28]. Shen L, Zhao W, Xing L, 2019. Patient-specific reconstruction of volumetric computed \\ntomography images from a single projection view via deep learning. Nature Biomedical \\nEngineering 3(11), 880–888.\\n[29]. Shen L, Zhu W, Wang X, Xing L, Pauly JM, Turkbey B, Harmon SA, Sanford TH, Mehralivand \\nS, Choyke P, Wood BJ, 2020. Multi-domain image completion for random missing input data. \\nIEEE Trans. on Medical Imaging\\n[30]. Shen L, Zhao W, Capaldi D, Pauly J, Xing L, 2021. A geometry-informed deep learning \\nframework for ultra-sparse computed tomography imaging. arXiv preprint arXiv:2105.11692\\n[31]. Sitzmann V, Zollhöfer M, Wetzstein G, 2019. Scene representation networks: Continuous 3d-\\nstructure-aware neural scene representations. In Advances in Neural Information Processing \\nSystems, pp. 1121–1132.\\nShen et al.\\nPage 17\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 16, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='[32]. Tatarchenko M, Richter SR, Ranftl R, Li Z, Koltun V, Brox T, 2019. What do single-view 3D \\nreconstruction networks learn In Proceedings of the IEEE Conference on Computer Vision and \\nPattern Recognition (CVPR), 3405–3414.\\n[33]. Ulyanov D, Vedaldi A, Lempitsky V, 2016. Instance normalization: The missing ingredient for \\nfast stylization. arXiv preprint arXiv:1607.08022\\n[34]. Wang Z, Bovik AC, Sheikh HR, Simoncelli EP, 2004. Image quality assessment: from error \\nvisibility to structural similarity. IEEE Trans. Image Process 13, 600–612. [PubMed: 15376593] \\n[35]. Wiles O, Gkioxari G, Szeliski R, Johnson J, 2020. Synsin: End-to-end view synthesis from \\na single image In Proceedings of the IEEE Conference on Computer Vision and Pattern \\nRecognition, pp. 7467–7477.\\n[36]. Winder M, Owczarek AJ, Chudek J, Pilch-Kowalczyk J, Baron J, 2021. Are We Overdoing It? \\nChanges in Diagnostic Imaging Workload during the Years 2010–2020 including the Impact of \\nthe SARS-CoV-2 Pandemic. In Healthcare (Vol. 9, No. 11, p. 1557). Multidisciplinary Digital \\nPublishing Institute. [PubMed: 34828603] \\n[37]. Xing L, Giger ML, Min JK (editors), 2020. Artificial intelligence in medicine: technical basis and \\nclinical applications, Academic Press, London, UK.\\n[38]. Yariv L, Kasten Y, Moran D, Galun M, Atzmon M, Ronen B, Lipman Y, 2020. Multiview \\nneural surface reconstruction by disentangling geometry and appearance. Advances in Neural \\nInformation Processing Systems, 33.\\n[39]. Ying X, Guo H, Ma K, Wu J, Weng Z, Zheng Y, 2019. X2CT-GAN: reconstructing CT from \\nbiplanar X-rays with generative adversarial networks In Proceedings of the IEEE Conference on \\nComputer Vision and Pattern Recognition, 10619–10628.\\n[40]. Zhang Z, Yang L, Zheng Y, 2018. Translating and segmenting multimodal medical volumes \\nwith cycle-and shape-consistency generative adversarial network In Proceedings of the IEEE \\nconference on Computer Vision and Pattern Recognition, pp. 9242–9251.\\n[41]. Zhao W, Shen L, Islam MT, Qin W, Zhang Z, Liang X, Zhang G, Xu S, Li X, 2021. Artificial \\nintelligence in image-guided radiotherapy: a review of treatment target localization. Quantitative \\nImaging in Medicine and Surgery, 11(12), p.4881. [PubMed: 34888196] \\n[42]. Zhu J-Y, Park T, Isola P, Efros AA, 2017. Unpaired image-to-image translation using cycle-\\nconsistent adversarial networks In Proceedings of the IEEE International Conference on \\nComputer Vision, pp. 2223–2232.\\n[43]. Zhu B, Liu JZ, Cauley SF, Rosen BR, Rosen MS, 2018. Image reconstruction by domain-\\ntransform manifold learning. Nature 555, 487–492. [PubMed: 29565357] \\nShen et al.\\nPage 18\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 17, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Highlights\\n•\\nWe for the first time investigate the novel-view projection synthesis problem \\nfor X-ray imaging. The approach can also be generalized to a more general \\nsynthesis from multi-views to multi-views projections.\\n•\\nWe propose a deep learning-based geometry-integrated projection synthesis \\nmodel (DL-GIPS) to generate novel-view X-ray projections through feature \\ndisentanglement and geometry transformation.\\n•\\nWe validate the feasibility of the proposed approach by experimenting on the \\none-to-one and multi-to-multi X-ray projection synthesis using lung imaging \\ncases across various patients.\\nShen et al.\\nPage 19\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 18, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 1. \\nSketch of X-ray projection imaging procedure. X-ray wave penetrates through the patient’s \\nbody and projects on the detector plane. When the X-ray source is located at different \\npositions, different projections at different view angles are obtained. The projections in the \\nview of anterior-posterior (AP) direction and lateral (LT) directions are shown in the figure.\\nShen et al.\\nPage 20\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 19, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 2. \\nIllustration of the proposed Deep Learning-based Geometry-Integrated Projection Synthesis \\nframework (DL-GIPS). The pipeline contains texture and geometry feature encoders, \\nprojection transformation and image generator. The projection transformation contains the \\ngeometric operation of backward projection and forward projection based on the X-ray \\nimaging physical model, and 3D feature refinement model.\\nShen et al.\\nPage 21\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 20, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 3. \\nIllustration of geometry transformation with backward projection (blue) and forward \\nprojection (purple). The back projector puts the pixel intensities in the source-view image \\nback to the corresponding voxels in the 3D volume according to the cone-beam geometry \\nof the physical model. When the X-ray source rotates to the target view angles, the forward \\nprojection operator integrates along the projection line and projects onto the detector plane.\\nShen et al.\\nPage 22\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 21, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 4. \\nResults of synthesized lateral projection from AP projection. Each row shows the results \\nof one testing sample. Regions of interest are zoomed in for more clear comparison in \\nstructural details. The columns are input projection, UNet synthesized projection, ReMIC \\nsynthesized projection, DL-GIPS synthesized projection, and the ground truth projection, \\nrespectively. (Red arrows highlight the compared difference among different images.)\\nShen et al.\\nPage 23\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 22, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 5. \\nResults of synthesized AP projection from lateral projection. Each row shows the results \\nof one testing sample. Regions of interest are zoomed in for more clear comparison in \\nstructural details. The columns are input projection, UNet synthesized projection, ReMIC \\nsynthesized projection, DL-GIPS synthesized projection, and the ground truth projection, \\nrespectively. (Red arrows highlight the compared difference among images.)\\nShen et al.\\nPage 24\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 23, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 6. \\nResults of synthesizing projections at the view angle of 30 degrees and 60 degrees from \\nAP and lateral projections. Each row shows the results of one testing sample. Regions of \\ninterest are zoomed in for more clear comparison in structural details. The columns are the \\ninput projections, UNet synthesized projections, ReMIC synthesized projections, DL-GIPS \\nsynthesized projections, and the ground truth projections respectively. Please note that the \\nmodel outputs the two target projections at 30 degrees and 60 degrees at one time. (Red \\narrows highlight the compared difference among images.)\\nShen et al.\\nPage 25\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 24, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 7. \\nFeature map visualization. We demonstrate the different features introduced in Fig. 2. The \\nfirst and the second rows show the geometry feature extracted from AP view and LT view \\nrespectively. The final row shows the texture features extracted from source view as shown \\nin Fig. 2.\\nShen et al.\\nPage 26\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 25, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fig. 8. \\nQualitative results of ablative study for synthesizing LT projection from AP projection. Each \\nrow shows results of one testing sample. Columns are synthesized projections for DL-GIPS \\nwithout adversarial loss, DL-GIPS synthesized projection, and ground truth projection.\\nShen et al.\\nPage 27\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 26, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Author Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nShen et al.\\nPage 28\\nTABLE I\\nRESULTS OF NOVEL-VIEW PROJECTION SYNTHESIS\\nMethods\\nUNet (Ronneberger et al.)\\nReMIC (Shen et al.)\\nDL-GIPS\\nMAE\\nRMSE\\nSSIM\\nPSNR\\nMAE\\nRMSE\\nSSIM\\nPSNR\\nMAE\\nRMSE\\nSSIM\\nPSNR\\nAP → LT\\n0.069\\n0.296\\n0.838\\n19.312\\n0.058\\n0.239\\n0.854\\n20.305\\n0.045\\n0.185\\n0.880\\n22.646\\nLT → AP\\n0.070\\n0.302\\n0.887\\n20.675\\n0.049\\n0.200\\n0.900\\n22.723\\n0.042\\n0.173\\n0.911\\n23.908\\nMulti→Multi\\n0.027\\n0.118\\n0.941\\n27.838\\n0.029\\n0.126\\n0.933\\n27.005\\n0.024\\n0.106\\n0.941\\n28.658\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 27, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Author Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nShen et al.\\nPage 29\\nTABLE II\\nRESULTS OF ABLATIVE STUDY ON LOSS FUNCTION\\nMethods\\nDL-GIPS w/o Consistency Loss\\nDL-GIPS\\nMAE\\nRMSE\\nSSIM\\nPSNR\\nMAE\\nRMSE\\nSSIM\\nPSNR\\nAP → LT\\n0.050\\n0.212\\n0.860\\n21.558\\n0.045\\n0.185\\n0.880\\n22.646\\nLT → AP\\n0.046\\n0.187\\n0.902\\n23.193\\n0.042\\n0.173\\n0.911\\n23.908\\nMulti→Multi\\n0.024\\n0.107\\n0.941\\n28.666\\n0.024\\n0.106\\n0.941\\n28.658\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 28, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Author Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nAuthor Manuscript\\nShen et al.\\nPage 30\\nTABLE III\\nRESULTS OF ABLATIVE STUDY ON MODEL STRUCTURE (AP → LT)\\nGeometry feature dimension\\nTexture feature dimension\\nMetrics\\nMAE\\nRMSE\\nSSIM\\nPSNR\\n32\\n64\\n0.045\\n0.185\\n0.880\\n22.646\\n32\\n32\\n0.050\\n0.212\\n0.862\\n21.560\\n16\\n32\\n0.058\\n0.239\\n0.854\\n20.602\\nMed Image Anal. Author manuscript; available in PMC 2023 April 01.\\n', metadata={'source': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'file_path': 'data/Shen et al. - 2022 - Novel-view X-ray projection synthesis through geom.pdf', 'page': 29, 'total_pages': 30, 'format': 'PDF 1.5', 'title': 'Novel-view X-ray Projection Synthesis through Geometry-integrated Deep Learning', 'author': 'Liyue Shen, Lequan Yu, Wei Zhao, John Pauly, Lei Xing', 'subject': '', 'keywords': 'projection view synthesis, X-ray imaging, geometry-integrated deep learning', 'creator': 'AH Formatter V7.2 MR3 for Linux64 : 7.2.4.55390 (2022-01-31T09:48+09)', 'producer': 'Antenna House PDF Output Library 7.2.1732', 'creationDate': \"D:20220311131918-05'00'\", 'modDate': \"D:20220311131918-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Direct Voxel Grid Optimization:\\nSuper-fast Convergence for Radiance Fields Reconstruction\\nCheng Sun1,2\\nchengsun@gapp.nthu.edu.tw\\nMin Sun1,3\\nsunmin@ee.nthu.edu.tw\\nHwann-Tzong Chen1,4\\nhtchen@cs.nthu.edu.tw\\nAbstract\\nWe present a super-fast convergence approach to recon-\\nstructing the per-scene radiance ﬁeld from a set of images\\nthat capture the scene with known poses. This task, which is\\noften applied to novel view synthesis, is recently revolution-\\nized by Neural Radiance Field (NeRF) for its state-of-the-art\\nquality and ﬂexibility. However, NeRF and its variants re-\\nquire a lengthy training time ranging from hours to days for\\na single scene. In contrast, our approach achieves NeRF-\\ncomparable quality and converges rapidly from scratch in\\nless than 15 minutes with a single GPU. We adopt a represen-\\ntation consisting of a density voxel grid for scene geometry\\nand a feature voxel grid with a shallow network for com-\\nplex view-dependent appearance. Modeling with explicit\\nand discretized volume representations is not new, but we\\npropose two simple yet non-trivial techniques that contribute\\nto fast convergence speed and high-quality output. First,\\nwe introduce the post-activation interpolation on voxel den-\\nsity, which is capable of producing sharp surfaces in lower\\ngrid resolution. Second, direct voxel density optimization\\nis prone to suboptimal geometry solutions, so we robustify\\nthe optimization process by imposing several priors. Finally,\\nevaluation on ﬁve inward-facing benchmarks shows that our\\nmethod matches, if not surpasses, NeRF’s quality, yet it only\\ntakes about 15 minutes to train from scratch for a new scene.\\nCode: https://github.com/sunset1995/DirectVoxGO.\\n1. Introduction\\nAchieving free-viewpoint navigation of 3D objects or\\nscenes from only a set of calibrated images as input is a de-\\nmanding task. For instance, it enables online product show-\\ncase to provide an immersive user experience comparing\\nto static image demonstration. Recently, Neural Radiance\\nFields (NeRFs) [37] have emerged as powerful representa-\\ntions yielding state-of-the-art quality on this task.\\n1National Tsing Hua University\\n2ASUS AICS Department\\n3Joint Research Center for AI Technology and All Vista Healthcare\\n4Aeolus Robotics\\n23.64 PSNR.\\n32.72 PSNR.\\n34.22 PSNR.\\nOurs at 2.33 mins.\\nOurs at 5.07 mins.\\nOurs at 13.72 mins.\\n(a) The synthesized novel view by our method at three training checkpoints.\\n(b) The training curves of different methods on Lego scene. The training time of each\\nmethod is measured on our machine with a single NVIDIA RTX 2080 Ti GPU.\\nFigure 1. Super-fast convergence by our method. The key to\\nour speedup is to optimize the volume density modeled in a dense\\nvoxel grid directly. Note that our method needs neither a conversion\\nstep from any trained implicit model (e.g., NeRF) nor a cross-\\nscene pretraining, i.e., our voxel grid representation is directly and\\nefﬁciently trained from scratch for each scene.\\nDespite its effectiveness in representing scenes, NeRF is\\nknown to be hampered by the need of lengthy training time\\nand the inefﬁciency in rendering new views. This makes\\nNeRF infeasible for many application scenarios. Several\\nfollow-up methods [15,18,29,30,42,43,66] have shown sig-\\nniﬁcant speedup of FPS in testing phase, some of which even\\nachieve real-time rendering. However, only few methods\\nshow training times speedup, and the improvements are not\\ncomparable to ours [1,10,31] or lead to worse quality [6,59].\\nOn a single GPU machine, several hours of per scene opti-\\nmization or a day of pretraining is typically required.\\nTo reconstruct a volumetric scene representation from a\\nset of images, NeRF uses multilayer perceptron (MLP) to\\nimplicitly learn the mapping from a queried 3D point (with\\na viewing direction) to its colors and densities. The queried\\nproperties along a camera ray can then be accumulated into a\\npixel color by volume rendering techniques. Our work takes\\ninspiration from the recent success [15, 18, 66] that uses\\nclassic voxel grid to explicitly store the scene properties,\\n5459\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='which enables real-time rendering and shows good quality.\\nHowever, their methods can not train from scratch and need\\na conversion step from the trained implicit model, which\\ncauses a bottleneck to the training time.\\nThe key to our speedup is to use a dense voxel grid to\\ndirectly model the 3D geometry (volume density). Develop-\\ning an elaborate strategy for view-dependent colors is not\\nin the main scope of this paper, and we simply use a hybrid\\nrepresentation (feature grid with shallow MLP) for colors.\\nDirectly optimizing the density voxel grid leads to super-\\nfast converges but is prone to suboptimal solutions, where\\nour method allocates “cloud” at free space and tries to ﬁt\\nthe photometric loss with the cloud instead of searching a\\ngeometry with better multi-view consistency. Our solution\\nto this problem is simple and effective. First, we initialize\\nthe density voxel grid to yield opacities very close to zero\\neverywhere to avoid the geometry solutions being biased\\ntoward the cameras’ near planes. Second, we give a lower\\nlearning rate to voxels visible to fewer views, which can\\navoid redundant voxels that are allocated just for explaining\\nthe observations from a small number of views. We show that\\nthe proposed solutions can successfully avoid the suboptimal\\ngeometry and work well on the ﬁve datasets.\\nUsing the voxel grid to model volume density still faces\\na challenge in scalability. For parsimony, our approach au-\\ntomatically ﬁnds a BBox tightly encloses the volume of\\ninterest to allocate the voxel grids. Besides, we propose\\npost-activation—applying all the activation functions after\\ntrilinearly interpolating the density voxel grid. Previous\\nwork either interpolates the voxel grid for the activated opac-\\nity or uses nearest-neighbor interpolation, which results in a\\nsmooth surface in each grid cell. Conversely, we prove math-\\nematically and empirically that the proposed post-activation\\ncan model (beyond) a sharp linear surface within a single\\ngrid cell. As a result, we can use fewer voxels to achieve\\nbetter qualities—our method with 1603 dense voxels already\\noutperforms NeRF in most cases.\\nIn summary, we have two main technical contributions.\\nFirst, we implement two priors to avoid suboptimal geometry\\nin direct voxel density optimization. Second, we propose the\\npost-activated voxel-grid interpolation, which enables sharp\\nboundary modeling in lower grid resolution. The resulting\\nkey merits of this work are highlighted as follows:\\n• Our convergence speed is about two orders of magni-\\ntude faster than NeRF—reducing training time from\\n10−20 hours to 15 minutes on our machine with a sin-\\ngle NVIDIA RTX 2080 Ti GPU, as shown in Fig. 1.\\n• We achieve visual quality comparable to NeRF at a\\nrendering speed that is about 45× faster.\\n• Our method does not need cross-scene pretraining.\\n• Our grid resolution is about 1603, while the grid reso-\\nlution in previous work [15,18,66] ranges from 5123\\nto 13003 to achieve NeRF-comparable quality.\\n2. Related work\\nRepresentations for novel view synthesis.\\nImages syn-\\nthesis from novel viewpoints given a set of images cap-\\nturing the scene is a long-standing task with rich stud-\\nies. Previous work has presented several scene represen-\\ntations reconstructed from the input images to synthesize\\nthe unobserved viewpoints. Lumigraph [4, 16] and light\\nﬁeld representation [7,23,24,46] directly synthesize novel\\nviews by interpolating the input images but require very\\ndense scene capture. Layered depth images [11,45,47,57]\\nwork for sparse input views but rely on depth maps or es-\\ntimated depth with sacriﬁced quality. Mesh-based repre-\\nsentations [8, 54, 58, 63] can run in real-time but have a\\nhard time with gradient-based optimization without template\\nmeshes provided. Recent approaches employ 2D/3D Con-\\nvolutional Neural Network (CNNs) to estimate multiplane\\nimages (MPIs) [12, 26, 36, 51, 56, 71] for forward-facing\\ncaptures; estimate voxel grid [17,32,48] for inward-facing\\ncaptures. Our method uses gradient-descent to optimize\\nvoxel grids directly and does not rely on neural networks to\\npredict the grid values, and we still outperform the previous\\nworks [17,32,48] with CNNs by a large margin.\\nNeural radiance ﬁelds.\\nRecently, NeRF [37] stands out\\nto be a prevalent method for novel view synthesis with rapid\\nprogress, which takes a moderate number of input images\\nwith known camera poses. Unlike traditional explicit and\\ndiscretized volumetric representations (e.g., voxel grids and\\nMPIs), NeRF uses coordinate-based multilayer perceptrons\\n(MLP) as an implicit and continuous volumetric represen-\\ntation. NeRF achieves appealing quality and has good ﬂex-\\nibility with many follow-up extensions to various setups,\\ne.g., relighting [2, 3, 50, 70], deformation [13, 38–40, 55],\\nself-calibration [19,27,28,35,61], meta-learning [52], dy-\\nnamic scene modeling [14, 25, 33, 41, 64], and generative\\nmodeling [5,22,44]. Nevertheless, NeRF has unfavorable\\nlimitations of lengthy training progress and slow rendering\\nspeed. In this work, we mainly follow NeRF’s original setup,\\nwhile our method can optimize the volume density explicitly\\nencoded in a voxel grid to speed up both training and testing\\nby a large margin with comparable quality.\\nHybrid volumetric representations.\\nTo combine NeRF’s\\nimplicit representation and traditional grid representations,\\nthe coordinate-based MLP is extended to also condition-\\ning on the local feature in the grid. Recently, hybrid vox-\\nels [18,30] and MPIs [62] representations have shown suc-\\ncess in fast rendering speed and result quality. We use hybrid\\nrepresentation to model view-dependent color as well.\\nFast NeRF rendering.\\nNSVF [30] uses octree in its hy-\\nbrid representation to avoid redundant MLP queries in free\\n5460\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='(a) Volume rendering\\n(b) Our scene representation\\nquery\\naccumulation\\nrendered pixel\\nobserved pixel\\nphotometric loss\\nshallow \\nMLP\\n3D position + \\nviewing-direction\\nqueried \\nfeature\\npost-activation\\n(c) Coarse geometry searching\\npriors\\n…\\ntraining views\\ninitialize\\nsupervise\\n(d) Fine detail reconstruction\\nnovel views\\nrender\\nfind tight bbox + \\nskip free space\\nsupervise\\n(Sec. 3)\\n(Sec. 5.2)\\n(Sec. 5.1)\\n(Sec. 5.2)\\n(Eq. (2))\\n(Eq. (3))\\n(Sec. 4)\\nFigure 2. Approach overview. We ﬁrst review NeRF in Sec. 3. In Sec. 4, we present a novel post-activated density voxel grid to support\\nsharp surface modeling in lower grid resolutions. In Sec. 5, we show our approach to the reconstruction of radiance ﬁeld with super-fast\\nconvergence, where we ﬁrst ﬁnd a coarse geometry in Sec. 5.1 and then reconstruct the ﬁne details and view-dependent effects in Sec. 5.2.\\nspace. However, NSVF still needs many training hours due\\nto the deep MLP in its representation. Recent methods fur-\\nther use thousands of tiny MLPs [43] or explicit volumetric\\nrepresentations [15,18,62,66] to achieve real-time rendering.\\nUnfortunately, gradient-based optimization is not directly\\napplicable to their methods due to their topological data\\nstructures or the lack of priors. As a result, these meth-\\nods [15, 18, 43, 62, 66] still need a conversion step from a\\ntrained implicit model (e.g., NeRF) to their ﬁnal representa-\\ntion that supports real-time rendering. Their training time is\\nstill burdened by the lengthy implicit model optimization.\\nFast NeRF convergence.\\nRecent works that focus on\\nfewer input views setup also bring faster convergence\\nas a side beneﬁt.\\nThese methods rely on generalizable\\npre-training [6, 59, 67] or external MVS depth informa-\\ntion [10,31], while ours does not. Further, they still require\\nseveral per-scene ﬁne-tuning hours [10] or fail to achieve\\nNeRF quality in the full input-view setup [6,59,67]. Most re-\\ncently, NeuRay [31] shows NeRF’s quality with 40 minutes\\nper-scene training time in the lower-resolution setup. Under\\nthe same GPU spec, our method achieves NeRF’s quality in\\n15 minutes per scene on the high-resolution setup and does\\nnot require depth guidance and cross-scene pre-training.\\n3. Preliminaries\\nTo represent a 3D scene for novel view synthesis, Neural\\nRadiance Fields (NeRFs) [37] employ multilayer perceptron\\n(MLP) networks to map a 3D position x and a viewing direc-\\ntion d to the corresponding density σ and view-dependent\\ncolor emission c:\\n(σ, e) = MLP(pos)(x) ,\\n(1a)\\nc = MLP(rgb)(e, d) ,\\n(1b)\\nwhere the learnable MLP parameters are omitted, and e\\nis an intermediate embedding to help the much shallower\\nMLP(rgb) to learn c (see NeRF++ [68] for more discussions\\non the architecture design). In practice, positional encod-\\ning is applied to x and d, which enables the MLPs to learn\\nthe high-frequency details from low-dimensional input [53].\\nFor output activation, Sigmoid is applied on c; ReLU or\\nSoftplus is applied on σ (see Mip-NeRF [1] for more dis-\\ncussion on output activation).\\nTo render the color of a pixel ˆC(r), we cast the ray r\\nfrom the camera center through the pixel; K points are then\\nsampled on r between the pre-deﬁned near and far planes;\\nthe K ordered sampled points are then used to query for\\ntheir densities and colors {(σi, ci)}K\\ni=1 (MLPs are queried\\nin NeRF). Finally, the K queried results are accumulated\\ninto a single color with the volume rendering quadrature in\\naccordance with the optical model given by Max [34]:\\nˆC(r) =\\n� K\\n�\\ni=1\\nTiαici\\n�\\n+ TK+1cbg ,\\n(2a)\\nαi = alpha(σi, δi) = 1 − exp(−σiδi) ,\\n(2b)\\nTi =\\ni−1\\n�\\nj=1\\n(1 − αj) ,\\n(2c)\\nwhere αi is the probability of termination at the point i; Ti is\\nthe accumulated transmittance from the near plane to point\\ni; δi is the distance to the adjacent sampled point, and cbg is\\na pre-deﬁned background color.\\nGiven the training images with known poses, NeRF model\\nis trained by minimizing the photometric MSE between the\\nobserved pixel color C(r) and the rendered color ˆC(r):\\nLphoto =\\n1\\n|R|\\n�\\nr∈R\\n��� ˆC(r) − C(r)\\n���\\n2\\n2 ,\\n(3)\\nwhere R is the set of rays in a sampled mini-batch.\\n5461\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Figure 3. A single grid cell with post-activation is capable of\\nmodeling sharp linear surfaces. Left: We depict the toy task\\nfor a 2D grid cell, where a grid cell is optimized for the linear\\nsurface (decision boundary) across it. Right: Each column shows\\nan example task for three different methods. The results show that a\\nsingle grid cell with post-activation (Eq. (6c)) is adequate to recover\\nfaithfully the linear surface. Conversely, pre-activation (Eq. (6a))\\nand in-activation (Eq. (6b)) fail to accomplish the tasks as they can\\nonly ﬁt into smooth results, and thus would require more grid cells\\nto recover the surface detail. See supplementary material for the\\nmathematical proof.\\n4. Post-activated density voxel grid\\nVoxel-grid representation.\\nA voxel-grid representation\\nmodels the modalities of interest (e.g., density, color, or\\nfeature) explicitly in its grid cells. Such an explicit scene\\nrepresentation is efﬁcient to query for any 3D positions via\\ninterpolation:\\ninterp(x, V ) :\\n�\\nR3, RC×Nx×Ny×Nz�\\n→ RC ,\\n(4)\\nwhere x is the queried 3D point, V is the voxel grid, C is\\nthe dimension of the modality, and Nx · Ny · Nz is the total\\nnumber of voxels. Trilinear interpolation is applied if not\\nspeciﬁed otherwise.\\nDensity voxel grid for volume rendering.\\nDensity voxel\\ngrid, V (density), is a special case with C = 1, which stores the\\ndensity values for volume rendering (Eq. (2)). We use ¨σ ∈ R\\nto denote the raw voxel density before applying the density\\nactivation (i.e., a mapping of R → R≥0). In this work, we\\nuse the shifted softplus mentioned in Mip-NeRF [1] as the\\ndensity activation:\\nσ = softplus(¨σ) = log(1 + exp(¨σ + b)) ,\\n(5)\\nwhere the shift b is a hyperparameter. Using softplus instead\\nof ReLU is crucial to optimize voxel density directly, as it\\nis irreparable when a voxel is falsely set to a negative value\\nwith ReLU as the density activation. Conversely, softplus\\nallows us to explore density very close to 0.\\nSharp decision boundary via post-activation.\\nThe inter-\\npolated voxel density is processed by softplus (Eq. (5)) and\\nalpha (Eq. (2b)) functions sequentially for volume render-\\ning. We consider three different orderings—pre-activation,\\nin-activation, and post-activation—of plugging in the tri-\\nlinear interpolation and performing the activation, given a\\n(a) Visual comparison of image ﬁtting results under grid resolution (H/5)×(W/5).\\nThe ﬁrst row is the results of pre-, in-, and post-activation. The second row is their\\nper-pixel absolute difference to the target image.\\n(b) PSNRs achieved by pre-, in- and post-activation under different grid strides. A\\ngrid stride s means that the grid resolution is (H/s) × (W/s). The black dashed\\nline highlights that post-activation with stride ≈ 8.5 can achieve the same PSNR as\\npre-activation with stride 2 in this example.\\nFigure 4. Toy example on image ﬁtting. The target 2D image is\\nbinary to imitate the scenario that most of the 3D space is either\\noccupied or free. The objective is to reconstruct the target image\\nby a low-resolution 2D grid. In each optimization step, the tunable\\n2D grid is queried by interpolation with pre-activation (Eq. (6a)),\\nin-activation (Eq. (6b)), or post-activation (Eq. (6c)) to minimize\\nthe mean squared error to the target image. The result reveals that\\nthe post-activation can produce sharp boundaries even with low\\ngrid resolution (Fig. 4a) and is much better than the other two under\\nvarious grid resolutions (Fig. 4b). This motivates us to model the\\n3D geometry directly via voxel grids with post-activation.\\nqueried 3D point x:\\nα(pre) = interp\\n�\\nx, alpha\\n�\\nsoftplus\\n�\\nV (density)���\\n, (6a)\\nα(in) = alpha\\n�\\ninterp\\n�\\nx, softplus\\n�\\nV (density)���\\n, (6b)\\nα(post) = alpha\\n�\\nsoftplus\\n�\\ninterp\\n�\\nx, V (density)���\\n. (6c)\\nThe input δ to the function alpha (Eq. (2b)) is omitted for\\nsimplicity. We show that the post-activation, i.e., applying\\nall the non-linear activation after the trilinear interpolation,\\nis capable of producing sharp surfaces (decision boundaries)\\nwith much fewer grid cells. In Fig. 3, we use a 2D grid cell\\nas an example to show that a grid cell with post-activation\\ncan produce a sharp linear boundary, while pre- and in-\\nactivation can only produce smooth results and thus require\\nmore cells for the surface detail. In Fig. 4, we further use\\nbinary image regression as a toy example to compare their\\ncapability, which also shows that post-activation can achieve\\na much better efﬁciency in grid cell usage.\\n5. Fast and direct voxel grid optimization\\nWe depict an overview of our approach in Fig. 2. In\\nSec. 5.1, we ﬁrst search the coarse geometry of a scene. In\\nSec. 5.2, we then reconstruct the ﬁne detail including view-\\ndependent effects. Hereinafter we use superscripts (c) and (f)\\nto denote variables in the coarse and ﬁne stages.\\n5462\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='5.1. Coarse geometry searching\\nTypically, a scene is dominated by free space (i.e., unoc-\\ncupied space). Motivated by this fact, we aim to efﬁciently\\nﬁnd the coarse 3D areas of interest before reconstructing the\\nﬁne detail and view-dependent effect that require more com-\\nputation resources. We can thus greatly reduce the number\\nof queried points on each ray in the later ﬁne stage.\\nCoarse scene representation.\\nWe use a coarse den-\\nsity voxel grid V (density)(c) ∈ R1×N (c)\\nx ×N (c)\\ny ×N (c)\\nz with post-\\nactivation (Eq. (6c)) to model scene geometry.\\nWe\\nonly model view-invariant color emissions by V (rgb)(c) ∈\\nR3×N (c)\\nx ×N (c)\\ny ×N (c)\\nz in the coarse stage. A query of any 3D\\npoint x is efﬁcient with interpolation:\\n¨σ(c) = interp\\n�\\nx, V (density)(c)�\\n,\\n(7a)\\nc(c) = interp\\n�\\nx, V (rgb)(c)�\\n,\\n(7b)\\nwhere c(c) ∈ R3 is the view-invariant color and ¨σ(c) ∈ R is\\nthe raw volume density.\\nCoarse voxels allocation.\\nWe ﬁrst ﬁnd a bounding box\\n(BBox) tightly enclosing the camera frustums of training\\nviews (See the red BBox in Fig. 2c for an example). Our\\nvoxel grids are aligned with the BBox. Let L(c)\\nx , L(c)\\ny , L(c)\\nz\\nbe the lengths of the BBox and M (c) be the hyperparameter\\nfor the expected total number of voxels in the coarse stage.\\nThe voxel size is s(c) =\\n3�\\nL(c)\\nx · L(c)\\ny · L(c)\\nz /M (c), so there are\\nN (c)\\nx , N (c)\\ny , N (c)\\nz\\n= ⌊L(c)\\nx /s(c)⌋, ⌊L(c)\\ny /s(c)⌋, ⌊L(c)\\nz /s(c)⌋ voxels\\non each side of the BBox.\\nCoarse-stage points sampling.\\nOn a pixel-rendering ray,\\nwe sample query points as\\nx0 = o + t(near)d ,\\n(8a)\\nxi = x0 + i · δ(c) ·\\nd\\n∥d∥2 ,\\n(8b)\\nwhere o is the camera center, d is the ray-casting direction,\\nt(near) is the camera near bound, and δ(c) is a hyperparameter\\nfor the step size that can be adaptively chosen according\\nto the voxel size s(c). The query index i ranges from 1 to\\n⌈t(far) · ∥d∥2/δ(c)⌉, where t(far) is the camera far bound, so\\nthe last sampled point stops nearby the far plane.\\nPrior 1: low-density initialization.\\nAt the start of train-\\ning, the importance of points far from a camera is down-\\nweighted due to the accumulated transmittance term in\\nEq. (2c). As a result, the coarse density voxel grid V (density)(c)\\ncould be accidentally trapped into a suboptimal “cloudy” ge-\\nometry with higher densities at camera near planes. We thus\\nhave to initialize V (density)(c) more carefully to ensure that\\nall sampled points on rays are visible to the cameras at the\\nbeginning, i.e., the accumulated transmittance rates Tis in\\nEq. (2c) are close to 1.\\nIn practice, we initialize all grid values in V (density)(c) to 0\\nand set the bias term in Eq. (5) to\\nb = log\\n��\\n1 − α(init)(c)�−\\n1\\ns(c) − 1\\n�\\n,\\n(9)\\nwhere α(init)(c) is a hyperparameter. Thereby, the accumu-\\nlated transmittance Ti is decayed by 1 − α(init)(c) ≈ 1 for a\\nray that traces forward a distance of a voxel size s(c). See\\nsupplementary material for the derivation and proof.\\nPrior 2: view-count-based learning rate.\\nThere could\\nbe some voxels visible to too few training views in real-\\nworld capturing, while we prefer a surface with consistency\\nin many views instead of a surface that can only explain few\\nviews. In practice, we set different learning rates for different\\ngrid points in V (density)(c). For each grid point indexed by j,\\nwe count the number of training views nj to which point j\\nis visible, and then scale its base learning rate by nj/nmax,\\nwhere nmax is the maximum view count over all grid points.\\nTraining objective for coarse representation.\\nThe scene\\nrepresentation is reconstructed by minimizing the mean\\nsquare error between the rendered and observed colors. To\\nregularize the reconstruction, we mainly use background\\nentropy loss to encourage the accumulated alpha values to\\nconcentrate on background or foreground. Please refer to\\nsupplementary material for more detail.\\n5.2. Fine detail reconstruction\\nGiven the optimized coarse geometry V (density)(c) in\\nSec. 5.1, we now can focus on a smaller subspace to recon-\\nstruct the surface details and view-dependent effects. The\\noptimized V (density)(c) is frozen in this stage.\\nFine scene representation.\\nIn the ﬁne stage, we use\\na higher-resolution density voxel grid V (density)(f)\\n∈\\nR1×N (f)\\nx ×N (f)\\ny ×N (f)\\nz with post-activated interpolation (Eq. (6c)).\\nNote that, alternatively, it is also possible to use a more ad-\\nvanced data structure [18, 30, 66] to reﬁne the voxel grid\\nbased on the current V (density)(c) but we leave that for future\\nwork. To model view-dependent color emission, we opt to\\nuse an explicit-implicit hybrid representation as we ﬁnd in\\nour prior experiments that an explicit representation tends to\\nproduce worse results, and an implicit representation entails\\na slower training speed. Our hybrid representation comprises\\ni) a feature voxel grid V (feat)(f) ∈ RD×N (f)\\nx ×N (f)\\ny ×N (f)\\nz , where\\nD is a hyperparameter for feature-space dimension, and ii) a\\nshallow MLP parameteriszed by Θ. Finally, queries of 3D\\npoints x and viewing-direction d are performed by\\n¨σ(f) = interp\\n�\\nx, V (density)(f)�\\n,\\n(10a)\\nc(f) = MLP(rgb)\\nΘ\\n�\\ninterp(x, V (feat)(f)), x, d\\n�\\n,\\n(10b)\\nwhere c(f) ∈ R3 is the view-dependent color emission and\\n¨σ(f) ∈ R is the raw volume density in the ﬁne stage. Posi-\\ntional embedding [37] is applied on x, d for the MLP(rgb)\\nΘ\\n.\\n5463\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Known free space and unknown space.\\nA query point\\nis in the known free space if the post-activated alpha value\\nfrom the optimized V (density)(c) is less than the threshold τ (c).\\nOtherwise, we say the query point is in the unknown space.\\nFine voxels allocation.\\nWe densely query V (density)(c) to\\nﬁnd a BBox tightly enclosing the unknown space, where\\nL(f)\\nx , L(f)\\ny , L(f)\\nz are the lengths of the BBox. The only hyper-\\nparameter is the expected total number of voxels M (f). The\\nvoxel size s(f) and the grid dimensions N (f)\\nx , N (f)\\ny , N (f)\\nz can\\nthen be derived automatically from M (f) as per Sec. 5.1.\\nProgressive scaling.\\nInspired by NSVF [30], we progres-\\nsively scale our voxel grid V (density)(f) and V (feat)(f). Let\\npg ckpt be the set of checkpoint steps. The initial number of\\nvoxels is set to ⌊M (f)/2|pg ckpt|⌋. When reaching the train-\\ning step in pg ckpt, we double the number of voxels such\\nthat the number of voxels after the last checkpoint is M (f);\\nthe voxel size s(f) and the grid dimensions N (f)\\nx , N (f)\\ny , N (f)\\nz\\nare updated accordingly. Scaling our scene representation\\nis much simpler. At each checkpoint, we resize our voxel\\ngrids, V (density)(f) and V (feat)(f), by trilinear interpolation.\\nFine-stage points sampling.\\nThe points sampling strategy\\nis similar to Eq. (8) with some modiﬁcations. We ﬁrst ﬁlter\\nout rays that do not intersect with the known free space. For\\neach ray, we adjust the near- and far-bound, t(near) and t(far),\\nto the two endpoints of the ray-box intersection. We do not\\nadjust t(near) if x0 is already inside the BBox.\\nFree space skipping.\\nQuerying V (density)(c) (Eq. (7a)) is\\nfaster than querying V (density)(f) (Eq. (10a)); querying for\\nview-dependent colors (Eq. (10b)) is the slowest. We im-\\nprove ﬁne-stage efﬁciency by free space skipping in both\\ntraining and testing. First, we skip sampled points that are in\\nthe known free space by checking the optimized V (density)(c)\\n(Eq. (7a)). Second, we further skip sampled points in un-\\nknown space with low activated alpha value (threshold at\\nτ (f)) by querying V (density)(f) (Eq. (10a)).\\nTraining objective for ﬁne representation.\\nWe use the\\nsame training losses as the coarse stage, but we use a smaller\\nweight for the regularization losses as we ﬁnd it empirically\\nleads to slightly better quality.\\n6. Experiments\\n6.1. Datasets\\nWe evaluate our approach on ﬁve inward-facing datasets.\\nSynthetic-NeRF [37] contains eight objects with realistic\\nimages synthesized by NeRF. Synthetic-NSVF [30] con-\\ntains another eight objects synthesized by NSVF. Strictly\\nfollowing NeRF’s and NSVF’s setups, we set the image\\nresolution to 800 × 800 pixels and let each scene have\\n100 views for training and 200 views for testing. Blend-\\nedMVS [65] is a synthetic MVS dataset that has realistic\\nambient lighting from real image blending. We use a subset\\nof four objects provided by NSVF. The image resolution\\nis 768 × 576 pixels, and one-eighth of the images are for\\ntesting. Tanks&Temples [21] is a real-world dataset. We\\nuse a subset of ﬁve scenes provided by NSVF, each con-\\ntaining views captured by an inward-facing camera circling\\nthe scene. The image resolution is 1920 × 1080 pixels, and\\none-eighth of the images are for testing. DeepVoxels [48]\\ndataset contains four simple Lambertian objects. The image\\nresolutions are 512 × 512, and each scene has 479 views for\\ntraining and 1000 views for testing.\\n6.2. Implementation details\\nWe choose the same hyperparameters generally for all\\nscenes. The expected numbers of voxels are set to M (c) =\\n1003 and M (f) = 1603 in coarse and ﬁne stages if not stated\\notherwise.\\nThe activated alpha values are initialized to\\nbe α(init)(c) = 10−6 in the coarse stage. We use a higher\\nα(init)(f) = 10−2 as the query points are concentrated on the\\noptimized coarse geometry in the ﬁne stage. The points\\nsampling step sizes are set to half of the voxel sizes, i.e.,\\nδ(c) = 0.5 · s(c) and δ(f) = 0.5 · s(f). The shallow MLP layer\\ncomprises two hidden layers with 128 channels. We use\\nthe Adam optimizer [20] with a batch size of 8,192 rays to\\noptimize the coarse and ﬁne scene representations for 10k\\nand 20k iterations. The base learning rates are 0.1 for all\\nvoxel grids and 10−3 for the shallow MLP. The exponential\\nlearning rate decay is applied. See supplementary material\\nfor detailed hyperparameter setups.\\n6.3. Comparisons\\nQuantitative evaluation on the synthesized novel view.\\nWe ﬁrst quantitatively compare the novel view synthesis\\nresults in Tab. 1. PSNR, SSIM [60], and LPIPS [69] are em-\\nployed as evaluation metrics. Our model with M (f) = 1603\\nvoxels already outperforms the original NeRF [37] and the\\nimproved JaxNeRF [9] re-implementation. Besides, our re-\\nsults are also comparable to most of the recent methods,\\nexcept JaxNeRF+ [9] and Mip-NeRF [1]. Moreover, our\\nper-scene optimization only takes about 15 minutes, while\\nall the methods after NeRF in Tab. 1 need quite a few hours\\nper scene. We also show our model with M (f) = 2563\\nvoxels, which signiﬁcantly improves our results under all\\nmetrics and achieves more comparable results to JaxNeRF+\\nand Mip-NeRF. We defer detail comparisons on the much\\nsimpler DeepVoxels [48] dataset to supplementary material,\\nwhere we achieve 45.83 averaged PSNR and outperform\\nNeRF’s 40.15 and IBRNet’s 42.93.\\nTraining time comparisons.\\nThe key merit of our work\\nis the signiﬁcant improvement in convergence speed with\\nNeRF-comparable quality. In Tab. 2, we show a training\\n5464\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Methods\\nSynthetic-NeRF\\nSynthetic-NSVF\\nBlendedMVS\\nTanks and Temples\\nPSNR↑ SSIM↑ LPIPS↓\\nPSNR↑ SSIM↑ LPIPS↓\\nPSNR↑ SSIM↑ LPIPS↓\\nPSNR↑ SSIM↑\\nLPIPS↓\\nSRN [49]\\n22.26\\n0.846\\n0.170vgg\\n24.33\\n0.882\\n0.141alex\\n20.51\\n0.770\\n0.294alex\\n24.10\\n0.847\\n0.251alex\\nNV [32]\\n26.05\\n0.893\\n0.160vgg\\n25.83\\n0.892\\n0.124alex\\n23.03\\n0.793\\n0.243alex\\n23.70\\n0.834\\n0.260alex\\nNeRF [37]\\n31.01\\n0.947\\n0.081vgg\\n30.81\\n0.952\\n0.043alex\\n24.15\\n0.828\\n0.192alex\\n25.78\\n0.864\\n0.198alex\\nImproved visual quality from NeRF\\nJaxNeRF [9]\\n31.69\\n0.953\\n0.068vgg\\n-\\n-\\n-\\n-\\n-\\n-\\n27.94\\n0.904\\n0.168vgg\\nJaxNeRF+ [9]\\n33.00\\n0.962\\n0.038\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMip-NeRF [1]\\n33.09\\n0.961\\n0.043vgg\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nImproved test-time rendering speed (and visual quality) from NeRF\\nAutoInt [29]\\n25.55\\n0.911\\n0.170\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nFastNeRF [15]\\n29.97\\n0.941\\n0.053\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSNeRG [18]\\n30.38\\n0.950\\n0.050\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nKiloNeRF [43]\\n31.00\\n0.95\\n0.03\\n33.37\\n0.97\\n0.02\\n27.39\\n0.92\\n0.06\\n28.41\\n0.91\\n0.09\\nPlenOctrees [66]\\n31.71\\n0.958\\n0.053vgg\\n-\\n-\\n-\\n-\\n-\\n-\\n27.99\\n0.917\\n0.131vgg\\nNSVF [30]\\n31.75\\n0.953\\n0.047alex\\n35.18\\n0.979\\n0.015alex\\n26.89\\n0.898\\n0.114alex\\n28.48\\n0.901\\n0.155alex\\nImproved convergence speed, test-time rendering speed, and visual quality from NeRF\\nours (M (f)=1603)\\n31.95\\n0.957\\n0.053vgg\\n0.035alex\\n35.08\\n0.975\\n0.033vgg\\n0.019alex\\n28.02\\n0.922\\n0.101vgg\\n0.075alex\\n28.41\\n0.911\\n0.155vgg\\n0.148alex\\nours (M (f)=2563)\\n32.80\\n0.961\\n0.045vgg\\n0.027alex\\n36.21\\n0.980\\n0.024vgg\\n0.012alex\\n28.64\\n0.933\\n0.081vgg\\n0.052alex\\n28.82\\n0.920\\n0.138vgg\\n0.124alex\\n* The superscript denotes the pre-trained models used in LPIPS. The gray numbers indicate that the code is unavailable or has a unconventional LPIPS implementation.\\nTable 1. Quantitative comparisons for novel view synthesis. Our method excels in convergence speed, i.e., 15 minutes per scene compared\\nto many hours or days per scene using other methods. Besides, our rendering quality is better than the original NeRF [37] and the improved\\nJaxNeRF [9] on the four datasets under all metrics. We also show comparable results to most of the recent methods.\\ntime comparison. We also show GPU speciﬁcations after\\neach reported time as it is the main factor affecting run-time.\\nNeRF [37] with a more powerful GPU needs 1–2 days per\\nscene to achieve 31.01 PSNR, while our method achieves a\\nsuperior 31.95 and 32.80 PSNR in about 15 an 22 minutes\\nper scene respectively. MVSNeRF [6], IBRNet [59], and\\nNeuRay [31] also show less per-scene training time than\\nNeRF but with the additional cost to run a generalizable\\ncross-scene pre-training. MVSNeRF [6], after pre-training,\\noptimizes a scene in 15 minutes as well, but the PSNR is\\ndegraded to 28.14. IBRNet [59] shows worse PSNR and\\nlonger training time than ours. NeuRay [31] originally re-\\nports time in lower-resolution (NeuRay-Lo) setup, and we\\nreceive the training time of the high-resolution (NeuRay-\\nHi) setup from the authors. NeuRay-Hi achieves 32.42\\nPSNR and requires 23 hours to train, while our method\\nwith M (f) = 2563 voxels achieves superior 32.80 in about\\n22 minutes. For the early-stopped NeuRay-Hi, unfortunately,\\nonly its training time is retained (early-stopped NeuRay-Lo\\nachieves NeRF-similar PSNR). NeuRay-Hi still needs 70\\nminutes to train with early stopping, while we only need 15\\nminutes to achieve NeRF-comparable quality and do not rely\\non generalizable pre-training or external depth information.\\nMip-NeRF [1] has similar run-time to NeRF but with much\\nbetter PSNRs, which also signiﬁes using less training time to\\nachieve NeRF’s PSNR. We train early-stopped Mip-NeRFs\\non our machine and show the averaged PSNR and training\\nMethods\\nPSNR↑\\ngeneralizable\\npre-training\\nper-scene\\noptimization\\nNeRF [37]\\n31.01\\nno need\\n1–2 days (V100)\\nMVSNeRF [6]\\n27.21\\n30 hrs (2080Ti)\\n15 mins (2080Ti)\\nIBRNet [59]\\n28.14\\n1 day (8xV100)\\n6 hrs\\n(V100)\\nNeuRay [31]†\\n32.42\\n2 days (2080Ti)\\n23 hrs\\n(2080Ti)\\nMip-NeRF [1]‡\\n30.85\\nno need\\n6 hrs\\n(2080Ti)\\nours (M (f)=1603)\\n31.95\\nno need\\n15 mins (2080Ti)\\nours (M (f)=2563)\\n32.80\\nno need\\n22 mins (2080Ti)\\n† Use external depth information.\\n‡ Our reproduction with early stopping on our machine.\\nTable 2. Training time comparisons. We take the training time\\nand GPU speciﬁcations reported in previous works directly. A\\nV100 GPU can run faster and has more storage than a 2080Ti GPU.\\nOur method achieves good PSNR in a signiﬁcantly less per-scene\\noptimization time.\\ntime. The early-stopped Mip-NeRF achieves 30.85 PSNR\\nafter 6 hours of training, while we can achieve 31.95 PSNR\\nin just 15 minutes.\\nRendering speed comparisons.\\nImproving test-time ren-\\ndering speed is not the main focus of this work, but we still\\nachieve ∼ 45× speedups from NeRF—0.64 seconds versus\\n29 seconds per 800 × 800 image on our machine.\\nQualitative comparison.\\nFig. 5 shows our rendering re-\\nsults on the challenging parts and compare them with the\\nresults (better than NeRF’s) provided by PlenOctrees [66].\\n5465\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='GT\\nOurs\\nPlenOctree\\nGT\\nOurs\\nPlenOctree\\nFigure 5. Qualitative comparisons on the challenging parts.\\nTop: On ﬁcus scene, we do not show blocking artifacts as PlenOc-\\ntree and recover the pot better. Middle: We produce blurrier results\\non ship’s body and rigging, but we do not have the background\\nartifacts. Bottom: On real-world captured Ignatius, we show better\\nquality without blocking artifacts (left) and recover the color tone\\nbetter (right). See supplementary material for more visualizations.\\n6.4. Ablation studies\\nWe mainly validate the effectiveness of the two proposed\\ntechniques—post-activation and the imposed priors—that\\nenable voxel grids to model scene geometry with NeRF-\\ncomparable quality. We subsample two scenes for each\\ndataset. See supplementary material for more detail and\\nadditional ablation studies on the number of voxels, point-\\nsampling step size, progressive scaling, free space skipping,\\nview-dependent colors modeling, and the losses.\\nEffectiveness of the post-activation.\\nWe show in Sec. 4\\nthat the proposed post-activated trilinear interpolation en-\\nables the discretized grid to model sharper surfaces. In Tab. 3,\\nwe compare the effectiveness of post-activation in scene re-\\nconstruction for novel view synthesis. Our grid in the ﬁne\\nstage consists of only 1603 voxels, where nearest-neighbor\\ninterpolation results in worse quality than trilinear interpola-\\ntion. The proposed post-activation can improve the results\\nfurther compared to pre- and in-activation. We ﬁnd that we\\ngain less in the real-world captured BlendedMVS and Tanks\\nand Temples datasets. The intuitive reason is that real-world\\ndata introduces more uncertainty (e.g., inconsistent light-\\nning, SfM error), which results in multi-view inconsistent\\nand blurrier surfaces. Thus, the advantage is lessened for\\nscene representations that can model sharper surfaces. We\\nspeculate that resolving the uncertainty in future work can\\nincrease the gain of the proposed post-activation.\\nEffectiveness of the imposed priors.\\nAs discussed in\\nSec. 5.1, it is crucial to initialize the voxel grid with low\\ndensity to avoid suboptimal geometry. The hyperparameter\\nα(init)(c) controls the initial activated alpha values via Eq. (9).\\nIn Tab. 4, we compare the quality with different α(init)(c) and\\nthe view-count-based learning rate. Without the low-density\\nInterp.\\nSyn.-NeRF\\nSyn.-NSVF\\nBlendedMVS\\nT&T\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\nNearest\\n28.61 -2.77\\n28.86 -6.22\\n25.49 -2.48\\n26.39 -1.27\\nTri.\\npre-\\n30.84 -0.55\\n32.66 -2.41\\n27.39 -0.58\\n27.44 -0.21\\nin-\\n29.91 -1.48\\n32.42 -2.66\\n27.29 -0.68\\n27.52 -0.13\\npost-\\n31.39\\n-\\n35.08\\n-\\n27.97\\n-\\n27.66\\n-\\nTable 3. Effectiveness of the post-activation. Geometry modeling\\nwith density voxel grid can achieve better PSNRs by using the\\nproposed post-activated trilinear interpolation.\\nα(init)(c)View.\\nlr.\\nSyn.-NeRF\\nSyn.-NSVF\\nBlendedMVS\\nT&T\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\nPSNR↑\\n∆\\n-\\n✓\\n28.88 -2.51\\n25.12 -9.96\\n22.17 -5.79\\n25.33 -2.33\\n10−3\\n✓\\n30.96 -0.42\\n27.24 -7.84\\n23.17 -4.79\\n26.04 -1.61\\n10−4\\n✓\\n31.29 -0.09\\n31.05 -4.03\\n26.09 -1.88\\n27.60 -0.05\\n10−5\\n✓\\n31.41 +0.02\\n35.04 -0.04\\n27.36 -0.61\\n27.63 -0.02\\n10−6\\n31.40 +0.01\\n35.03 -0.04\\n27.37 -0.60\\n27.59 -0.07\\n10−7\\n✓\\n31.36 -0.02\\n35.03 -0.05\\n27.73 -0.23\\n27.59 -0.06\\n10−6\\n✓\\n31.39\\n-\\n35.08\\n-\\n27.97\\n-\\n27.66\\n-\\n- / ✓\\n10−3 / ✓\\n10−6 / -\\n10−6 / ✓\\nTable 4. Effectiveness of the imposed priors. We compare our\\ndifferent settings in the coarse geometry search. Top: We show\\ntheir impacts on the ﬁnal PSNRs after the ﬁne stage reconstruction.\\nBottom: We visualize the allocated voxels by coarse geometry\\nsearch on the Truck scene. Overall, low-density initialization is\\nessential; using α(init)(c) = 10−6 and view-count-based learning\\nrate generally achieves cleaner voxels allocation in the coarse stage\\nand better PSNR after the ﬁne stage.\\ninitialization, the quality drops severely for all the scenes.\\nWhen α(init)(c) = 10−7, we have to train the coarse stage\\nof some scenes for more iterations. The effective range of\\nα(init)(c) is scene-dependent. We ﬁnd α(init)(c) = 10−6 gener-\\nally works well on all the scenes in this work. Finally, using\\na view-count-based learning rate can further improve the\\nresults and allocate noiseless voxels in the coarse stage.\\n7. Conclusion\\nOur method directly optimizes the voxel grid and achieves\\nsuper-fast convergence in per-scene optimization with NeRF-\\ncomparable quality—reducing training time from many\\nhours to 15 minutes. However, we do not deal with the\\nunbounded or forward-facing scenes, while we believe our\\nmethod can be a stepping stone toward fast convergence in\\nsuch scenarios. We hope our method can boost the progress\\nof NeRF-based scene reconstruction and its applications.\\nAcknowledgements: This work was supported in part by\\nthe MOST grants 110-2634-F-001-009 and 110-2622-8-007-\\n010-TE2 of Taiwan. We are grateful to National Center for\\nHigh-performance Computing for providing computational\\nresources and facilities.\\n5466\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\\nMip-nerf: A multiscale representation for anti-aliasing neural\\nradiance ﬁelds. In ICCV, 2021. 1, 3, 4, 6, 7\\n[2] Sai Bi, Zexiang Xu, Pratul P. Srinivasan, Ben Mildenhall,\\nKalyan Sunkavalli, Milos Hasan, Yannick Hold-Geoffroy,\\nDavid J. Kriegman, and Ravi Ramamoorthi.\\nNeural re-\\nﬂectance ﬁelds for appearance acquisition.\\narxiv CS.CV\\n2106.01970, 2020. 2\\n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\\nron, Ce Liu, and Hendrik P. A. Lensch. Nerd: Neural re-\\nﬂectance decomposition from image collections. In ICCV,\\n2021. 2\\n[4] Chris Buehler, Michael Bosse, Leonard McMillan, Steven J.\\nGortler, and Michael F. Cohen. Unstructured lumigraph ren-\\ndering. In SIGGRAPH, 2001. 2\\n[5] Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\\nand Gordon Wetzstein. Pi-gan: Periodic implicit generative\\nadversarial networks for 3d-aware image synthesis. In CVPR,\\n2021. 2\\n[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\\nizable radiance ﬁeld reconstruction from multi-view stereo.\\nIn ICCV, 2021. 1, 3, 7\\n[7] Abe Davis, Marc Levoy, and Fr´edo Durand. Unstructured\\nlight ﬁelds. Comput. Graph. Forum, 2012. 2\\n[8] Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Mod-\\neling and rendering architecture from photographs: A hybrid\\ngeometry- and image-based approach. In SIGGRAPH, 1996.\\n2\\n[9] Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan.\\nJaxNeRF: an efﬁcient JAX implementation of NeRF, 2020.\\n6, 7\\n[10] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.\\nDepth-supervised nerf: Fewer views and faster training for\\nfree. arxiv CS.CV 2107.02791, 2021. 1, 3\\n[11] Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, and\\nFederico Tombari. Peeking behind objects: Layered depth\\nprediction from a single image. Pattern Recognit. Lett., 2019.\\n2\\n[12] John Flynn, Michael Broxton, Paul E. Debevec, Matthew\\nDuVall, Graham Fyffe, Ryan S. Overbeck, Noah Snavely,\\nand Richard Tucker. Deepview: View synthesis with learned\\ngradient descent. In CVPR, 2019. 2\\n[13] Guy Gafni, Justus Thies, Michael Zollh¨ofer, and Matthias\\nNießner. Dynamic neural radiance ﬁelds for monocular 4d\\nfacial avatar reconstruction. In CVPR, 2021. 2\\n[14] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\\nDynamic view synthesis from dynamic monocular video. In\\nICCV, 2021. 2\\n[15] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie\\nShotton, and Julien P. C. Valentin. Fastnerf: High-ﬁdelity\\nneural rendering at 200fps. In ICCV, 2021. 1, 2, 3, 7\\n[16] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and\\nMichael F. Cohen. The lumigraph. In SIGGRAPH, 1996. 2\\n[17] Tong He, John P. Collomosse, Hailin Jin, and Stefano Soatto.\\nDeepvoxels++: Enhancing the ﬁdelity of novel view synthesis\\nfrom 3d voxel embeddings. In Hiroshi Ishikawa, Cheng-Lin\\nLiu, Tom´as Pajdla, and Jianbo Shi, editors, ACCV, 2020. 2\\n[18] Peter Hedman,\\nPratul P. Srinivasan,\\nBen Mildenhall,\\nJonathan T. Barron, and Paul E. Debevec. Baking neural\\nradiance ﬁelds for real-time view synthesis. In ICCV, 2021.\\n1, 2, 3, 5, 7\\n[19] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Ani-\\nmashree Anandkumar, Minsu Cho, and Jaesik Park. Self-\\ncalibrating neural radiance ﬁelds. In ICCV, 2021. 2\\n[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for\\nstochastic optimization. In ICLR, 2015. 6\\n[21] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\\nKoltun. Tanks and temples: benchmarking large-scale scene\\nreconstruction. ACM Trans. Graph., 2017. 6\\n[22] Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol\\nMoreno, Rosalia Schneider, Sona Mokr´a, and Danilo Jimenez\\nRezende. Nerf-vae: A geometry aware 3d scene generative\\nmodel. In ICML, 2021. 2\\n[23] Anat Levin and Fr´edo Durand. Linear view synthesis using a\\ndimensionality gap light ﬁeld prior. In CVPR, 2010. 2\\n[24] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering. In\\nSIGGRAPH, 1996. 2\\n[25] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\\nNeural scene ﬂow ﬁelds for space-time view synthesis of\\ndynamic scenes. In CVPR, 2021. 2\\n[26] Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely.\\nCrowdsampling the plenoptic function. In ECCV, 2020. 2\\n[27] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\\nLucey. BARF: bundle-adjusting neural radiance ﬁelds. In\\nICCV, 2021. 2\\n[28] Yen-Chen Lin, Pete Florence, Jonathan T. Barron, Alberto\\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting\\nneural radiance ﬁelds for pose estimation. In IROS, 2021. 2\\n[29] David B. Lindell, Julien N. P. Martel, and Gordon Wetzstein.\\nAutoint: Automatic integration for fast neural volume render-\\ning. In CVPR, 2021. 1, 7\\n[30] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\\nChristian Theobalt. Neural sparse voxel ﬁelds. In NeurIPS,\\n2020. 1, 2, 5, 6, 7\\n[31] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng\\nWang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang.\\nNeural rays for occlusion-aware image-based rendering. arxiv\\nCS.CV 2107.13421, 2021. 1, 3, 7\\n[32] Stephen Lombardi, Tomas Simon, Jason M. Saragih, Gabriel\\nSchwartz, Andreas M. Lehrmann, and Yaser Sheikh. Neural\\nvolumes: learning dynamic renderable volumes from images.\\nACM Trans. Graph., 2019. 2, 7\\n[33] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj-\\njadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel\\nDuckworth. Nerf in the wild: Neural radiance ﬁelds for\\nunconstrained photo collections. In CVPR, 2021. 2\\n[34] Nelson L. Max. Optical models for direct volume rendering.\\nIEEE Trans. Vis. Comput. Graph., 1995. 3\\n[35] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\\nLan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neural\\nradiance ﬁeld without posed camera. In ICCV, 2021. 2\\n5467\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[36] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz Cayon,\\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\\nAbhishek Kar. Local light ﬁeld fusion: practical view syn-\\nthesis with prescriptive sampling guidelines. ACM Trans.\\nGraph., 2019. 2\\n[37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance ﬁelds for view synthe-\\nsis. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[38] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya\\nHarada. Neural articulated radiance ﬁeld. In ICCV, 2021. 2\\n[39] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Soﬁen\\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\\nMartin-Brualla. Deformable neural radiance ﬁelds. In ICCV,\\n2021. 2\\n[40] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\\nBarron, Soﬁen Bouaziz, Dan B. Goldman, Ricardo Martin-\\nBrualla, and Steven M. Seitz.\\nHypernerf:\\nA higher-\\ndimensional representation for topologically varying neural\\nradiance ﬁelds. arxiv CS.CV 2106.13228, 2021. 2\\n[41] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\\nFrancesc Moreno-Noguer. D-nerf: Neural radiance ﬁelds\\nfor dynamic scenes. In CVPR, 2021. 2\\n[42] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,\\nKwang Moo Yi, and Andrea Tagliasacchi. Derf: Decom-\\nposed radiance ﬁelds. In CVPR, 2021. 1\\n[43] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\\nGeiger. Kilonerf: Speeding up neural radiance ﬁelds with\\nthousands of tiny mlps. In ICCV, 2021. 1, 3, 7\\n[44] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\\nGeiger. GRAF: generative radiance ﬁelds for 3d-aware image\\nsynthesis. In NeurIPS, 2020. 2\\n[45] Jonathan Shade, Steven J. Gortler, Li-wei He, and Richard\\nSzeliski. Layered depth images. In SIGGRAPH, 1998. 2\\n[46] Lixin Shi, Haitham Hassanieh, Abe Davis, Dina Katabi, and\\nFr´edo Durand. Light ﬁeld reconstruction using sparsity in the\\ncontinuous fourier domain. ACM Trans. Graph., 2014. 2\\n[47] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\\nHuang. 3d photography using context-aware layered depth\\ninpainting. In CVPR, 2020. 2\\n[48] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\\nNießner, Gordon Wetzstein, and Michael Zollh¨ofer. Deep-\\nvoxels: Learning persistent 3d feature embeddings. In CVPR,\\n2019. 2, 6\\n[49] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein.\\nScene representation networks: Continuous 3d-structure-\\naware neural scene representations. In NeurIPS, 2019. 7\\n[50] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\\nTancik, Ben Mildenhall, and Jonathan T. Barron. Nerv: Neu-\\nral reﬂectance and visibility ﬁelds for relighting and view\\nsynthesis. In CVPR, 2021. 2\\n[51] Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron,\\nRavi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the\\nboundaries of view extrapolation with multiplane images. In\\nCVPR, 2019. 2\\n[52] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\\nSchmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren\\nNg. Learned initializations for optimizing coordinate-based\\nneural representations. In CVPR, 2021. 2\\n[53] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features\\nlet networks learn high frequency functions in low dimen-\\nsional domains. In NeurIPS, 2020. 3\\n[54] Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. De-\\nferred neural rendering: image synthesis using neural textures.\\nACM Trans. Graph., 2019. 2\\n[55] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\\nZollh¨ofer, Christoph Lassner, and Christian Theobalt. Non-\\nrigid neural radiance ﬁelds: Reconstruction and novel view\\nsynthesis of a deforming scene from monocular video. In\\nICCV, 2021. 2\\n[56] Richard Tucker and Noah Snavely. Single-view view synthe-\\nsis with multiplane images. In CVPR, 2020. 2\\n[57] Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer-\\nstructured 3d scene inference via view synthesis. In ECCV,\\n2018. 2\\n[58] Michael Waechter, Nils Moehrle, and Michael Goesele. Let\\nthere be color! large-scale texturing of 3d reconstructions. In\\nECCV, 2014. 2\\n[59] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srini-\\nvasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-\\nBrualla, Noah Snavely, and Thomas A. Funkhouser. Ibrnet:\\nLearning multi-view image-based rendering. In CVPR, 2021.\\n1, 3, 7\\n[60] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.\\nSimoncelli. Image quality assessment: from error visibility\\nto structural similarity. IEEE TIP, 2004. 6\\n[61] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\\ntor Adrian Prisacariu. Nerf-: Neural radiance ﬁelds without\\nknown camera parameters. arxiv CS.CV 2102.07064, 2021. 2\\n[62] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\\nview synthesis with neural basis expansion. In CVPR, 2021.\\n2, 3\\n[63] Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Cur-\\nless, Tom Duchamp, David Salesin, and Werner Stuetzle.\\nSurface light ﬁelds for 3d photography. In SIGGRAPH, 2000.\\n2\\n[64] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\\nKim. Space-time neural irradiance ﬁelds for free-viewpoint\\nvideo. In CVPR, 2021. 2\\n[65] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\\nscale dataset for generalized multi-view stereo networks. In\\nCVPR, 2020. 6\\n[66] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\\nneural radiance ﬁelds. In ICCV, 2021. 1, 2, 3, 5, 7\\n[67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\\npixelnerf: Neural radiance ﬁelds from one or few images. In\\nCVPR, 2021. 3\\n[68] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\\nKoltun. Nerf++: Analyzing and improving neural radiance\\nﬁelds. arxiv CS.CV 2010.07492, 2020. 3\\n5468\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[69] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. In CVPR, 2018. 6\\n[70] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul E.\\nDebevec, William T. Freeman, and Jonathan T. Barron. Ner-\\nfactor: Neural factorization of shape and reﬂectance under an\\nunknown illumination. arxiv CS.CV 2106.01970, 2021. 2\\n[71] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magniﬁcation: learning view syn-\\nthesis using multiplane images. ACM Trans. Graph., 2018.\\n2\\n5469\\n', metadata={'source': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'file_path': 'data/Sun et al. - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': 'Direct Voxel Grid Optimization: Super-Fast Convergence for Radiance Fields Reconstruction', 'author': 'Cheng Sun;  Min Sun;  Hwann-Tzong Chen', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\nAvailable online 15 April 2022\\n0895-6111/© 2022 Elsevier Ltd. All rights reserved.\\nXctNet: Reconstruction network of volumetric images from a single \\nX-ray image \\nZhiqiang Tan a,b, Jun Li a,b, Huiren Tao c, Shibo Li a,*, Ying Hu a,* \\na Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen Key Laboratory of Minimally Invasive Surgical Robotics and System, Shenzhen \\n518055, China \\nb University of Chinese Academy of Sciences, CAS, Beijing 100049, China \\nc Department of Orthopaedics, Shenzhen University General Hospital，Shenzhen University Clinical Medical Academy, Shenzhen 518055, China   \\nA R T I C L E  I N F O   \\nKeywords: \\n3D reconstruction \\nSelf-attention module \\nDeep learning \\nX-ray \\nVolumetric images \\nA B S T R A C T   \\nConventional Computed Tomography (CT) produces volumetric images by computing inverse Radon trans-\\nformation using X-ray projections from different angles, which results in high dose radiation, long reconstruction \\ntime and artifacts. Biologically, prior knowledge or experience can be utilized to identify volumetric information \\nfrom 2D images to certain extents. a deep learning network, XctNet, is proposed to gain this prior knowledge \\nfrom 2D pixels and produce volumetric data. In the proposed framework, self-attention mechanism is used for \\nfeature adaptive optimization; multiscale feature fusion is used to further improve the reconstruction accuracy; a \\n3D branch generation module is proposed to generate the details of different generation fields. Comparisons are \\nmade with the state-of-arts methods using public dataset and XctNet shows significantly higher image quality as \\nwell as better accuracy (SSIM and PSNR values of XctNet are 0.8681 and 29.2823 respectively).   \\n1. Introduction \\nImage based 3D reconstruction, which is to infer 3D shapes from \\nsingle or multiple 2D images, has been explored in the field of computer \\nvision for decades (Han et al., 2021) and has become the basis of many \\nfields, such as robot navigation, 3D modeling and animation, object \\nrecognition, scene understanding, medical diagnosis, etc. However, it is \\nnot straightforward to extract volumetric information from digital im-\\nages without disparity knowledge from stereo correspondence, due to \\nthe lack of approaches to derive depth information from pixels. Pro-\\njectional radiography, a conventional way to observe the inside of ob-\\njects or bodies, is no different than normal photography, except that the \\npixels carry rich observations of transparent volumetric structure other \\nthan opaque surface. Specifically, in X-ray radiographs, each pixel is the \\nline integral of attenuation data following Radon transformation in 2D \\nspace, so that the inversion of radiographs is not 2D–3D point conver-\\nsion but the conversion from 2D pixel to spatial line distribution. \\nTherefore, 3D reconstruction from radiographs is an even more chal-\\nlenging task. \\nClinically, there are only limited number of available approaches for \\n3D reconstruction. Computed tomographic (CT), a well-developed way \\nof radiograph-based 3D reconstruction, is the commonly used way to \\nobtain patients’ volumetric information and has many variations such as \\nCBCT, PET-CT, etc. CT is inherently an inverse Radon transformation \\nprocess, in which the spatial distribution function of the X-ray attenu-\\nation is solved by Inverse Fourier Transformation (IFT), so that the \\nangular integral of projection views from all directions would be \\ncalculated. In practice, projections from a large number of different \\nangular positions are requisite in order to maintain acceptable resolu-\\ntion and mitigate physics-based artifacts of the tomographs. The \\nreconstruction process intrinsically determines the inevitable limita-\\ntions of CT, such as high radiation, long reconstruction time and patient- \\nmovement-based artifacts. Other than CT, the novel EOS imaging system \\noffers a better alternative for full-body biplanar X-ray scan and 3D \\nreconstruction of the whole skeleton. The new technology is extremely \\nhelpful in diagnosis of orthopedic diseases, such as adolescent idiopathic \\nscoliosis (AIS) and adult degenerative knee arthritis (Lenke et al., 2001; \\nOvadia, 2013). However, the reconstruction process of EOS imaging is \\nbased on statistical shape models (SSMs), so that the obtained model is \\nnot the exactly same reflection of the patient but a semantically similar \\nvirtual one instead. \\nBiologically, although our eye-brain vision system does not make 3D \\nreconstruction from plain pixels, we can still partially obtain the hidden \\nspatial information from subtle evidence like: shadow, occlusion, light/ \\n* Corresponding authors. \\nE-mail addresses: zq.tan@siat.ac.cn (Z. Tan), jun.li@siat.ac.cn (J. Li), huiren_tao@163.com (H. Tao), sb.li@siat.ac.cn (S. Li), ying.hu@siat.ac.cn (Y. Hu).  \\nContents lists available at ScienceDirect \\nComputerized Medical Imaging and Graphics \\njournal homepage: www.elsevier.com/locate/compmedimag \\nhttps://doi.org/10.1016/j.compmedimag.2022.102067 \\nReceived 18 December 2021; Received in revised form 15 March 2022; Accepted 8 April 2022   \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 0, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n2\\nshade, relative size, etc. The knowledge we use in this evidence-based \\nstereo reconstruction process can be defined as prior knowledge, \\nwhich plays an essential part in human vision-based judgment and \\nidentification system. When we look at photographs or images by bare \\neyes, the relative spatial relation of the objects or bodies could always be \\ndeduced by combining pixel information with prior knowledge. Simi-\\nlarly, radiologists are able to tell the spatial information of human \\nbodies from radiographs by applying prior knowledge from anatomy \\nand everyday practice. Therefore, from the aspect of biomimetic, prior \\nknowledge has potential to reconstruct 3D information at least partially \\nfrom radiographs theoretically. \\nDeep learning, which shows great advantages over traditional \\nmethods in fitting complex nonlinear mathematical relations, has \\nbrought an evolution to numerous medical fields such as medical image \\nsegmentation, lesion area recognition, medical image registration, etc. \\n(Feng et al., 2020; Schwartz et al., 2019; Singh et al., 2020). The pos-\\nsibilities of 3D reconstruction from 2D radiographs have not been \\nobserved for long, but only in recent years does the attempts of the in-\\nverse mapping emerge. Henzler et al. (2018) first, to our knowledge, \\napplied \\na \\ndeep \\nConvolutional \\nNeural \\nNetwork \\n(CNN) \\nto \\nsingle-radiograph tomography and reconstructed 3D cranial volumes \\nfrom 2D X-rays. Kasten et al. (2020) used an end-to-end CNN for 3D \\nreconstruction of knee bones from bi-planar X-ray images. Shen et al. \\n(2019) developed a deep network system with representation, trans-\\nformation, generation modules to generate volumetric tomography im-\\nages from single or multiple 2D X-rays. Through the current literature \\nresearch, it can be found that the current CNN-based reconstruction \\nmethods use the end-to-end network structure, which will cause a \\ncertain loss of image resolution due to the network sampling process; \\nSecondly, the task of CT volumetric images reconstruction based on \\nX-ray image is quite computationally expensive. Thus, this paper con-\\nstructs a lightweight CNN-based reconstruction network, XctNet, which \\ncan not only improve the information loss caused by the sampling pro-\\ncess, but also greatly reduce the required computing resources. To \\nsummarize, the contribution can be seen as follow:  \\n• This paper constructs a lightweight CNN based reconstruction \\nnetwork, XctNet, which can also ensure the reconstruction accuracy \\nof the network on the premise of greatly reducing the required \\ncomputing resources.  \\n• We attempt to add attention mechanism and multi-scale feature \\nfusion module into the feature extraction process to redundant fea-\\ntures on the reconstructed image and further improve the pixel loss \\nin the reconstruction process.  \\n• We propose a 3D branch generation module, namely New Inception \\nmodule, which can better generate the details of different generation \\nfields by using different sizes of convolution kernels. \\n2. Related work \\n2.1. 2D–3D reconstruction via deep learning \\nVarious deep learning algorithms have been proposed in 3D recon-\\nstruction of natural images, including supervised learning, unsupervised \\nlearning and semi supervised learning etc. (Han et al., 2019). Wu et al. \\n(2015) proposed a convolutional deep belief network to represent a \\ngeometric 3D shape (3D ShapeNet) as a probability distribution of bi-\\nnary variables on a 3D voxel grid and also constructed ModelNet in \\norder to train 3D deep learning model. Wu et al. (2016) used generative \\nadversarial network to generate 3D objects from a probabilistic space by \\nleveraging recent advances in volumetric convolutional networks and \\ngenerative adversarial nets (3D-GAN). Wang et al. (2017) introduced a \\nhybrid framework, which combined a 3D Encoder-Decoder Generative \\nAdversarial Network (3D-ED-GAN) and a Long-term Recurrent Con-\\nvolutional Network (LRCN), and their model was fit into GPU memory \\ncompared with other 3D CNN methods. Li et al. (2017) introduced a \\nGenerative Recursive Autoencoder for Shape Structures (GRASS) and \\nproved that without supervision, their network can learn meaningful \\nstructural hierarchies. Yan et al. (2016) formulated an encoder-decoder \\nnetwork for predicting 3D models from a single-view 2D image. Choy \\net al. (2016) designed a recurrent network to reconstruct 3D models \\nfrom a sequence of multi-view images. \\n2.2. Reconstruction of volumetric images from X-rays \\nCT reconstruction is an inverse mapping mathematical process, \\nwhich generates tomographic images from X-ray projection data ac-\\nquired at many different angles around the patient (Stierstorfer et al., \\n2004). The quality of reconstruction has a fundamental impact on the \\nradiation dose used and the researchers are trying to find better recon-\\nstruction algorithm to ensure both the accuracy and resolution of the \\nreconstructed image while minimizing radiation dose (Kak and Slaney, \\n1987; Hsieh, 2003). \\nA multi-detector spiral CT reconstruction method is proposed based \\non cone beam geometry (Taguchi and Aradate, 1998). Hu (1999) stud-\\nied the scanning and reconstruction principles of multi-slice spiral CT, \\nespecially the scanning and reconstruction principles of 4-slice spiral CT, \\nand concluded that the volume coverage speed of 4-slice spiral CT is 2–3 \\ntimes that of single-slice spiral CT, which can provide the same image \\nquality. Schaller et al. (2001) introduced a high-quality image recon-\\nstruction approach for helical CBCT and Flohr et al. (2003) proved its \\neffectiveness in a 16-slice CT scanner. \\nThe EOS system, originated from the Nobel prize-winning invention \\nMWPC (the Multiwire Proportional Chamber) particle detector by Dr. \\nGeorges Charpak, is able to produce full-body stereo images of patients \\nusing biplanar low-dose X-ray scan and is regarded as the most advanced \\nimage acquisition equipment in orthopedics at present (Melhem et al., \\n2016; Song et al., 2020). Rehm et al. (2017) compared EOS imaging \\nequipment with CT imaging equipment and showed that the EOS system \\ncan obtain high-quality images with less doses. Post et al. (2018) pro-\\nposed a three-dimensional spine classification method based on the EOS \\nsystem. However, the 3D reconstruction of EOS depends on parametric \\nmodels and statistical inferences from collected biplanar X-ray scans, so \\nthat the generated skeleton model is only a parametric virtual substitute \\nand is limited in circumstances of severe skeletal malformations or ab-\\nnormalities like congenital scoliosis (CS) and ankylosing spondylitis \\n(AS). \\nIn recent years, deep learning has been widely adopted in the field of \\nmedical imaging. Meng et al. (2020) used semi-supervised learning to \\nreconstruct high-dose volumetric images from low-dose volumetric \\nimages. Henzler et al. (2018) proposed a deep convolution to generate \\n3D images from a single X-ray animal skull image. Its network archi-\\ntecture adopts an end-to-end structure, and compared with some pre-\\nviously proposed network structures, it proved that their network can \\nachieve better reconstruction results. Shen et al. (2019) proposed a 2D to \\n3D network model architecture and brought the idea of converting the \\n2D feature information into a spatial tensor in order to perform a 3D \\ndeconvolution. However, Shen’s reconstruction network includes \\nenormous amount of parameters to update, which leads to computa-\\ntional inefficiency. Multiple studies on machine-learning-based 3D \\nreconstruction have also been carried out in the field of dentistry, spine, \\nchest, etc (Ying et al., 2019; Bayat et al., 2020; ˇCavojsk´a et al., 2020). \\nThe mentioned works also have limitations in generalization and tend to \\nunderperform on different datasets in practice. In this paper, a more \\nlightweight CNN-based network is constructed to improve the recon-\\nstruction accuracy and reduce the computational cost. \\n3. Methodology \\nThe architecture of XctNet reconstruction network, shown in Fig. 1, \\nincludes the two major parts:The X-ray feature extraction module, \\nMulti-scale feature fusion module and the volumetric image generation \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 1, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n3\\nmodule. The details of each module and the loss function will be \\nexplained in the following sections. \\n3.1. XctNet reconstruction network architecture \\nX-ray feature extraction module: The introduction of deep residual \\nnetwork is to solve the problem of gradient disappearance caused by too \\nmany network layers in the training process. The most representative \\nstructure of the residual network is ResNet (He et al., 2016), which \\ndirectly connects the input terminal to the following layer through the \\nshortcut structure, thereby protecting the integrity of the transmitted \\ndata. The feature extraction module is built based on ResNet34. The \\ninput is a single X-ray image which size is\\n128 × 128, the first layer of \\nthe model is composed of a convolutional layer with a kernel size of \\n7 × 7 and stride 2, and the second to fifth layers are composed of four \\nresidual blocks which contains two\\n3 × 3 convolutional layers. The \\nnumber of channels of the convolutional layer in each residual block is \\nkept the same to ensure that the shortcut path and the residual path can \\nmaintain the same size during the element-wise addition operation. The \\nsize of the feature representation output is\\n4 × 4. In addition, through \\nexperiments, we find that when using the encoder/decoder structure for \\npixel level vision tasks, the convolution layer can only use local infor-\\nmation to calculate the target pixel value. Therefore, the lack of global \\ninformation will undoubtedly lead to deviation. The error caused by the \\nconvolution layer can be described by the covariance between the pixel \\nvalues shown in Eq. (1). Each pixel value xi in the feature map obtained \\nby the convolution layer can be used as a random variable, and x\\nis the \\nmean value of the feature map. The similarity between the two variables \\ncan be evaluated by calculating the covariance of the two random var-\\niables. The attention mechanism is to use the similarity between pixels \\nto improve the performance of convolution layer. \\nCov(x, y) =\\n1\\nN − 1\\n∑\\nN\\ni=1\\n(xi − x)(yi − y)\\n(1) \\nIn order to reduce the error caused by the convolution process, this \\npaper introduces two attention mechanisms, CBAM (Convolutional \\nBlock Attention module) (Woo et al., 2018) and ECA (Efficient channel \\nAttention module) module (Wang et al., 2020), to adaptively improve \\nthe feature extraction ability and reduce the error. Specifically, CBAM \\nderives the attention graph from the 2D information of space and \\nchannel, then, the attention graph with the input features will be \\nmultiplied to adaptively optimize the eigenvalues. The module structure \\nis shown in Fig. 1(b). In the 2D feature extraction module, CBAM is \\nmainly used for convolution feature extraction of the first layer and the \\nlast layer, so as to improve the ability of feature adaptive extraction on \\nthe premise of ensuring that the overall network structure is not \\naffected. For the intermediate convolution layer, ECA module is used to \\nimprove its feature extraction performance. As a local cross-channel \\ninteraction module that does not reduce the feature dimension, ECA \\nmodule obtains local cross-channel interaction information by \\ncombining each channel and its adjacent k channels. ECA module can be \\nrealized by one-dimensional convolution layer with the size of k. It is \\nworth noting that ECA, as a lightweight module, does not add a large \\nnumber of additional parameters. The network structure diagram is \\nshown in Fig. 1(c). In this paper, the feature extraction capability of the \\nmiddle layer is improved by combining the residual module and ECA \\nmodule. The feature map obtained by the residual module will be input \\ninto the ECA module for adaptive optimization. In addition, the feature \\nmap obtained by the residual module will also be combined with the \\nadaptively optimized feature map through element-wise product, so as \\nto get a refined feature map. \\nMulti-scale feature fusion module: CNN based reconstruction \\nnetwork structure extracts 2D features layer by layer through down \\nFig. 1. Architecture of XctNet. The model contains X-ray feature extraction module, multi-scale feature fusion module and the volumetric images generation \\nmodule. The input of the model is a single 2D projection image. The X-ray feature extraction module extract feature information from the input X-ray image. The \\nmulti-scale feature extraction module converts 2D features into 3D features and performs feature fusion with the corresponding 3D generation module. The volu-\\nmetric images generation module, which consists of a series of New Inception module, uses the extracted feature data to generate the corresponding volu-\\nmetric image. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 2, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n4\\nsampling and then reconstructs 3D information through up sampling \\noperation. In the down sampling process, the shallow network has \\nstrong semantic information representation ability, but lacks spatial \\ngeometric details; the deep network has strong representation ability of \\ngeometric detail information, but it lacks semantic representation abil-\\nity. The traditional encoder/decoder structure directly inputs the fea-\\ntures to the decoder for up sampling after the down sampling process. \\nFor the volume image reconstruction task, volume images usually have \\naplenty of detail information. While the image generated based on the \\ntraditional encoder/decoder structure will lack a lot of detail informa-\\ntion. In order to solve the loss of detail in volumetric images and improve \\nthe fine-grained features of network generated images, a multi-scale \\nfeature fusion method is proposed in this paper. \\nThe process of multi-scale convolution mainly includes two factors: \\nfeature propagation and cross-scale communication (Feng et al., 2020). \\nIn the multi-scale feature extraction structure proposed in this paper, the \\ninput feature will be divided into high-scale feature\\nXhigh and low-scale \\nfeature\\nXlow to obtain the corresponding high-scale feature out-\\nput\\nYhigh\\nand low-scale feature output Ylow respectively. The \\nmulti-scale feature transform process can be seen as follow: \\n[\\nYhigh\\nYlow\\n]\\n=\\n[\\nI\\n0\\n0\\nω\\n][\\nXhigh\\nXlow\\n]\\n(2) \\nEq. (2) can be encapsulated as the aggregating transformation per-\\nformed on the input feature maps. Where I refers to the information \\nmapping, ω represents the transformation in the same scale. In other \\nwords, high-scale features will be connected with the corresponding \\ngeneration module through layer skipping connection, while low-scale \\nfeatures will be down sampled through a series of convolution layers. \\nThe overall network structure is shown in Fig. 1(a). Feature maps of \\ndifferent layers are extracted from the original network structure and \\nconverted into corresponding 3D feature maps through transform \\nmodule and then the multi-scale 3D feature maps are combined with \\ncorresponding 3D feature generation layers, so as to improve the fine \\ngranularity of generation results and reduce the loss of information in \\nthe reconstruction process. For details, to convert 2D projection data to \\nvolumetric images data requires data conversion. A transform module is \\nadded to bridge the 2D feature extraction module and the 3D generation \\nmodule. the multi-scale 2D features, which size is(C, H, W), are con-\\nverted to (C, 1, H, W) by the dimension conversion function, after that, \\nthe converted 3D feature map, which size is (C, D, H, W) can be obtained \\nthrough a deconvolution operation with a kernel size of \\nD × 1 × 1. In \\naddition, the ReLU activation function and the batch normalization \\nfunction are also included to better learn the transformation relationship \\nin the transform process. \\nVolumetric image generation module: Inspired by the Inception \\nstructure of GoogleNet (Szegedy et al., 2015), which can solve over-\\nfitting and gradient disappearance problems, a 3D deconvolution form \\nof the Inception structure (New Inception) is added to the 3D generation \\nnetwork, which is composed of a 3D point-wise convolution with a \\nkernel size of 1 × 1 × 1 and two 3D deconvolution with kernel size of \\n3 × 3 × 3 as well as\\n5 × 5 × 5. As can be seen in the Fig. 1(d), the 3D \\npoint-wise convolution layer for the deconvolution module can super-\\nimpose more deconvolutions in the generation field of the same size, \\nthereby more details could be obtained in the generated images. In \\naddition, 3D point-wise convolution also plays a fundamental role in \\ndimensionality reduction. Performing 3D deconvolution operations will \\ngenerate a huge amount of calculation. The number of input features can \\nbe effectively reduced by adding 3D point-wise convolution, so as to \\nincrease the computational efficiency. The New Inception structure is \\ncomposed of two branches, each of which uses filters of different sizes \\nfor deconvolution. The branches can generate information of different \\nscales and generate richer results. The New Inception structure uses the \\nprinciple of decomposing a sparse matrix into a dense matrix for \\ncalculation. The feature dimension is decomposed into multiple densely \\ndistributed sub-feature sets. The highly correlated features are clustered \\ntogether and the unrelated features will be weakened. Finally, they will \\nbe spliced together in the feature dimension and consistent with the \\ninput dimension. This approach reduces the calculation cost and ensures \\nthat the final training results will not be affected. \\n3.2. Data pre-processing \\nThe original data needs to be preprocessed before fed into the \\nnetwork model for training. First of all, all input data need to be resized \\nto the same size. The 2D images and the corresponding 3D CT images \\nused for training are resized to 128 × 128 and 128 × 128 × 128 sepa-\\nrately. In practice, 2D–3D data pairs should be composed of X-ray and \\nCT images, owing to the lack of corresponding paired images, this paper \\nuses digitally reconstructed radio algorithm (DRR) to generate an \\napproximate single 2D projection image to obtain the corresponding \\n2D–3D data pairs. As shown in the Fig. 2, this article uses point source \\nvision based DRR projection algorithm to generated 2D projection \\n(Moturu and Chang, 2018). The advantage of this method is that the \\npoint source can be randomly selected to obtain X-rays, which makes the \\ndata change slightly. Specifically, after the light source point is selected \\nand the projection distance is fixed (centered on the front of the CT \\nvolumetric image), 2D projections are generated according to Beer’s law \\n(Feeman, 2010), in which the intensity loss measurement of X-rays \\npassing through the body is modeled by Beer’s law. The information \\n(spacing, size, direction) of the CT image is obtained and the image is \\nused as the input of the DRR algorithm, and then the image is resampled \\nby coordinate transformation. Moreover, we set the distance from the \\nlight source to the projection plane to 400 mm, and the default pixel \\nspacing of the projection pixel plane is 0.8 × 0.8 and set the threshold to \\n− 80, and the bilinear interpolation is used to integrate each voxel plane \\ntraversed, so as to obtain the anterior-posterior positions of the 2D \\nprojected image. In order to enrich the sample size of training data, data \\naugmentation, which includes scale change, rotation change, mirror \\nimage and translation change, brightness change, chroma change, \\ncontrast change and sharpness change, is performed before training. \\nMoreover, the pixel-wise input data is normalized to the interval [0,1]. \\n3.3. Evaluation metrics \\nIn order to evaluate the performance of the model, we tested the \\ntrained model on the test set and used different evaluation metrics to \\nevaluate the predicted reconstruction results. Four evaluation functions \\nis used in this paper for model evaluation, namely: MSE (mean squared \\nerror), MAE (mean absolute error), SSIM (structural similarity) and \\nPSNR (peak signal noise ratio). MSE and MAE are used to evaluate the \\ndeviation between the predicted reconstruction result and the target \\nvalue. The smaller MSE/MAE value, the closer the reconstruction result \\nis to the real situation. The image evaluation metric SSIM, incorporating \\nthe information of luminance, contrast and structures, is used to eval-\\nuate the degree of similarity between images. The commonly used PSNR \\nis applied to evaluate the quality of our reconstructed volumetric im-\\nages. Generally, the resultant images with better structural and higher \\nresolution will have higher SSIM and PSNR values. Each metric value is \\naveraged for all test samples and different methods are compared as \\nFig. 2. DRR projection algorithm based on point source vision.  \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 3, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n5\\nshown in Table 2. \\n4. Reconstruction experiments results \\n4.1. Datasets \\nThe input sample consists of a single 2D projection image X and \\nvolumetric CT images Y. In the training process, with the single X-ray \\nimage as input X ∈ RH×W, the model output is volumetric \\nimages\\nYpre ∈ RC×H×W, while YGT ∈ RC×H×Wis the ground truth which \\nis the reference standards for model training. \\nIn order to verify the effectiveness of the model, we used the public \\ndataset, the Lung Image Database Consortium image collection (LIDC- \\nIDRI) (Armato et al., 2011), which contains 1081 CT volumetric image \\ncases. The original data will be divided into training set, test set and \\nverification set separately, on this basis, the original data is expanded to \\n59,708 cases through data augmentation. Among them, the training set \\nis 35,825 cases, the verification set is 11,941 cases, and the test set is 11, \\n942 cases. At the same time, the corresponding input X of each case is \\ngenerated by DRR projection. \\n4.2. Training details \\nThe size of the input X is 128 × 128 and the size of the ground truth \\nYGT\\nis 128 × 128 × 128. The network is trained on a device with three \\nNVIDIA Tesla V100 graphic processing units, and the training platform \\nis Pytorch. Training epoch is 64. As an important parameter in deep \\nlearning training, it is particularly important to select the appropriate \\nlearning rate. This paper constructs an adaptive learning rate adjustment \\nstrategy, which can modify the learning rate according to the specific \\nsituation in the training process, so as to ensure the best effect of \\ntraining. As shown in Eq. (3), the loss function used in all three trained \\nnetworks was MSE. The training results were shown in Fig. 3. \\nLMSE(Y, YGT) = 1\\nN\\n∑\\nN\\nn=1\\n(YGTn − Yn)2\\n(3) \\nIn addition, to verify the effectiveness of the multi-scale feature \\nmodule and attention mechanism feature extraction module proposed in \\nthis paper, we construct three network models with different structures \\n(ResXct, CBAM/ECAXct, XctNet) and reproduce the network structure \\nproposed by Shen et al. (2019), namely ReconNet, The specific structure \\nis shown in the Table 1: \\n4.3. Training result analysis \\nWe show the reconstruction results of ResXct, CBAM/ECAXct, XctNet \\nas well as ReconNet on the LIDC-IDRI data set. These abnormalities can \\nfurther prove the effectiveness of the XctNet model. \\nAs shown in Fig. 3, a test sample is randomly selected to show the \\ngeneration results of the different model and the numbers of the selected \\nslices are 3, 15, 35, 65 and 85. The results shown in Fig. 3(a)–(d) are slice \\nimages which are randomly selected from the test sample; Fig. 3(e) is the \\ncorresponding ground truth. From the overall result of reconstruction, \\nthe result generated by our XctNet is closer to the ground truth. In terms \\nof details, the content of the slice image generated by the original \\nversion model is relatively vague; compared with the ReconNet model, \\nthe overall contour of the intermediate version model is clearer, and \\nsome internal details, such as rib areas, can be reconstructed. On the \\nother hand, from the chest slice data at different positions, the best \\nreconstruction detail is the bony area, while the reconstruction accuracy \\nof internal organs in the chest is blurred to varying degrees. Besides, by \\nrandomly selecting multiple test data for analysis, it can be found that in \\nthe reconstructed volumetric data, the reconstruction performance of \\nthe middle of the volume is generally better than that of the front of the \\nvolume and the end of the volume. The main reasons for this phenom-\\nenon are as follows: First of all, owing to this paper is to reconstruct the \\nFig. 3. Volumetric image examples from the test set. (a)–(d) represent the results generated by the ReconNet, ResXct, CBAM/ECAXct and XctNet respectively; (e) \\nis the ground truth. The results shown in the figure comes from different slice graphs in a volumetric image randomly selected. \\nTable 1 \\nStructural details between different networks.   \\nRepresentation \\nnetwork (Layer) \\nVolumetric \\nimage \\ngeneration \\nmodule (Layer) \\nAttention \\nmodule \\nMulti-scale \\nfeature \\nfusion \\nmodule \\nReconNet \\n10 \\n10 \\n/ \\n/ \\nResXct \\n5 \\n5 \\n/ \\n/ \\nCBAM/ \\nECAXct \\n10 \\n5 \\n√ \\n/ \\nXctNet \\n10 \\n5 \\n√ \\n√  \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 4, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n6\\nwhole thoracic cavity, we do not preprocess according to the HU values \\nof different tissues and organs, but process the CT data of the whole \\nthoracic cavity. Therefore, there will be some deviation for the details \\nsuch as internal organs in the reconstruction process. Secondly, due to \\nthe different sources of the data sets, the quality of the thoracic cavity \\narea can not be guaranteed to be completely consistent, resulting in the \\nquality of the front and end volume data of the reconstructed CT volu-\\nmetric image will be relatively worse than that of the middle volume. It \\ncan be seen that the main reason for this phenomenon is due to the \\ncomplexity of data, but this does not mean that our model has limita-\\ntions. As can be seen, compared with the above different models, XctNet \\nhas the best reconstruction performance in terms of overall contour and \\ninternal details. \\nTo show the details of the differences in results between different \\nmodels, as can be seen in Fig. 4, gray values indicate areas with insig-\\nnificant differences, while white and black values represent areas with \\nlarge differences. It can be seen from the figure that XctNet has the \\nsmallest difference with ground truth compared with other models. \\n4.4. Comparison with state-of-the-art \\nIn order to make a quantitative analysis of XctNet and the proposed \\ncontrast network, four evaluation metric functions are used to analyze \\nthe difference between the predicted reconstructed image and the \\nground truth. In addition, by comparing the evaluation metric differ-\\nences between the models, the effectiveness of attention mechanism and \\nmulti-scale fusion module can be further illustrated. \\nIt is worth noting that the volumetric data used in this paper is \\ncomposed of 128 slices of data. It is worth noting that the volumetric \\ndata used in this paper are composed of 128 slices of data. The 128 slices \\nof volumetric data is evaluated separately by using different evaluation \\nmetric functions, and then obtain the final evaluation result by taking \\nthe average of all slices.As shown in Table 2, our XctNet can achieve the \\nbest evaluation results, and its PSNR and SSIM can reach 29.2823 and \\n0.8681 respectively. Incidentally, all the evaluation metric values in \\nTable 2 are the mean values of the test samples. On the other hand, the \\nevaluation results obtained by ReconNet perform better than our base-\\nline model, RexXct, which shows that increasing the network depth is \\neffective for the reconstruction results. In addition, the evaluation re-\\nsults of CBAM/ECAXct are similar to ReconNet, that is, adding light-\\nweight attention mechanism is also an effective method to enhance the \\nperformance of the model without increasing the network depth. \\nFrom the overall distribution of the evaluation results of the test set, \\nas shown in Fig. 5, the four violin graphs represent the results of \\ndifferent evaluation functions. Overall, XctNet achieves the better re-\\nsults in all evaluation metrics. On the other hand, as shown in Fig. 5(c), \\nthe distribution of PSNR values of all models are mostly concentrated \\nnear the inferior quartile, that is mainly because PSNR evaluates the \\ngray difference between images and due to the data set used in this paper \\nis complicated, the prediction results are usually different. From the \\ncomparison results, the interquartile range (IQR) of XctNet is smaller \\nthan the other three models, which shows that XctNet model is more \\nstable. Through the above data analysis, we can get the conclusions that \\nself-attention mechanism and multi-scale feature fusion module can \\nFig. 4. Comparison of deviation with respect to ground truth. The first column correspond to the ground truth. The second column shows the difference between \\nReconNet and ground truth. Other columns represent the difference between the corresponding model and the ground truth. \\nTable 2 \\nEvaluation on reconstruction results of lung CT cases.   \\nMAE \\nMSE \\nPSNR \\nSSIM \\nReconNet  \\n0.0211  \\n0.0017  \\n28.4891  \\n0.8349 \\nResXct  \\n0.0213  \\n0.0019  \\n27.8489  \\n0.8250 \\nCBAM/ECAXct  \\n0.0205  \\n0.0017  \\n28.1859  \\n0.8351 \\nXctNet  \\n0.01764  \\n0.0013  \\n29.2823  \\n0.8681  \\nFig. 5. Distribution of evaluation results of different models. (a)–(d) rep-\\nresents the distribution of evaluation results of MAE, MSE, PSNR and SSIM on \\nLIDC-IDRI data set respectively. It can be seen that the result distribution in (a)– \\n(c) approach to the inferior quartile and (d) approach to the superior quartile. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 5, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n7\\ngreatly improve the output accuracy of the reconstructed model. \\nBy summarizing and analyzing the training results of the four groups \\nof models, the evaluation metrics obtained by our XctNet exceeds the \\nReconNet. This result confirms that without additional network depth, \\nattention mechanism and multi-scale feature fusion module can also \\nimprove the accuracy of the model. \\n5. Discussion \\nThe advantages of the XctNet model is further illustrated by \\nanalyzing the semantic representation of the model. For the 2D projec-\\ntion image based reconstruction task, only when the feature extraction \\nmodule extracts the key and useful feature information can the volu-\\nmetric images be correctly reconstructed. The three models constructed \\nin this paper contain 512 feature maps with a size of \\n4 × 4. For better \\nvisualization, 16 feature maps are randomly selected to illustrate the \\nfeature representation between different models. As shown in Fig. 6(a)– \\n(b), it can be seen that the feature map generated by the feature \\nextraction module with self-attention mechanism is more concise than \\nthe feature map without self-attention mechanism. Comparing (b)–(c), it \\ncan be seen that CBAM/ECA attention mechanism can further remove \\nredundant features. On the other hand, from the perspective of the \\ngenerated volumetric image, the image quality generated by the model \\nwith self-attention mechanism is significantly better. In other words, the \\nfeature extraction module without self-attention mechanism learns a lot \\nof redundant information, which leads to the phenomenon of fuzzy \\ngeneration results. Therefore, the conclusion can be got that the self- \\nattention mechanism can help the model to better learn the feature \\ninformation. \\nIn order to further verify the performance of XctNet model, the \\ncurrent state of art CNN models are compared. As shown in Table 3, \\nReconNet model has the highest model complexity, and its FLOPs \\n(floating point operations) reaches 1.304 × 1012. ResXct as our baseline \\nmodel, which complexity is lower than ReconNet, and the error rate is \\nalmost the same. Our XctNet has greatly improved its error rate with \\nonly a little increase in complexity. This phenomenon shows the \\nfollowing two aspects. Firstly, the lightweight attention mechanism can \\nimprove the performance of the model without increasing the \\ncomplexity of the model. Secondly, the New Inception module proposed \\nin this paper can greatly reduce the amount of model calculation and \\ngenerate volumetric images with richer content. \\nAccording to the definition of information entropy, it represents the \\noverall characteristics of an information source in an average sense. For \\nimage information, we can describe the amount of information con-\\ntained in the image according to image entropy. As shown in Eq. (4), x \\nrepresents each pixel in the image, the image entropy reflects the \\naverage information of an image and the image entropy obtained for \\nspecific image information is unique. Therefore, the generation quality \\nof volumetric images can be evaluated from the perspective of image \\nentropy. \\nHEntropy(X) = −\\n∑\\nm\\nn=1\\npi(x)logpi(x)\\n(4) \\nAs shown in Fig. 7, two test samples are randomly selected to illus-\\ntrate the distribution of image entropy comes from different model, \\nwhich shows the ground truth and the entropy map of the different \\nmodels. It can be seen that, the area in which the entropy map tends to \\nbe cold indicates that it contains less information and the area in which \\nthe entropy map tends to be warm indicates that it contains more in-\\nformation. By comparing the results of the three models with the ground \\ntruth, we can find that XctNet can get a distribution map more inclined \\nto the ground truth by adding self-attention mechanism and multi-scale \\nfusion module. By comparing the ReconNet and ResXct with the other \\ntwo models, it further shows that the traditional end-to-end network will \\nbring deviation in the convolution process, and also verifies the effec-\\ntiveness of the improved method proposed in this paper. \\nTo verify the performance of the model on clinical X-ray images, we \\nobtained 10 original X-ray chest images through the spine surgery of the \\nGeneral Hospital of Shenzhen University. As shown in Fig. 8, the \\nreconstruction results of two groups of clinical X-ray images are shown. \\nIt can be seen that the accuracy of the reconstruction result are worse \\nthan that of the 2D projection used in this paper. The main reason for \\nthis phenomenon is that there are some differences between clinical X- \\nray images and 2D projections. Therefore, in the future work, more in- \\ndepth research from clinical X-rays need to conducted. However, from \\nthe generation results of ReconNet and XctNet proposed in this paper, \\nXctNet performs better in clinical X-rays data, which also confirms that \\nthe network we constructed has considerable superiority. \\n6. Conclusion \\nIn this paper, we focus on the reconstruction quality of volumetric \\nimage. In order to obtain more accurate reconstruction results, a light-\\nweight reconstruction network, XctNet, is constructed. The network \\nstructure has the following three innovations: \\nFirstly, self-attention mechanism is be added to the original residual \\nfeature extraction module to remove redundant features; Secondly, a \\nmulti-scale feature fusion module is proposed in this paper to improve \\nFig. 6. Feature extraction and network structure analysis. a) Feature map learned from 2D feature extraction module without attention mechanism; b) Feature \\nmap learned from 2D feature extraction module only with CBAM attention mechanism; c) Feature map learned from 2D feature extraction module with CBAM/ECA \\nattention mechanism. \\nTable 3 \\nComparison with different CNNs models on the LIDC-IDRI data set.   \\n#. Param. ( × 107) \\nFLOPs ( × 109) \\nError rate (%) \\nReconNet \\n58.85914 \\n1304  \\n2.125 \\nResXct \\n3.02648 \\n144.359  \\n2.126 \\nCBAMXct \\n3.03315 \\n144.360  \\n2.059 \\nECAXct \\n3.02648 \\n144.359  \\n2.004 \\nCBAM/ECAXct \\n3.03316 \\n144.360  \\n2.001 \\nXctNet \\n3.45363 \\n211.109  \\n1.791  \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 6, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n8\\nthe quality of reconstructed image and other details. Finally, a feature \\ngeneration module called New Inception module is constructed to obtain \\nricher feature information and more accurate reconstruction results. At \\nthe same time, there are still some problems to be solved in this paper. In \\nthe actual application scenario, the corresponding 2D projection image \\nshould be an X-ray image, but the 2D projection used in this article is \\nprojected by the DRR algorithm. To solve this problem, using style \\ntransfer algorithm may considered to solve the difference between \\nclinical X-rays and DRR projection. In conclusion, XctNet as a light-\\nweight framework can further improve the results of volumetric image \\nreconstruction. \\nCRediT authorship contribution statement \\nZhiqiang Tan: Conceptualization, Methodology, Software, Investi-\\ngation. Jun Li: Data curation, Software Huiren Tao: Resources, Vali-\\ndation. Shibo Li: Visualization, Writing – review & editing. Ying Hu: \\nSupervision, Project administration. \\nDeclaration of Competing Interest \\nThe authors declare that they have no known competing financial \\ninterests or personal relationships that could have appeared to influence \\nthe work reported in this paper. \\nFig. 7. The entropy map generated by different network structures. a–b) represent different test samples and their corresponding entry information. The \\nvariation of image entropy is closely related to the content contained in the image. As can be seen, the less content the image contains, the lower the image in-\\nformation entropy, that is, the color of the entropy map tends to be cold. \\nFig. 8. Clinical X-ray reconstruction results of difference cases. a–b) represent volumetric images reconstructed from different X-ray images. The number of \\nslices shown in the figure are 30, 60, 70, and 90. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 7, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computerized Medical Imaging and Graphics 98 (2022) 102067\\n9\\nAcknowledgements \\nThis work was supported in part by Key-Area Research and Devel-\\nopment Program of Guangdong Province (No.2020B0909020002), Na-\\ntional Natural Science Foundation of China (Grant No. 62003330), \\nShenzhen \\nFundamental \\nResearch \\nFunds \\n(Grant \\nNo. \\nJCYJ20200109114233670，No.JCYJ20190807170407391, \\nJCYJ20180507182415428), Natural Science Foundation of Guangdong \\nProvince (Grant No. 2019A1515011699) and Guangdong-Hong Kong- \\nMacao Joint Laboratory of Human-Machine Intelligence-Synergy Sys-\\ntems, Shenzhen Institute of Advanced Technology. \\nReferences \\nArmato, Samuel G., McLennan, Geoffrey, Bidaut, Luc M., McNitt-Gray, Michael F., \\nMeyer, C.R., Reeves, Anthony P., Zhao, Binsheng, Aberle, Denise R., Henschke, \\nClaudia I., Hoffman, Eric A., Kazerooni, Ella A., MacMahon, Heber, Van Beeke, \\nEdwin J.R., Yankelevitz, David F., Biancardi, Alberto M., Bland, Peyton H., Brown, \\nMatthew S., Engelmann, Roger M., Laderach, G.E., Max, Daniel, Pais, Richard C., \\nQing, D.P., Roberts, Rachael Y., Smith, Amanda R., Starkey, Adam, Batrah, Poonam, \\nCaligiuri, Philip, Farooqi, Ali O., Gladish, Gregory W., Jude, Cecilia Matilda, \\nMunden, Reginald F., Petkovska, Iva, Quint, Leslie E., Schwartz, Lawrence H., \\nSundaram, Baskaran, Dodd, Lori E., Fenimore, Charles, Gur, David, Petrick, Nicholas \\nA., Freymann, John B., Kirby, Justin S., Hughes, Brian, Casteele, Alessi Vande, \\nGupte, Sangeeta, Sallamm, Maha, Heath,Michael, Kuhn, M., Dharaiya, Ekta, Burns, \\nRichard, Fryd, David, Salganicoff, Marcos, Anand, V., Shreter, Uri, Vastagh, Stephen, \\nCroft, Barbara Y., 2011. The lung image database consortium (LIDC) and image \\ndatabase resource initiative (IDRI): a completed reference database of lung nodules \\non CT scans. Med. Phys. vol. 38, issue 2, pp. 915–31. \\nBayat, Amirhossein, Kumar Sekuboyina, Anjany, Paetzold, Johannes C., Payer, Christian, \\nˇStern, Darko, Urschler, Martin, Kirschke, Jan S., Menze, Bjoern H., 2020. Inferring \\nthe 3D standing spine posture from 2D radiographs. MICCAI. \\nˇCavojsk´a, Jana, Petrasch, Julian, Mattern, Denny, Lehmann, Nicolas J., Voisard, Agn`es, \\nB¨ottcher, Peter, 2020. Estimating and abstracting the 3D structure of feline bones \\nusing neural networks on X-ray (2D) images. Commun. Biol. 3 (n. pag).  \\nChoy, Christopher Bongsoo, Xu, Danfei, Gwak, JunYoung, Chen, Kevin, Savarese, Silvio, \\n2016. 3D-R2N2: a unified approach for single and multi-view 3D object \\nreconstruction. ECCV 628–644. \\nFeeman, Timothy G., 2010. The Mathematics of Medical Imaging. \\nFeng, Ruicheng, Guan, Weipeng, Qiao, Yu, Dong, Chao, 2020. Exploring Multi-Scale \\nFeature Propagation and Communication for Image Super Resolution. ArXiv abs/ \\n2008.00239, n. pag. \\nFlohr, T., Stierstorfer, K., Bruder, H., Simon, J., Polacin, A., Schaller, S., 2003. Image \\nreconstruction and image quality evaluation for a 16-slice CT scanner. Med. Phys. \\n832–845. \\nHan, Xian-Feng, Laga, Hamid, Bennamoun, Mohammed, 2019. Image-based 3D object \\nreconstruction: state-of-the-art and trends in the deep learning era. IEEE Trans. \\nPattern Anal. Mach. Intell. 43 (5), 1578–1604. \\nHan, Xian-Feng, Laga, Hamid, Bennamoun, Mohammed, 2021. Image-based 3D object \\nreconstruction: state-of-the-art and trends in the deep learning era. IEEE Trans. \\nPattern Anal. Mach. Intell. 43, 1578–1604. \\nHe, Kaiming, Zhang, X., Ren, Shaoqing, Sun, Jian, 2016. Deep residual learning for image \\nrecognition. In: Proceedings of the 2016 IEEE Conference on Computer Vision and \\nPattern Recognition (CVPR), pp. 770–8. \\nHenzler, Philipp, Rasche, Volker, Ropinski, Timo, Ritschel, Tobias, 2018. Single-image \\ntomography: 3D volumes from 2D cranial X-rays. Comput. Graph. Forum 37 (n. pag).  \\nHsieh, Jiang, 2003. Computed Tomography: Principles, Design, Artifacts, and Recent \\nAdvances. \\nHu, H., 1999. Multi-slice helical CT: scan and reconstruction. Med Phys. 26, 5–18. \\nKak, A.C., Slaney, M., 1987. Principles of Computed Tomographic Imaging. SIAM, \\nPhiladelphia, PA.  \\nKasten, Yoni, Doktofsky, Daniel, Kovler I., 2020. End-To-End Convolutional Neural \\nNetwork for 3D Reconstruction of Knee Bones from Bi-Planar X-Ray Images. ArXiv \\nabs/2004.00871, n. pag. \\nLenke, Lawrence G., Betz, Randal R., Harms, Jürgen, Bridwell, Keith H., Clements, David \\nH., Lowe, Thomas G., Blanke, Kathy M., 2001. Adolescent idiopathic scoliosis: a new \\nclassification to determine extent of spinal arthrodesis. J. Bone Jt. Surg. 83, \\n1169–1181. \\nLi, Jun, Xu, Kai, Chaudhuri, Siddhartha, Yumer, Ersin, Zhang, Hao, Guibas, Leonidas J., \\n2017. GRASS: Generative Recursive Autoencoders for Shape Structures. ArXiv abs/ \\n1705.02090, n. pag. \\nMelhem, E., Assi, A., El Rachkidi, R., Ghanem, I., 2016. EOS® biplanar X-ray imaging: \\nconcept, developments, benefits, and limitations. J. Child.’s Orthop. 1–4. \\nMeng, M., Li, S., Yao, L., Li, D., Zhu, M., Gao, Q., Xie, Q., Zhao, Q., Bian, Z., Huang, J., \\nMeng, D., 2020. Semi-supervised learned sinogram restoration network for low-dose \\nCT image reconstruction. International Society for Optics and Photonics. Phys. Med. \\nImaging 11312, 113120B. \\nMoturu, Abhishek, Chang, Alex, 2018. Creation of Synthetic X-Rays to Train a Neural \\nNetwork to Detect Lung Cancer. \\nOvadia, Dror, 2013. Classification of adolescent idiopathic scoliosis (AIS). J. Child.’s \\nOrthop. 7, 25–28. \\nPost, Mareille, Verdun, St´ephane, Roussouly, Pierre, Abelin-Genevois, Kariman, 2018. \\nNew sagittal classification of AIS: validation by 3D characterization. Eur. Spine J. 28, \\n551–558. \\nRehm, Johannes, Germann, Thomas, Akbar, Michael, Pepke, Wojciech, Kauczor, Hans \\nUlrich, Weber, Marc-Andr´e, Spira, Daniel, 2017. 3D-modeling of the spine using EOS \\nimaging system: inter-reader reproducibility and reliability. PLoS One 12 (n. pag).  \\nSchaller, Stefan, Stierstorfer, Karl, Bruder, Herbert, Kachelriess, Marc, Flohr, Thomas G., \\n2001. Novel approximate approach for high-quality image reconstruction in helical \\ncone-beam CT at arbitrary pitch. SPIE Med. Imaging. \\nSchwartz, John T., Gao, Michae C., Geng, Eric, Mody, Kush S., Mikhail, Christopher M., \\nCho, Samuel K., 2019. Applications of machine learning using electronic medical \\nrecords in spine surgery. Neurospine 16, 643–653. \\nShen, Liyue, Zhao, Wei, Xing, Lei, 2019. Patient-specific reconstruction of volumetric \\ncomputed tomography images from a single projection view via deep learning. Nat. \\nBiomed. Eng. 3, 880–888. \\nSingh, Amitojdeep, Sengupta, Sourya, Lakshminarayanan, Vasudevan, 2020. Explainable \\ndeep learning models in medical image analysis. J. Imaging 6 (n. pag).  \\nSong, Weinan, Liang, Yuan, Wang, Kun, He, Lei, 2020. Oral-3D: reconstructing the 3D \\nbone structure of oral cavity from 2D panoramic X-ray. ArXiv abs/2003.08413, n. \\npag. \\nStierstorfer, Karl, Rauscher, Annabella, Boese, Jan, Bruder, Herbert, Schaller, Stefan, \\nFlohr, Thomas G., 2004. Weighted FBP–a simple approximate 3D FBP algorithm for \\nmultislice spiral CT with good dose usage for arbitrary pitch. Phys. Med. Biol. 49 \\n(11), 2209–2218. \\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott E., Anguelov, \\nDragomir, Erhan, D., Vanhoucke, Vincent, Rabinovich, Andrew, 2015. Going deeper \\nwith convolutions. In: Proceedings of the 2015 IEEE Conference on Computer Vision \\nand Pattern Recognition (CVPR), pp. 1–9. \\nTaguchi, Katsuyuki, Aradate, Hiroshi, 1998. Algorithm for image reconstruction in multi- \\nslice helical CT. Med. Phys. 25 (4), 550–561. \\nWang, Qilong, Wu, Banggu, Zhu, Pengfei, Li, P., Zuo, Wangmeng, Hu, Qinghua, 2020. \\nECA-net: efficient channel attention for deep convolutional neural networks. In: \\nProceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition (CVPR), pp. 11531–9. \\nWang, Weiyue, Huang, Qiangui, You, Suya, Yang, Chao, Neumann, Ulrich, 2017. Shape \\nInpainting using 3D generative adversarial network and recurrent convolutional \\nnetworks. In: Proceedings of the 2017 IEEE International Conference on Computer \\nVision (ICCV), pp. 2317–25. \\nWoo, Sanghyun, Park, Jongchan, Lee, Joon-Young, Kweon, In-So, 2018. CBAM: \\nconvolutional block attention module. ECCV. \\nWu, Jiajun, Zhang, Chengkai, Xue, Tianfan, Freeman, Bill, Tenenbaum, Joshua B., 2016. \\nLearning a probabilistic latent space of object shapes via 3D generative-adversarial \\nmodeling. NIPS. \\nWu, Zhirong, Song, Shuran, Khosla, Aditya, Yu, Fisher, Zhang, Linguang, Tang, Xiaoou, \\nXiao, Jianxiong, 2015. 3D shapenets: a deep representation for volumetric shapes. \\nIn: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern \\nRecognition (CVPR), pp. 1912–20. \\nYan, Xinchen, Yang, Jimei, Yumer, Ersin, Guo, Yijie, Lee, Honglak, 2016. Perspective \\ntransformer nets: learning single-view 3D object reconstruction without 3D \\nsupervision. NIPS 1696–1704. \\nYing, Xingde, Guo, Heng, Ma, Kai, Wu, Jian, Weng, Zhengxin, Zheng, Yefeng, 2019. \\nX2CT-GAN: reconstructing CT from biplanar X-rays with generative adversarial \\nnetworks. In: Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and \\nPattern Recognition (CVPR), pp. 10611–20. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'file_path': 'data/Tan et al. - 2022 - XctNet Reconstruction network of volumetric image.pdf', 'page': 8, 'total_pages': 9, 'format': 'PDF 1.7', 'title': 'XctNet: Reconstruction network of volumetric images from a single X-ray image', 'author': 'Zhiqiang Tan', 'subject': 'Computerized Medical Imaging and Graphics, 98 (2022) 102067. doi:10.1016/j.compmedimag.2022.102067', 'keywords': '3D reconstruction,Self-attention module,Deep learning,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20220525170227Z', 'modDate': 'D:20220526023359Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\nAvailable online 13 February 2023\\n0010-4825/© 2023 Elsevier Ltd. All rights reserved.\\nSemi-XctNet: Volumetric images reconstruction network from a single \\nprojection image via semi-supervised learning \\nZhiqiang Tan a,b, Shibo Li a,*, Ying Hu a,**, Huiren Tao c, Lihai Zhang d \\na Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen Key Laboratory of Minimally Invasive Surgical Robotics and System, Shenzhen, \\n518055, China \\nb University of Chinese Academy of Sciences, CAS, Beijing, 100049, China \\nc Department of Orthopaedics, Shenzhen University General Hospital, Shenzhen University Clinical Medical Academy, Shenzhen, 518055, China \\nd Department of Orthopaedics, Chinese PLA General Hospital, Beijing, 100853, China   \\nA R T I C L E  I N F O   \\nKeywords: \\n3D Reconstruction \\nSemi-supervised learning \\nData augmentation \\nX-ray \\nVolumetric images \\nA B S T R A C T   \\nDeep learning networks have achieved remarkable progress in various tasks of medical imaging. Most of the \\nrecent success in computer vision highly depend on large amounts of carefully annotated data, whereas labelling \\nis arduous, time-consuming and in need of expertise. In this paper, a semi-supervised learning method, Semi- \\nXctNet, is proposed for volumetric images reconstruction from a single X-ray image. In our framework, the ef-\\nfect of regularization on pixel-level prediction is enhanced by introducing a transformation consistent strategy \\ninto the model. Furthermore, a multi-stage training strategy is designed to ameliorate the generalization per-\\nformance of the teacher network. An assistant module is also introduced to improve the pixel quality of pseudo- \\nlabels, thereby further improving the reconstruction accuracy of the semi-supervised model. The semi-supervised \\nmethod proposed in this paper has been extensively validated on the LIDC-IDRI lung cancer detection public data \\nset. Quantitative results show that SSIM (structural similarity measurement) and PSNR (peak signal noise ratio) \\nare 0.8384 and 28.7344 respectively. Compared with the state-of-the-arts, Semi-XctNet exhibits excellent \\nreconstruction performance, thus demonstrating the effectiveness of our method on the task of volumetric images \\nreconstruction network from a single X-ray image.   \\n1. Introduction \\nAs a common medical diagnostic imaging tool, computed tomogra-\\nphy (CT) can assist doctors to observe the pathological state of patients, \\nwhich is of great significance to preoperative planning. The CT image \\ndata currently used for clinical diagnosis generates line integrals of the \\nscanned bodies. Within the CT image reconstruction process, the inverse \\nRadon transformation is performed and the inverse Fourier transform \\n(IFT) is commonly implemented to recover the spatial distribution of the \\nX-ray attenuation. The reconstruction quality of traditional methods is \\nacceptable for clinical diagnosis in ideal cases, however, the perfor-\\nmance is limited by non-ideal factors such as patient movement, radi-\\nation, and non-uniform field gradients [1]. The issue of blurry and \\nartifact may also be revealed in the reconstructed image due to various \\nreasons including factors from patients, physics, and hardware etc. \\nAdditionally, high radiation doses and sophisticated operating \\ncircumstances also restrict both intraoperative and postoperative ap-\\nplications of CT. \\nIn recent years, deep learning methods have been proven to have \\nenormous potential in medical data processing [2–4]. Wang et al. [5] \\npoint out that as artificial intelligence technology continues to develop, \\nnew breakthroughs will be made in the field of tomographic imaging. In \\naddition, Sahiner et al. [6] provide a summary of current research in \\nmedical radiography and indicate that combining deep learning model \\ninnovations will be a key research direction for the future [7]. The \\ndeep-learning approaches avoid playing with the pixels, but instead \\nfocus on the driving data as well as the learning architectures. In terms of \\n3D construction, the construction from single image is an ill-posed \\nproblem and is almost unsolvable through traditional algorithms, \\nnonetheless, this problem is already partially settled by various teams \\nusing deep learning methods. Shen et al. [8]. achieved the reconstruc-\\ntion of CT volumetric images from 2D projections for the first time \\n* Corresponding author. \\n** Corresponding author. \\nE-mail addresses: zq.tan@siat.ac.cn (Z. Tan), sb.li@siat.ac.cn (S. Li), ying.hu@siat.ac.cn (Y. Hu), huiren_tao@163.com (H. Tao), zhanglihai74@qq.com (L. Zhang). \\nContents lists available at ScienceDirect \\nComputers in Biology and Medicine \\njournal homepage: www.elsevier.com/locate/compbiomed \\nhttps://doi.org/10.1016/j.compbiomed.2023.106663 \\nReceived 17 October 2022; Received in revised form 29 January 2023; Accepted 10 February 2023   \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n2\\nthrough deep learning. The substantial progress can be explained as that \\nthe learning mechanism takes the advantage of prior knowledge from \\ndata and succeeds in recovering the spatial information of the pixels \\nfrom images, which is in analogy with the way doctors grasp anatomical \\ninformation from radiographs. \\nDifferent from natural images, large-scale data is not always avail-\\nable due to various reasons like privacy, patient volume, the database \\nconstruction of hospital and so on. In addition, the annotation of medical \\nimage data is integral to supervised learning, which requires clinical \\nexperience and is time costly. In the process of clinical diagnosis, pa-\\ntients usually take X-ray images for preliminary investigation, and then \\nCT scans are performed for further diagnosis, so that the amount of X-ray \\ndata is usually greater than CT data, which makes the data for volu-\\nmetric construction a partially labeled dataset. To solve this problem, \\nthis paper proposes a method for reconstructing CT volumetric images \\nfrom 2D projection images based on semi-supervised learning. This \\nmethod can achieve the reconstruction tasks from a large number of \\nunpaired 2D projection images and a small fraction of paired 2D pro-\\njection/CT volumetric image data. \\nThe semi-supervised reconstruction algorithm proposed in this paper \\nincludes three models, namely, a teacher model, an assistant model and \\na student model. The teacher model is used to generate pseudo-labels to \\ntrain the teacher model, the assistant model is designed to enhance the \\ndetails of pseudo-labels, while the student model is created for self- \\nsupervised training of volumetric image reconstruction. The main con-\\ntributions of this paper are as follows.  \\n• An algorithm for reconstructing CT volumetric images from a single \\nX-ray image based on semi-supervised learning is proposed. Teacher \\nmodel and assistant model generate pseudo-labels to help the student \\nmodel to perform self-supervised learning.  \\n• A multi-stage training strategy is proposed in the training process. \\nThree stages are included: a pre-training stage based on unlabeled \\ndata, a self-supervised training stage using distinct augmentation \\nmethods, and a supervised fine-tuning stage. The generated pseudo- \\nlabels and prediction results are mixed based on similarity to learn \\nthe semantic level mapping from X-ray image to volumetric images.  \\n• An assistant network model is incorporated into the reconstruction \\nframework to improve the quality of pseudo-labels generated by the \\nteacher network. Pseudo-labels with higher qualities are created \\nthrough the assistant network to ensure better reconstruction results \\nby the student model. \\n2. Related work \\nVarious significant research findings have been established in the \\nfield of image-based 3D reconstruction, which aim to infer the 3D \\nstructure of objects from single-view or multi-view 2D images. Nozawa \\net al. [9] proposed a deep learning model for reconstructing 3D car \\nshape from a single 2D sketch image. The model acts as a variational \\nautoencoder deep neural network that takes a 2D sketch and generates a \\nset of multi-view depth and mask images, forming a more efficient \\nrepresentation and can be efficiently fused to generate a 3D car shape. \\nFeng et al. [10]. proposed an efficient end-to-end deep learning frame-\\nwork for reconstructing the 3D structure of porous media, which could \\nbe trained by inputting 2D slices into a deep learning model, and the \\ncorresponding 3D structure could be reconstructed instantaneously. Fu \\net al. [11] reviewed recent work on 3D structure reconstruction from a \\nsingle image and introduced the encoder structure and training details \\nfor this task. \\nLearning-based 3D reconstruction has been gaining growing popu-\\nlarities. Henzler et al. [12] employed Convolutional Neural Network \\n(CNN) to generate 3D cranial volumes from single 2D projections. Ying \\net al. [13] reconstructed CT images by inputting orthogonal X-ray im-\\nages into the general advertising network (GAN). This method trained \\nthe generator by combining mean square error (MSE) with \\ncountermeasure loss and proved its effectiveness on public data set. \\nRatul et al. [14] proposed a conditional deep network, CCX-rayNet, \\nwhich can recapture the shape and texture with a priori semantic in-\\nformation in the generated CT volume. This method combines the \\nadaptive feature fusion (AFF) module and uses the similarity matrix to \\nalleviate the registration problem of unconstrained input data. From the \\nexperimental results, this method was better than the current baseline \\nmethod. \\nCurrent approaches for volumetric data construction from single \\nimage rely on the large amount of paired data in order to obtain satis-\\nfactory reconstruction results. For medical images, labelling requires \\nclinical prior knowledge, which includes both complex anatomical \\ntheories and practical experiences, and it also takes plenty of time. Semi- \\nsupervised learning aims to enhance the learning performance of the \\nmodel by making full use of a large number of unlabeled samples under \\nthe guidance of a small number of sample labels, so as to avoid waste of \\ndata resources, and to solve the problem of poor generalization caused \\nby few labeled data in supervised learning methods [15]. \\nLee [16] proposed the concept of Pseudo-label by taking the target \\nclass with the highest prediction probability of unlabeled samples as the \\nreal label. The algorithm proved that training with pseudo-labels can \\nmake the entropy of unlabeled data samples smaller, so that the classi-\\nfication and generalization performance can be improved. Laine et al. \\n[17] proposed a simple and effective method for training deep neural \\nnetworks in a semi-supervised environment, in which only a small part \\nof the training data were labeled. This method introduces self-awareness \\nand forms a consistent prediction for unknown labels under different \\nregularization and input enhancement conditions. For unknown labels, \\nthe prediction set can be regarded as a better predictor than the network \\noutput in the recent training period, so it can be used as a training \\ntarget. Sohn et al. [18] proposed a semi-supervised learning algorithm \\nnamed FixMatch by combining two common self-supervised learning \\nmethods (consistency regularization and pseudo-labeling). State--\\nof-the-art performance was achieved on supervised learning bench-\\nmarks. In the same year, Google released a self-supervised learning \\nframework [19], SimCLR. This paper is not only simple in method, but \\nalso deepens our understanding of self-supervised learning and \\ncontrastive learning. On this basis, the Google team proposed SimCLR \\nV2 [20] for application, which mainly carries out unsupervised training \\non a large number of unlabeled samples, fine-tuned through a small \\nnumber of labels and distills knowledge on unlabeled data. In addition, \\nthere are many advanced research results in semi-supervised learning in \\nthe field of medical image analysis. Li et al. [21] proposed a new \\nsemi-supervised method for medical image segmentation, in which the \\nnetwork is optimized by a weighted combination of common supervi-\\nsion loss only for labeled input and regularization loss of labeled and \\nunlabeled data. Yu et al. [22] proposed a novel uncertainty-aware \\nsemi-supervised framework for 3D MR left atrial image segmentation, \\nwhich enables student models to gradually learn from meaningful and \\nreliable targets by exploiting uncertainty information. Wang et al. [23] \\nconstructed a 3D medical image detection framework named FocalMix, \\nwhich performed extensive experiments on two widely used lung nodule \\ndetection datasets. Results showed that FocalMix achieved substantial \\nimprovements of up to 17.3% over state-of-the-art supervised learning \\nmethods over 400 unlabeled CT scans. \\nMost current deep learning-based image reconstruction tasks are \\nbased on supervised learning models, owing to data in the medical im-\\naging field is often not available on a large scale, supervised learning for \\nimage reconstruction tasks is usually not well suited to drive clinical \\napplications. From the above literature, it can be seen that semi- \\nsupervised learning has shown performance effects that can surpass \\nsupervised learning, so it is a feasible novel method to apply semi- \\nsupervised learning to the field of medical image reconstruction. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n3\\n3. Methodology \\n3.1. Semi-supervised learning network model architecture \\nIn this paper, we propose a self-supervised learning-based single X- \\nray image reconstruction algorithm, Semi-XctNet, which aims to address \\nthe current imbalance in medical imaging data by using a small amount \\nof X-ray/CT paired data and a large amount of unlabeled X-ray data. The \\nlabeled input X ∈ R H×W and unlabeled input ̃X ∈ R H×W are single X-ray \\nimage, ground truth Y ∈ R C×H×W is CT volumetric images. A small \\namount of labeled data will be input into the teacher model to perform \\nsupervised training. In addition, the framework introduces a super- \\nresolution reconstruction network as an auxiliary module to obtain \\nhigh-resolution prediction results to improve the reconstruction accu-\\nracy of the teacher model. On the other hand, semi-supervised learning \\nbased on consistent regularization has made significant progress in \\nimage classification [24,25]. Consequently, a learning strategy based on \\nconsistent \\nregularization \\nand \\npseudo-label \\nis \\nproposed \\nfor \\nFig. 1. The Schematic diagram of training process for volumetric images reconstruction. The teacher model uses labeled data for supervised training and \\nprovides pseudo-labels for unlabeled data training; the teacher model is trained based on pseudo-labels fused by different data augmentation methods and fine-tuned \\nin the labeled data set. \\nFig. 2. The main architecture of XctNet. The model contains X-ray feature extraction module, the volumetric images generation module and multi-scale \\nfeature fusion module. The feature extraction module and volumetric images generation module are connected using multi-scale feature fusion module. The input \\nof the model is a single X-ray image. The feature extraction module can be used to extract 2D feature information and optimize features adaptively. The New \\nInception module used in the volumetric images generation module can obtain more detailed results. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n4\\nself-supervised learning of unlabeled data. As can be seen in Fig. 1, the \\ncorresponding augmented images from unlabeled data set are obtained \\nthrough different data augmentation methods and the related volu-\\nmetric images are obtained through the teacher model. According to the \\nprinciple of consistent regulation, the generated volumetric images is \\nused as pseudo-label to train the student model. Furthermore, this paper \\nproposes a multi-stage training strategy to promote the robustness of \\npseudo-label by continuously improving the teacher model. \\nTeacher/Student model：Using the end-to-end network model for \\nCT volumetric images reconstruction task will lose abundant informa-\\ntion due to the randomness of extracting X-ray feature information \\nduring down sampling operation, resulting in blurring and other phe-\\nnomena in CT volumetric images reconstruction. To solve this problem, \\nwe propose a new network model, XctNet [26], by adding a series of \\nimproved methods on the basis of the baseline model. As can be seen in \\nFig. 2, a multi-scale feature fusion module is added based on the baseline \\nmodel to improve the fine-grained features of the image generated by \\nthe model. On the other hand, channel attention and spatial attention \\nmechanism [27,28] are taken into XctNet model. The attention module \\nfor feedforward convolutional neural network will infer relevant \\nattention attempts from two different dimensions of channel and space \\nin turn, thus, the obtained attention map is multiplied by the corre-\\nsponding input feature map to adaptively optimize the features. In \\naddition, XctNet, as a lightweight network model, although many novel \\nmodules are added to improve the performance of the model, it will not \\nincrease the amount of calculation. On the contrary, it can greatly \\nprogress the reconstruction performance of the model. \\nIn the part of encoder module, residual block is used as the feature \\nextraction module, as can be seen in Table 1. The first layer of the model \\nis composed of a convolutional layer with a kernel size of 7× 7 and stride \\n2, and the second to fifth layers are composed of four residual blocks \\nwhich contains two convolutional layers. The number of channels of the \\nconvolutional layer in each residual block is kept the same to ensure that \\nthe shortcut path and the residual path can maintain the same size \\nduring the element-wise addition operation. The size of the feature \\nrepresentation output is 4 × 4. We built a transform module to convert \\nX-ray image feature information into volumetric feature information. \\nThe output of the representation network is reshaped from 512 × 4 × 4 \\ninto 256 × 4 × 4 × 4 through transformation. The converted 3D feature \\nmap can be obtained through a deconvolution operation with a kernel \\nsize of 1 × 1 × 1. In addition, the ReLU activation function and the batch \\nnormalization function are also included to better learn the trans-\\nformation relationship in the transform process. In the decode part, as \\nshown in Table 2, a 3D Inception module, which can solve the problems \\nof gradient disappearance and data over-fitting, is used for teacher/ \\nagnostic model to generate more detailed volumetric images. The 3D \\nInception module is composed of a 3D point-wise convolution with a \\nkernel size of 1 × 1 × 1 and two 3D deconvolution with kernel size of \\n3 × 3 × 3 as well as 5 × 5 × 5. Among them, the 3D point-wise convo-\\nlution layer can not only generate more image detail information, but \\nalso effectively reduce the amount of feature calculations, thereby \\nimproving computing efficiency. In addition, the 3D Inception module \\nobtains two 3D deconvolutions in the form of branches to generate \\nimages at different scales, so as to obtain more abundant generation \\nresults. It is worth noting that the Teacher model represents the teacher \\nmodel, and the Teacher model represents the student model in this \\narticle. \\nAttention-based SRCNN: Under normal circumstances, the use of \\nend-to-end neural networks for vision-related tasks will cause certain \\npixel loss. For generating CT volumetric images from a single X-ray \\nimage, since the X-ray only has a small amount of spatial information \\nrelationship corresponding to the CT volumetric image, a lot of pixel \\ninformation will be lost in the reconstruction process. In this paper, an \\nattention mechanism is added to the main network structure of SRCNN \\n[29] to enhance the reconstruction accuracy of the model and use it as \\nan assistant model to reconstruct the generated volume images. The \\nnetwork mainly contains three convolution modules. Among them, the \\nnumber of input channels is 128 × 128 × 128 and the convolution kernel \\nand padding layer parameters of each layer adopt different sizes through \\ncalculation to ensure that the final output is consistent with the size of \\nthe input. \\n3.2. Multi-stage training strategy \\nThe overall process of the semi-supervised learning-based volumetric \\nimage reconstruction algorithm proposed in this paper is as shown in \\nAlg.1. The algorithm requires two X-ray image data sets, among which \\none contains ground truth and the other does not. First, the teacher \\nmodel is pre-trained on a large image public data, ImageNet [30], and \\nthen fine-tuned in the X-ray data set with ground truth in order to obtain \\ntask-related hyperparameters and its prediction results will be input into \\nthe assistant model for reconstruction. In addition, unlabeled data is \\ngenerated through weakly-augmentation (translation, flip, etc) and \\nstrongly-augmentation (similar to RandAugment [31]) to generate \\npaired images and input the generated images into the teacher model to \\ngenerate the corresponding pseudo-labels, the generated pseudo-labels \\nwill be blended together to get a new pseudo-labels and the generated \\npaired images will also be input into the teacher model for \\nTable 1 \\nMain Architecture of X-ray feature extraction Module.  \\nLayers \\nConv 2d \\nResidual Block \\nResidual Block \\nResidual Block \\nResidual Block \\nOutput Size \\n64 × 64 \\n32 × 32 \\n16 × 16 \\n8 × 8 \\n4 × 4 \\nKernel size/depth \\n7 × 7,64,stride 2 \\n[\\n3 × 3, 64\\n3 × 3, 64\\n]\\n× 3 \\n[\\n3 × 3, 128\\n3 × 3, 128\\n]\\n× 4 \\n[\\n3 × 3, 256\\n3 × 3, 256\\n]\\n× 6 \\n[\\n3 × 3, 512\\n3 × 3, 512\\n]\\n× 3  \\nTable 2 \\nMain architecture of volumetric image generation module.  \\nLayers \\nBasicTranspose3d \\nNew Inception \\nBasicTranspose3d \\nNew Inception \\nOutput Size \\n8 × 8 × 8 \\n8 × 8 × 8 \\n16 × 16 × 16 \\n16 × 16 × 16 \\nKernel size/depth \\n4 × 4 × 4 \\n256,stride 2 \\n1 × 1 × 1，80 \\n{\\n3 × 3 × 3, 156, padding 1\\n5 × 5 × 5, 100, padding 2 \\n4 × 4 × 4 \\n128,stride 2 \\n1 × 1 × 1，50 \\n{\\n3 × 3 × 3, 68, padding 1\\n5 × 5 × 5, 60, padding 2  \\nBasicTranspose3d \\nNew Inception \\nBasicTranspose3d \\nNew Inception \\nBasicTranspose3d \\n32 × 32 × 32 \\n32 × 32 × 32 \\n64 × 64 × 64 \\n64 × 64 × 64 \\n128 × 128 × 128 \\n4 × 4 × 4 \\n64,stride 2 \\n1 × 1 × 1，20 \\n{\\n3 × 3 × 3, 34, padding 1\\n5 × 5 × 5, 30, padding 2 \\n4 × 4 × 4 \\n64,stride 2 \\n1 × 1 × 1，10 \\n{\\n3 × 3 × 3, 18, padding 1\\n5 × 5 × 5, 14, padding 2 \\n4 × 4 × 4 \\n16,stride 2  \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n5\\nself-supervised training with the new pseudo-labels. To be noticed, the \\nreconstruction effect of the assistant model in the initial training stage is \\nnot satisfying, which may affect the overall training effect. Therefore, \\nthe assistant model is added to training process of self-supervised \\nlearning to enhance the details of pseudo-labels after several rounds of \\ntraining in the labeled data. After that, the teacher model trained in the \\nfirst iteration will be used as a new teacher model to generate \\npseudo-labels to guide the new round of training. Among them, the new \\nteacher model will integrate with a series of improvement methods on \\nthe basis of the first iteration of baseline model, so that the final model \\ncan show better reconstruction results. \\nAlgorithm 1.\\nSemi-supervised learning for volumetric images \\nreconstruction. \\n3.3. Self-supervised learning \\nTo better generate medical reconstruction images, a loss function \\nbased on similarity measurement will be used in the self-supervised \\nlearning task. The ground truth YP provided for self-supervised \\nlearning is pseudo-labels generated by teacher model, Ypre represent \\nvolumetric images generated by teacher model. The optimization used \\nin self-supervised learning process can be seen as Eq. (1), among them, \\nLGCC(Ypre, YP) is the similarity measurement function, which is used to \\nevaluate the consistent regularization in the self-supervised training \\nprocess and the λ‖w‖2 is the spatial norm regularization term, which is \\nused to constrain the spatial smoothness of the generated volumetric \\nimage and improve the anti-interference ability of the model. \\nL\\n(\\nYpre, YP, ω\\n)\\n= LGCC\\n(\\nYpre, YP\\n)\\n+ λ‖w‖2\\n(1) \\nVoxelMorph [32] summarized the similarity of images used to \\nmeasure the image, using mean squared voxel difference as the evalu-\\nation metric for images that are susceptible to grayscale distribution and \\ncontrast and the ground truth used in the self-supervised learning pro-\\ncess is pseudo-labels. The poor quality of the pseudo-labels at the \\nbeginning of training may adversely affect the training process. Conse-\\nquently, this article uses global cross-correlation (GCC) as the similarity \\nmeasure function to improve the robustness of the training process, as \\nshown in Eq. (2). \\nLGCC\\n(\\nYpre, YP\\n)\\n=\\n∑\\nM\\nm=1\\n∑\\nN\\nn=1\\n(\\nYpre − ̂Y pre\\n)(\\nYP − ̂Y p\\n)\\n̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅\\n∑\\nM\\nm=1\\n∑\\nN\\nn=1\\n(\\nYpre − ̂Y pre\\n)2\\n√\\n̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅\\n∑\\nM\\nm=1\\n∑\\nN\\nn=1\\n(YP − ̂Y P)2\\n√\\n(2) \\nM, N are the pixel size in the image, ̂Ypre, YP are the generated \\nvolumetric image and the mean square error of pseudo-labels. On the \\nother hand, in the task of X-ray reconstruction of CT volumetric images, \\nthe source of the collected CT volumetric images cannot be completely \\nguaranteed to be collected from the same instrument. This uncertainty \\ncomes from the process of data collection, which will undoubtedly bring \\ncertain deviation to the training results. The uncertainty caused by the \\ndata can be solved by preprocessing operations such as normalization, \\nbut in fact, the weights of different tasks will also lead to uncertainty, \\nthat is, the homoscedastic uncertainty [33]. In deep learning based \\nmedical image reconstruction tasks, homoscedastic uncertainty is \\nclosely related to the parameter weights of the task. The higher the \\nhomoscedastic uncertainty, the higher the noise in the output of the \\nassociated task and the harder the task is to learn. On the other hand, \\nthis paper introduces different image augmentation algorithms into the \\nmulti-stage training strategy. Ideally, in a sufficiently robust model, \\ndifferent image enhancement algorithms would generate the same re-\\nsults. However, the use of convolutional layers for sampling in deep \\nlearning algorithms causes some loss of detail, which results in different \\nimage enhancement algorithms generating different results. This can \\nlead to serious medical errors in clinical applications. To address this \\nissue, we incorporate an uncertainty loss function to optimize the model \\nparameters and enhance the robustness of the model. As shown in Eq. \\n(3), in the training process, the weight of the reconstruction network is \\nadaptively changed according to the homoscedastic uncertainty of the \\ntask, so as to make the training of the task model more effective. Among \\nthem, L (w) is the loss function shown in Eq. (1) and σ is the standard \\ndeviation of the Gaussian distribution. \\nLTask−agnostic(w, σ) = 1\\n2σ2 L(w) + log σ\\n(3)  \\n3.4. Supervised learning \\nIn the supervised learning task, the teacher and assistant models will \\nbe trained and the teacher model will be fine-tuned. Therefore, the loss \\nfunction of this part includes two parts: The loss function shown in Eq. \\n(4) used in the teacher/agnostic model is MSE (Mean Squared Error). \\nThe assistant model uses the Smooth L1 loss function for optimization \\niterations, as shown in Eq. (5) and (6). \\nLMSE(Y, y) = 1\\nK\\n∑\\nK\\nk=1\\n(Yk − yk)2\\n(4)  \\nLAssistant\\n(\\nyA, Y\\n)\\n=\\n∑\\ni∈{X,Y}\\nsmoothL1\\n(\\nyA\\ni , Yi\\n)\\n(5)  \\nsmoothL1 =\\n{\\n0.5(Y − y)2, if |Y − y| < 1\\n|Y − y| − 0.5, otherwise\\n(6) \\nIn summary, the core of the algorithm in this paper is to implement a \\nsemi-supervised learning-based CT volumetric image reconstruction \\ntask by combining GCC and an uncertainty loss function. On the other \\nhand, a multi-stage training strategy is proposed in this paper, which can \\nobtain more accurate generation results through multiple rounds of \\ntraining, thus helping doctors to perform more accurate preoperative or \\nintraoperative planning tasks in practical clinical applications. \\n4. Experimental design \\n4.1. Datasets \\nThe training data used in the self-supervised learning framework \\nconstructed in this paper comes from the LIDC-IDRI lung cancer detec-\\ntion public data set [34]. The data set consists of 1081 cases of chest CT \\nvolumetric images. Owing to the lack of X-ray images corresponding to \\nCT volumetric images, the training data input to the network model is a \\n2D projection which projected from the data set through the digitally \\nreconstructed radio algorithm (DRR) [35,36]. The DDR algorithm refers \\nto the result of viewing the 3D reconstructed image from the field \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n6\\ndirection or from the direction of the X-ray target similar to the analog \\npositioning machine. The algorithm is mainly divided into parallel-ray \\nmethod and point-source method. Among them, the point-source \\nmethod can randomly select the light source point, thereby changing \\nthe image slightly. Therefore, we adopt this method as the projection \\nalgorithm to obtain the corresponding 2D projection. Specifically, in \\nterms of specific algorithm settings, we obtain the information such as \\nthe spacing, size, and orientation of the CT image and use it as the input \\nof the DRR algorithm, and then resample the image through coordinate \\ntransformation. In addition, we set the distance from the light source to \\nthe projection plane to 400 mm, the default pixel spacing of the pro-\\njection pixel plane is 0.8 × 0.8, and the threshold is set to −80, then \\nchange the image size to 128 × 128. The ground truth is the corre-\\nsponding chest CT volumetric image, the data size of each slice in this \\ndataset is 512 × 512. Since the data set is collected from different data \\nsources, the number of slices is not uniform. Therefore, in order to \\nfacilitate network training, the CT volumetric image data are organized \\ninto 128 × 128 × 128 through preprocessing methods such as center \\ncropping, scale scaling, and normalization. The paired X-ray image is \\nalso generated through the DRR algorithm. To be noticed, the current \\nresolution is 128 × 128, which may affect the accuracy of reconstruc-\\ntion, but it does not represent the limitations of the method based on \\ndeep learning. The spatial resolution can be improved through \\nsuper-resolution algorithms to obtain higher resolution images. The \\noriginal data sets will be divided into paired CT-DRR data and unpaired \\nDRR data, and the two data sets will be augmented to 9412 and 38,226 \\nseparately by data augmentation method. \\n4.2. Evaluation metrics \\nIn order to evaluate the performance of the model, the predicted \\nreconstruction results from the Semi-XctNet model are evaluated on the \\ntest set. This paper uses four evaluation functions for model evaluation, \\nnamely: MSE (mean squared error), MAE (mean absolute error), SSIM \\n(structural similarity measurement) and PSNR (peak signal noise ratio). \\nMSE and MAE are used to evaluate the deviation between the predicted \\nreconstruction result and the target value. The image evaluation metric \\nSSIM, incorporating the information of luminance, contrast and struc-\\ntures, is used to evaluate the degree of similarity between images. The \\ncommonly used PSNR is applied to evaluate the quality of our recon-\\nstructed volumetric images. Generally, reconstructed volumetric images \\nwith better structure and higher resolution will have higher SSIM and \\nPSNR values. \\n4.3. Training details \\nThe model proposed in this paper are trained using Pytorch on a \\ndevice with three NVIDIA Tesla V100 graphic processing units. Firstly, \\nthe baseline model is trained in the labeled data set and then the model \\nis applied as the first iteration of teacher model to provide pseudo-labels \\nfor teacher model training. Secondly, an adaptive learning rate is \\nadopted to adjust the learning rate during the training process according \\nto the training situation of the network during the training process. Each \\nround of training iterates 32 epochs, for a total of four rounds of training. \\nThose models obtained in the three rounds is named as ResXct (baseline \\nmodel), CBAM/ECAXct and XctNet. \\n5. Results and discussion \\nAs shown in Fig. 3, the performance of different models is shown in \\nthe test set. The HU (Hounsfiled Unit) value reflects the degree of tissue \\nabsorption of X-rays. Values within different ranges can represent \\ndifferent organs. Since our algorithm reconstructs the entire thoracic \\ndata, in order to better visualize the reconstruction effect, we adaptively \\ndisplay the reconstruction effect according to the HU value of the bony \\narea. Fig. 3 (a) reveals the ground truth of a 2D projection randomly \\nselected from the test set and the visualization results of 3D bony areas \\ngenerated for the convenience of observing the reconstruction effect. \\nFig. 3 (b)–(c) respectively represent the volumetric images and 3D bone \\nregion visualization results obtained based on the supervised learning \\nmodel. It is worth noting that, for the convenience of comparison, the \\nsupervised learning dataset and semi-supervised dataset on which the \\nmodels presented are of the same order of magnitude. The number of \\nslices shown in Fig. 3 are 5, 25, 45, 65 and 85 respectively. It can be seen \\nthat the results reconstructed by XctNet network model are more ac-\\ncurate. Fig. 3 (d) shows the volumetric images and 3D visualization of \\nbony regions based on the semi-supervised learning strategy proposed in \\nthis paper. From the reconstruction results, we can see that the details of \\nthe reconstruction results from the Semi-XctNet model are richer than \\nReconNet [8]. \\nIn order to better show the superiority of the reconstruction results of \\nSemi-XctNet, four evaluation functions have adopted to verify the \\nreconstruction results on the test set. Among them, the evaluation \\nmethod we use is to evaluate each test set data which contain 128 \\nFig. 3. Volumetric images comparison of ReconNet, XctNet and Semi-XctNet on LIDC-IDRI data set. (a)–(d) represent the ground truth of a randomly selected \\n2D projection and the corresponding reconstruction results from different network, respectively. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n7\\nvolumetric images, and take the average of the evaluation results of all \\nsamples in the test set as the final reference value, which are shown in \\nTable 3. We analyze from the two learning strategies of supervised \\nlearning and semi-supervised learning respectively. From the evaluation \\nvalues of all models based on supervised learning, the XctNet model we \\nconstructed can achieve the highest performance. The performance of \\nReconNet is very close to the baseline model, it can be seen that adding \\nmulti-scale feature fusion module and attention mechanism is effective \\nto improve the quality of reconstruction results. In addition, comparing \\nthe evaluation values of the model based on semi-supervised learning, it \\ncan be seen that although the Semi-XctNet model does not perform as \\nwell as the supervised learning based XctNet model, the Semi-XctNet \\nmodel can achieve better evaluation results compared to the results of \\nother supervised learning models. Therefore, through the above anal-\\nysis, we conclude that when the model structure is more excellent, the \\nreconstruction performance obtained by the training mode of semi- \\nsupervised learning can also surpass the existing supervised learning \\nmethods. \\nThis paper mainly discusses how to solve the problem of difficult \\nlabeling of current medical data from the training method of semi- \\nsupervised learning. To achieve our expected results, a series of strate-\\ngies are adopted to improve the reconstruction accuracy of the model. \\nMulti-stage Semi-Supervised Learning: Pseudo-label is the main \\nmethod used in this paper to guide model training on unlabeled data. \\nTherefore, the accuracy of pseudo-label will largely determine the \\nreconstruction quality of semi-supervised learning. In this paper, a \\nmulti-stage semi-supervised learning strategy is designed to endow \\nsemi-supervised learning with better performance by constantly \\nTable 3 \\nThe Reconstruction Results Based on Semi-supervised Learning are Evaluated on \\nthe Verification Set.   \\nTraining Method \\nMAE \\nMSE \\nPSNR \\nSSIM \\nReconNet \\nSupervised \\n0.0211 \\n0.0017 \\n28.4891 \\n0.8349 \\nResXct(baseline \\nmodel) \\nSupervised \\n0.0213 \\n0.0019 \\n27.8489 \\n0.8250 \\nCBAM/ECAXct \\nSupervised \\n0.0205 \\n0.0017 \\n28.1859 \\n0.8351 \\nXctNet \\nSupervised \\n0.0176 \\n0.0013 \\n29.2823 \\n0.8681 \\nIterative Semi- \\nSupervised \\n0.0198 \\n0.0015 \\n28.7344 \\n0.8384  \\nTable 4 \\nMulti-stage semi-supervised learning training results.  \\nItr \\nNetwork Structure \\nTraining Set \\nValidation Set \\nStudent \\nTeacher \\nLabeled \\nPseudo-labeled \\nMAE \\nMSE \\nSSIM \\nPSNR \\n0 \\nResXct \\n/ \\n✓ \\n/ \\n0.025 \\n0.0022 \\n26.922 \\n0.761 \\n1 \\nCBAM/ECAXct \\nResXct \\n✓ \\n✓ \\n0.033 \\n0.0033 \\n25.457 \\n0.691 \\n2 \\nXctNet \\nCBAM/ECAXct \\n✓ \\n✓ \\n0.022 \\n0.0018 \\n28.084 \\n0.804 \\n3 \\nXctNet \\nXctNet \\n✓ \\n✓ \\n0.020 \\n0.0015 \\n28.734 \\n0.838  \\nFig. 4. Model performance of semi-supervised learning with different numbers of labeled data. The red part represents the performance of the model based on \\nsemi-supervised learning, and the black part represents the performance of the model based on supervised learning. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n8\\nchanging the teacher network model. The effectiveness of this strategy is \\nverified as shown in Table 4. First, the baseline model is adopted to \\nperform pre-training on the labeled dataset in the first stage. Second, the \\npre-trained model is used from the first stage as a teacher network to \\nguide the training of the CBAM/ECAXct model in the second stage. This \\ntraining strategy is also adopted in the subsequent rounds of training, \\nand our final training model, Semi-XctNet, can be obtained. From the \\nevaluation results in Table 4, the reconstructed model is gradually \\nincreasing with the increase of the number of iterations. Therefore, it \\ncan be proved that the semi-supervised learning method based on the \\nmulti-stage training strategy can effectively improve the reconstruction \\nperformance of the model. \\nSemi-supervised learning vs. Supervised learning: The performance \\nof semi-supervised learning is explored by setting different amounts of \\nground truth but fixed amount of pseudo-label. As can be seen from the \\nprevious experimental settings, there are two data sets used for semi- \\nsupervised learning model training, namely paired CT-DRR data set \\nand unpaired DRR data set. In order to verify the effectiveness of semi- \\nsupervised learning, we fixed the amount of data in the unpaired DRR \\ndata set, and only used 10%, 20%, 30%, and 40% of the data in the \\npaired CT-DRR data set for model training. The sizes of the four datasets \\nare 1159, 2172, 3330 and 4644. As shown in Fig. 4(a)-(b), the MAE and \\nMSE values obtained by the semi-supervised learning model are evenly \\ndistributed around 0.024 and 0.0022 and the minimum MAE and MSE \\nvalues obtained by the supervised learning model are 0.028 and 0.0024, \\nrespectively. As shown in Fig. 4(c)–(d), the PSNR and SSIM values ob-\\ntained by the semi-supervised learning model are evenly distributed \\naround 27.24 and 0.7732 and the maximum PSNR and SSIM values \\nobtained by the supervised learning model are 26.434 and 0.7658, \\nrespectively. Therefore, it can be seen that the performance of semi- \\nsupervised learning outperforms supervised learning on the four data-\\nsets containing different labeled data. Moreover, the performance of the \\nmodel based on semi-supervised learning tends to be the average in the \\nfour training sets containing different amounts of labeled data, while the \\nperformance of the supervised learning algorithm tends to increase with \\nthe increase of labeled data. \\nThe performance of the supervised learning-based model tends to \\nincrease as the amount of data increases. This phenomenon also dem-\\nonstrates that a semi-supervised learning-based model for volumetric \\ndata reconstruction can achieve satisfactory results with only a small \\namount of annotated data. It is worth noting that this result is not only \\napplicable to the reconstruction task in this paper, but also to other \\nFig. 5. Validation results of appending assistant modules. The degree of change in image entropy is closely related to the content and gray value of the image. It \\ncan be seen that the richer the content of the sliced image, the higher the image information entropy, that is, the color of the entropy map tends to be warmer. \\nFig. 6. Visualization of volumetric image reconstruction results based on 2D projection. (a)–(b) are the 3D visualization results of the Semi-XctNet model \\ncombined with the assistant module and the Semi-XctNet model without the assistant module on three randomly selected volumetric data, respectively. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n9\\nmedical clinical applications where it is difficult to annotate the data. \\nAvailability of Assistant modules: To reduce the pixel loss caused by \\nusing the end-to-end neural network model for the volumetric image \\nreconstruction task. An assistant module based on super-resolution \\nreconstruction technique is used to enhance the image quality of \\npseudo-label, thereby improving the reconstruction effect based on \\nsemi-supervised learning. As shown in Fig. 5, we randomly selected an \\nexample from the test set and showed some slice image data generated \\nby the Semi-XctNet model with or without assistant module. The \\nnumbers of these slice data are 7, 43, 67, 91 and 111 respectively. From \\nthe heat map of the volumetric images, the model combined with the \\nassistant module has a clearer division of image details. The volume data \\nreconstructed from the network contains 128 slice data. From the \\nextracted slices, the reconstruction quality of the whole volume data can \\nbe observed. As can be seen from Fig. 5 (a)–(b), the overall recon-\\nstruction quality of volumetric data can be further improved in combi-\\nnation with the assistant module. \\nFurthermore, in order to verify the specificity and sensitivity of the \\nSemi-XctNet with assistant model, we randomly selected three groups of \\ntest samples and performed 3D visualization of their reconstruction re-\\nsults, which are shown in Fig. 6. From the three sets of reconstruction \\nresults, more detailed information can be reconstructed by combining \\nthe assistant module. Specifically, the network model combined with the \\nassistant module allows a more accurate reconstruction of the bony re-\\ngions and each region (thoracic vertebrae, ribs, etc.) can also be well \\nobserved from the visualization results. Thus, it can be demonstrated \\nthat the combination of the assistant modules can greatly improve the \\nspecificity and sensitivity of the model. \\nThe effectiveness of the semi-XctNet model can be verified by the \\nvarious arguments mentioned above. From the performance comparison \\nof supervised and semi-supervised learning, it can be found that semi- \\nsupervised learning has the potential to meet or even exceed the per-\\nformance of supervised learning. In addition, we propose an auxiliary \\nmodule to promote the accuracy of pseudo-labels, and the results are \\nobvious from the comparison of model reconstruction performance with \\nor without the assistant module. However, this dataset used in this paper \\nobtained from different sources and the data format is not highly uni-\\nform. In addition, this data has only raw CT volume data and no clinical \\ncounterpart of X-ray data. Using the network model trained in this paper \\nfor real clinical applications may lead to poor generalization perfor-\\nmance, which is one of the limitations of this paper. However, this \\nlimitation is caused by the data and is not limited by the network model \\nproposed in this paper. Later, we will consider collecting experimental \\ndata from clinical pairs for in-depth study from a clinical perspective, or \\ncombining image style transfer algorithms to transfer features from X- \\nrays to 2D projection images to generate the corresponding volume data. \\n6. Conclusion \\nIn this paper, a novel and effective semi-supervised learning algo-\\nrithm is proposed for the task of reconstructing volumetric CT images \\nfrom a single X-ray image. The whole framework adopts the teacher- \\nassistant-student scheme for training and optimizes the supervised loss \\nand self-supervised loss by combining the image similarity and uncer-\\ntainty factor. To further enhance the robustness of pseudo-labels, a \\nmulti-stage training strategy is proposed to ameliorate the performance \\nof the teacher network. On the other hand, an assistant module is added \\nin this paper to increase the accuracy of pseudo-labels, so that the stu-\\ndent model can be guided to generate more accurate reconstructed im-\\nages. The effectiveness of our method is demonstrated by \\ncomprehensive experimental analysis on public datasets. Furthermore, \\nour proposed semi-supervised learning reconstruction algorithm, as a \\ngeneral algorithm, can be applied to other semi-supervised medical \\nimage analysis tasks. \\nDeclaration of competing interest \\nThe authors declare no competing interest. \\nAcknowledgment \\nThis work was supported in part by National Natural Science Foun-\\ndation of China (Grant No. 62003330, No.62050410349), Shenzhen \\nFundamental Research Funds (Grant No. JCYJ20220818101608019， \\nNo.JCYJ20190807170407391), Natural Science Foundation of Guang-\\ndong Province(Grant No. 2019A1515011699) and Guangdong-Hong \\nKong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy \\nSystems, Shenzhen Institute of Advanced Technology. \\nReferences \\n[1] G. Wang, J.C. Ye, B.D. Man, Deep learning for tomographic image reconstruction, \\nNat. Mach. Intell. 2 (2020) 737–748. \\n[2] B. He, W. Hu, K. Zhang, S. Yuan, X. Han, C. Su, J. Zhao, G. Wang, G. Wang, \\nL. Zhang, Image segmentation algorithm of lung cancer based on neural network \\nmodel, Expet Syst. 39 (2021). \\n[3] K. Hu, L. Zhao, S. Feng, S. Zhang, Q. Zhou, X. Gao, Y. Guo, Colorectal polyp region \\nextraction using saliency detection network with neutrosophic enhancement, \\nComput. Biol. Med. 147 (2022), 105760. \\n[4] A. Qi, D. Zhao, F. Yu, A. Heidari, Z. Wu, Z. Cai, F.S. Alenezi, R.F. Mansour, H. Chen, \\nM. Chen, Directional mutation and crossover boosted ant colony optimization with \\napplication to COVID-19 X-ray image segmentation, Comput. Biol. Med. (2022). \\n[5] G. Wang, J.C. Ye, K. Mueller, J.A. Fessler, Image reconstruction is a new frontier of \\nmachine learning, IEEE Trans. Med. Imag. 37 (6) (2018) 1289–1296. \\n[6] B. Sahiner, A. Pezeshk, L.M. Hadjiiski, X. Wang, K. Drukker, K.H. Cha, M.L. Giger, \\nDeep learning in medical imaging and radiation therapy, Med. Phys. 46 (1) (2019) \\ne1–e36. \\n[7] E. Melhem, A. Assi, R. El Rachkidi, I. Ghanem, EOS® biplanar X-ray imaging: \\nconcept, developments, benefits, and limitations, J. children’s orthopaedics 10 (1) \\n(2016) 1–14. \\n[8] L. Shen, W. Zhao, L. Xing, Patient-specific reconstruction of volumetric computed \\ntomography images from a single projection view via deep learning, Nature \\nbiomedical engineering 3 (11) (2019) 880–888. \\n[9] N. Nozawa, H. Shum, E. Ho, S. Morishima, Single Sketch Image Based 3D Car Shape \\nReconstruction with Deep Learning and Lazy Learning, 2020. \\n[10] J. Feng, Q. Teng, B. Li, X. He, H. Chen, Y. Li, An end-to-end three-dimensional \\nreconstruction framework of porous media from a single two-dimensional image \\nbased on deep learning, Comput. Methods Appl. Mech. Eng. 368 (2020), 113043. \\n[11] K. Fu, J. Peng, Q. He, H. Zhang, Single image 3D object reconstruction based on \\ndeep learning: a review, Multimed. Tool. Appl. 80 (1) (2021) 463–498. \\n[12] P. Henzler, V. Rasche, T. Ropinski, T. Ritschel, Single-image tomography: 3d \\nvolumes from 2d x-rays (2017) arXiv preprint arXiv:1710.04867. \\n[13] X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, Y. Zheng, X2CT-GAN: reconstructing CT \\nfrom biplanar X-rays with generative adversarial networks, in: Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, \\npp. 10619–10628. \\n[14] M.A.R. Ratul, K. Yuan, W. Lee, CCX-rayNet: a class conditioned convolutional \\nneural network for biplanar X-rays to CT volume, in: 2021 IEEE 18th International \\nSymposium on Biomedical Imaging (ISBI), IEEE, 2021, pp. 1655–1659. \\n[15] J.E. Van Engelen, H.H. Hoos, A survey on semi-supervised learning, Mach. Learn. \\n109 (2019) 373–440. \\n[16] D.H. Lee, Pseudo-label: the simple and efficient semi-supervised learning method \\nfor deep neural networks, in: Workshop on challenges in representation learning, \\nICML 3, 2013, p. 896, 2. \\n[17] S. Laine, T. Aila, Temporal Ensembling for Semi-supervised Learning, 2016 arXiv \\npreprint arXiv:1610.02242. \\n[18] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C.A. Raffel, C.L. Li, Fixmatch: \\nsimplifying semi-supervised learning with consistency and confidence, Adv. Neural \\nInf. Process. Syst. 33 (2020) 596–608. \\n[19] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework for contrastive \\nlearning of visual representations, in: International Conference on Machine \\nLearning, PMLR, 2020, pp. 1597–1607. \\n[20] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, G.E. Hinton, Big self-supervised \\nmodels are strong semi-supervised learners, Adv. Neural Inf. Process. Syst. 33 \\n(2020) 22243–22255. \\n[21] X. Li, L. Yu, H. Chen, C.W. Fu, L. Xing, P.A. Heng, Transformation-consistent self- \\nensembling model for semisupervised medical image segmentation, IEEE Transact. \\nNeural Networks Learn. Syst. 32 (2) (2020) 523–534. \\n[22] L. Yu, S. Wang, X. Li, C.W. Fu, P.A. Heng, Uncertainty-aware self-ensembling \\nmodel for semi-supervised 3D left atrium segmentation, in: International \\nConference on Medical Image Computing and Computer-Assisted Intervention, \\nSpringer, Cham, 2019, pp. 605–613. \\n[23] D. Wang, Y. Zhang, K. Zhang, L. Wang, Focalmix: semi-supervised learning for 3d \\nmedical image detection, in: Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition, 2020, pp. 3951–3960. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Computers in Biology and Medicine 155 (2023) 106663\\n10\\n[24] Y. Luo, J. Zhu, M. Li, Y. Ren, B. Zhang, Smooth neighbors on teacher graphs for \\nsemi-supervised learning, in: Proceedings of the IEEE Conference on Computer \\nVision and Pattern Recognition, 2018, pp. 8896–8905. \\n[25] A. Tarvainen, H. Valpola, Mean teachers are better role models: weight-averaged \\nconsistency targets improve semi-supervised deep learning results, Adv. Neural Inf. \\nProcess. Syst. 30 (2017). \\n[26] Z. Tan, J.Y. Li, H. Tao, S. Li, Y. Hu, XctNet: reconstruction network of volumetric \\nimages from a single X-ray image, Comput. Med. Imag. Graph.: Off. J. Computer. \\nMed. Imag. Soc. 98 (2022), 102067. \\n[27] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, Q. Hu, ECA-net: efficient channel attention \\nfor deep convolutional neural networks, in: 2020 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition (CVPR), 2020, pp. 11531–11539. \\n[28] S. Woo, J. Park, J.Y. Lee, I.S. Kweon, CBAM: convolutional block attention module, \\nin: Proceedings of the European Conference on Computer Vision, ECCV), 2018, \\npp. 3–19. \\n[29] C. Dong, C.C. Loy, K. He, X. Tang, Image super-resolution using deep convolutional \\nnetworks, IEEE Trans. Pattern Anal. Mach. Intell. 38 (2) (2015) 295–307. \\n[30] O. R Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, \\nA. Karpathy, A. Khosla, M.S. Bernstein, A.C. Berg, L. Fei-Fei, ImageNet large scale \\nvisual recognition challenge, Int. J. Comput. Vis. 115 (2015) 211–252. \\n[31] E.D. Cubuk, B. Zoph, J. Shlens, Q.V. Le, Randaugment: practical automated data \\naugmentation with a reduced search space, in: 2020 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition Workshops (CVPRW), 2020, \\npp. 3008–3017. \\n[32] G. Balakrishnan, A. Zhao, M.R. Sabuncu, J.V. Guttag, A.V. Dalca, VoxelMorph: a \\nlearning framework for deformable medical image registration, IEEE Trans. Med. \\nImag. 38 (2019) 1788–1800. \\n[33] A. Kendall, Y. Gal, What Uncertainties Do We Need in Bayesian Deep Learning for \\nComputer Vision? NIPS, 2017. \\n[34] S.G. Armato, G. McLennan, L.M. Bidaut, M.F. McNitt-Gray, Meyer, B.Y. Croft, The \\nlung image database consortium (LIDC) and image database resource initiative \\n(IDRI): a completed reference database of lung nodules on CT scans, Med. Phys. 38 \\n2 (2011) 915–931. \\n[35] A. Moturu, A. Chang, Creation of synthetic X-rays to train a neural network to \\ndetect lung cancer, J. Beyond Sci. Initiat. Univ. Toronto (2018). \\n[36] T.G. Feeman, The Mathematics of Medical Imaging, Sringer, 2010. \\nZ. Tan et al.                                                                                                                                                                                                                                      \\n', metadata={'source': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'file_path': 'data/Tan et al. - 2023 - Semi-XctNet Volumetric images reconstruction netw.pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'Semi-XctNet: Volumetric images reconstruction network from a single projection image via semi-supervised learning', 'author': 'Zhiqiang Tan', 'subject': 'Computers in Biology and Medicine, 155 (2023) 106663. doi:10.1016/j.compbiomed.2023.106663', 'keywords': '3D Reconstruction,Semi-supervised learning,Data augmentation,X-ray,Volumetric images', 'creator': 'Elsevier', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': 'D:20230307163741Z', 'modDate': 'D:20230307213429Z', 'trapped': ''}),\n",
       " Document(page_content='Block-NeRF: Scalable Large Scene Neural View Synthesis\\nMatthew Tancik1⇤\\nVincent Casser2\\nXinchen Yan2\\nSabeek Pradhan2\\nBen P. Mildenhall3\\nPratul Srinivasan3\\nJonathan T. Barron3\\nHenrik Kretzschmar2\\n1UC Berkeley\\n2Waymo\\n3Google Research\\nAlamo Square, SF\\n1 km\\nBlock-NeRF\\nSept.\\nJune\\nFigure 1. Block-NeRF is a method that enables large-scale scene reconstruction by representing the environment using multiple compact\\nNeRFs that each ﬁt into memory. At inference time, Block-NeRF seamlessly combines renderings of the relevant NeRFs for the given area.\\nIn this example, we reconstruct the Alamo Square neighborhood in San Francisco using data collected over 3 months. Block-NeRF can\\nupdate individual blocks of the environment without retraining on the entire scene, as demonstrated by the construction on the right. Video\\nresults can be found on the project website waymo.com/research/block-nerf.\\nAbstract\\nWe present Block-NeRF, a variant of Neural Radiance\\nFields that can represent large-scale environments. Specif-\\nically, we demonstrate that when scaling NeRF to render\\ncity-scale scenes spanning multiple blocks, it is vital to de-\\ncompose the scene into individually trained NeRFs. This\\ndecomposition decouples rendering time from scene size, en-\\nables rendering to scale to arbitrarily large environments,\\nand allows per-block updates of the environment. We adopt\\nseveral architectural changes to make NeRF robust to data\\ncaptured over months under different environmental condi-\\ntions. We add appearance embeddings, learned pose reﬁne-\\nment, and controllable exposure to each individual NeRF,\\nand introduce a procedure for aligning appearance between\\nadjacent NeRFs so that they can be seamlessly combined. We\\nbuild a grid of Block-NeRFs from 2.8 million images to cre-\\nate the largest neural scene representation to date, capable\\nof rendering an entire neighborhood of San Francisco.\\n1. Introduction\\nRecent advancements in neural rendering such as Neural\\nRadiance Fields [40] have enabled photo-realistic reconstruc-\\n*Work done as an intern at Waymo.\\ntion and novel view synthesis given a set of posed camera im-\\nages [3,38,44]. Earlier works tended to focus on small-scale\\nand object-centric reconstruction. Though some methods\\nnow address scenes the size of a single room or building,\\nthese are generally still limited and do not na¨ıvely scale up\\nto city-scale environments. Applying these methods to large\\nenvironments typically leads to signiﬁcant artifacts and low\\nvisual ﬁdelity due to limited model capacity.\\nReconstructing large-scale environments enables several\\nimportant use-cases in domains such as autonomous driv-\\ning [30,43,69] and aerial surveying [14,33]. For example,\\na high-ﬁdelity map of the operating domain can serve as\\na prior for robot navigation. Large-scale scene reconstruc-\\ntions can be used for closed-loop robotic simulations [13].\\nAutonomous driving systems are commonly evaluated by re-\\nsimulating previously encountered scenarios. Any deviation\\nfrom the recorded encounter, however, may change the vehi-\\ncle’s trajectory, requiring high-ﬁdelity novel view renderings\\nalong the altered path. Scene conditioned NeRFs can further\\naugment simulation scenarios by changing environmental\\nlighting conditions, such as camera exposure, weather, or\\ntime of day.\\nReconstructing such large-scale environments introduces\\nadditional challenges, including the presence of transient\\nobjects (cars and pedestrians), limitations in model capacity,\\n18248\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='along with memory and compute constraints. Furthermore,\\ntraining data for such large environments is highly unlikely\\nto be collected in a single capture under consistent condi-\\ntions. Rather, data for different parts of the environment may\\nneed to be sourced from different data collection efforts, in-\\ntroducing variance in both scene geometry (e.g., construction\\nwork and parked cars), as well as appearance (e.g., weather\\nconditions and time of day).\\nWe extend NeRF with appearance embeddings and\\nlearned pose reﬁnement to address the environmental\\nchanges and pose errors in the collected data. We addi-\\ntionally add exposure conditioning to provide the ability\\nto modify the exposure during inference. We refer to this\\nmodiﬁed model as a Block-NeRF. Scaling up the network\\ncapacity of Block-NeRF enables the ability to represent in-\\ncreasingly large scenes. However this approach comes with a\\nnumber of limitations; rendering time scales with the size of\\nthe network, networks can no longer ﬁt on a single compute\\ndevice, and updating or expanding the environment requires\\nretraining the entire network.\\nTo address these challenges, we propose dividing up large\\nenvironments into individually trained Block-NeRFs, which\\nare then rendered and combined dynamically at inference\\ntime. Modeling these Block-NeRFs independently allows\\nfor maximum ﬂexibility, scales up to arbitrarily large en-\\nvironments and provides the ability to update or introduce\\nnew regions in a piecewise manner without retraining the\\nentire environment as demonstrated in Figure 1. To com-\\npute a target view, only a subset of the Block-NeRFs are\\nrendered and then composited based on their geographic lo-\\ncation compared to the camera. To allow for more seamless\\ncompositing, we propose an appearance matching technique\\nwhich brings different Block-NeRFs into visual alignment\\nby optimizing their appearance embeddings.\\n2. Related Work\\n2.1. Large Scale 3D Reconstruction\\nResearchers have been developing and reﬁning tech-\\nniques for 3D reconstruction from large image collections\\nfor decades [1,16,31,46,56,78], and much current work re-\\nlies on mature and robust software implementations such as\\nCOLMAP to perform this task [54]. Nearly all of these recon-\\nstruction methods share a common pipeline: extract 2D im-\\nage features (such as SIFT [37]), match these features across\\ndifferent images, and jointly optimize a set of 3D points and\\ncamera poses to be consistent with these matches (the well-\\nexplored problem of bundle adjustment [23,64]). Extending\\nthis pipeline to city-scale data is largely a matter of imple-\\nmenting highly robust and parallelized versions of these\\nalgorithms, as explored in work such as Photo Tourism [56]\\nand Building Rome in a Day [1]. Core graphics research\\nhas also explored breaking up scenes for fast high quality\\nrendering [36].\\nBlock-NeRF Origin\\nBlock-NeRF Training Radius\\nVisibility Prediction\\nColor Prediction\\nCombined\\nColor Prediction\\nTarget View\\nDiscarded\\nFigure 2. The scene is split into multiple Block-NeRFs that are each\\ntrained on data within some radius (dotted orange line) of a speciﬁc\\nBlock-NeRF origin coordinate (orange dot). To render a target\\nview in the scene, the visibility maps are computed for all of the\\nNeRFs within a given radius. Block-NeRFs with low visibility are\\ndiscarded (bottom Block-NeRF) and the color output is rendered\\nfor the remaining blocks. The renderings are then merged based on\\neach block origin’s distance to the target view.\\nThese approaches typically output a camera pose for each\\ninput image and a sparse 3D point cloud. To get a complete\\n3D scene model, these outputs must be further processed by\\na dense multi-view stereo algorithm (e.g., PMVS [18]) to\\nproduce a dense point cloud or triangle mesh. This process\\npresents its own scaling difﬁculties [17]. The resulting 3D\\nmodels often contain artifacts or holes in areas with limited\\ntexture or specular reﬂections as they are challenging to\\ntriangulate across images. As such, they frequently require\\nfurther postprocessing to create models that can be used to\\nrender convincing imagery [55]. However, this task is mainly\\nthe domain of novel view synthesis, and 3D reconstruction\\ntechniques primarily focus on geometric accuracy.\\nIn contrast, our approach does not rely on large-scale\\nSfM to produce camera poses, instead performing odome-\\ntry using various sensors on the vehicle as the images are\\ncollected [63].\\n2.2. Novel View Synthesis\\nGiven a set of input images of a given scene and their\\ncamera poses, novel view synthesis seeks to render observed\\nscene content from previously unobserved viewpoints, al-\\nlowing a user to navigate through a recreated environment\\nwith high visual ﬁdelity.\\nGeometry-based\\nImage\\nReprojection.\\nMany\\nap-\\nproaches to view synthesis start by applying traditional 3D\\nreconstruction techniques to build a point cloud or triangle\\nmesh representing the scene. This geometric “proxy” is\\n28249\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='then used to reproject pixels from the input images into\\nnew camera views, where they are blended by heuristic [6]\\nor learning-based methods [24,51,52]. This approach has\\nbeen scaled to long trajectories of ﬁrst-person video [29],\\npanoramas collected along a city street [28], and single\\nlandmarks from the Photo Tourism dataset [39]. Methods\\nreliant on geometry proxies are limited by the quality of the\\ninitial 3D reconstruction, which hurts their performance in\\nscenes with complex geometry or reﬂectance effects.\\nVolumetric Scene Representations.\\nRecent view synthe-\\nsis work has focused on unifying reconstruction and render-\\ning and learning this pipeline end-to-end, typically using\\na volumetric scene representation. Methods for rendering\\nsmall baseline view interpolation often use feed-forward\\nnetworks to learn a mapping directly from input images to\\nan output volume [15, 77], while methods such as Neural\\nVolumes [35] that target larger-baseline view synthesis run\\na global optimization over all input images to reconstruct\\nevery new scene, similar to traditional bundle adjustment.\\nNeural Radiance Fields (NeRF) [40] combines this single-\\nscene optimization setting with a neural scene representation\\ncapable of representing complex scenes much more efﬁ-\\nciently than a discrete 3D voxel grid; however, its rendering\\nmodel scales very poorly to large-scale scenes in terms of\\ncompute. Followup work has proposed making NeRF more\\nefﬁcient by partitioning space into smaller regions, each\\ncontaining its own lightweight NeRF network [42,47,48].\\nUnlike our method, these network ensembles must be trained\\njointly, limiting their ﬂexibility. Another approach is to pro-\\nvide extra capacity in the form of a coarse 3D grid of latent\\ncodes [34]. This approach has also been applied to compress\\ndetailed 3D shapes into neural signed distance functions [61]\\nand to represent large scenes using occupancy networks [45].\\nConcurrent works Mega-NeRF [65] and CityNeRF [67]\\nutilize NeRFs to represent large scenes. Mega-NeRF splits\\ndata captured from drones into multiple partitions to train\\nspecialized NeRFs. CityNeRF learns a multi-scale represen-\\ntation from satellite imagery.\\nWe build our Block-NeRF implementation on top of mip-\\nNeRF [3], which improves aliasing issues that hurt NeRF’s\\nperformance in scenes where the input images observe the\\nscene from many different distances. We incorporate tech-\\nniques from NeRF in the Wild (NeRF-W) [38], which adds\\na latent code per training image to handle inconsistent scene\\nappearance when applying NeRF to landmarks from the\\nPhoto Tourism dataset. NeRF-W creates a separate NeRF\\nfor each landmark from thousands of images, whereas our\\napproach combines many NeRFs to reconstruct a coherent\\nlarge environment from millions of images. Our model also\\nincorporates a learned camera pose reﬁnement which has\\nbeen explored in previous works [32,58,66,70,71].\\nSome NeRF-based methods use segmentation data to\\nd\\nd\\nf\\nRGB\\nx\\nσ\\nσ\\nfc\\nExposure\\nIntegrated Positional\\nEncoding\\nPositional Encoding\\nAppearance Embedding\\nf\\nx\\nv\\nVisibility\\nFigure 3. Our model is an extension of the model presented in\\nmip-NeRF [3]. The ﬁrst MLP fσ predicts the density σ for a\\nposition x in space. The network also outputs a feature vector\\nthat is concatenated with viewing direction d, the exposure level,\\nand an appearance embedding. These are fed into a second MLP\\nfc that outputs the color for the point. We additionally train a\\nvisibility network fv to predict whether a point in space was visible\\nin the training views, which is used for culling Block-NeRFs during\\ninference.\\nisolate and reconstruct static [68] or moving objects (such\\nas people or cars) [43,74] across video sequences. As we\\nfocus primarily on reconstructing the environment itself, we\\nchoose to simply mask out dynamic objects during training.\\n2.3. Urban Scene Camera Simulation\\nCamera simulation has become a popular data source\\nfor training and validating autonomous driving systems on\\ninteractive platforms [2,27]. Early works [13,19,50,53] syn-\\nthesized data from scripted scenarios and manually created\\n3D assets. These methods suffered from domain mismatch\\nand limited scene-level diversity. Several recent works tackle\\nthe simulation-to-reality gaps by minimizing the distribution\\nshifts in the simulation and rendering pipeline. Kar et al. [26]\\nand Devaranjan et al. [12] proposed to minimize the scene-\\nlevel distribution shift from rendered outputs to real camera\\nsensor data through a learned scenario generation frame-\\nwork. Richter et al. [49] leveraged intermediate rendering\\nbuffers in the graphics pipeline to improve photorealism of\\nsynthetically generated camera images.\\nTowards the goal of building photo-realistic and scalable\\ncamera simulation, prior methods [9, 30, 69] leverage rich\\nmulti-sensor driving data collected during a single drive to\\nreconstruct 3D scenes for object injection [9] and novel view\\nsynthesis [69] using modern machine learning techniques, in-\\ncluding image GANs for 2D neural rendering. Relying on a\\nsophisticated surfel reconstruction pipeline, SurfelGAN [69]\\nis still susceptible to errors in graphical reconstruction and\\ncan suffer from the limited range and vertical ﬁeld-of-view\\nof LiDAR scans. In contrast to existing efforts, our work\\ntackles the 3D rendering problem and is capable of modeling\\nthe real camera data captured from multiple drives under\\nvarying environmental conditions, such as weather and time\\nof day, which is a prerequisite for reconstructing large-scale\\nareas.\\n38250\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='3. Background\\nWe build upon NeRF [40] and its extension mip-NeRF [3].\\nHere, we summarize relevant parts of these methods. For\\ndetails, please refer to the original papers.\\n3.1. NeRF and mip-NeRF Preliminaries\\nNeural Radiance Fields (NeRF) [40] is a coordinate-based\\nneural scene representation that is optimized through a dif-\\nferentiable rendering loss to reproduce the appearance of a\\nset of input images from known camera poses. After opti-\\nmization, the NeRF model can be used to render previously\\nunseen viewpoints.\\nThe NeRF scene representation is a pair of multilayer\\nperceptrons (MLPs). The ﬁrst MLP fσ takes in a 3D position\\nx and outputs volume density σ and a feature vector. This\\nfeature vector is concatenated with a 2D viewing direction\\nd and fed into the second MLP fc, which outputs an RGB\\ncolor c. This architecture ensures that the output color can\\nvary when observed from different angles, allowing NeRF\\nto represent reﬂections and glossy materials, but that the\\nunderlying geometry represented by σ is only a function of\\nposition.\\nEach pixel in an image corresponds to a ray r(t) = o +\\ntd through 3D space. To calculate the color of r, NeRF\\nrandomly samples distances {ti}N\\ni=0 along the ray and passes\\nthe points r(ti) and direction d through its MLPs to calculate\\nσi and ci. The resulting output color is\\ncout =\\nN\\nX\\ni=1\\nwici,\\nwhere wi = Ti(1 − e−∆iσi),\\n(1)\\nTi = exp\\n0\\n@−\\nX\\nj<i\\n∆jσj\\n1\\nA ,\\n∆i = ti − ti−1 .\\n(2)\\nThe full implementation of NeRF iteratively resamples the\\npoints ti (by treating the weights wi as a probability distribu-\\ntion) in order to better concentrate samples in areas of high\\ndensity.\\nTo enable the NeRF MLPs to represent higher frequency\\ndetail [62], the inputs x and d are each preprocessed by a\\ncomponentwise sinusoidal positional encoding γPE:\\nγPE(z) = [sin(20z), cos(20z), . . . , sin(2L−1z), cos(2L−1z)] (3)\\nwhere L is the number of levels of positional encoding.\\nNeRF’s MLP fσ takes a single 3D point as input. How-\\never, this ignores both the relative footprint of the corre-\\nsponding image pixel and the length of the interval [ti−1, ti]\\nalong the ray r containing the point, resulting in aliasing\\nartifacts when rendering novel camera trajectories. Mip-\\nNeRF [3] remedies this issue by using the projected pixel\\nfootprint to sample conical frustums along the ray rather than\\nintervals. To feed these frustums into the MLP, mip-NeRF\\napproximates each of them as Gaussian distributions with\\nparameters µi, ⌃i and replaces the positional encoding γPE\\nwith its expectation over the input Gaussian\\nγIPE(µ, ⌃) = EX⇠N (µ,⌃)[γPE(X)] ,\\n(4)\\nreferred to as an integrated positional encoding.\\n4. Method\\nTraining a single NeRF does not scale when trying to\\nrepresent scenes as large as cities. We instead propose split-\\nting the environment into a set of Block-NeRFs that can\\nbe independently trained in parallel and composited during\\ninference. This independence enables the ability to expand\\nthe environment with additional Block-NeRFs or update\\nblocks without retraining the entire environment (see Fig-\\nure 1). We dynamically select relevant Block-NeRFs for\\nrendering, which are then composited in a smooth manner\\nwhen traversing the scene. To aid with this compositing,\\nwe optimize the appearances codes to match lighting condi-\\ntions and use interpolation weights computed based on each\\nBlock-NeRF’s distance to the novel view.\\n4.1. Block Size and Placement\\nThe individual Block-NeRFs should be arranged such\\nthat they collectively achieve full coverage of the target en-\\nvironment. We typically place one Block-NeRF at each\\nintersection, covering the intersection itself and any con-\\nnected street 75% of the way until it converges into the next\\nintersection (see Figure 1). This results in a 50% overlap\\nbetween any two adjacent blocks on the connecting street\\nsegment, making appearance alignment easier between them.\\nWe make sure to train each Block-NeRF on data that is con-\\nﬁned to a geographic area. This can be automated and only\\nrelies on basic map data, such as OpenStreetMap [22].\\nOther placement heuristics are conceivable. For example,\\nfor some of our experiments, we place Block-NeRFs along a\\nstreet segment at uniform distances and deﬁne the block size\\nto be a sphere around the origin of the blocks (see Figure 2).\\n4.2. Training Individual Block-NeRFs\\n4.2.1\\nAppearance Embeddings\\nGiven that different parts of our data may be captured under\\ndifferent environmental conditions, we follow NeRF-W [38]\\nand use Generative Latent Optimization [5] to optimize per-\\nimage appearance embedding vectors, as shown in Figure 3.\\nThis allows the NeRF to explain away several appearance-\\nchanging conditions, such as varying weather and lighting.\\nWe can additionally manipulate these appearance embed-\\ndings to interpolate between different conditions observed\\nin the training data (such as cloudy versus clear skies, or\\n48251\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Figure 4. The appearance codes allow the model to represent different lighting and weather conditions.\\nday and night). Examples of rendering with different appear-\\nances can be seen in Figure 4. In § 4.3.3, we use test-time\\noptimization over these embeddings to match the appear-\\nance of adjacent Block-NeRFs, which is important when\\ncombining multiple renderings.\\n4.2.2\\nLearned Pose Reﬁnement\\nAlthough we assume that camera poses are provided, we ﬁnd\\nit advantageous to learn regularized pose offsets for further\\nalignment. Pose reﬁnement has been explored in previous\\nNeRF based models [32,58,66,71]. These offsets are learned\\nper driving segment and include both a translation and a\\nrotation component. We optimize these offsets jointly with\\nthe NeRF itself, signiﬁcantly regularizing the offsets in the\\nearly phase of training to allow the network to ﬁrst learn a\\nrough structure prior to modifying the poses.\\n4.2.3\\nExposure Input\\nTraining images may be captured across a wide range of\\nexposure levels, which can impact NeRF training if left\\nunaccounted for. We ﬁnd that feeding the camera exposure\\ninformation to the appearance prediction part of the model\\nallows the NeRF to compensate for the visual differences\\n(see Figure 3). Speciﬁcally, the exposure information is\\nprocessed as γPE(shutter speed ⇥ analog gain/t) where γPE\\nis a sinusoidal positional encoding with 4 levels, and t is\\na scaling factor (we use 1,000 in practice). An example of\\ndifferent learned exposures can be found in Figure 5.\\n4.2.4\\nTransient Objects\\nWhile the appearance embeddings account for variation in\\nappearance, we assume that the scene geometry is consis-\\ntent across the training data. Movable objects (e.g. cars,\\npedestrians) typically violate this assumption. We therefore\\nuse a semantic segmentation model [10] to ignore masks\\nof common movable objects during training. Note that this\\ndoes not account for changes in otherwise static parts of\\nthe environment, e.g. construction, it accommodates most\\ncommon types of geometric inconsistency.\\n4.2.5\\nVisibility Prediction\\nWhen merging multiple Block-NeRFs, it can be useful to\\nknow whether a speciﬁc region of space was visible to a\\ngiven NeRF during training. We extend our model with an\\nadditional small MLP fv that is trained to learn an approx-\\nimation of the visibility of a sampled point (see Figure 3).\\nFor each sample along a training ray, fv takes in the lo-\\ncation and view direction and regresses the corresponding\\ntransmittance of the point (Ti in Equation 2). The model\\nis trained alongside fσ, which provides supervision. Trans-\\nmittance represents how visible a point is from a particular\\ninput camera: points in free space or on the surface of the\\nﬁrst intersected object will have transmittance near 1, and\\npoints inside or behind the ﬁrst visible object will have trans-\\nmittance near 0. If a point is seen from some viewpoints\\nbut not others, the regressed transmittance value will be the\\naverage over all training cameras and lie between zero and\\none, indicating that the point is partially observed. Our visi-\\nbility prediction is similar to the visibility ﬁelds proposed by\\nSrinivasan et al. [57]. However, they used an MLP to predict\\nvisibility to environment lighting to recover a relightable\\nNeRF model, while we predict visibility to training rays.\\nThe visibility network is small and can be run indepen-\\ndently from the color and density networks. This proves\\nuseful when merging multiple NeRFs, since it can help to\\ndetermine whether a speciﬁc NeRF is likely to produce mean-\\ningful outputs for a given location, as explained in § 4.3.1.\\nThe visibility predictions can also be used to determine loca-\\ntions to perform appearance matching between two NeRFs,\\nas detailed in § 4.3.3.\\n4.3. Merging Multiple Block-NeRFs\\n4.3.1\\nBlock-NeRF Selection\\nThe environment can be composed of an arbitrary number\\nof Block-NeRFs. For efﬁciency, we utilize two ﬁltering\\nmechanisms to only render relevant blocks for the given\\ntarget viewpoint. We only consider Block-NeRFs that are\\nwithin a set radius of the target viewpoint. Additionally,\\nfor each of these candidates, we compute the associated\\nvisibility. If the mean visibility is below a threshold, we\\ndiscard the Block-NeRF. An example of visibility ﬁltering\\nis provided in Figure 2. Visibility can be computed quickly\\nbecause its network is independent of the color network, and\\n58252\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Figure 5. Our model is conditioned on exposure, which helps\\naccount for exposure changes present in the training data. This\\nallows users to alter the appearance of the output images in a\\nhuman-interpretable manner during inference.\\nit does not need to be rendered at the target image resolution.\\nAfter ﬁltering, there are typically one to three Block-NeRFs\\nleft to merge.\\n4.3.2\\nBlock-NeRF Compositing\\nWe render color images from each of the ﬁltered Block-\\nNeRFs and interpolate between them using inverse distance\\nweighting between the camera origin c and the centers xi of\\neach Block-NeRF. Speciﬁcally, we calculate the respective\\nweights as wi / distance(c, xi)−p, where p inﬂuences the\\nrate of blending between Block-NeRF renders. The inter-\\npolation is done in 2D image space and produces smooth\\ntransitions between Block-NeRFs. We also explore other\\ninterpolation methods in § 5.4.\\n4.3.3\\nAppearance Matching\\nWe can control the appearance of our learned models by\\nan appearance latent code after the Block-NeRF has been\\ntrained. These codes are randomly initialized during train-\\ning and the same code therefore typically leads to different\\nappearances when fed into different Block-NeRFs. This is\\nundesirable when compositing as it may lead to inconsis-\\ntencies between views. Given a target appearance in one of\\nthe Block-NeRFs, we match its appearance in the remaining\\nblocks. To this end, we ﬁrst select a 3D matching location\\nbetween pairs of adjacent Block-NeRFs. The visibility pre-\\ndiction at this location should be high for both Block-NeRFs.\\nGiven the matching location, we freeze the Block-NeRF net-\\nwork weights and only optimize the appearance code of the\\ntarget in order to reduce the `2 loss between the respective\\narea renders. This optimization is quick, converging within\\n100 iterations. While not necessarily yielding perfect align-\\nment, this procedure aligns most global and low-frequency\\nattributes of the scene, such as time of day, color balance, and\\nweather, which is a prerequisite for successful compositing.\\nFigure 6 shows an example optimization, where appearance\\nmatching turns a daytime scene into nighttime to match the\\nadjacent Block-NeRF. Starting from a root Block-NeRF, we\\npropagate the optimized appearance through the scene by\\niteratively optimizing the appearance of its neighbors. If mul-\\ntiple blocks surrounding a target Block-NeRF have already\\nbeen optimized, we consider each of them when computing\\nthe loss.\\n5. Results and Experiments\\nIn this section we will discuss our datasets and exper-\\niments.\\nWe provide the architectural and optimization\\nspeciﬁcs in the supplement. The supplement also provides\\ncomparisons to reconstructions from COLMAP [54], a tradi-\\ntional Structure from Motion approach. This reconstruction\\nis sparse and fails to represent reﬂective surfaces and the sky.\\n5.1. Datasets\\nWe perform experiments on datasets that we collected\\nfor novel view synthesis of large-scale scenes using data\\ncollection vehicles driving on public roads. Existing public\\nlarge-scale driving datasets are not designed for the task of\\nview synthesis. For example, some datasets lack sufﬁcient\\ncamera coverage (e.g., KITTI [21], Cityscapes [11]) or pri-\\noritize visual diversity over repeated observations of a target\\narea (e.g., NuScenes [7], Waymo Open Dataset [60], Argov-\\nerse [8]). Instead, these datasets are typically designed for\\ntasks such as object detection and tracking.\\nOur dataset includes both long-term sequence data (100 s\\nor more) and distinct sequences captured repeatedly in a\\nparticular target area over a period of several months. We\\nuse image data captured by 12 cameras, where 8 cameras\\nmounted on the roof of the car provide a 360° surround\\nview, and 4 cameras located at the front of the vehicle point\\nforward and sideways. Each camera captures images at\\n10 Hz and stores a scalar exposure value. The vehicle pose\\nis known and all cameras are calibrated. We calculate the\\ncorresponding camera ray origins and directions in a com-\\nmon coordinate system, accounting for the rolling shutter\\nof the cameras. As described in § 4.2.4, we use a semantic\\nsegmentation model [10] to detect movable objects.\\nSan Francisco Alamo Square Dataset.\\nWe select San\\nFrancisco’s Alamo Square neighborhood as the target area\\nfor our scalability experiments. The dataset spans an area\\nof approximately 960 m ⇥ 570 m, and was recorded in June,\\nJuly, and August of 2021. We divide this dataset into 35\\nBlock-NeRFs. Example renderings and Block-NeRF place-\\nments can be seen in Figure 1. To best appreciate the scale\\nof the reconstruction, please refer to supplementary videos.\\nEach Block-NeRF was trained on data from 38 to 48 differ-\\nent data collection runs, adding up to a total driving time of\\n18 to 28 minutes each. After ﬁltering out some redundant\\nimage captures (e.g. stationary captures), each Block-NeRF\\nis trained on between 64,575 to 108,216 images. The over-\\nall dataset is composed of 13.4 h of driving time sourced\\nfrom 1,330 different data collection runs, with a total of\\n2,818,745 training images.\\n68253\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Before Appearance Matching\\nAfter Appearance Matching\\nBase Block-NeRF\\nAdjacent Block-NeRF\\nFigure 6. When rendering scenes based on multiple Block-NeRFs, we use appearance matching to obtain a consistent appearance across the\\nscene. Given a ﬁxed target appearance for one of the Block-NeRFs (left image), we optimize the appearances of the adjacent Block-NeRFs\\nto match. In this example, appearance matching produces a consistent night appearance across Block-NeRFs.\\nSan Francisco Mission Bay Dataset.\\nWe choose San\\nFrancisco’s Mission Bay District as the target area for our\\nbaseline, block size, and placement experiments. Mission\\nBay is an urban environment with challenging geometry and\\nreﬂective facades. We identiﬁed a long stretch on Third\\nStreet with far-range visibility, making it an interesting test\\ncase. Notably, this dataset was recorded in a single capture in\\nNovember 2020, with consistent environmental conditions al-\\nlowing for simple evaluation. This dataset was recorded over\\n100 s, in which the data collection vehicle traveled 1.08 km\\nand captured 12,000 total images from 12 cameras. We will\\nrelease this single-capture dataset to aid reproducibility.\\n5.2. Model Ablations\\nNeRFs\\nPSNR\"\\nSSIM\"\\nLPIPS#\\nmip-NeRF\\n17.86\\n0.563\\n0.509\\nOurs\\n-Appearance\\n20.13\\n0.611\\n0.458\\n-Exposure\\n23.55\\n0.649\\n0.418\\n-Pose Opt.\\n23.05\\n0.625\\n0.442\\nFull\\n23.60\\n0.649\\n0.417\\nTable 1. Ablations of different Block-NeRF components on a\\nsingle intersection in the Alamo Square dataset. We show the\\nperformance of mip-NeRF as a baseline, as well as the effect of\\nremoving individual components from our method.\\nWe ablate our model modiﬁcations on a single intersec-\\ntion from the Alamo Square dataset. We report PSNR, SSIM,\\nand LPIPS [76] metrics for the test image reconstructions\\nin Table 1. The test images are split in half vertically, with\\nthe appearance embeddings being optimized on one half and\\ntested on the other. We also provide qualitative examples\\nin Figure 7. Mip-NeRF alone fails to properly reconstruct\\nthe scene and is prone to adding non-existent geometry and\\ncloudy artifacts to explain the differences in appearance.\\nWhen our method is not trained with appearance embed-\\ndings, these artifacts are still present. If our method is not\\ntrained with pose optimization, the resulting scene is blurrier\\nand can contain duplicated objects due to pose misalignment.\\nFinally, the exposure input marginally improves the recon-\\nstruction, but more importantly provides us with the ability\\nto change the exposure during inference.\\n5.3. Block-NeRF Size and Placement\\n# Blocks\\nWeights / Total\\nSize\\nCompute\\nPSNR\"\\nSSIM\"\\nLPIPS#\\n1\\n0.25M / 0.25M\\n544 m\\n1⇥\\n23.83\\n0.825\\n0.381\\n4\\n0.25M / 1.00M\\n271 m\\n2⇥\\n25.55\\n0.868\\n0.318\\n8\\n0.25M / 2.00M\\n116 m\\n2⇥\\n26.59\\n0.890\\n0.278\\n16\\n0.25M / 4.00M\\n54 m\\n2⇥\\n27.40\\n0.907\\n0.242\\n1\\n1.00M / 1.00M\\n544 m\\n1⇥\\n24.90\\n0.852\\n0.340\\n4\\n0.25M / 1.00M\\n271 m\\n0.5⇥\\n25.55\\n0.868\\n0.318\\n8\\n0.13M / 1.00M\\n116 m\\n0.25⇥\\n25.92\\n0.875\\n0.306\\n16\\n0.07M / 1.00M\\n54 m\\n0.125⇥\\n25.98\\n0.877\\n0.305\\nTable 2. Comparison of different numbers of Block-NeRFs for\\nreconstructing the Mission Bay dataset. Splitting the scene into\\nmultiple Block-NeRFs improves the reconstruction accuracy, even\\nwhen holding the total number of weights constant (bottom section).\\nThe number of blocks determines the size of the area each block is\\ntrained on and the relative compute expense at inference time.\\nWe compare performance on our Mission Bay dataset\\nversus the number of Block-NeRFs used. We show details\\nin Table 2, where depending on granularity, the Block-NeRF\\nsizes range from as small as 54 m to as large as 544 m. We\\nensure that each pair of adjacent blocks overlaps by 50%\\nand compare other overlap percentages in the supplement.\\nAll were evaluated on the same set of held-out test images\\nspanning the entire trajectory. We consider two regimes,\\none where each Block-NeRF contains the same number of\\nweights (top section) and one where the total number of\\nweights across all Block-NeRFs is ﬁxed (bottom section).\\nIn both cases, we observe that increasing the number of\\nmodels improves the reconstruction metrics. In terms of\\ncomputational expense, parallelization during training is\\ntrivial as each model can be optimized independently across\\ndevices. At inference, our method only requires rendering\\nBlock-NeRFs near the target view. Depending on the scene\\nand NeRF layout, we typically render between one to three\\nNeRFs. We report the relative compute expense in each\\nsetting without assuming any parallelization, which would\\nalso be possible and lead to an additional speed-up. We ﬁnd\\nthat splitting the scene into multiple lower capacity models\\n78254\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Ground Truth\\nmip-NeRF\\nFull\\nBlock-NeRF\\n-Pose Opt.\\n-Exposure\\n-Appearance\\nFigure 7. Model ablation results on multi segment data. Appearance embeddings help the network avoid adding cloudy geometry to explain\\naway changes in the environment like weather and lighting. Removing exposure slightly decreases the accuracy. The pose optimization\\nhelps sharpen the results and removes ghosting from repeated objects, as observed with the telephone pole in the ﬁrst row.\\ncan reduce the overall computational cost as not all of the\\nmodels need to be evaluated (see bottom section of Table 2).\\n5.4. Interpolation Methods\\nInterpolation\\nConsistent?\\nPSNR\"\\nSSIM\"\\nLPIPS#\\nNearest\\n–\\n26.40\\n0.887\\n0.280\\nIDW 2D\\n3\\n26.59\\n0.890\\n0.278\\nIDW 3D\\n–\\n26.57\\n0.890\\n0.278\\nPixelwise Visibility\\n–\\n27.39\\n0.906\\n0.242\\nImagewise Visibility\\n–\\n27.41\\n0.907\\n0.242\\nTable 3. Comparison of interpolation methods. For our ﬂythrough\\nvideo results, we opt for 2D inverse distance weighting (IDW) as it\\nproduces temporally consistent results.\\nWe explore interpolation techniques in Table 3. The sim-\\nple method of only rendering the nearest Block-NeRF to\\nthe camera requires the least amount of compute but results\\nin harsh jumps when transitioning between blocks. These\\ntransitions can be smoothed by using inverse distance weight-\\ning (IDW) between the camera and Block-NeRF centers, as\\ndescribed in § 4.3.2. We also explored a variant of IDW\\nwhere the interpolation was performed over projected 3D\\npoints predicted by the expected Block-NeRF depth. This\\nmethod suffers when the depth prediction is incorrect, lead-\\ning to artifacts and temporal incoherence.\\nFinally, we experiment with weighing the Block-NeRFs\\nbased on per-pixel and per-image predicted visibility. This\\nproduces sharper reconstructions of further-away areas but is\\nprone to temporal inconsistency. Therefore, these methods\\nare best used only when rendering still images. We provide\\nfurther details in the supplement.\\n6. Limitations and Future Work\\nThe proposed method ﬁlters out transient objects during\\ntraining via masking using a segmentation model. Objects\\nthat are not properly masked can cause artifacts, such as\\nremaining shadows of cars that have been removed from\\nthe scene. Temporal inconsistencies in the training data,\\nsuch as changing vegetation or temporary construction work,\\nbreak our assumptions and may result in blurred renderings.\\nThe inability to handle dynamic objects currently limits ap-\\nplications to closed-loop robotic simulation. These issues\\ncould be addressed by learning transient objects [38] or di-\\nrectly modeling dynamic objects [43,68]. Our model does\\nnot sample distant objects with the same density as nearby\\nobjects. This issue with sampling unbounded volumetric\\nrepresentations can lead to blurrier reconstructions. Tech-\\nniques proposed in NeRF++ [75] and Mip-NeRF 360 [4]\\ncould potentially be used to produce sharper renderings of\\ndistant objects. In many applications, real-time rendering\\nis key. NeRFs, however, are computationally expensive to\\nrender. NeRF caching techniques [20,25,73] or sparse voxel\\ngrids [34] could enable real-time Block-NeRF rendering.\\nRecent work has demonstrated techniques to speed up NeRF\\ntraining by multiple orders of magnitude [41,59,72].\\n7. Conclusion\\nWe propose Block-NeRF, a method that reconstructs arbi-\\ntrarily large environments using NeRFs. We demonstrate the\\nefﬁcacy of the method by building an entire neighborhood\\nin San Francisco from 2.8M images, forming the largest neu-\\nral scene representation to date. We accomplish this scale\\nby splitting our representation into multiple blocks that can\\nbe optimized independently. At such a scale, the data col-\\nlected will necessarily have transient objects and variations\\nin appearance, which we account for by modifying the un-\\nderlying NeRF architecture. We hope that this can inspire\\nfuture work in large-scale scene reconstruction using modern\\nneural rendering methods.\\n88255\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian\\nSimon, Brian Curless, Steven M Seitz, and Richard Szeliski.\\nBuilding rome in a day. Communications of the ACM, 2011.\\n2\\n[2] Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia\\nMoseyko, Rohan Banerjee, Sertac Karaman, and Daniela Rus.\\nLearning robust control policies for end-to-end autonomous\\ndriving from data-driven simulation. IEEE Robotics and\\nAutomation Letters, 2020. 3\\n[3] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\\nMip-NeRF: A multiscale representation for anti-aliasing neu-\\nral radiance ﬁelds. ICCV, 2021. 1, 3, 4\\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\\nSrinivasan, and Peter Hedman.\\nMip-nerf 360:\\nUn-\\nbounded anti-aliased neural radiance ﬁelds. arXiv preprint\\narXiv:2111.12077, 2021. 8\\n[5] Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and\\nArthur Szlam.\\nOptimizing the latent space of generative\\nnetworks. arXiv:1707.05776, 2017. 4\\n[6] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\\nGortler, and Michael Cohen. Unstructured lumigraph ren-\\ndering. Computer graphics and interactive techniques, 2001.\\n3\\n[7] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal\\ndataset for autonomous driving. CVPR, 2020. 6\\n[8] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet\\nSingh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr,\\nSimon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking\\nand forecasting with rich maps. CVPR, 2019. 6\\n[9] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang,\\nXinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin\\nYumer, and Raquel Urtasun. Geosim: Realistic video simula-\\ntion via geometry-aware composition for self-driving. CVPR,\\n2021. 3\\n[10] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\\nPanoptic-deeplab: A simple, strong, and fast baseline for\\nbottom-up panoptic segmentation. CVPR, 2020. 5, 6\\n[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\\nsemantic urban scene understanding. CVPR, 2016. 6\\n[12] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2:\\nUnsupervised learning of scene structure for synthetic data\\ngeneration. ECCV, 2020. 3\\n[13] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\\nLopez, and Vladlen Koltun. Carla: An open urban driving\\nsimulator. Conference on robot learning, 2017. 1, 3\\n[14] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen\\nDuan, Guorong Li, Weigang Zhang, Qingming Huang, and\\nQi Tian. The unmanned aerial vehicle benchmark: Object\\ndetection and tracking. ECCV, 2018. 1\\n[15] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely.\\nDeepstereo: Learning to predict new views from the world’s\\nimagery. CVPR, 2016. 3\\n[16] Christian Fr¨uh and Avideh Zakhor. An automated method for\\nlarge-scale, ground-based city model acquisition. IJCV, 2004.\\n2\\n[17] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and\\nRichard Szeliski. Towards internet-scale multi-view stereo.\\nCVPR, 2010. 2\\n[18] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and\\nrobust multi-view stereopsis. IEEE TPAMI, 2010. 2\\n[19] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora\\nVig. Virtual worlds as proxy for multi-object tracking analysis.\\nCVPR, 2016. 3\\n[20] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\\nShotton, and Julien Valentin. Fastnerf: High-ﬁdelity neural\\nrendering at 200fps. arXiv:2103.10380, 2021. 8\\n[21] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\\nready for autonomous driving? the kitti vision benchmark\\nsuite. CVPR, 2012. 6\\n[22] Mordechai Haklay and Patrick Weber. Openstreetmap: User-\\ngenerated street maps. IEEE Pervasive computing, 2008. 4\\n[23] R. I. Hartley and A. Zisserman. Multiple View Geometry\\nin Computer Vision. Cambridge University Press, second\\nedition, 2004. 2\\n[24] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\\nfree-viewpoint image-based rendering. ACM Transactions on\\nGraphics (TOG), 2018. 3\\n[25] Peter Hedman,\\nPratul P Srinivasan,\\nBen Mildenhall,\\nJonathan T Barron, and Paul Debevec. Baking neural ra-\\ndiance ﬁelds for real-time view synthesis. arXiv:2103.14645,\\n2021. 8\\n[26] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,\\nJustin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba,\\nand Sanja Fidler. Meta-sim: Learning to generate synthetic\\ndatasets. ICCV, 2019. 3\\n[27] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja\\nFidler. Drivegan: Towards a controllable high-quality neural\\nsimulation. CVPR, 2021. 3\\n[28] Johannes Kopf, Billy Chen, Richard Szeliski, and Michael\\nCohen. Street slide: browsing street level imagery. ACM\\nTransactions on Graphics (TOG), 2010. 3\\n[29] Johannes Kopf, Michael Cohen, and Rick Szeliski. First-\\nperson hyperlapse videos. SIGGRAPH, 2014. 3\\n[30] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL\\nYan, QC Geng, XY Huang, HJ Gong, et al. Aads: Augmented\\nautonomous driving simulation using data-driven algorithms.\\nScience robotics, 2019. 1, 3\\n[31] Xiaowei Li, Changchang Wu, Christopher Zach, Svetlana\\nLazebnik, and Jan-Michael Frahm. Modeling and recognition\\nof landmark image collections using iconic scene graphs.\\nECCV, 2008. 2\\n[32] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\\nLucey. Barf: Bundle-adjusting neural radiance ﬁelds. arXiv\\npreprint arXiv:2104.06405, 2021. 3, 5\\n98256\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[33] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Maka-\\ndia, Noah Snavely, and Angjoo Kanazawa. Inﬁnite nature:\\nPerpetual view generation of natural scenes from a single\\nimage. ICCV, 2021. 1\\n[34] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\\nChristian Theobalt. Neural sparse voxel ﬁelds. NeurIPS,\\n2020. 3, 8\\n[35] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural\\nvolumes: Learning dynamic renderable volumes from images.\\nSIGGRAPH, 2019. 3\\n[36] Frank Losasso and Hugues Hoppe. Geometry clipmaps: ter-\\nrain rendering using nested regular grids. Siggraph, 2004.\\n2\\n[37] David G Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. IJCV, 2004. 2\\n[38] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\\nworth. Nerf in the wild: Neural radiance ﬁelds for uncon-\\nstrained photo collections. CVPR, 2021. 1, 3, 4, 8\\n[39] Moustafa Meshry, Dan B. Goldman, Sameh Khamis, Hugues\\nHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-\\nBrualla. Neural rerendering in the wild. CVPR, 2019. 3\\n[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\\nRepresenting scenes as neural radiance ﬁelds for view synthe-\\nsis. ECCV, 2020. 1, 3, 4\\n[41] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan-\\nder Keller. Instant neural graphics primitives with a mul-\\ntiresolution hash encoding. arXiv:2201.05989, Jan. 2022.\\n8\\n[42] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\\nKurz, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and\\nMarkus Steinberger. Donerf: Towards real-time rendering\\nof neural radiance ﬁelds using depth oracle networks. arXiv\\ne-prints, pages arXiv–2103, 2021. 3\\n[43] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and\\nFelix Heide. Neural scene graphs for dynamic scenes. CVPR,\\n2021. 1, 3, 8\\n[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen\\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\\nMartin-Brualla. Nerﬁes: Deformable neural radiance ﬁelds.\\nICCV, 2021. 1\\n[45] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\\nPollefeys, and Andreas Geiger. Convolutional occupancy\\nnetworks. In Computer Vision–ECCV 2020: 16th European\\nConference, Glasgow, UK, August 23–28, 2020, Proceedings,\\nPart III 16, pages 523–540. Springer, 2020. 3\\n[46] Marc Pollefeys, David Nist´er, J-M Frahm, Amir Akbarzadeh,\\nPhilippos Mordohai, Brian Clipp, Chris Engels, David Gallup,\\nS-J Kim, Paul Merrell, et al. Detailed real-time urban 3d\\nreconstruction from video. IJCV, 2008. 2\\n[47] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li,\\nKwang Moo Yi, and Andrea Tagliasacchi. Derf: Decom-\\nposed radiance ﬁelds. CVPR, 2021. 3\\n[48] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\\nGeiger. KiloNeRF: Speeding up neural radiance ﬁelds with\\nthousands of tiny MLPs. ICCV, 2021. 3\\n[49] Stephan R Richter, Hassan Abu AlHaija, and Vladlen Koltun.\\nEnhancing photorealism enhancement. arXiv:2105.04619,\\n2021. 3\\n[50] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen\\nKoltun. Playing for data: Ground truth from computer games.\\nECCV, 2016. 3\\n[51] Gernot Riegler and Vladlen Koltun. Free view synthesis.\\nECCV, 2020. 3\\n[52] Gernot Riegler and Vladlen Koltun. Stable view synthesis.\\nCVPR, 2021. 3\\n[53] German Ros, Laura Sellart, Joanna Materzynska, David\\nVazquez, and Antonio M Lopez. The synthia dataset: A large\\ncollection of synthetic images for semantic segmentation of\\nurban scenes. CVPR, 2016. 3\\n[54] Johannes L Schonberger and Jan-Michael Frahm. Structure-\\nfrom-motion revisited. CVPR, 2016. 2, 6\\n[55] Qi Shan, Riley Adams, Brian Curless, Yasutaka Furukawa,\\nand Steven M. Seitz. The visual turing test for scene recon-\\nstruction. 3DV, 2013. 2\\n[56] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo\\ntourism: Exploring photo collections in 3d. SIGGRAPH,\\n2006. 2\\n[57] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\\nTancik, Ben Mildenhall, and Jonathan T. Barron. NeRV:\\nNeural reﬂectance and visibility ﬁelds for relighting and view\\nsynthesis. CVPR, 2021. 5\\n[58] Shih-Yang Su, Frank Yu, Michael Zollh¨ofer, and Helge\\nRhodin. A-nerf: Articulated neural radiance ﬁelds for learn-\\ning human shape, appearance, and pose. Advances in Neural\\nInformation Processing Systems, 34, 2021. 3, 5\\n[59] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\\ngrid optimization: Super-fast convergence for radiance ﬁelds\\nreconstruction. arXiv preprint arXiv:2111.11215, 2021. 8\\n[60] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\\nYuning Chai, Benjamin Caine, et al. Scalability in perception\\nfor autonomous driving: Waymo open dataset. CVPR, 2020.\\n6\\n[61] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis,\\nCharles Loop, Derek Nowrouzezahrai, Alec Jacobson, Mor-\\ngan McGuire, and Sanja Fidler. Neural geometric level of\\ndetail: Real-time rendering with implicit 3D shapes. CVPR,\\n2021. 3\\n[62] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features\\nlet networks learn high frequency functions in low dimen-\\nsional domains. NeurIPS, 2020. 4\\n[63] Sebastian Thrun. Probabilistic robotics. Communications of\\nthe ACM, 2002. 2\\n[64] Bill Triggs, Philip F McLauchlan, Richard I Hartley, and An-\\ndrew W Fitzgibbon. Bundle adjustment—a modern synthesis.\\nInternational workshop on vision algorithms, 1999. 2\\n[65] Haithem Turki, Deva Ramanan, and Mahadev Satya-\\nnarayanan.\\nMega-nerf:\\nScalable construction of large-\\nscale nerfs for virtual ﬂy-throughs.\\narXiv preprint\\narXiv:2112.10703, 2021. 3\\n108257\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[66] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\\ntor Adrian Prisacariu. Nerf–: Neural radiance ﬁelds without\\nknown camera parameters. arXiv preprint arXiv:2102.07064,\\n2021. 3, 5\\n[67] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\\nCitynerf:\\nBuilding nerf at city scale.\\narXiv preprint\\narXiv:2112.05504, 2021. 3\\n[68] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learn-\\ning object-compositional neural radiance ﬁeld for editable\\nscene rendering. ICCV, 2021. 3, 8\\n[69] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou,\\nPei Sun, Dumitru Erhan, Sean Rafferty, and Henrik Kret-\\nzschmar. Surfelgan: Synthesizing realistic sensor data for\\nautonomous driving. CVPR, 2020. 1, 3\\n[70] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neural\\nsurface reconstruction by disentangling geometry and appear-\\nance. Advances in Neural Information Processing Systems,\\n33, 2020. 3\\n[71] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto\\nRodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF: Invert-\\ning neural radiance ﬁelds for pose estimation. In IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems\\n(IROS), 2021. 3, 5\\n[72] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\\nRadiance ﬁelds without neural networks.\\narXiv preprint\\narXiv:2112.05131, 2021. 8\\n[73] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\\nneural radiance ﬁelds. arXiv:2103.14024, 2021. 8\\n[74] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun\\nZhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi\\nYu. Editable free-viewpoint video using a layered neural\\nrepresentation. ACM Transactions on Graphics (TOG), 2021.\\n3\\n[75] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\\nKoltun. Nerf++: Analyzing and improving neural radiance\\nﬁelds. arXiv preprint arXiv:2010.07492, 2020. 8\\n[76] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. CVPR, 2018. 7\\n[77] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\\nand Noah Snavely. Stereo magniﬁcation: Learning view\\nsynthesis using multiplane images. arXiv:1805.09817, 2018.\\n3\\n[78] Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang,\\nPing Tan, and Long Quan. Very large-scale global SFM by\\ndistributed motion averaging. CVPR, 2018. 2\\n118258\\n', metadata={'source': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'file_path': 'data/Tancik et al. - 2022 - Block-NeRF Scalable Large Scene Neural View Synth.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.4', 'title': 'Block-NeRF: Scalable Large Scene Neural View Synthesis', 'author': 'Matthew Tancik;  Vincent Casser;  Xinchen Yan;  Sabeek Pradhan;  Ben Mildenhall;  Pratul P. Srinivasan;  Jonathan T. Barron;  Henrik Kretzschmar', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'pikepdf 5.1.3', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction\\nof Deformable Tissues in Robotic Surgery\\nYuehao Wang1, Yonghao Long1, Siu Hin Fan2, and Qi Dou1(�)\\n1 Dept. of Computer Science and Engineering, The Chinese University of Hong Kong\\n2 Dept. of Biomedical Engineering, The Chinese University of Hong Kong\\nAbstract. Reconstruction of the soft tissues in robotic surgery from en-\\ndoscopic stereo videos is important for many applications such as intra-\\noperative navigation and image-guided robotic surgery automation. Pre-\\nvious works on this task mainly rely on SLAM-based approaches, which\\nstruggle to handle complex surgical scenes. Inspired by recent progress in\\nneural rendering, we present a novel framework for deformable tissue re-\\nconstruction from binocular captures in robotic surgery under the single-\\nviewpoint setting. Our framework adopts dynamic neural radiance ﬁelds\\nto represent deformable surgical scenes in MLPs and optimize shapes\\nand deformations in a learning-based manner. In addition to non-rigid\\ndeformations, tool occlusion and poor 3D clues from a single viewpoint\\nare also particular challenges in soft tissue reconstruction. To overcome\\nthese diﬃculties, we present a series of strategies of tool mask-guided ray\\ncasting, stereo depth-cueing ray marching and stereo depth-supervised\\noptimization. With experiments on DaVinci robotic surgery videos, our\\nmethod signiﬁcantly outperforms the current state-of-the-art reconstruc-\\ntion method for handling various complex non-rigid deformations. To our\\nbest knowledge, this is the ﬁrst work leveraging neural rendering for sur-\\ngical scene 3D reconstruction with remarkable potential demonstrated.\\nCode is available at: https://github.com/med-air/EndoNeRF.\\nKeywords: 3D Reconstruction · Neural Rendering · Robotic Surgery.\\n1\\nIntroduction\\nSurgical scene reconstruction from endoscope stereo video is an important but\\ndiﬃcult task in robotic minimally invasive surgery. It is a prerequisite for many\\ndownstream clinical applications, including intra-operative navigation and aug-\\nmented reality, surgical environment simulation, immersive education, and robotic\\nsurgery automation [2,12,20,25]. Despite much recent progress [10,22,28,29,30,33],\\nseveral key challenges still remain unsolved. First, surgical scenes are deformable\\nwith signiﬁcant topology changes, requiring dynamic reconstruction to capture\\na high degree of non-rigidity. Second, endoscopic videos show sparse viewpoints\\ndue to constrained camera movement in conﬁned space, resulting in limited 3D\\nclues of soft tissues. Third, the surgical instruments always occlude part of the\\nsoft tissues, which aﬀects the completeness of surgical scene reconstruction.\\narXiv:2206.15255v1  [cs.CV]  30 Jun 2022\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='2\\nY. Wang et al.\\nPrevious works [1,13] explored the eﬀectiveness of surgical scene reconstruc-\\ntion via depth estimation. Since most of the endoscopes are equipped with stereo\\ncameras, depth can be estimated from binocular vision. Follow-up SLAM-based\\nmethods [23,31,32] fuse depth maps in 3D space to reconstruct surgical scenes\\nunder more complex settings. Nevertheless, these methods either hypothesize\\nscenes as static or surgical tools not present, limiting their practical use in real\\nscenarios. Recent work SuPer [8] and E-DSSR [11] present frameworks consisting\\nof tool masking, stereo depth estimation and SurfelWarp [4] to perform single-\\nview 3D reconstruction of deformable tissues. However, all these methods track\\ndeformation based on a sparse warp ﬁeld [16], which degenerates when deforma-\\ntions are signiﬁcantly beyond the scope of non-topological changes.\\nAs an emerging technology, neural rendering [6,27,26] is recently developed to\\nbreak through the limited performance of traditional 3D reconstruction by lever-\\naging diﬀerentiable rendering and neural networks. In particular, neural radiance\\nﬁelds (NeRF) [15], a popular pioneering work of neural rendering, proposes to\\nuse neural implicit ﬁeld for continuous scene representations and achieves great\\nsuccess in producing high-quality view synthesis and 3D reconstruction on di-\\nverse scenarios [14,15,17]. Meanwhile, recent variants of NeRF [18,19,21] target-\\ning dynamic scenes have managed to track deformations through various neural\\nrepresentations on non-rigid objects.\\nIn this paper, we endeavor to reconstruct highly deformable surgical scenes\\ncaptured from single-viewpoint stereo endoscopes. We embark on adapting the\\nemerging neural rendering framework to the regime of deformable surgical scene\\nreconstruction. We summarize our contributions as follows: 1) To accommodate\\na wide range of geometry and deformation representations on soft tissues, we\\nleverage neural implicit ﬁelds to represent dynamic surgical scenes. 2) To ad-\\ndress the particular tool occlusion problem in surgical scenes, we design a new\\nmask-guided ray casting strategy for resolving tool occlusion. 3) We incorporate\\na depth-cueing ray marching and depth-supervised optimization scheme, using\\nstereo prior to enable neural implicit ﬁeld reconstruction for single-viewpoint in-\\nput. To the best of our knowledge, this is the ﬁrst work introducing cutting-edge\\nneural rendering to surgical scene reconstruction. We evaluate our method on 6\\ntypical in-vivo surgical scenes of robotic prostatectomy. Compared with previ-\\nous methods, our results exhibit great performance gain, both quantitatively and\\nqualitatively, on 3D reconstruction and deformation tracking of surgical scenes.\\n2\\nMethod\\n2.1\\nOverview of the Neural Rendering-based Framework\\nGiven a single-viewpoint stereo video of a dynamic surgical scene, we aim to\\nreconstruct 3D structures and textures of surgical scenes without occlusion of\\nsurgical instruments. We denote {(Il\\ni, Ir\\ni )}T\\ni=1 as a sequence of input stereo video\\nframes, where T is the total number of frames and (Il\\ni, Ir\\ni ) is the pair of left and\\nright images at the i-th frame. The video duration is normalized to [0, 1]. Thus,\\ntime of the i-th frame is i/T. We also extract binary tool masks {M i}T\\ni=1 for the\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction in Robotic Surgery\\n3\\n𝛾(𝐱, 𝑡)\\n∆𝐱\\nTool Mask\\n𝛾 𝐱 + ∆𝐱 , 𝒅\\nc , 𝜎\\n𝐫 𝑠 = 𝐨 + 𝑠𝐝\\n𝐺!\\n𝐹\"\\nVolume \\nRendering\\nDeformed Space\\nCanonical Space\\n.𝐶\\n.𝐷\\nDepth Loss\\nColor Loss\\nStereo Depth\\nGround Truth Images\\nTransfer Function\\nFig. 1: Illustration of our proposed novel approach of neural rendering for stereo\\n3D reconstruction of deformable tissues in robotic surgery.\\nleft views to identify the region of surgical instruments. To utilize stereo clues,\\nwe estimate coarse depth maps {Di}T\\ni=1 for the left views from the binocular\\ncaptures. We follow the modeling in D-NeRF [21] and represent deformable\\nsurgical scenes as a canonical neural radiance ﬁeld along with a time-dependent\\nneural displacement ﬁeld (cf. Sec. 2.2). In our pipeline, each training iteration\\nconsists of the following six stages: i) randomly pick a frame for training, ii)\\nrun tool-guided ray casting (cf. Sec. 2.3) to shoot camera rays into the scene, iii)\\nsample points along each camera ray via depth-cueing ray marching (cf. Sec. 2.4),\\niv) send sampled points to networks to obtain color and space occupancy of\\neach point, v) evaluate volume rendering integral on sampled points to produce\\nrendering results, vi) optimize the rendering loss plus depth loss to reconstruct\\nshapes, colors and deformations of the surgical scene (cf. Sec. 2.5). The overview\\nof key components in our approach is illustrated in Fig. 1. We will describe the\\ndetailed methods in the following subsections.\\n2.2\\nDeformable Surgical Scene Representations\\nWe represent a surgical scene as a canonical radiance ﬁeld and a time-dependent\\ndisplacement ﬁeld. Accordingly, each frame of the surgical scene can be regarded\\nas a deformation of the canonical ﬁeld. The canonical ﬁeld, denoted as FΘ(x, d),\\nis an 8-layer MLP with network parameter Θ, mapping coordinates x ∈ R3 and\\nunit view-in directions d ∈ R3 to RGB colors c(x, d) ∈ R3 and space occupancy\\nσ(x) ∈ R. The time-dependent displacement ﬁeld GΦ(x, t) is encoded in another\\n8-layer MLP with network parameters Φ and maps input space-time coordinates\\n(x, t) into displacement between the point x at time t and the corresponding\\npoint in the canonical ﬁeld. For any time t, the color and occupancy at x can\\nbe retrieved as FΘ(x + GΦ(x, t), d). Compared with other dynamic modeling\\napproaches [18,19], a displacement ﬁeld is suﬃcient to explicitly and physically\\nexpress all tissue deformations. To capture high-frequency details, we use posi-\\ntional encoding γ(·) to map the input coordinates and time into Fourier features\\n[24] before feeding them to the networks.\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='4\\nY. Wang et al.\\n2.3\\nTool Mask-Guided Ray Casting\\nWith scene representations, we further leverage the diﬀerentiable volume ren-\\ndering used in NeRF to yield renderings for supervision. The diﬀerentiable vol-\\nume rendering begins with shooting a batch of camera rays into the surgical\\nscene from a ﬁxed viewpoint at an arbitrary time t. Every ray is formulated as\\nr(s) = o + sd, where o is a ﬁxed origin of the ray, d is the pointing direction of\\nthe ray and s is the ray parameter. In the original NeRF, rays are shot towards\\na batch of randomly selected pixels on the entire image plane. However, there\\nare many pixels of surgical tools on the captured images, while our goal is to\\nreconstruct underlying tissues. Thus, training on these tool pixels is unexpected.\\nOur main idea for solving this issue is to bypass those rays traveling through tool\\npixels over the training stage. We utilize binary tool masks {M i}T\\ni=1, where 0\\nstands for tissue pixels and 1 stands for tool pixels, to inform which rays should\\nbe neglected. In this regard, we create importance maps {V}T\\ni=1 according to\\nM i and perform importance sampling to avoid shooting rays for those pixels of\\nsurgical tools. Eq. (1) exhibits the construction of importance maps, where ⊗ is\\nelement-wise multiplication, ∥·∥F is Frobenius norm and 1 is a matrix with the\\nsame shape as M i while ﬁlled with ones:\\nVi = Λ ⊗\\n�\\n1 − M i\\n�\\n,\\nΛ =\\n�\\n1 +\\n�T\\nj=1 M j\\n��� �T\\nj=1 M j\\n��\\nF\\n�\\n.\\n(1)\\nThe 1 − M i term initializes the importance of tissue pixels to 1 and the impor-\\ntance of tool pixels to 0. To balance the sampling rate of occluded pixels across\\nframes, the scaling term Λ speciﬁes higher importance scaling for those tissue\\nareas with higher occlusion frequencies. Normalizing each importance map as\\n�Vi = Vi/∥Vi∥F will yield a probability mass function over the image plane.\\nDuring our ray casting stage for the i-th frame, we sample pixels from the distri-\\nbution �Vi using inverse transform sampling and cast rays towards these sampled\\npixels. In this way, the probability of shooting rays for tool pixels is guaranteed\\nto be zero as the importance of tool pixels is constantly zero.\\n2.4\\nStereo Depth-Cueing Ray Marching\\nAfter shooting camera rays over tool occlusion, we proceed ray marching to\\nsample points in the space. Speciﬁcally, we discretize each camera ray r(s) into\\nbatch of points {xj|xj = r(sj)}m\\nj=1 by sampling a sequence of ray steps s1 ≤ s2 ≤\\n· · · ≤ sm. The original NeRF proposes hierarchical stratiﬁed sampling to obtain\\n{sj}m\\nj=1. However, this sampling strategy hardly exploits accurate 3D structures\\nwhen NeRF models are trained on single-view input. Drawing inspiration from\\nearly work in iso-surface rendering [7], we create Gaussian transfer functions with\\nstereo depth to guide point sampling near tissue surfaces. For the i-th frame, the\\ntransfer function for a ray r(s) shooting towards pixel (u, v) is formulated as:\\nδ(s; u, v, i) = exp\\n�\\n−(s − Di[u, v])2�\\n2ξ2�\\n.\\n(2)\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction in Robotic Surgery\\n5\\nThe transfer function δ(s; u, v, i) depicts an impulse distribution that contin-\\nuously allocates sampling weights for every location on r(s). The impulse is\\ncentered at Di[u, v], i.e., the depth at the (u, v) pixel. The width of the impulse\\nis controlled by the hyperparameter ξ, which is set to a small value to mimic\\nDirac delta impulse. In our ray marching, s1 ≤ s2, ≤ · · · ≤ sm are drawn from the\\nnormalized impulse distribution\\n1\\nξ\\n√\\n2πδ(s; u, v, i). By this means, sampled points\\nare concentrated around tissue surfaces, imposing stereo prior in rendering.\\n2.5\\nOptimization for Deformable Radiance Fields\\nOnce we obtain the sampled points in the space, the emitted color �C and optical\\ndepth �D of a camera ray r(s) can be evaluated by volume rendering [5] as:\\n�C\\n�\\nr(s)\\n�\\n=\\n�m−1\\nj=1 wjc(xj, d),\\n�D\\n�\\nr(s)\\n�\\n=\\n�m−1\\nj=1 wjsj,\\nwj = (1 − exp (−σ(xj)∆sj)) exp (−\\n�j−1\\nk=1 σ(xk)∆sk), ∆sj = sj+1 − sj.\\n(3)\\nTo reconstruct the canonical and displacement ﬁelds from single-view captures,\\nwe optimize the network parameters Θ and Φ by jointly supervising the rendered\\ncolor and optical depth [3]. Speciﬁcally, the loss function for training the networks\\nis deﬁned as:\\nL(r(s)) =\\n�� �C(r(s)) − Ii[u, v]\\n��2\\n2 + λ\\n�� �D(r(s)) − Di[u, v]\\n��,\\n(4)\\nwhere (u, v) is the location of the pixel that r(s) shoots towards, λ is a hyper-\\nparameter weighting the depth loss.\\nLast but not least, we conduct statistical depth reﬁnement to handle corrupt\\nstereo depth caused by fuzzy pixels and specular highlights on the images of\\nsurgical scenes. Direct supervision on the estimated depth will overﬁt corrupt\\ndepth in the end, leading to abrupt artifacts in reconstruction results (Fig. 3).\\nOur preliminary ﬁndings reveal that our model at the early training stage would\\nproduce smoother results both in color and depth since the underﬁtting model\\ntends to average learned colors and occupancy. Thus, minority corrupt depth\\nis smoothed by majority normal depth. Based on this observation, we propose\\nto patch the corrupt depth with the output from underﬁtting radiance ﬁelds.\\nDenoting �D\\nK\\ni\\nas the underﬁtting output depth maps for the i-th frame after\\nK iterations of training, we ﬁrstly ﬁnd residual maps through ϵi = | �D\\nK\\ni − Di|,\\nthen we compute a probabilistic distribution over the residual maps. After that,\\nwe set a small number α ∈ [0, 1] and locate those pixels with the last α-quantile\\nresiduals. Since those located pixels statistically correspond to large residuals,\\nwe can identify them as occurrences of corrupt depth. Finally, we replace those\\nidentiﬁed corrupt depth pixels with smoother depth pixels in �D\\nK\\ni . After this\\nreﬁnement procedure, the radiance ﬁelds are optimized on the patched depth\\nmaps in the subsequent training iterations, alleviating corrupt depth ﬁtting.\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='6\\nY. Wang et al.\\n3\\nExperiments\\n3.1\\nDataset and Evaluation Metrics\\nWe evaluate our proposed method on typical robotic surgery stereo videos from 6\\ncases of our in-house DaVinci robotic prostatectomy data. We totally extracted\\n6 clips with a total of 807 frames. Each clip lasts for 4∼8s with 15fps. Each case\\nis captured from stereo cameras at a single viewpoint and encompasses challeng-\\ning scenes with non-rigid deformation and tool occlusion. Among the selected\\n6 cases, 2 cases contain traction on thin structures such as fascia, 2 cases con-\\ntain signiﬁcant pushing and pulling of tissue, and 2 cases contain tissue cutting,\\nwhich altogether present the typical soft tissue situations in robotic surgery. For\\ncomparison, we take the most recent state-of-the-art surgical scene reconstruc-\\ntion method of E-DSSR [11] as a strong comparison. For qualitative evaluation,\\nWe exhibit our reconstructed point clouds and compare textural and geometric\\ndetails obtained by diﬀerent methods. We also conduct an ablation study on\\nour depth-related modules through qualitative comparison. Due to clinical reg-\\nulation in practice, it is impossible to collect ground truth depth for numerical\\nevaluation on 3D structures. Following the evaluation method in [11] and wide\\nliterature in neural rendering, we alternatively use photometric errors, including\\nPSNR, SSIM and LPIPS, as evaluation metrics for quantitative comparisons.\\n3.2\\nImplementation Details\\nIn our implementation, we empirically set the width of the transfer function\\nξ = 1, the weight of depth loss λ = 1, depth reﬁnement iteration K = 4000 and\\nα=0.1. Other training hyper-parameters follow the settings in the state-of-the-\\nart D-NeRF [21]. We calibrate the endoscope in advance to acquire its intrinsics.\\nIn all of our experiments, tool masks are obtained by manually labeling and\\ncoarse stereo depth maps are generated by STTR-light [9] pretrained on Scene\\nFlow. We optimize each model over 100K iterations on a single case. To recover\\nexplicit geometry from implicit ﬁelds, we render optimized radiance ﬁelds to\\nRGBD maps, smooth rendered depth maps via bilateral ﬁltering, and back-\\nproject RGBD into point clouds based on the endoscope intrinsics.\\n3.3\\nQualitative and Quantitative Results\\nFor qualitative evaluation, Fig. 2 illustrates the reconstruction results of our ap-\\nproach and the comparison method, along with a reference to the original video.\\nIn the test case of Fig. 2(a), the tissues are pulled by surgical instruments, yield-\\ning relatively large deformations. Beneﬁtting from the underlying continuous\\nscene representations, our method can reconstruct water-tight tissues without\\nbeing aﬀected by the tool occlusion. More importantly, per-frame deformations\\nare captured continuously, achieving stable results over the episode of consec-\\nutive pulling. In contrast, the current state-of-the-art method [11] could not\\nfully track these large deformations and its reconstruction results include holes\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction in Robotic Surgery\\n7\\nOurs\\nE-DSSR [11]\\nReference\\nt=0s\\nt=1.42s\\nt=0.95s\\nt=1.23s\\nt=1.08s\\nConsecutive Pulling on Soft Tissues within 1.5s \\n(a) Results on the case “pulling tissues”, where soft tissues are drastically pulled within\\n2s. We exhibit 5 reconstruction results of our method and E-DSSR over time.\\nOurs\\nE-DSSR [11]\\nReference\\nt=1.23s\\nt=2.42s\\nt=5.00s\\nBefore Cutting on Tissues\\nCutting on Tissues\\nTwice Cutting on Tissues\\n(b) Results on the case “cutting tissues twice”. We show 3 frames corresponding to no\\ndeformation, cutting once and cutting twice, respectively. The close-ups of the cutting\\nareas display reconstructed tissue details before and after cutting.\\nFig. 2: Qualitative comparisons of 2 cases, demonstrating reconstruction of soft\\ntissues with large deformations and topology changes.\\nand noisy points under such a challenging situation. We further demonstrate a\\nmore diﬃcult case in Fig. 2(b) which includes soft tissue cutting with topol-\\nogy changes. From the reconstruction results, it is observed that our method\\nmanages to track the detailed cutting procedures, owing to the powerful neural\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='8\\nY. Wang et al.\\nComplete Model\\nw/o Depth-Cueing \\nRay Marching\\nw/o Depth \\nRefinement\\nw/o Depth-\\nSupervised Loss\\nCorrupt \\nStereo Depth\\nFig. 3: Ablation study on our depth-related modules, i.e., depth-supervised loss,\\ndepth reﬁnement and depth-cueing ray marching.\\nTable 1: Quantitative evaluation on photometric errors of the dynamic recon-\\nstruction on metrics of PSNR, SSIM and LPIPS.\\nMethods\\nPSNR ↑\\nSSIM ↑\\nLPIPS ↓\\nE-DSSR [11]\\n13.398 ± 1.387\\n0.630 ± 0.057\\n0.423 ± 0.047\\nOurs w/o D\\n24.088 ± 2.567\\n0.849±0.023\\n0.230 ± 0.023\\nOurs\\n29.831 ± 2.208\\n0.925 ± 0.020\\n0.081 ± 0.022\\nrepresentation of displacement ﬁelds. In addition, it can bypass the issue of tool\\nocclusion and recover the hidden tissues, which is cooperatively achieved by our\\nmask-guided ray casting and the interpolation property of neural implicit ﬁelds.\\nOn the other hand, the comparison method is not able to capture these small\\nchanges on soft tissues nor patch all the tool-occluded areas. Table 1 summarizes\\nour quantitative experiments, showing overall performance on the dataset. Our\\nmethod dramatically outperforms E-DSSR by ↑ 16.433 PSNR, ↑ 0.295 SSIM\\nand ↓ 0.342 LPIPS. To assess the contribution of the dynamics modeling, we\\nalso evaluate our model without neural displacement ﬁeld (Ours w/o D). As ex-\\npected, removing this component leads to a noticeable performance drop, which\\nreﬂects the eﬀectiveness of the displacement modeling.\\nWe present a qualitative ablation study on our depth-related modules in Fig.\\n3. Without depth-supervision loss, we observe that the pipeline is not capable\\nof learning correct geometry from single-viewpoint input. Moreover, when depth\\nreﬁnement is disabled, abrupt artifacts occur on the reconstruction results due\\nto corruption in stereo depth estimation. Our depth-cueing ray marching can\\nfurther diminish artifacts on 3D structures, especially for boundary points.\\n4\\nConclusion\\nThis paper presents a novel neural rendering-based framework for dynamic sur-\\ngical scene reconstruction from single-viewpoint binocular images, as well as\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction in Robotic Surgery\\n9\\naddressing complex tissue deformation and tool occlusion. We adopt the cutting-\\nedge dynamic neural radiance ﬁeld method to represent surgical scenes. In ad-\\ndition, we propose mask-guided ray casting to handle tool occlusion and im-\\npose stereo depth prior upon the single-viewpoint situation. Our approach has\\nachieved superior performance on various scenarios in robotic surgery data such\\nas large elastic deformations and tissue cutting. We hope the emerging NeRF-\\nbased 3D reconstruction techniques could inspire new pathways for robotic surgery\\nscene understanding, and empower various down-stream clinical-oriented tasks.\\nAcknowledgements. This work was supported in part by CUHK Shun Hing In-\\nstitute of Advanced Engineering (project MMT-p5-20), in part by Shenzhen-HK\\nCollaborative Development Zone, and in part by Multi-Scale Medical Robotics\\nCentre InnoHK.\\nReferences\\n1. Brandao, P., Psychogyios, D., Mazomenos, E., Stoyanov, D., Janatka, M.: Hapnet:\\nhierarchically aggregated pyramid network for real-time stereo matching. Com-\\nputer Methods in Biomechanics and Biomedical Engineering: Imaging & Visual-\\nization 9(3), 219–224 (2021) 2\\n2. Chen, L., Tang, W., John, N.W., Wan, T.R., Zhang, J.J.: Slam-based dense sur-\\nface reconstruction in monocular minimally invasive surgery and its application to\\naugmented reality. Computer methods and programs in biomedicine 158, 135–146\\n(2018) 1\\n3. Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised nerf: Fewer views\\nand faster training for free. arXiv preprint arXiv:2107.02791 (2021) 5\\n4. Gao, W., Tedrake, R.: Surfelwarp: Eﬃcient non-volumetric single view dynamic\\nreconstruction. In: Robotics: Science and Systems XIV (2019) 2\\n5. Kajiya, J.T., Von Herzen, B.P.: Ray tracing volume densities. ACM SIGGRAPH\\ncomputer graphics 18(3), 165–174 (1984) 5\\n6. Kato, H., Ushiku, Y., Harada, T.: Neural 3d mesh renderer. In: CVPR. pp. 3907–\\n3916 (2018) 2\\n7. Kniss, J., Ikits, M., Lefohn, A., Hansen, C., Praun, E., et al.: Gaussian transfer\\nfunctions for multi-ﬁeld volume visualization. In: IEEE Visualization, 2003. VIS\\n2003. pp. 497–504. IEEE (2003) 4\\n8. Li, Y., Richter, F., Lu, J., Funk, E.K., Orosco, R.K., Zhu, J., Yip, M.C.: Super:\\nA surgical perception framework for endoscopic tissue manipulation with surgical\\nrobotics. IEEE Robotics and Automation Letters 5(2), 2294–2301 (2020) 2\\n9. Li, Z., Liu, X., Drenkow, N., Ding, A., Creighton, F.X., Taylor, R.H., Unberath,\\nM.: Revisiting stereo depth estimation from a sequence-to-sequence perspective\\nwith transformers. In: ICCV. pp. 6197–6206 (2021) 6\\n10. Liu, X., Stiber, M., Huang, J., Ishii, M., Hager, G.D., Taylor, R.H., Unberath,\\nM.: Reconstructing sinus anatomy from endoscopic video–towards a radiation-free\\napproach for quantitative longitudinal assessment. In: MICCAI. pp. 3–13. Springer\\n(2020) 1\\n11. Long, Y., Li, Z., Yee, C.H., Ng, C.F., Taylor, R.H., Unberath, M., Dou, Q.: E-dssr:\\neﬃcient dynamic surgical scene reconstruction with transformer-based stereoscopic\\ndepth perception. In: MICCAI. pp. 415–425. Springer (2021) 2, 6, 8\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='10\\nY. Wang et al.\\n12. Lu, J., Jayakumari, A., Richter, F., Li, Y., Yip, M.C.: Super deep: A surgical per-\\nception framework for robotic tissue manipulation using deep learning for feature\\nextraction. In: ICRA. pp. 4783–4789. IEEE (2021) 1\\n13. Luo, H., Wang, C., Duan, X., Liu, H., Wang, P., Hu, Q., Jia, F.: Unsupervised\\nlearning of depth estimation from imperfect rectiﬁed stereo laparoscopic images.\\nComputers in biology and medicine 140, 105109 (2022) 2\\n14. Martin-Brualla, R., Radwan, N., Sajjadi, M.S., Barron, J.T., Dosovitskiy, A., Duck-\\nworth, D.: Nerf in the wild: Neural radiance ﬁelds for unconstrained photo collec-\\ntions. In: CVPR. pp. 7210–7219 (2021) 2\\n15. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,\\nR.: Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In: ECCV.\\npp. 405–421. Springer (2020) 2\\n16. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and tracking\\nof non-rigid scenes in real-time. In: CVPR. pp. 343–352 (2015) 2\\n17. Niemeyer, M., Geiger, A.: Giraﬀe: Representing scenes as compositional generative\\nneural feature ﬁelds. In: CVPR. pp. 11453–11464 (2021) 2\\n18. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin-\\nBrualla, R.: Nerﬁes: Deformable neural radiance ﬁelds. In: CVPR. pp. 5865–5874\\n(2021) 2, 3\\n19. Park, K., Sinha, U., Hedman, P., Barron, J.T., Bouaziz, S., Goldman, D.B., Martin-\\nBrualla, R., Seitz, S.M.: Hypernerf: a higher-dimensional representation for topo-\\nlogically varying neural radiance ﬁelds. ACM Transactions on Graphics (TOG)\\n40(6), 1–12 (2021) 2, 3\\n20. Penza, V., De Momi, E., Enayati, N., Chupin, T., Ortiz, J., Mattos, L.S.: envisors:\\nenhanced vision system for robotic surgery. a user-deﬁned safety volume tracking\\nto minimize the risk of intraoperative bleeding. Frontiers in Robotics and AI 4, 15\\n(2017) 1\\n21. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural ra-\\ndiance ﬁelds for dynamic scenes. In: CVPR. pp. 10318–10327 (2021) 2, 3, 6\\n22. Recasens, D., Lamarca, J., Fácil, J.M., Montiel, J., Civera, J.: Endo-depth-and-\\nmotion: Reconstruction and tracking in endoscopic videos using depth networks\\nand photometric constraints. IEEE Robotics and Automation Letters 6(4), 7225–\\n7232 (2021) 1\\n23. Song, J., Wang, J., Zhao, L., Huang, S., Dissanayake, G.: Dynamic reconstruction of\\ndeformable soft-tissue with stereo scope in minimal invasive surgery. IEEE Robotics\\nand Automation Letters 3(1), 155–162 (2017) 2\\n24. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Sing-\\nhal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn\\nhigh frequency functions in low dimensional domains. NeurIPS 33, 7537–7547\\n(2020) 3\\n25. Tang, R., Ma, L.F., Rong, Z.X., Li, M.D., Zeng, J.P., Wang, X.D., Liao, H.E.,\\nDong, J.H.: Augmented reality technology for preoperative planning and intra-\\noperative navigation during hepatobiliary surgery: a review of current methods.\\nHepatobiliary & Pancreatic Diseases International 17(2), 101–112 (2018) 1\\n26. Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Xu, Z., Simon, T.,\\nNießner, M., Tretschk, E., Liu, L., et al.: Advances in neural rendering. In: ACM\\nSIGGRAPH 2021 Courses, pp. 1–320 (2021) 2\\n27. Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Sunkavalli, K., Martin-\\nBrualla, R., Simon, T., Saragih, J., Nießner, M., et al.: State of the art on neural\\nrendering. In: Computer Graphics Forum. vol. 39, pp. 701–727. Wiley Online Li-\\nbrary (2020) 2\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='Neural Rendering for Stereo 3D Reconstruction in Robotic Surgery\\n11\\n28. Tukra, S., Marcus, H.J., Giannarou, S.: See-through vision with unsupervised scene\\nocclusion reconstruction. IEEE Transactions on Pattern Analysis & Machine In-\\ntelligence (01), 1–1 (2021) 1\\n29. Wei, G., Yang, H., Shi, W., Jiang, Z., Chen, T., Wang, Y.: Laparoscopic scene\\nreconstruction based on multiscale feature patch tracking method. In: 2021 Inter-\\nnational Conference on Electronic Information Engineering and Computer Science\\n(EIECS). pp. 588–592. IEEE (2021) 1\\n30. Wei, R., Li, B., Mo, H., Lu, B., Long, Y., Yang, B., Dou, Q., Liu, Y., Sun, D.: Stereo\\ndense scene reconstruction and accurate laparoscope localization for learning-based\\nnavigation in robot-assisted surgery. arXiv preprint arXiv:2110.03912 (2021) 1\\n31. Zhou, H., Jagadeesan, J.: Real-time dense reconstruction of tissue surface from\\nstereo optical video. IEEE transactions on medical imaging 39(2), 400–412 (2019)\\n2\\n32. Zhou, H., Jayender, J.: Emdq-slam: Real-time high-resolution reconstruction of soft\\ntissue surface from stereo laparoscopy videos. In: MICCAI. pp. 331–340. Springer\\n(2021) 2\\n33. Zhou, H., Jayender, J.: Real-time nonrigid mosaicking of laparoscopy images. IEEE\\nTransactions on Medical Imaging 40(6), 1726–1736 (2021) 1\\n', metadata={'source': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'file_path': 'data/Wang et al. - 2022 - Neural Rendering for\\xa0Stereo 3D Reconstruction of\\xa0D.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220701003804Z', 'modDate': 'D:20220701003804Z', 'trapped': ''}),\n",
       " Document(page_content='iNeRF: Inverting Neural Radiance Fields for Pose Estimation\\nLin Yen-Chen1,2\\nPete Florence1\\nJonathan T. Barron1\\nAlberto Rodriguez2\\nPhillip Isola2\\nTsung-Yi Lin1\\n1Google Research\\n2Massachusetts Institute of Technology\\nt=180\\nt=90\\nt=0\\nt=270\\nIterative Pose Estimation\\nw/ NeRF Model\\nObserved Image \\nw/ Unknown Pose\\nPose Estimation Results: \\nOverlaid NeRF Rendering and Observed Image\\nt=0\\nt=90\\nt=180\\nt=270\\nFig. 1: We present iNeRF which performs mesh-free pose estimation by inverting a neural radiance ﬁeld of an object or scene. The middle\\nﬁgure shows the trajectory of estimated poses (gray) and the ground truth pose (green) in iNeRF’s iterative pose estimation procedure. By\\ncomparing the observed and rendered images, we perform gradient-based optimization to estimate the camera’s pose without accessing\\nthe object’s mesh model. Click the image to play the video in a browser.\\nAbstract— We present iNeRF, a framework that performs\\nmesh-free pose estimation by “inverting” a Neural Radiance\\nField (NeRF). NeRFs have been shown to be remarkably effec-\\ntive for the task of view synthesis — synthesizing photorealistic\\nnovel views of real-world scenes or objects. In this work,\\nwe investigate whether we can apply analysis-by-synthesis via\\nNeRF for mesh-free, RGB-only 6DoF pose estimation – given\\nan image, ﬁnd the translation and rotation of a camera relative\\nto a 3D object or scene. Our method assumes that no object\\nmesh models are available during either training or test time.\\nStarting from an initial pose estimate, we use gradient descent\\nto minimize the residual between pixels rendered from a NeRF\\nand pixels in an observed image. In our experiments, we ﬁrst\\nstudy 1) how to sample rays during pose reﬁnement for iNeRF\\nto collect informative gradients and 2) how different batch sizes\\nof rays affect iNeRF on a synthetic dataset. We then show\\nthat for complex real-world scenes from the LLFF dataset,\\niNeRF can improve NeRF by estimating the camera poses of\\nnovel images and using these images as additional training\\ndata for NeRF. Finally, we show iNeRF can perform category-\\nlevel object pose estimation, including object instances not seen\\nduring training, with RGB images by inverting a NeRF model\\ninferred from a single view.\\nI. INTRODUCTION\\nSix degree of freedom (6DoF) pose estimation has a wide\\nrange of applications, including robot manipulation, and mo-\\nbile robotics, and augmented reality, [16], [17], [6]. Recent\\nprogress in differentiable rendering has sparked interest in\\nsolving pose estimation via analysis-by-synthesis [3], [15],\\n[26], [43]. However, techniques built around differentiable\\nrendering engines typically require a high-quality watertight\\n3D model, e.g., mesh model, of the object for use in\\nrendering. Obtaining such models can be difﬁcult and labor-\\nintensive, and objects with unusual transparencies, shapes,\\nor material properties may not be amenable to 3D model\\nformats used in rendering engines.\\nThe\\nrecent\\nadvances\\nof\\nNeural\\nRadiance\\nFields\\n(NeRF [22]) provide a mechanism for capturing complex\\n3D and optical structures from only one or a few RGB\\nimages, which opens up the opportunity to apply analysis-\\nby-synthesis to broader real-world scenarios without mesh\\nmodels during training or test times. NeRF representations\\nparameterize the density and color of the scene as a function\\nof 3D scene coordinates. The function can either be learned\\nfrom multi-view images with given camera poses [18], [22]\\nor directly predicted by a generative model given one or\\nfew input images [45], [47].\\nHere we present iNeRF, a new framework for 6 DoF pose\\nestimation by inverting a NeRF model. . iNeRF takes three\\ninputs: an observed image, an initial estimate of the pose,\\nand a NeRF model representing a 3D scene or an object\\nin the image. We adopt an analysis-by-synthesis approach\\nto compute the appearance differences between the pixels\\nrendered from the NeRF model and the pixels from the\\nobserved image. The gradients from these residuals are then\\nbackpropagated through the NeRF model to produce the\\ngradients for the estimated pose. As illustrated in Figure 1,\\nthis procedure is repeated iteratively until the rendered and\\nobserved images are aligned, thereby yielding an accurate\\npose estimate.\\nDespite its compelling reconstruction ability, using NeRF\\nas a differentiable renderer for pose estimation through\\ngradient-based optimization presents several challenges. For\\none, NeRF renders each pixel in an image by shooting a ray\\nthrough that pixel and repeatedly querying a 3D radiance\\narXiv:2012.05877v3  [cs.CV]  10 Aug 2021\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 0, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='ﬁeld (parameterized by a neural network) while marching\\nalong that ray to produce estimates of volume density and\\ncolor that are then alpha-composited into a pixel color. This\\nrendering procedure is expensive, which presents a problem\\nfor an analysis-by-synthesis approach which, naively, should\\nrequire rendering a complete image and backpropagating\\nthe loss contributed by all pixels. For iNeRF, we address\\nthis issue by capitalizing on the fact that NeRF’s ray-\\nmarching structure allows rays and pixels to be rendered\\nindividually, and we present an interest point-based sampling\\napproach that allows us to identify which rays should be\\nsampled to best inform the pose of the object. This sampling\\nstrategy allows for accurate pose estimation while using two\\norders of magnitude fewer pixels than a full-image sampling.\\nFurthermore, we demonstrate iNeRF can improve NeRF’s\\nreconstruction quality by annotating images without pose\\nlabels and adding them to the training set. We show that\\nthis procedure can reduce the number of required labeled\\nimages by 25% while maintaining reconstruction quality.\\nFinally, we show iNeRF can perform category-level object\\npose estimation, including object instances not seen during\\ntraining, with RGB inputs by inverting a NeRF model\\ninferred by pixelNeRF [47] given a single view of the object.\\nThe only prior work we are aware of that similarly provides\\nRGB-only category-level pose estimation is the recent work\\nof Chen et al. [3]. In Sec. II we compare differences between\\n[3] and our work, which mostly arise from the opportunities\\nand challenges presented by a continuous, implicit NeRF\\nparameterization.\\nTo summarize, our primary contributions are as follows.\\n(i) We show that iNeRF can use a NeRF model to estimate\\n6 DoF pose for scenes and objects with complex geometry,\\nwithout the use of 3D mesh models or depth sensing — only\\nRGB images are used as input. (ii) We perform a thorough\\ninvestigation of ray sampling and the batch sizes for gradient\\noptimization to characterize the robustness and limitations\\nof iNeRF. (iii) We show that iNeRF can improve NeRF\\nby predicting the camera poses of additional images, that\\ncan then be added into NeRF’s training set. (iv) We show\\ncategory-level pose estimation results, for unseen objects,\\nincluding a real-world demonstration.\\nII. RELATED WORKS\\nNeural 3D shape representations. Recently, several\\nworks have investigated representing 3D shapes implicitly\\nwith neural networks. In this formulation, the geometric or\\nappearance properties of a 3D point x = (x, y, z) is param-\\neterized as the output of a neural network. The advantage\\nof this approach is that scenes with complex topologies can\\nbe represented at high resolution with low memory usage.\\nWhen ground truth 3D geometry is available as supervision,\\nneural networks can be optimized to represent the signed\\ndistance function [25] or occupancy function [20]. However,\\nground truth 3D shapes are hard to obtain in practice.\\nThis motivates subsequent work on relaxing this constraint\\nby formulating differentiable rendering pipelines that allow\\nneural 3D shape representations to be learned using only 2D\\nimages as supervision [8], [11], [12]. Niemeyer et al. [23]\\nrepresent a surface as a neural 3D occupancy ﬁeld and texture\\nas a neural 3D texture ﬁeld. Ray intersection locations are\\nﬁrst computed with numerical methods using the occupancy\\nﬁeld and then provided as inputs to the texture ﬁeld to\\noutput the colors. Scene Representation Networks [34] learn\\na neural 3D representation that outputs a feature vector and\\nRGB color at each continuous 3D coordinate and employs\\na recurrent neural network to perform differentiable ray-\\nmarching. NeRF [22] shows that by taking view directions\\nas additional inputs, a learned neural network works well\\nin tandem with volume rendering techniques and enables\\nphoto-realistic view synthesis. NeRF in the Wild [18] extends\\nNeRF to additionally model each image’s individual appear-\\nance and transient content, thereby allowing high-quality\\n3D reconstruction of landmarks using unconstrained photo\\ncollections. NSVF [13] improves NeRF by incorporating a\\nsparse voxel octree structure into the scene representation,\\nwhich accelerates rendering by allowing voxels without\\nscene content to be omitted during rendering. To generalize\\nacross scenes or objects, pixelNeRF [47] and IBRNet [45]\\npredict NeRF models conditioned on input images. Unlike\\nNeRF and its variants, which learn to represent a scene’s\\nstructure from posed RGB images, we address the inverse\\nproblem: how to localize new observations whose camera\\nposes are unknown, using a NeRF.\\nPose Estimation from RGB Images. Classical methods\\nfor object pose estimation address the task by detecting\\nand matching keypoints with known 3D models [1], [4],\\n[5], [29]. Recent approaches based on deep learning have\\nproposed to 1) directly estimate objects pose using CNN-\\nbased architectures [32], [40], [46] or 2) estimate 2D key-\\npoints [27], [35], [37], [38] and solve for pose using the\\nPnP-RANSAC algorithm. Differentiable mesh renderers [2],\\n[24] have also been explored for pose estimation. Although\\ntheir results are impressive, all the aforementioned works\\nrequire access to objects’ 3D models during both training and\\ntesting, which signiﬁcantly limits the applicability of these\\napproaches. Recently, Chen et al. [3] address category-level\\nobject pose estimation [44], in particular they impressively\\nestimate object shape and pose across a category from a\\nsingle image. They use a single-image reconstruction with\\na 3D voxel-based feature volume and then estimating pose\\nusing iterative image alignment. In contrast, in our work\\nwe use continuous implicit 3D representations in the form\\nof NeRF models, which have been empirically shown to\\nproduce more photorealistic novel-image rendering [22], [18]\\nand scale to large, building-scale volumes [18], which we\\nhypothesize will enable higher-ﬁdelity pose estimation. This\\nalso presents challenges, however, due to the expensive\\ncomputational cost of NeRF rendering, for which we intro-\\nduce a novel importance-sampling approach in Sec. IV-B.\\nAnother practical difference in our approach to category-\\nlevel pose estimation – while [3] optimizes for shape with\\ngradient descent, we show we can instead allow pixelNeRF\\nto predict a NeRF model with just a forward pass of a\\nnetwork. Additionally, since NeRF models scale well to large\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 1, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='NeRF\\nEstimated \\nPose\\nSampled Pixels\\nRays\\nRendered Pixels\\nObserved Pixels\\nLoss\\nBackpropagation\\nF⇥\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>F⇥\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\nsMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>\\n<latexit sha1_base64=\"wIZvrwMZbsarO/wb5mO5nuEZUb\\n8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNREI8V+oVtKJvtpl262YTdiVBC/4UXD4p49d9489+4bXPQ1gcDj/\\ndmJkXJFIYdN1vp7C2vrG5Vdwu7ezu7R+UD49aJk4140Wy1h3Amq4FIo3UaDknURzGgWSt4Px7cxvP3FtRKwaOEm4H9Gh\\nEqFgFK30eNfPeo0Rzrtlytu1Z2DrBIvJxXIUe+Xv3qDmKURV8gkNabruQn6GdUomOTUi81PKFsTIe8a6miETd+Nr94Ss6\\n7xKWhdVz616D5eV2k0eRxFO4BTOwYMrqME91KEJDBQ8wyu8OcZ5cd6dj0VrwclnjuEPnM8fg/mQzQ=</latexit>sMiBhrG0pJHP190RGI2MmUWA7I4ojs+zNxP+8borhtZ8JlaTIFVsClNJMCaz98lAaM5QTiyhTAt7K2EjqilDG1LJhuAtv\\n+\\n<latexit sha1_ba\\nse64=\"aWOcHJrcbrsxJSLO3n80z068C\\nGE=\">AB6HicbVBNS8NAEJ3Ur1q/qh\\n69LBZBEoigh6LXjy2YD+gDWznbRrN\\n5uwuxFK6C/w4kERr/4kb/4bt20O2vpg\\n4PHeDPzgkRwbVz32ymsrW9sbhW3Szu\\n7e/sH5cOjlo5TxbDJYhGrTkA1Ci6xab\\ngR2EkU0igQ2A7GdzO/YRK81g+mEmCf\\nkSHkoecUWOlxkW/XHGr7hxklXg5qUCO\\ner/81RvELI1QGiao1l3PTYyfUWU4Ezg\\nt9VKNCWVjOsSupZJGqP1sfuiUnFlQM\\nJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/\\nud1UxPe+BmXSWpQsWiMBXExGT2NRlw\\nhcyIiSWUKW5vJWxEFWXGZlOyIXjL6+S\\n1mXVc6te46pSu83jKMIJnMI5eHANbi\\nHOjSBAcIzvMKb8+i8O/Ox6K14OQzx/\\nAHzucPcYWMrw=</latexit>\\n<latexit sha1_ba\\nse64=\"aWOcHJrcbrsxJSLO3n80z068C\\nGE=\">AB6HicbVBNS8NAEJ3Ur1q/qh\\n69LBZBEoigh6LXjy2YD+gDWznbRrN\\n5uwuxFK6C/w4kERr/4kb/4bt20O2vpg\\n4PHeDPzgkRwbVz32ymsrW9sbhW3Szu\\n7e/sH5cOjlo5TxbDJYhGrTkA1Ci6xab\\ngR2EkU0igQ2A7GdzO/YRK81g+mEmCf\\nkSHkoecUWOlxkW/XHGr7hxklXg5qUCO\\ner/81RvELI1QGiao1l3PTYyfUWU4Ezg\\nt9VKNCWVjOsSupZJGqP1sfuiUnFlQM\\nJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/\\nud1UxPe+BmXSWpQsWiMBXExGT2NRlw\\nhcyIiSWUKW5vJWxEFWXGZlOyIXjL6+S\\n1mXVc6te46pSu83jKMIJnMI5eHANbi\\nHOjSBAcIzvMKb8+i8O/Ox6K14OQzx/\\nAHzucPcYWMrw=</latexit>\\n<latexit sha1_ba\\nse64=\"aWOcHJrcbrsxJSLO3n80z068C\\nGE=\">AB6HicbVBNS8NAEJ3Ur1q/qh\\n69LBZBEoigh6LXjy2YD+gDWznbRrN\\n5uwuxFK6C/w4kERr/4kb/4bt20O2vpg\\n4PHeDPzgkRwbVz32ymsrW9sbhW3Szu\\n7e/sH5cOjlo5TxbDJYhGrTkA1Ci6xab\\ngR2EkU0igQ2A7GdzO/YRK81g+mEmCf\\nkSHkoecUWOlxkW/XHGr7hxklXg5qUCO\\ner/81RvELI1QGiao1l3PTYyfUWU4Ezg\\nt9VKNCWVjOsSupZJGqP1sfuiUnFlQM\\nJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/\\nud1UxPe+BmXSWpQsWiMBXExGT2NRlw\\nhcyIiSWUKW5vJWxEFWXGZlOyIXjL6+S\\n1mXVc6te46pSu83jKMIJnMI5eHANbi\\nHOjSBAcIzvMKb8+i8O/Ox6K14OQzx/\\nAHzucPcYWMrw=</latexit>\\n<latexit sha1_ba\\nse64=\"aWOcHJrcbrsxJSLO3n80z068C\\nGE=\">AB6HicbVBNS8NAEJ3Ur1q/qh\\n69LBZBEoigh6LXjy2YD+gDWznbRrN\\n5uwuxFK6C/w4kERr/4kb/4bt20O2vpg\\n4PHeDPzgkRwbVz32ymsrW9sbhW3Szu\\n7e/sH5cOjlo5TxbDJYhGrTkA1Ci6xab\\ngR2EkU0igQ2A7GdzO/YRK81g+mEmCf\\nkSHkoecUWOlxkW/XHGr7hxklXg5qUCO\\ner/81RvELI1QGiao1l3PTYyfUWU4Ezg\\nt9VKNCWVjOsSupZJGqP1sfuiUnFlQM\\nJY2ZKGzNXfExmNtJ5Ege2MqBnpZW8m/\\nud1UxPe+BmXSWpQsWiMBXExGT2NRlw\\nhcyIiSWUKW5vJWxEFWXGZlOyIXjL6+S\\n1mXVc6te46pSu83jKMIJnMI5eHANbi\\nHOjSBAcIzvMKb8+i8O/Ox6K14OQzx/\\nAHzucPcYWMrw=</latexit>\\nT\\n<latexit sha1_ba\\nse64=\"+PBvf+lnsnFY014vABWwPiSxY\\no=\">AB6HicbVBNS8NAEJ3Ur1q/qh69\\nLBbBU0lE0GPRi8cW+gVtKJvtpF272YTd\\njVBCf4EXD4p49Sd589+4bXPQ1gcDj/dm\\nmJkXJIJr47rfTmFjc2t7p7hb2ts/ODw\\nqH5+0dZwqhi0Wi1h1A6pRcIktw43AbqK\\nQRoHATjC5n/udJ1Sax7Jpgn6ER1JHnJ\\nGjZUazUG54lbdBcg68XJSgRz1QfmrP4x\\nZGqE0TFCte56bGD+jynAmcFbqpxoTyi\\nZ0hD1LJY1Q+9ni0Bm5sMqQhLGyJQ1ZqL\\n8nMhpPY0C2xlRM9ar3lz8z+ulJrz1My\\n6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N\\n5K2JgqyozNpmRD8FZfXiftq6rnVr3Gda\\nV2l8dRhDM4h0vw4AZq8AB1aAEDhGd4h\\nTfn0Xlx3p2PZWvByWdO4Q+czx+vqYzY<\\n/latexit>\\n<latexit sha1_ba\\nse64=\"+PBvf+lnsnFY014vABWwPiSxY\\no=\">AB6HicbVBNS8NAEJ3Ur1q/qh69\\nLBbBU0lE0GPRi8cW+gVtKJvtpF272YTd\\njVBCf4EXD4p49Sd589+4bXPQ1gcDj/dm\\nmJkXJIJr47rfTmFjc2t7p7hb2ts/ODw\\nqH5+0dZwqhi0Wi1h1A6pRcIktw43AbqK\\nQRoHATjC5n/udJ1Sax7Jpgn6ER1JHnJ\\nGjZUazUG54lbdBcg68XJSgRz1QfmrP4x\\nZGqE0TFCte56bGD+jynAmcFbqpxoTyi\\nZ0hD1LJY1Q+9ni0Bm5sMqQhLGyJQ1ZqL\\n8nMhpPY0C2xlRM9ar3lz8z+ulJrz1My\\n6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N\\n5K2JgqyozNpmRD8FZfXiftq6rnVr3Gda\\nV2l8dRhDM4h0vw4AZq8AB1aAEDhGd4h\\nTfn0Xlx3p2PZWvByWdO4Q+czx+vqYzY<\\n/latexit>\\n<latexit sha1_ba\\nse64=\"+PBvf+lnsnFY014vABWwPiSxY\\no=\">AB6HicbVBNS8NAEJ3Ur1q/qh69\\nLBbBU0lE0GPRi8cW+gVtKJvtpF272YTd\\njVBCf4EXD4p49Sd589+4bXPQ1gcDj/dm\\nmJkXJIJr47rfTmFjc2t7p7hb2ts/ODw\\nqH5+0dZwqhi0Wi1h1A6pRcIktw43AbqK\\nQRoHATjC5n/udJ1Sax7Jpgn6ER1JHnJ\\nGjZUazUG54lbdBcg68XJSgRz1QfmrP4x\\nZGqE0TFCte56bGD+jynAmcFbqpxoTyi\\nZ0hD1LJY1Q+9ni0Bm5sMqQhLGyJQ1ZqL\\n8nMhpPY0C2xlRM9ar3lz8z+ulJrz1My\\n6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N\\n5K2JgqyozNpmRD8FZfXiftq6rnVr3Gda\\nV2l8dRhDM4h0vw4AZq8AB1aAEDhGd4h\\nTfn0Xlx3p2PZWvByWdO4Q+czx+vqYzY<\\n/latexit>\\n<latexit sha1_ba\\nse64=\"+PBvf+lnsnFY014vABWwPiSxY\\no=\">AB6HicbVBNS8NAEJ3Ur1q/qh69\\nLBbBU0lE0GPRi8cW+gVtKJvtpF272YTd\\njVBCf4EXD4p49Sd589+4bXPQ1gcDj/dm\\nmJkXJIJr47rfTmFjc2t7p7hb2ts/ODw\\nqH5+0dZwqhi0Wi1h1A6pRcIktw43AbqK\\nQRoHATjC5n/udJ1Sax7Jpgn6ER1JHnJ\\nGjZUazUG54lbdBcg68XJSgRz1QfmrP4x\\nZGqE0TFCte56bGD+jynAmcFbqpxoTyi\\nZ0hD1LJY1Q+9ni0Bm5sMqQhLGyJQ1ZqL\\n8nMhpPY0C2xlRM9ar3lz8z+ulJrz1My\\n6T1KBky0VhKoiJyfxrMuQKmRFTSyhT3N\\n5K2JgqyozNpmRD8FZfXiftq6rnVr3Gda\\nV2l8dRhDM4h0vw4AZq8AB1aAEDhGd4h\\nTfn0Xlx3p2PZWvByWdO4Q+czx+vqYzY<\\n/latexit>\\nC(r)\\n<latexit sha1_base64=\"wiGoFXnOuVJfdeJP9sAqg09UY=\">AB9HicbVDLSgMxFL2pr1pfVZdugkWomzIjgi6L3bisYB/QDiWT\\nZtrQTGZMoUy9DvcuFDErR/jzr8x085CWw8EDufcyz05fiy4No7zjQobm1vbO8Xd0t7+weFR+fikraNEUdaikYhU1yeaCS5Zy3AjWDdWjIS+YB1/0sj8zpQpzSP5aGYx80IykjzglBgreY1qPyRm7Aepml8OyhWn5iyA14mbkwrkaA7KX/1hRJOQSUMF0brnOrHxUqIMp4LNS/1\\nEs5jQCRmxnqWShEx76SL0HF9YZYiDSNknDV6ovzdSEmo9C307mUXUq14m/uf1EhPceimXcWKYpMtDQSKwiXDWAB5yxagRM0sIVdxmxXRMFKHG9lSyJbirX14n7aua69Tch+tK/S6vowhncA5VcOEG6nAPTWgBhSd4hld4Q1P0gt7Rx3K0gPKdU/gD9PkDTIWRyA=</latexit>\\n<latexit sha1_base64=\"wiGoFXnOuVJfdeJP9sAqg09UY=\">AB9HicbVDLSgMxFL2pr1pfVZdugkWomzIjgi6L3bisYB/QDiWT\\nZtrQTGZMoUy9DvcuFDErR/jzr8x085CWw8EDufcyz05fiy4No7zjQobm1vbO8Xd0t7+weFR+fikraNEUdaikYhU1yeaCS5Zy3AjWDdWjIS+YB1/0sj8zpQpzSP5aGYx80IykjzglBgreY1qPyRm7Aepml8OyhWn5iyA14mbkwrkaA7KX/1hRJOQSUMF0brnOrHxUqIMp4LNS/1\\nEs5jQCRmxnqWShEx76SL0HF9YZYiDSNknDV6ovzdSEmo9C307mUXUq14m/uf1EhPceimXcWKYpMtDQSKwiXDWAB5yxagRM0sIVdxmxXRMFKHG9lSyJbirX14n7aua69Tch+tK/S6vowhncA5VcOEG6nAPTWgBhSd4hld4Q1P0gt7Rx3K0gPKdU/gD9PkDTIWRyA=</latexit>\\n<latexit sha1_base64=\"wiGoFXnOuVJfdeJP9sAqg09UY=\">AB9HicbVDLSgMxFL2pr1pfVZdugkWomzIjgi6L3bisYB/QDiWT\\nZtrQTGZMoUy9DvcuFDErR/jzr8x085CWw8EDufcyz05fiy4No7zjQobm1vbO8Xd0t7+weFR+fikraNEUdaikYhU1yeaCS5Zy3AjWDdWjIS+YB1/0sj8zpQpzSP5aGYx80IykjzglBgreY1qPyRm7Aepml8OyhWn5iyA14mbkwrkaA7KX/1hRJOQSUMF0brnOrHxUqIMp4LNS/1\\nEs5jQCRmxnqWShEx76SL0HF9YZYiDSNknDV6ovzdSEmo9C307mUXUq14m/uf1EhPceimXcWKYpMtDQSKwiXDWAB5yxagRM0sIVdxmxXRMFKHG9lSyJbirX14n7aua69Tch+tK/S6vowhncA5VcOEG6nAPTWgBhSd4hld4Q1P0gt7Rx3K0gPKdU/gD9PkDTIWRyA=</latexit>\\n<latexit sha1_base64=\"wiGoFXnOuVJfdeJP9sAqg09UY=\">AB9HicbVDLSgMxFL2pr1pfVZdugkWomzIjgi6L3bisYB/QDiWT\\nEs5jQCRmxnqWShEx76SL0HF9YZYiDSNknDV6ovzdSEmo9C307mUXUq14m/uf1EhPceimXcWKYpMtDQSKwiXDWAB5yxagRM0sIVdxmxXRMFKHG9lSyJbirX14n7aua69Tch+tK/S6vowhncA5VcOEG6nAPTWgBhSd4hld4Q1P0gt7Rx3K0gPKdU/gD9PkDTIWRyA=</latexit>ZtrQTGZMoUy9DvcuFDErR/jzr8x085CWw8EDufcyz05fiy4No7zjQobm1vbO8Xd0t7+weFR+fikraNEUdaikYhU1yeaCS5Zy3AjWDdWjIS+YB1/0sj8zpQpzSP5aGYx80IykjzglBgreY1qPyRm7Aepml8OyhWn5iyA14mbkwrkaA7KX/1hRJOQSUMF0brnOrHxUqIMp4LNS/1\\nˆC(r)\\n<latexit sha1_base64=\"qDYNst2S5gZgEJWc7+iUrRyNjA=\">AB/HicbVDLSsNAFL2pr1pf0S7dBItQNyURQZfFblxWsA9oQplM\\nJ+3QySTMTIQ4q+4caGIWz/EnX/jpM1CWw8MHM65l3vm+DGjUtn2t1HZ2Nza3qnu1vb2Dw6PzOTvowSgUkPRywSQx9JwignPUVI8NYEBT6jAz8eafwB49ESBrxB5XGxAvRlNOAYqS0NDbr7gyprJM3RCpmR9kIr8Ymw27ZS9grROnJA0o0R2bX+4kwklIuMIMSTly7Fh5GRK\\nKYkbymptIEiM8R1My0pSjkEgvW4TPrXOtTKwgEvpxZS3U3xsZCqVMQ19PFhHlqleI/3mjRAU3XkZ5nCjC8fJQkDBLRVbRhDWhgmDFUk0QFlRntfAMCYSV7qumS3BWv7xO+pctx24591eN9m1ZRxVO4Qya4MA1tOEOutADCk8wyu8GU/Gi/FufCxHK0a5U4c/MD5/AK3glMY=</\\nlatexit>\\n<latexit sha1_base64=\"qDYNst2S5gZgEJWc7+iUrRyNjA=\">AB/HicbVDLSsNAFL2pr1pf0S7dBItQNyURQZfFblxWsA9oQplM\\nJ+3QySTMTIQ4q+4caGIWz/EnX/jpM1CWw8MHM65l3vm+DGjUtn2t1HZ2Nza3qnu1vb2Dw6PzOTvowSgUkPRywSQx9JwignPUVI8NYEBT6jAz8eafwB49ESBrxB5XGxAvRlNOAYqS0NDbr7gyprJM3RCpmR9kIr8Ymw27ZS9grROnJA0o0R2bX+4kwklIuMIMSTly7Fh5GRK\\nKYkbymptIEiM8R1My0pSjkEgvW4TPrXOtTKwgEvpxZS3U3xsZCqVMQ19PFhHlqleI/3mjRAU3XkZ5nCjC8fJQkDBLRVbRhDWhgmDFUk0QFlRntfAMCYSV7qumS3BWv7xO+pctx24591eN9m1ZRxVO4Qya4MA1tOEOutADCk8wyu8GU/Gi/FufCxHK0a5U4c/MD5/AK3glMY=</\\nlatexit>\\n<latexit sha1_base64=\"qDYNst2S5gZgEJWc7+iUrRyNjA=\">AB/HicbVDLSsNAFL2pr1pf0S7dBItQNyURQZfFblxWsA9oQplM\\nJ+3QySTMTIQ4q+4caGIWz/EnX/jpM1CWw8MHM65l3vm+DGjUtn2t1HZ2Nza3qnu1vb2Dw6PzOTvowSgUkPRywSQx9JwignPUVI8NYEBT6jAz8eafwB49ESBrxB5XGxAvRlNOAYqS0NDbr7gyprJM3RCpmR9kIr8Ymw27ZS9grROnJA0o0R2bX+4kwklIuMIMSTly7Fh5GRK\\nKYkbymptIEiM8R1My0pSjkEgvW4TPrXOtTKwgEvpxZS3U3xsZCqVMQ19PFhHlqleI/3mjRAU3XkZ5nCjC8fJQkDBLRVbRhDWhgmDFUk0QFlRntfAMCYSV7qumS3BWv7xO+pctx24591eN9m1ZRxVO4Qya4MA1tOEOutADCk8wyu8GU/Gi/FufCxHK0a5U4c/MD5/AK3glMY=</\\nlatexit>\\n<latexit sha1_base64=\"qDYNst2S5gZgEJWc7+iUrRyNjA=\">AB/HicbVDLSsNAFL2pr1pf0S7dBItQNyURQZfFblxWsA9oQplM\\nJ+3QySTMTIQ4q+4caGIWz/EnX/jpM1CWw8MHM65l3vm+DGjUtn2t1HZ2Nza3qnu1vb2Dw6PzOTvowSgUkPRywSQx9JwignPUVI8NYEBT6jAz8eafwB49ESBrxB5XGxAvRlNOAYqS0NDbr7gyprJM3RCpmR9kIr8Ymw27ZS9grROnJA0o0R2bX+4kwklIuMIMSTly7Fh5GRK\\nKYkbymptIEiM8R1My0pSjkEgvW4TPrXOtTKwgEvpxZS3U3xsZCqVMQ19PFhHlqleI/3mjRAU3XkZ5nCjC8fJQkDBLRVbRhDWhgmDFUk0QFlRntfAMCYSV7qumS3BWv7xO+pctx24591eN9m1ZRxVO4Qya4MA1tOEOutADCk8wyu8GU/Gi/FufCxHK0a5U4c/MD5/AK3glMY=</\\nlatexit>\\nX\\nr2R\\n|| ˆC(r) − C(r)||2\\n2\\n<latexit sha1_base64=\"gmFn0/BhefSvtKT+mBHsO9QYJ1U=\">ACNnicbVDLSsNAFJ34rPVdelmsAh1YUmKoMtiN26EKvYBTQ2T6aQdOpmEmYlQ0nyVG7/DXTcu\\nFHrJzhpA9bWAwPnMvc+9xQ0alMs2JsbK6tr6xmdvKb+/s7u0XDg6bMogEJg0csEC0XSQJo5w0FWMtENBkO8y0nKHtdRvPREhacAf1CgkXR/1OfUoRkpLTuHWlpHvxLaP1MD1YpFAm3I4LTFi8X2SwPHYHiAV15LSb9cZPIe1+Xo8diqPFadQNMvmFHCZWBkpgx1p/Bq9wIc+YQrzJCUHcsMVTdGQlHMSJK3I0lChIeoTzqacuQT2Y2nZy\\nfwVCs96AVCP67gVJ2fiJEv5ch3dWe6qFz0UvE/rxMp76obUx5GinA8+8iLGFQBTDOEPSoIVmykCcKC6l0hHiCBsNJ53UI1uLJy6RZKVtm2bq7KFavszhy4BicgBKwCWoghtQBw2AwTOYgHfwYbwYb8an8TVrXTGymSPwB8b3DxjqrKc=</latexit>\\n<latexit sha1_base64=\"gmFn0/BhefSvtKT+mBHsO9QYJ1U=\">ACNnicbVDLSsNAFJ34rPVdelmsAh1YUmKoMtiN26EKvYBTQ2T6aQdOpmEmYlQ0nyVG7/DXTcu\\nFHrJzhpA9bWAwPnMvc+9xQ0alMs2JsbK6tr6xmdvKb+/s7u0XDg6bMogEJg0csEC0XSQJo5w0FWMtENBkO8y0nKHtdRvPREhacAf1CgkXR/1OfUoRkpLTuHWlpHvxLaP1MD1YpFAm3I4LTFi8X2SwPHYHiAV15LSb9cZPIe1+Xo8diqPFadQNMvmFHCZWBkpgx1p/Bq9wIc+YQrzJCUHcsMVTdGQlHMSJK3I0lChIeoTzqacuQT2Y2nZy\\nfwVCs96AVCP67gVJ2fiJEv5ch3dWe6qFz0UvE/rxMp76obUx5GinA8+8iLGFQBTDOEPSoIVmykCcKC6l0hHiCBsNJ53UI1uLJy6RZKVtm2bq7KFavszhy4BicgBKwCWoghtQBw2AwTOYgHfwYbwYb8an8TVrXTGymSPwB8b3DxjqrKc=</latexit>\\n<latexit sha1_base64=\"gmFn0/BhefSvtKT+mBHsO9QYJ1U=\">ACNnicbVDLSsNAFJ34rPVdelmsAh1YUmKoMtiN26EKvYBTQ2T6aQdOpmEmYlQ0nyVG7/DXTcu\\nFHrJzhpA9bWAwPnMvc+9xQ0alMs2JsbK6tr6xmdvKb+/s7u0XDg6bMogEJg0csEC0XSQJo5w0FWMtENBkO8y0nKHtdRvPREhacAf1CgkXR/1OfUoRkpLTuHWlpHvxLaP1MD1YpFAm3I4LTFi8X2SwPHYHiAV15LSb9cZPIe1+Xo8diqPFadQNMvmFHCZWBkpgx1p/Bq9wIc+YQrzJCUHcsMVTdGQlHMSJK3I0lChIeoTzqacuQT2Y2nZy\\nfwVCs96AVCP67gVJ2fiJEv5ch3dWe6qFz0UvE/rxMp76obUx5GinA8+8iLGFQBTDOEPSoIVmykCcKC6l0hHiCBsNJ53UI1uLJy6RZKVtm2bq7KFavszhy4BicgBKwCWoghtQBw2AwTOYgHfwYbwYb8an8TVrXTGymSPwB8b3DxjqrKc=</latexit>\\n<latexit sha1_base64=\"gmFn0/BhefSvtKT+mBHsO9QYJ1U=\">ACNnicbVDLSsNAFJ34rPVdelmsAh1YUmKoMtiN26EKvYBTQ2T6aQdOpmEmYlQ0nyVG7/DXTcu\\nfwVCs96AVCP67gVJ2fiJEv5ch3dWe6qFz0UvE/rxMp76obUx5GinA8+8iLGFQBTDOEPSoIVmykCcKC6l0hHiCBsNJ53UI1uLJy6RZKVtm2bq7KFavszhy4BicgBKwCWoghtQBw2AwTOYgHfwYbwYb8an8TVrXTGymSPwB8b3DxjqrKc=</latexit>FHrJzhpA9bWAwPnMvc+9xQ0alMs2JsbK6tr6xmdvKb+/s7u0XDg6bMogEJg0csEC0XSQJo5w0FWMtENBkO8y0nKHtdRvPREhacAf1CgkXR/1OfUoRkpLTuHWlpHvxLaP1MD1YpFAm3I4LTFi8X2SwPHYHiAV15LSb9cZPIe1+Xo8diqPFadQNMvmFHCZWBkpgx1p/Bq9wIc+YQrzJCUHcsMVTdGQlHMSJK3I0lChIeoTzqacuQT2Y2nZy\\nr 2 R\\n<latexit sha1_base64=\"grFlHRQurE\\nZvpvB+iDEi4/5lTv4=\">ACA3icbVBNS8NAEJ3Ur1q/ot70slgETyURQY9FLx6r\\n2FpoQtlsN+3SzSbsboQSCl78K148KOLVP+HNf+MmzUFbHy8fW+GmXlBwpnSjvN\\ntVZaWV1bXqu1jc2t7R17d6+j4lQS2iYxj2U3wIpyJmhbM81pN5EURwGn98H4Kvf\\nvH6hULBZ3epJQP8JDwUJGsDZS3z7wIqxHQZjJKfKYQMWXYJ7dTvt23Wk4BdAicUt\\nShxKtv3lDWKSRlRowrFSPdJtJ9hqRnhdFrzUkUTMZ4SHuGChxR5WfFDVN0bJ\\nQBCmNpntCoUH93ZDhSahIFpjJfUc17ufif10t1eOFnTCSpoLMBoUpRzpGeSBowC\\nQlmk8MwUQysysiIywx0Sa2mgnBnT95kXROG67TcG/O6s3LMo4qHMIRnIAL59CEa\\n2hBGwg8wjO8wpv1ZL1Y79bHrLRilT378AfW5w8u2Zfa</latexit>\\n<latexit sha1_base64=\"grFlHRQurE\\nZvpvB+iDEi4/5lTv4=\">ACA3icbVBNS8NAEJ3Ur1q/ot70slgETyURQY9FLx6r\\n2FpoQtlsN+3SzSbsboQSCl78K148KOLVP+HNf+MmzUFbHy8fW+GmXlBwpnSjvN\\ntVZaWV1bXqu1jc2t7R17d6+j4lQS2iYxj2U3wIpyJmhbM81pN5EURwGn98H4Kvf\\nvH6hULBZ3epJQP8JDwUJGsDZS3z7wIqxHQZjJKfKYQMWXYJ7dTvt23Wk4BdAicUt\\nShxKtv3lDWKSRlRowrFSPdJtJ9hqRnhdFrzUkUTMZ4SHuGChxR5WfFDVN0bJ\\nQBCmNpntCoUH93ZDhSahIFpjJfUc17ufif10t1eOFnTCSpoLMBoUpRzpGeSBowC\\nQlmk8MwUQysysiIywx0Sa2mgnBnT95kXROG67TcG/O6s3LMo4qHMIRnIAL59CEa\\n2hBGwg8wjO8wpv1ZL1Y79bHrLRilT378AfW5w8u2Zfa</latexit>\\n<latexit sha1_base64=\"grFlHRQurE\\nZvpvB+iDEi4/5lTv4=\">ACA3icbVBNS8NAEJ3Ur1q/ot70slgETyURQY9FLx6r\\n2FpoQtlsN+3SzSbsboQSCl78K148KOLVP+HNf+MmzUFbHy8fW+GmXlBwpnSjvN\\ntVZaWV1bXqu1jc2t7R17d6+j4lQS2iYxj2U3wIpyJmhbM81pN5EURwGn98H4Kvf\\nvH6hULBZ3epJQP8JDwUJGsDZS3z7wIqxHQZjJKfKYQMWXYJ7dTvt23Wk4BdAicUt\\nShxKtv3lDWKSRlRowrFSPdJtJ9hqRnhdFrzUkUTMZ4SHuGChxR5WfFDVN0bJ\\nQBCmNpntCoUH93ZDhSahIFpjJfUc17ufif10t1eOFnTCSpoLMBoUpRzpGeSBowC\\nQlmk8MwUQysysiIywx0Sa2mgnBnT95kXROG67TcG/O6s3LMo4qHMIRnIAL59CEa\\n2hBGwg8wjO8wpv1ZL1Y79bHrLRilT378AfW5w8u2Zfa</latexit>\\n<latexit sha1_base64=\"grFlHRQurE\\nZvpvB+iDEi4/5lTv4=\">ACA3icbVBNS8NAEJ3Ur1q/ot70slgETyURQY9FLx6r\\n2FpoQtlsN+3SzSbsboQSCl78K148KOLVP+HNf+MmzUFbHy8fW+GmXlBwpnSjvN\\ntVZaWV1bXqu1jc2t7R17d6+j4lQS2iYxj2U3wIpyJmhbM81pN5EURwGn98H4Kvf\\nvH6hULBZ3epJQP8JDwUJGsDZS3z7wIqxHQZjJKfKYQMWXYJ7dTvt23Wk4BdAicUt\\nShxKtv3lDWKSRlRowrFSPdJtJ9hqRnhdFrzUkUTMZ4SHuGChxR5WfFDVN0bJ\\nQBCmNpntCoUH93ZDhSahIFpjJfUc17ufif10t1eOFnTCSpoLMBoUpRzpGeSBowC\\nQlmk8MwUQysysiIywx0Sa2mgnBnT95kXROG67TcG/O6s3LMo4qHMIRnIAL59CEa\\n2hBGwg8wjO8wpv1ZL1Y79bHrLRilT378AfW5w8u2Zfa</latexit>\\nFig. 2: An overview of our pose estimation pipeline which inverts an optimized neural radiance ﬁeld (NeRF). Given an initially estimated\\npose, we ﬁrst decide which rays to emit. Sampled points along the ray and the corresponding viewing direction are fed into NeRF’s\\nvolume rendering procedure to output rendered pixels. Since the whole pipeline is differentiable, we can reﬁne our estimated pose by\\nminimizing the residual between the rendered and observed pixels.\\nscenes, we can use the same iNeRF formulation to perform\\nlocalization, for example in challenging real-world LLFF\\nscenes – this capability was not demonstrated in [3], and\\nmay be challenging due to the memory limitations of voxel\\nrepresentations for sufﬁcient ﬁdelity in large scenes. While\\nobject pose estimation methods are often separate from\\nmethods used for visual localization of a camera in a scene as\\nin the SfM literature (i.e. [33], [41], [31]), because NeRF and\\niNeRF only require posed RGB images as training, iNeRF\\ncan be applied to localization as well.\\nIII. BACKGROUND\\nGiven a collection of N RGB images {Ii}N\\ni=1, Ii ∈\\n[0, 1]H×W ×3 with known camera poses {Ti}N\\ni=1, NeRF\\nlearns to synthesize novel views associated with unseen\\ncamera poses. NeRF does this by representing a scene as\\na “radiance ﬁeld”: a volumetric density that models the\\nshape of the scene, and a view-dependent color that models\\nthe appearance of occupied regions of the scene, both of\\nwhich lie within a bounded 3D volume. The density σ\\nand RGB color c of each point are parameterized by the\\nweights Θ of a multilayer perceptron (MLP) F that takes\\nas input the 3D position of that point x = (x, y, z) and the\\nunit-norm viewing direction of that point d = (dx, dy, dz),\\nwhere (σ, c) ← FΘ(x, d). To render a pixel, NeRF emits\\na camera ray from the center of the projection of a camera\\nthrough that pixel on the image plane. Along the ray, a set\\nof points are sampled for use as input to the MLP which\\noutputs a set of densities and colors. These values are then\\nused to approximate the image formation behind volume\\nrendering [7] using numerical quadrature [19], producing\\nan estimate of the color of that pixel. NeRF is trained to\\nminimize a photometric loss L = �\\nr∈R || ˆC(r) − C(r)||2\\n2,\\nusing some sampled set of rays r ∈ R where C(r) is the\\nobserved RGB value of the pixel corresponding to ray r in\\nsome image, and ˆC(r) is the prediction produced from neural\\nvolume rendering. To improve rendering efﬁciency one may\\ntrain two MLPs: one “coarse” and one “ﬁne”, where the\\ncoarse model serves to bias the samples that are used for the\\nﬁne model. For more details, we refer readers to Mildenhall\\net al. [22].\\nAlthough NeRF originally needs to optimize the represen-\\ntation for every scene independently, several extensions [28],\\n[39], [45], [47] have been proposed to directly predict a\\ncontinuous neural scene representation conditioned on one or\\nfew input images. In our experiments, we show that iNeRF\\ncan be used to perform 6D pose estimation with either an\\noptimized or predicted NeRF model.\\nIV. INERF FORMULATION\\nWe now present iNeRF, a framework that performs 6 DoF\\npose estimation by “inverting” a trained NeRF. Let us assume\\nthat the NeRF of a scene or object parameterized by Θ has\\nalready been recovered and that the camera intrinsics are\\nknown, but the camera pose T of an image observation I\\nare as-yet undetermined. Unlike NeRF, which optimizes Θ\\nusing a set of given camera poses and image observations, we\\ninstead solve the inverse problem of recovering the camera\\npose T given the weights Θ and the image I as input:\\nˆT = argmin\\nT ∈SE(3)\\nL(T | I, Θ)\\n(1)\\nTo solve this optimization, we use the ability from NeRF to\\ntake some estimated camera pose T ∈ SE(3) in the coordi-\\nnate frame of the NeRF model and render a corresponding\\nimage observation. We can then use the same photometric\\nloss function L as was used in NeRF (Sec. III), but rather\\nthan backpropagate to update the weights Θ of the MLP, we\\ninstead update the pose T to minimize L. The overall proce-\\ndure is shown in Figure 2. While the concept of inverting a\\nNeRF to perform pose estimation can be concisely stated, it\\nis not obvious that such a problem can be practically solved\\nto a useful degree. The loss function L is non-convex over\\nthe 6DoF space of SE(3), and full-image NeRF renderings\\nare computationally expensive, particularly if used in the\\nloop of an optimization procedure. Our formulation and\\nexperimentation (Sec. V) aim to address these challenges.\\nIn the next sections, we discuss (i) the gradient-based SE(3)\\noptimization procedure, (ii) ray sampling strategies, and (iii)\\nhow to use iNeRF’s predicted poses to improve NeRF.\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 2, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='A. Gradient-Based SE(3) Optimization\\nLet Θ be the parameters of a trained and ﬁxed NeRF,\\nˆTi the estimated camera pose at current optimization step\\ni, I the observed image, and L( ˆTi | I, Θ) be the loss used\\nto train the ﬁne model in NeRF. We employ gradient-based\\noptimization to solve for ˆT as deﬁned in Equation 1. To\\nensure that the estimated pose ˆTi continues to lie on the\\nSE(3) manifold during gradient-based optimization, we pa-\\nrameterize ˆTi with exponential coordinates. Given an initial\\npose estimate ˆT0 ∈ SE(3) from the camera frame to the\\nmodel frame, we represent ˆTi as:\\nˆTi = e[Si]θi ˆT0 ,\\nwhere\\ne[S]θ =\\n�\\ne[ω]θ\\nK(S, θ)\\n0\\n1\\n�\\n,\\nwhere S = [ω, ν]T represents the screw axis, θ the magni-\\ntude, [w] represents the skew-symmetric 3 × 3 matrix of w,\\nand K(S, θ) = (Iθ +(1−cos θ)[ω]+(θ −sin θ)[ω]2)ν [14].\\nWith this parameterization, our goal is to solve the optimal\\nrelative transformation from an initial estimated pose T0:\\n�\\nSθ = argmin\\nSθ∈R6 L(e[S]θT0 | I, Θ).\\n(2)\\nWe iteratively differentiate the loss function through the\\nMLP to obtain the gradient ∇SθL(e[S]θT0 | I, Θ) that is\\nused to update the estimated relative transformation. We\\nuse Adam optimizer [9] with an exponentially decaying\\nlearning rate (See Supplementary for parameters). For each\\nobserved image, we initialize Sθ near 0, where each element\\nis drawn at random from a zero-mean normal distribution\\nN(0, σ = 10−6). In practice, parameterizing with e[S]θ T0\\nrather than T0 e[S]θ results in a center-of-rotation at the initial\\nestimate’s center, rather than at the camera frame’s center.\\nThis alleviates coupling between rotations and translations\\nduring optimization.\\nB. Sampling Rays\\nIn a typical differentiable render-and-compare pipeline,\\none would want to leverage the gradients contributed by all of\\nthe output pixels in the rendered image [43]. However, with\\nNeRF, each output pixel’s value is computed by weighing the\\nvalues of n sampled points along each ray r ∈ R during ray\\nmarching, so given the amount of sampled rays in a batch b =\\n|R|, then O(bn) forward/backward passes of the underlying\\nNeRF MLP will be queried. Computing and backpropagating\\nthe loss of all pixels in an image (i.e., , b = HW, where H\\nand W represent the height and width of a high-resolution\\nimage) therefore require signiﬁcantly more memory than is\\npresent on any commercial GPU. While we may perform\\nmultiple forward and backward passes to accumulate these\\ngradients, this becomes prohibitively slow to perform each\\nstep of our already-iterative optimization procedure. In the\\nfollowing, we explore strategies for selecting a sampled\\nset of rays R for use in evaluating the loss function L\\nat each optimization step. In our experiments we ﬁnd that\\nwe are able to recover accurate poses while sampling only\\nRandom\\nInterest Region\\nInterest Point\\n+\\nx\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+ +\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\no\\no\\no\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\no\\noo\\no\\no\\nFig. 3: An illustration of 3 sampling strategies. The input image and\\nthe rendering corresponding to the estimated pose of the scene are\\naveraged. We use x to represent sampled pixels on the background;\\n+ to represent sampled pixels that are covered by both rendered\\nand observed images; o to represent sampled pixels that are only\\ncovered by either the rendered or the input image. When performing\\nrandom sampling (left) many sampled pixels are x, which provide\\nno gradients for updating the pose. For “interest point” sampling\\n(middle) some of the sampled pixels are already aligned and\\ntherefore provide little information. For “interest region” sampling,\\nmany sampled pixels are o, which helps pose estimation achieve\\nhigher accuracy and faster convergence.\\nb = 2048 rays per gradient step, which corresponds to a\\nsingle forward/backward pass that ﬁts within GPU memory\\nand provides 150× faster gradient steps on a 640 × 480\\nimage.\\na) Random Sampling.: An intuitive strategy is to sam-\\nple M pixel locations {pi\\nx, pi\\ny}M\\ni=0 on the image plane\\nrandomly and compute their corresponding rays. Indeed,\\nNeRF itself uses this strategy when optimizing Θ (assuming\\nimage batching is not used). We found this random sampling\\nstrategy’s performance to be ineffective when the batch size\\nof rays b is small. Most randomly-sampled pixels correspond\\nto ﬂat, textureless regions of the image, which provide little\\ninformation with regards to pose (which is consistent with\\nthe well-known aperture problem [42]). See Figure 3 for an\\nillustration.\\nb) Interest Point Sampling.: Inspired by the literature\\nof image alignment [36], we propose interest point sam-\\npling to guide iNeRF optimization, where we ﬁrst employ\\ninterest point detectors to localize a set of candidate pixel\\nlocations in the observed image. We then sample M points\\nfrom the detected interest points and fall back to random\\nsampling if not enough interest points are detected. Although\\nthis strategy makes optimization converge faster since less\\nstochasticity is introduced, we found that it is prone to local\\nminima as it only considers interest points on the observed\\nimage instead of interest points from both the observed\\nand rendered images. However, obtaining the interest points\\nin the rendered image requires O(HWn) forward MLP\\npasses and thus prohibitively expensive to be used in the\\noptimization.\\nc) Interest Region Sampling.:\\nTo prevent the local\\nminima caused by only sampling from interest points, we\\npropose using “Interest Region” Sampling, a strategy that\\nrelaxes Interest Point Sampling and samples from the dilated\\nmasks centered on the interest points. After the interest\\npoint detector localizes the interest points, we apply a 5 × 5\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 3, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='morphological dilation for I iterations to enlarge the sampled\\nregion. In practice, we ﬁnd this to speed up the optimization\\nwhen the batch size of rays is small. Note that if I is set\\nto a large number, Interest Region Sampling falls back to\\nRandom Sampling.\\nC. Self-Supervising NeRF with iNeRF\\nIn addition to using iNeRF to perform pose estimation\\ngiven a trained NeRF, we also explore using the estimated\\nposes to feed back into training the NeRF representation.\\nSpeciﬁcally, we ﬁrst (1) train a NeRF given a set of training\\nRGB images with known camera poses {(Ii, Ti)}Ntrain\\ni=1 , yield-\\ning NeRF parameters Θtrain. We then (2) use iNeRF to take\\nin additional unknown-pose observed images {Ii}Ntest\\ni=1 and\\nsolve for estimated poses { ˆTi}Ntest\\ni=1. Given these estimated\\nposes, we can then (3) use the self-supervised pose labels to\\nadd {(Ii, ˆTi)}Ntest\\ni=1 into the training set. This procedure allows\\nNeRF to be trained in a semi-supervised setting.\\nV. RESULTS\\nWe ﬁrst conduct extensive experiments on the synthetic\\ndataset from NeRF [22] and the real-world complex scenes\\nfrom LLFF [21] to evaluate iNeRF for 6DoF pose esti-\\nmation. Speciﬁcally, we study how the batch size of rays\\nand sampling strategy affect iNeRF. We then show that\\niNeRF can improve NeRF by estimating the camera poses\\nof images with unknown poses and using these images as\\nadditional training data for NeRF. Finally, we show that\\niNeRF works well in tandem with pixelNeRF [47] which\\npredicts a NeRF model conditioned on a single RGB image.\\nWe test our method for category-level object pose estimation\\nin both simulation and the real world. We found that iNeRF\\nachieving competitive results against feature-based methods\\nwithout accessing object mesh models during either training\\nor test time.\\nA. Synthetic Dataset\\na) Setting: We test iNeRF on 8 scenes from NeRF’s\\nsynthetic dataset. For each scene, we choose 5 test images\\nand generate 5 different pose initializations by ﬁrst randomly\\nsampling an axis from the unit sphere and rotating the camera\\npose around the axis by a random amount within [−40, 40]\\ndegrees. Then, we translate the camera along each axis by a\\nrandom offset within [−0.2, 0.2] meters.\\nb) Results: We report the percentage of predicted poses\\nwhose error is less than 5◦ or 5cm at different numbers\\nof steps. It is a metric widely used in the pose estimation\\ncommunity [6]. Quantitative results are shown in Figure 6a.\\nWe verify that under the same sampling strategy, larger\\nbatch sizes of rays achieve not only better pose estimation\\naccuracy, but also faster convergence. On the other hand,\\nwhen the batch size of rays is ﬁxed, interest region sampling\\nis able to provide better accuracy and efﬁciency. Speciﬁcally,\\nthe qualitative results shown in Figure 4 clearly indicate that\\nrandom sampling is inefﬁcient as many sampled points lie on\\nthe common background and therefore provide no gradient\\nfor matching.\\nLabel Fraction\\n100%\\n50%\\n25%\\niNeRF Supervision\\nNo\\nYes\\nNo\\nYes\\nNo\\nPSNR\\n24.94\\n24.64\\n24.18\\n23.89\\n21.85\\nTABLE I: Benchmark on Fern scene. NeRFs trained with pose\\nlabels generated by iNeRF can achieve higher PSNR.\\nB. LLFF Dataset\\na) Setting: We use 4 complex scenes: Fern, Fortress,\\nHorns, and Room from the LLFF dataset [21]. For each test\\nimage, we generate 5 different pose initializations following\\nthe procedures outlined in Section V-A but instead translate\\nthe camera along each axis by a random offset within\\n[−0.1, 0.1] meters. Unlike the synthetic dataset where the\\nimages are captured on a surrounding hemisphere, images\\nin the LLFF dataset are all captured with a forward-facing\\nhandheld cellphone.\\nb) Pose Estimation Results: The percentage of pre-\\ndicted poses whose error is less than 5◦ or 5cm at different\\nnumber of steps is reported in Figure 6b. Similar to Sec-\\ntion V-A, we ﬁnd that the batch size of rays signiﬁcantly\\naffects iNeRF’s visual localization performance. Also, we\\nnotice that iNeRF performs worse on the LLFF dataset\\ncompared to the synthetic dataset. When the batch size of\\nrays is set to 1024, the percentage of < 5◦ rotation errors\\ndrops from 71% to 55%, and the percentage of < 5cm\\ntranslation errors drops from 73% to 39%. This difference\\nacross datasets may be due to the fact that the LLFF use-\\ncase in NeRF uses a normalized device coordinate (NDC)\\nspace, or may simply be a byproduct of the difference in\\nscene content.\\nc) Self-Supervising NeRF with iNeRF Results: We take\\nthe Fern scene from the LLFF dataset and train it with\\n25%, 50%, and 100% of the training data, respectively. Then,\\nNeRFs trained with 25% and 50% data are used by iNeRF to\\nestimate the remaining training images’ camera poses. The\\nestimated camera poses, together with existing camera poses,\\nare used as supervision to re-train NeRF from scratch. We\\nreport PSNRs in Table I. All of the models are trained for\\n200k iterations using the same learning rate. We ﬁnd that\\nmodels that use the additional data made available through\\nthe use of iNeRF’s estimated poses perform better. This ﬁnd-\\ning is consistent with NeRF’s well-understood sensitivity to\\nthe pose of its input cameras being accurate [22]. Qualitative\\nresults can be found in Figure 5.\\nC. ShapeNet-SRN Cars\\na) Setting:\\nWe\\nevaluate\\niNeRF\\nfor\\nRGB-based\\ncategory-level object pose estimation. Given two images I0\\nand I1 of an unseen instance, we seek to estimate its relative\\npose T 1\\n0\\n= (R, t) ∈ SO(3) × S2 between images. The\\ntranslation t ∈ S2 ˙={t ∈ R3 | ∥t∥ = 1} is ambiguous up to\\na scale. We perform the experiments on the “car” classes of\\nShapeNet using the dataset introduced in Stizmann et al. [34].\\nThe dataset contains 3514 cars that are split for training,\\nvalidation, and test set. All the images have resolution of\\n128 × 128. The test images of the objects are rendered from\\n251 views on an archimedean spiral. For each object in the\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 4, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='t=0\\nt=50\\nt=100\\nt=150\\nt=200\\nRandom \\nSampling\\nInterest Region \\nSampling\\nFig. 4: We visualize the average of rendered images based on the estimated pose at time t and the test image to compare different\\nsampling methods. Adopting Interest Region Sampling helps our method to recover camera poses that align the rendered and test image\\nto ﬁne details. Random Sampling aligns the mic’s rigging, but fails to align the wire. Click the image to play the video in a browser.\\n100%\\n50%+iNeRF\\n50%\\n25%+iNeRF\\n25%\\nGround Truth\\nFern\\nFig. 5: iNeRF can be used to improve NeRF by augmenting training data with images whose camera poses are unknown. We present\\nan ablation study using 25% and 50% of training images to train NeRF models. These models are compared with models trained using\\n100% of the training images but where a fraction of that data use estimated poses from iNeRF rather than ground-truth poses from the\\ndataset.\\ntest set, I0 is selected randomly from one of the 251 views\\nand the other image I1 is selected from views whose rotation\\nand translation are within 30-degree from I0. At test time,\\nour method uses a pre-trained pixelNeRF to predict a NeRF\\nmodel conditioned on image I0. Then, we apply iNeRF to\\nalign against I1 for estimating the relative pose T 1\\n0 .\\nb) Pose Estimation Results: As shown in Table II,\\nour method achieves lower rotation and translation errors\\nthan a strong feature-based baseline, using SuperGlue [30].\\nImportantly, iNeRF receives much fewer outliers: 8.7% vs.\\n33.3%. (Outliers are deﬁned in Table II). We note that in our\\nmethod, our pose estimate is deﬁned relative to a reference\\nview of the object – this is in contrast to [3], which depends\\non a canonical pose deﬁnition – the subtleties of canonical\\npose deﬁnitions are discussed in [16], [44]. While [3]’s\\nmethod could be used in our setting, it would not make use\\nof the reference image, and code was not available to run\\nthe comparison.\\nD. Sim2Real Cars\\nRotation (◦)\\nTranslation (◦)\\nOutlier\\nMethods\\nMean\\nMedian\\nMean\\nMedian\\n%\\nSuperGlue [30]\\n9.27\\n6.22\\n18.2\\n5.4\\n33.3\\nOurs\\n4.39\\n2.01\\n4.81\\n1.77\\n8.7\\nTABLE II: Quantitative results for the ShapeNet Cars dataset. We\\nreport performance using the mean and median of the translation\\nand rotation error. A prediction is deﬁned as an outlier when either\\nthe translation error or the rotation error is larger than 20◦.\\na) Setting: We explore the performance of using iNeRF\\nto perform category-level object pose estimation on real-\\nworld images. 10 unseen cars, as shown in Figure 7 are used\\nas the test data. We apply a pixelNeRF model trained on\\nsynthetic ShapeNet dataset to infer the NeRF model from a\\nsingle real image without extra ﬁne-tuning. Then we apply\\niNeRF with the inferred NeRF model to estimate the relative\\npose to the target real image.\\nb) Pose Estimation Results: We show the qualitative\\nresults of pose tracking in Figure 8. At each time step t, iN-\\neRF inverts a NeRF model conditioned on the frame at time\\nt − 1 to estimate the object’s pose. The reconstructed frame\\nand estimated poses are also visualized. Since pixelNeRF\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 5, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='(a)\\n(b)\\n(a) Results on the synthetic dataset.\\n(a)\\n(b)\\n(b) Results on real-world scenes from the LLFF dataset.\\nFig. 6: (a) Quantitative results on the synthetic dataset. “s” stands for sampling strategy and “b” stands for the batch size. Applying Interest\\nRegion Sampling (s=region) improves the accuracy by 15% across various batch sizes. (b) Quantitative results on LLFF. Interest Region\\nSampling is always applied and we show the effect of various batch sizes on performance. Larger batch sizes can improve accuracy while\\nreducing the number of gradient steps needed for convergence.\\nFig. 7: Real-world test data, toy cars with unknown mesh models.\\nrequires a segmented image as input, we use PointRend [10]\\nto remove the background for frames that pixelNeRF takes as\\ninputs. In this iterative tracking setting, iNeRF only requires\\nless than 10 iterations of optimization to converge which\\nenables tracking at approximately 1Hz.\\nVI. LIMITATIONS AND FUTURE WORK\\nWhile iNeRF has shown promising results on pose estima-\\ntion, it is not without limitations. Both lighting and occlusion\\ncan severely affect the performance of iNeRF and are not\\nmodeled by our current formulation. One potential solution\\nis to model appearance variation using transient latent codes\\nas was done in NeRF-W [18] when training NeRFs, and\\njointly optimize these appearance codes alongside camera\\npose within iNeRF. Also, currently iNeRF takes around 20\\nseconds to run 100 optimization steps, which prevents it\\nfrom being practical for real-time use. We expect that this\\nissue may be mitigated with recent improvements in NeRF’s\\nrendering speed [13].\\nt=1\\nt=T\\nTarget Image\\nPredicted Pose  \\n& Image\\n…\\nt=T-1\\n…\\nFig. 8: Qualitative results of pose tracking in real-world images\\nwithout the need for mesh/CAD model. In the left column, we\\nshow input video frames at different time steps. At each time t,\\niNeRF leverages a NeRF model inferred by pixelNeRF based on\\ninput frame at time t − 1 to estimate the object’s pose. In the\\nright column, we show the resulting reconstructed frames and the\\nestimated poses at each time step. The background has been masked\\nout using PointRend [10] before feeding the frame into pixelNeRF.\\nThe views are rotations about the view-space vertical axis.Click\\nthe image to play the video in a browser.\\nVII. CONCLUSION\\nWe have presented iNeRF, a framework for mesh-free,\\nRGB-only pose estimation that works by inverting a NeRF\\nmodel. We have demonstrated that iNeRF is able to perform\\naccurate pose estimation using gradient-based optimization.\\nWe have thoroughly investigated how to best construct mini-\\nbatches of sampled rays for iNeRF and have demonstrated its\\nperformance on both synthetic and real datasets. Lastly, we\\nhave shown how iNeRF can perform category-level object\\npose estimation and track pose for novel object instances\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 6, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='Fig. 9: Histogram of pose errors on real-world scenes from the\\nLLFF dataset.\\nwith an image conditioned generative NeRF model.\\nAPPENDIX\\nVIII. IMPLEMENTATION DETAILS\\na) Adam Optimizer and Learning Rate Schedule.: The\\nhyperparameter β1 is set to 0.9 and β2 is set to 0.999 in\\nAdam optimizer. We set the initial learning rate α0 to 0.01.\\nThe learning rate at step t is set as follow:\\nαt = α00.8t/100 .\\nb) Loss for LineMOD.: To compare the rendered and\\nobserved images, we transform both of them from RGB to\\nYUV space using the standard deﬁnition:\\n\\uf8ee\\n\\uf8f0\\nY\\nU\\nV\\n\\uf8f9\\n\\uf8fb =\\n\\uf8ee\\n\\uf8f0\\n0.2126\\n0.7152\\n0.0722\\n−0.09991\\n−0.33609\\n0.436\\n0/615\\n−0.55861\\n−0.05639\\n\\uf8f9\\n\\uf8fb\\n\\uf8ee\\n\\uf8f0\\nR\\nG\\nB\\n\\uf8f9\\n\\uf8fb\\nThe Y channel is not considered in the computation of loss.\\nIX. HISTOGRAM OF POSE ERRORS\\nWe visualize the histogram of pose errors, before and after\\niNeRF optimization, on the LLFF dataset in Figure 9 using\\nthe data from Section 5.2. The data is generated by applying\\nrandom perturbations within [−40, 40] degrees for rotation\\nand [−0.1, 0.1] meters along each axis for translation. Note\\nthat when the batch size is 2048, more than 70% of the data\\nhas < 5◦ and < 5 cm error after iNeRF is applied.\\nX. MORE ANALYSIS IN SELF-SUPERVISED NERF\\nFor the Fern scene, we found that when only 10% of\\nlabeled camera poses are used, it worsens the PSNR from\\n18.5 to 15.64. The results show that having enough labels\\nfor a good initalization is important.\\nREFERENCES\\n[1] Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C Russell,\\nand Josef Sivic.\\nSeeing 3D chairs: exemplar part-based 2D-3D\\nalignment using a large dataset of CAD models. CVPR, 2014.\\n[2] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehti-\\nnen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects\\nwith an interpolation-based differentiable renderer. NeurIPS, 2019.\\n[3] Xu Chen, Zijian Dong, Jie Song, Andreas Geiger, and Otmar Hilliges.\\nCategory level object pose estimation via neural analysis-by-synthesis.\\nECCV, 2020.\\n[4] Alvaro Collet, Manuel Martinez, and Siddhartha S Srinivasa.\\nThe\\nmoped framework: Object recognition and pose estimation for manip-\\nulation. IJRR, 2011.\\n[5] Vittorio Ferrari, Tinne Tuytelaars, and Luc Van Gool. Simultaneous\\nobject recognition and segmentation from single or multiple model\\nviews. IJCV, 2006.\\n[6] Tomas Hodan, Frank Michel, Eric Brachmann, Wadim Kehl, Anders\\nGlentBuch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke,\\nXenophon Zabulis, et al.\\nBop: Benchmark for 6d object pose\\nestimation. ECCV, 2018.\\n[7] James T. Kajiya and Brian P. Von Herzen.\\nRay tracing volume\\ndensities. SIGGRAPH, 1984.\\n[8] Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru\\nMatsuoka, Wadim Kehl, and Adrien Gaidon. Differentiable rendering:\\nA survey. arXiv preprint arXiv:2006.12057, 2020.\\n[9] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\\noptimization. ICLR, 2015.\\n[10] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick.\\nPointrend: Image segmentation as rendering. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition,\\npages 9799–9808, 2020.\\n[11] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey.\\nSdf-srn:\\nLearning signed distance 3d object reconstruction from static images.\\nNeurIPS, 2020.\\n[12] Chen-Hsuan Lin, Oliver Wang, Bryan C Russell, Eli Shechtman,\\nVladimir G Kim, Matthew Fisher, and Simon Lucey.\\nPhotometric\\nmesh optimization for video-aligned 3d object reconstruction.\\nIn\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 969–978, 2019.\\n[13] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian\\nTheobalt. Neural sparse voxel ﬁelds. NeurIPS, 2020.\\n[14] Kevin M Lynch and Frank C Park. Modern Robotics. Cambridge\\nUniversity Press, 2017.\\n[15] Wei-Chiu Ma, Shenlong Wang, Jiayuan Gu, Sivabalan Manivasagam,\\nAntonio Torralba, and Raquel Urtasun. Deep feedback inverse problem\\nsolver. ECCV, 2020.\\n[16] Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake. kPAM:\\nKeypoint affordances for category-level robotic manipulation. ISRR,\\n2019.\\n[17] Pat Marion, Peter R Florence, Lucas Manuelli, and Russ Tedrake.\\nLabel fusion: A pipeline for generating ground truth labels for real\\nrgbd data of cluttered scenes. ICRA, 2018.\\n[18] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T\\nBarron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild:\\nNeural radiance ﬁelds for unconstrained photo collections.\\narXiv\\npreprint arXiv:2008.02268, 2020.\\n[19] Nelson Max. Optical models for direct volume rendering. IEEE TVCG,\\n1995.\\n[20] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian\\nNowozin, and Andreas Geiger.\\nOccupancy networks: Learning 3d\\nreconstruction in function space. CVPR, 2019.\\n[21] Ben\\nMildenhall,\\nPratul\\nP\\nSrinivasan,\\nRodrigo\\nOrtiz-Cayon,\\nNima\\nKhademi\\nKalantari,\\nRavi\\nRamamoorthi,\\nRen\\nNg,\\nand\\nAbhishek Kar. Local light ﬁeld fusion: Practical view synthesis with\\nprescriptive sampling guidelines. ACM TOG, 2019.\\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T\\nBarron, Ravi Ramamoorthi, and Ren Ng.\\nNerf: Representing\\nscenes as neural radiance ﬁelds for view synthesis.\\narXiv preprint\\narXiv:2003.08934, 2020.\\n[23] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas\\nGeiger.\\nDifferentiable volumetric rendering: Learning implicit 3d\\nrepresentations without 3d supervision. CVPR, 2020.\\n[24] Andrea Palazzi, Luca Bergamini, Simone Calderara, and Rita Cuc-\\nchiara. End-to-end 6-DOF object pose estimation through differen-\\ntiable rasterization. ECCV, 2018.\\n[25] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe,\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 7, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='and Steven Lovegrove. Deepsdf: Learning continuous signed distance\\nfunctions for shape representation. CVPR, 2019.\\n[26] Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox.\\nLatentfusion: End-to-end differentiable reconstruction and rendering\\nfor unseen object pose estimation. CVPR, 2020.\\n[27] Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstantinos G\\nDerpanis, and Kostas Daniilidis. 6-DOF object pose from semantic\\nkeypoints. ICRA, 2017.\\n[28] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio Fer-\\nrari. ShaRF: Shape-conditioned radiance ﬁelds from a single view.\\nhttps://arxiv.org/pdf/2102.08860.pdf, 2021.\\n[29] Fred Rothganger, Svetlana Lazebnik, Cordelia Schmid, and Jean\\nPonce. 3d object modeling and recognition using local afﬁne-invariant\\nimage descriptors and multi-view spatial constraints. IJCV, 2006.\\n[30] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and An-\\ndrew Rabinovich. Superglue: Learning feature matching with graph\\nneural networks. CVPR, 2020.\\n[31] Tanner Schmidt, Richard Newcombe, and Dieter Fox. Self-supervised\\nvisual descriptor learning for dense correspondence. IEEE Robotics\\nand Automation Letters, 2016.\\n[32] Max Schwarz, Hannes Schulz, and Sven Behnke.\\nRgb-d object\\nrecognition and pose estimation based on pre-trained convolutional\\nneural network features. ICRA, 2015.\\n[33] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Anto-\\nnio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression\\nforests for camera relocalization in rgb-d images. CVPR, 2013.\\n[34] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene\\nrepresentation networks: Continuous 3d-structure-aware neural scene\\nrepresentations. NeurIPS, 2019.\\n[35] Supasorn Suwajanakorn, Noah Snavely, Jonathan J Tompson, and\\nMohammad Norouzi. Discovery of latent 3d keypoints via end-to-\\nend geometric reasoning. NeurIPS, 2018.\\n[36] Richard Szeliski. Image alignment and stitching: A tutorial. Founda-\\ntions and Trends® in Computer Graphics and Vision, 2006.\\n[37] Bugra Tekin, Sudipta N Sinha, and Pascal Fua. Real-time seamless\\nsingle shot 6D object pose prediction. CVPR, 2018.\\n[38] Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang,\\nDieter Fox, and Stan Birchﬁeld.\\nDeep object pose estimation for\\nsemantic robotic grasping of household objects. CoRL, 2018.\\n[39] Alex\\nTrevithick\\nand\\nBo\\nYang.\\nGRF:\\nLearning\\na\\ngen-\\neral radiance ﬁeld for 3D scene representation and rendering.\\nhttps://arxiv.org/abs/2010.04595, 2020.\\n[40] Shubham Tulsiani and Jitendra Malik.\\nViewpoints and keypoints.\\nCVPR, 2015.\\n[41] Julien Valentin, Matthias Nießner, Jamie Shotton, Andrew Fitzgibbon,\\nShahram Izadi, and Philip HS Torr. Exploiting uncertainty in regres-\\nsion forests for accurate camera relocalization. CVPR, 2015.\\n[42] Hans Wallach.\\n¨Uber visuell wahrgenommene bewegungsrichtung.\\nPsychologische Forschung, 1935.\\n[43] Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir\\nNavab, and Federico Tombari. Self6d: Self-supervised monocular 6d\\nobject pose estimation. arXiv preprint arXiv:2004.06468, 2020.\\n[44] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran\\nSong, and Leonidas J Guibas. Normalized object coordinate space for\\ncategory-level 6d object pose and size estimation. CVPR, 2019.\\n[45] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan,\\nHoward Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah\\nSnavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-\\nbased rendering. arXiv preprint arXiv:2102.13090, 2021.\\n[46] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter\\nFox. PoseCNN: A convolutional neural network for 6D object pose\\nestimation in cluttered scenes. RSS, 2018.\\n[47] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixel-\\nnerf: Neural radiance ﬁelds from one or few images. CVPR, 2021.\\n', metadata={'source': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'file_path': 'data/Yen-Chen et al. - 2021 - iNeRF Inverting Neural Radiance Fields for Pose E.pdf', 'page': 8, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210811013632Z', 'modDate': 'D:20210811013632Z', 'trapped': ''}),\n",
       " Document(page_content='X2CT-GAN: Reconstructing CT from Biplanar X-Rays with Generative\\nAdversarial Networks\\nXingde Ying∗ ,†,♭, Heng Guo∗ ,§,♭, Kai Ma♭, Jian Wu†, Zhengxin Weng§, and Yefeng Zheng♭\\n♭ YouTu Lab, Tencent\\n† Zhejiang University\\n§ Shanghai Jiao Tong University\\n{kylekma, yefengzheng}@tencent.com\\n{yingxingde, wujian2000}@zju.edu.cn\\n{ghgood, zxweng}@sjtu.edu.cn\\nAbstract\\nComputed tomography (CT) can provide a 3D view of the\\npatient’s internal organs, facilitating disease diagnosis, but\\nit incurs more radiation dose to a patient and a CT scan-\\nner is much more cost prohibitive than an X-ray machine\\ntoo. Traditional CT reconstruction methods require hun-\\ndreds of X-ray projections through a full rotational scan of\\nthe body, which cannot be performed on a typical X-ray ma-\\nchine. In this work, we propose to reconstruct CT from two\\northogonal X-rays using the generative adversarial network\\n(GAN) framework. A specially designed generator network\\nis exploited to increase data dimension from 2D (X-rays)\\nto 3D (CT), which is not addressed in previous research of\\nGAN. A novel feature fusion method is proposed to com-\\nbine information from two X-rays. The mean squared error\\n(MSE) loss and adversarial loss are combined to train the\\ngenerator, resulting in a high-quality CT volume both visu-\\nally and quantitatively. Extensive experiments on a publicly\\navailable chest CT dataset demonstrate the effectiveness of\\nthe proposed method. It could be a nice enhancement of a\\nlow-cost X-ray machine to provide physicians a CT-like 3D\\nvolume in several niche applications.\\n1. Introduction\\nImmediately after its discovery by Wilhelm Rntgen in\\n1895, X-ray found wide applications in clinical practice. It\\nis the ﬁrst imaging modality enabling us to non-invasively\\nsee through a human body and diagnose changes of internal\\nanatomies. However, all tissues are projected onto a 2D im-\\nage, overlaying each other. While bones are clearly visible,\\nsoft tissues are often difﬁcult to discern. Computed tomog-\\nraphy (CT) is an imaging modality that reconstructs a 3D\\nvolume from a set of X-rays (usually, at least 100 images)\\ncaptured in a full rotation of the X-ray apparatus around\\nthe body. One prominent advantage of CT is that tissues\\n∗Equally-contributed.\\n�����\\n���������\\n�����\\n�����\\n������\\n��\\nFigure 1. Illustration of the proposed method. The network takes\\n2D biplanar X-rays as input and outputs a 3D CT volume.\\nare presented in the 3D space, which completely solves the\\noverlaying issue. However, a CT scan incurs far more radi-\\nation dose to a patient (depending on the number of X-rays\\nacquired for CT reconstruction). Moreover, a CT scanner\\nis often much more cost prohibitive than an X-ray machine,\\nmaking its less accessible in developing countries [37].\\nWith hundreds of X-ray projections, standard recon-\\nstruction algorithms, e.g., ﬁltered back projection or itera-\\ntive reconstruction, can accurately reconstruct a CT volume\\n[14]. However, the data acquisition requires a fast rotation\\nof the X-ray apparatus around the patient, which cannot be\\nperformed on a typical X-ray machine. In this work, we\\npropose to reconstruct a CT volume from biplanar X-rays\\nthat are captured from two orthogonal view planes. The\\nmajor challenge is that the X-ray image suffers from severe\\nambiguity of internal body information, where numbers of\\nCT volumes can exactly match the same input X-rays once\\nprojected onto 2D. It seems to be unsolvable if we look for\\ngeneral solutions with traditional CT reconstruction algo-\\nrithms. However, human body anatomy is well constrained\\nand we may be able to learn the mapping from X-rays to\\nCT from a large training set through machine learning tech-\\nnology, especially deep learning (DL) methods. Recently,\\nthe generative adversarial network (GAN) [11] has been\\n10611\\n2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\n978-1-7281-3293-8/19/$31.00 ©2019 IEEE\\nDOI 10.1109/CVPR.2019.01087\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='used for cross-modality image transfer in medical imag-\\ning [3, 5, 30, 39] and has demonstrated the effectiveness.\\nHowever, the previous works only deal with the input and\\noutput data having the same dimension. Here we propose\\nX2CT-GAN that can reconstruct CT from biplanar X-rays,\\nsurpassing the data limitations of different modalities and\\ndimensionality (Fig. 1).\\nThe purpose of this work is not to replace CT with X-\\nrays. Though the proposed method can reconstruct the gen-\\neral structure accurately, small anatomies still suffer from\\nsome artifacts. However, the proposed method may ﬁnd\\nsome niche applications in clinical practice. For example,\\nwe can measure the size of major organs (e.g., lungs, heart,\\nand liver) accurately, or diagnose ill-positioned organs on\\nthe reconstructed CT scan. It may also be used for dose\\nplanning in radiation therapy, or pre-operative planning and\\nintra-operative guidance in minimally invasive intervention.\\nIt could be a nice enhancement of a low-cost X-ray machine\\nas physicians may also get a CT-like 3D volume that has\\ncertain clinical values.\\nThough the proposed network can also be used to recon-\\nstruct CT from a single X-ray, we argue that using bipla-\\nnar X-rays is a more practical solution. First, CT recon-\\nstruction from a single X-ray subjects to too much ambigu-\\nity while biplanar X-rays offer additional information from\\nboth views that is complementary to each other. More accu-\\nrate results, 4 dB improvement in peak signal-to-noise ratio\\n(PSNR), are achieved in our comparison experiment. Sec-\\nond, biplanar X-ray machines are already clinically avail-\\nable, which can capture two orthogonal X-ray images si-\\nmultaneously. And, it is also clinically practicable to cap-\\nture two orthogonal X-rays with a mono-planar machine,\\nby rotating the X-ray apparatus to a new orientation for the\\nsecond X-ray imaging.\\nOne practical issue to train X2CT-GAN is lacking of\\npaired X-ray and CT 1. It is expensive to collect such paired\\ndata from patients and it is also unethical to subject patients\\nto additional radiation doses. In this work, we train the net-\\nwork with synthesized X-rays generated from large public-\\navailable chest CT datasets [1]. Given a CT volume, we\\nsimulate two X-rays, one from the posterior-anterior (PA)\\nview and the other from the lateral view, using the digi-\\ntally reconstructed radiographs (DRR) technology [28]. Al-\\nthough DRR synthesized X-rays are quite photo-realistic,\\nthere still exits a gap between real and synthesized X-rays,\\nespecially in ﬁner anatomy structures, e.g., blood vessels.\\nTherefore we further resort CycleGAN [41] to learn the\\ngenuine X-ray style that can be transferred to the synthe-\\nsized data. More information about the style transfer oper-\\nation can be found in supplement materials.\\n1Sometimes X-rays are captured as topogram before a CT scan. How-\\never, without the calibrated back-projection matrix, we cannot perfectly\\nalign the two modalities.\\n���������������� ������������\\n�������������\\n����������������\\n�\\n���������\\n��\\n��\\n��\\n����������\\nFigure 2. Overview of the X2CT-GAN model. RL and PL are\\nabbreviations of the reconstruction loss and projection loss.\\nTo summarize, we make the following contributions:\\n• We are the ﬁrst to explore CT reconstruction from bi-\\nplanar X-rays with deep learning. To fully utilize the\\ninput information from two different views, a novel\\nfeature fusion method is proposed.\\n• We propose X2CT-GAN, as illustrated in Fig. 2, to\\nincrease the data dimension from input to output (i.e.,\\n2D X-ray to 3D CT), which is not addressed in previ-\\nous research on GAN.\\n• We propose a novel skip connection module that could\\nbridge 2D and 3D feature maps more naturally.\\n• We use synthesized X-rays to learn the mapping from\\n2D to 3D, and CycleGAN to transfer real X-rays to\\nthe synthesized style before feeding into the network.\\nTherefore, although our network is trained with syn-\\nthesized X-rays, it can still reconstruct CT from real\\nX-rays.\\n• Compared to other reconstruction algorithms using\\nvisible light [7, 9, 18], our X-ray based approach can\\nreconstruct both surface and internal structures.\\n2. Related Work\\nCross-Modality Transfer A DL based model often suf-\\nfers from lacking enough training data so as to fall into a\\nsuboptimal point during training or even overﬁt the small\\ndataset. To alleviate this problem, synthetic data has been\\nused to boost the training process [33, 39]. So synthesiz-\\ning realistic images close to the target distribution is a critic\\npremise. Previous research such as pix2pix [17] could do\\nthe pixel level image to image transfer and CycleGAN [41]\\nhas the ability to learn the mapping between two unpaired\\ndatasets. In medical imaging community, quite some efforts\\nhave been put into this area to transfer a source modality to\\na target modality, e.g., 3T MRI to 7T MRI [3], MRI to CT\\n[5, 30], MRI and CT bidirectional transfer [39] etc. Our\\napproach differs from the previous cross-modality transfer\\nworks in two ways. First, in all the above works, the di-\\nmensions of the input and output are consistent, e.g., 2D to\\n10612\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='�������\\n�����\\n�����\\n�����\\n�����\\n�����\\n�������\\n��\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�����\\n�����\\n�����\\n�����\\n�����\\n�������\\n��\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�\\n�������\\n�������\\n�������\\n�������\\n�������\\n��\\n������������\\n������������\\n������������\\n������������\\n������������\\n����\\n�����������\\n��������\\n�����\\n�\\n��\\n����\\n����������\\n����\\n�\\n��\\n����\\n�����������\\n��������\\n�\\n�������\\n��\\n�������\\n��\\n�\\n������\\n��\\n����\\n�������\\n�\\n��������\\n��\\n����\\n��\\n�\\n������\\n��\\n����\\n�������\\n�\\n������������\\n�\\nFigure 3. Network architecture of the X2CT-GAN generator. Two encoder-decoder networks with the same architecture work in parallel for\\nposterior-anterior (PA) and lateral X-rays, respectively. Another fusion network between these two encoder-decoder networks is responsible\\nfor fusing information coming from two views. For more details about Connection-A, B and C, please refer to Fig. 4.\\n2D or 3D to 3D. Here, we want to transfer 2D X-rays to a\\n3D volume. To handle this challenge, we propose X2CT-\\nGAN, which incorporates two mechanisms to increase the\\ndata dimension. Second, our goal is to reconstruct accurate\\n3D anatomy from biplanar X-rays with clinical values in-\\nstead of enriching the training set. A photo-realistic image\\n(e.g., one generated from pure noise input [11]) may already\\nbe beneﬁcial for training. However, our application further\\nrequires the image to be anatomically accurate as well.\\n3D Model Extraction from 2D Projections 3D model\\nextraction from 2D projections is a well studied topic in\\ncomputer vision [7, 9, 18]. Since most objects are opaque\\nto light, only the outer surface model can be reconstructed.\\nX-ray can penetrate most objects (except thick metal) and\\ndifferent structures overlay on a 2D image. Therefore, the\\nmethods used in 3D model extraction from X-rays are quite\\ndifferent to those used in the computer vision community.\\nEarly in 1990, Caponetti and Fanelli reconstructed a bone\\nmodel from two X-rays based on back-lighting projections,\\npolygon mesh and B-spline interpolation [6].\\nIn recent\\nyears, several works have investigated the reconstruction of\\nbones, rib cages and lungs through statistical shape models\\nor other prior knowledge [8, 2, 19, 24, 21, 23, 22]. Different\\nto ours, these methods could not generate a 3D CT-like im-\\nage. Furthermore, although they may be able to get precise\\nmodels, if we generalize these to reconstruct other organs,\\nan elaborate geometric model has to be prepared in advance,\\nwhich limits their application scenarios.\\nCT Reconstruction from X-ray Classical CT recon-\\nstruction algorithms, e.g., ﬁltered back projection and it-\\nerative reconstruction methods [14], require hundreds of\\nX-rays captured during a full rotational scan of the body.\\nMethods based on deep learning have also been used to im-\\nprove the performance in recent works [38, 12]. The input\\nof [38] is an X-ray sinogram, while ours are human readable\\nbiplanar X-rays. And, [12] mainly deals with the limited-\\nangle CT compensation problem. More relevant to our work\\nis [13], which uses a convolutional neural network (CNN) to\\npredict the underlying 3D object as a volume from a single-\\nimage tomography. However, we argue that a single X-ray\\nis not enough to accurately reconstruct 3D anatomy since\\nit is subject to too much ambiguity. For example, we can\\nstretch or ﬂip an object along the projection direction with-\\nout changing the projected image. As shown in our experi-\\nments, biplanar X-rays with two orthogonal projections can\\nsigniﬁcantly improve the reconstruction accuracy, beneﬁt-\\ning from more constraints provided by an additional view.\\nFurthermore, the images reconstructed by [13] are quite\\nblurry, thus with limited clinical values. Combining adver-\\nsarial training and reconstruction constraints, our method\\n10613\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='could extract much ﬁner anatomical structures (e.g. blood\\nvessels inside lungs), which signiﬁcantly improves the vi-\\nsual quality.\\n3. Objective Functions of X2CT-GAN\\nGAN [11] is a recent proposal to effectively train a gen-\\nerative model that has demonstrated the ability to capture\\nreal data distribution. Conditional GAN [29], as an exten-\\nsion of the original GAN, further improves the data gener-\\nation process by conditioning the generative model on ad-\\nditional inputs, which could be class labels, partial data, or\\neven data from a different modality. Inspired by the suc-\\ncesses of conditional GANs, we propose a novel solution\\nto train a generative model that can reconstruct a 3D CT\\nvolume from biplanar 2D X-rays. In this section, we ﬁrst\\nintroduce several loss functions that are used to constrain\\nthe generative model.\\n3.1. Adversarial Loss\\nThe original intention of GAN is to learn deep genera-\\ntive models while avoiding approximating many intractable\\nprobabilistic computations that arise in other strategies, i.e.,\\nmaximum likelihood estimation. The learning procedure is\\na two-player game, where a discriminator D and a genera-\\ntor G would compete with each other. The ultimate goal is\\nto learn a generator distribution pG(x) that matches the real\\ndata distribution pdata(x). An ideal generator could gener-\\nate samples that are indistinguishable from the real samples\\nby the discriminator. More formally, the minmax game is\\nsummarized by the following expression:\\nmin\\nG max\\nD V (G, D) =Ex∼pdata[log D(x)]+\\nEz∼noise[log (1 − D(G(z)))],\\n(1)\\nwhere z is sampled from a noise distribution.\\nAs we want to learn a non-linear mapping from X-rays to\\nCT, the generated CT volume should be consistent with the\\nsemantic information provided by the input X-rays. After\\ntrying different mutants of the conditional GAN, we ﬁnd out\\nthat LSGAN [27] is more suitable for our task and apply it\\nto guide the training process. The conditional LSGAN loss\\nis deﬁned as:\\nLLSGAN(D) = 1\\n2[Ey∼p(CT )(D(y|x) − 1)2+\\nEx∼p(Xray)(D(G(x)|x) − 0)2],\\nLLSGAN(G) = 1\\n2[Ex∼p(Xray)(D(G(x)|x) − 1)2],\\n(2)\\nwhere x is composed of two orthogonal biplanar X-\\nrays, and y is the corresponding CT volume. Compared\\nto the original objective function deﬁned in Eq. (1), LS-\\nGAN replaces the logarithmic loss with a least-square loss,\\nwhich helps to stabilize the adversarial training process and\\nachieve more realistic details.\\n3.2. Reconstruction Loss\\nThe conditional adversarial loss tries to make prediction\\nlook real. However, it does not guarantee that G can gener-\\nate a sample maintaining the structural consistency with the\\ninput. Moreover, CT scans, different from natural images\\nthat have more diversity in color and shape, require higher\\nprecision of internal structures in 3D. Consequently, an ad-\\nditional constraint is required to enforce the reconstructed\\nCT to be voxel-wise close to the ground truth. Some pre-\\nvious work has combined the reconstruction loss [32] with\\nthe adversarial loss and got positive improvements. We also\\nfollow this strategy and acquire a high PSNR as shown in\\nTable 1. Our reconstruction loss is deﬁned as MSE:\\nLre = Ex,y∥y − G(x)∥2\\n2.\\n(3)\\n3.3. Projection Loss\\nThe aforementioned reconstruction loss is a voxel-wise\\nloss that enforces the structural consistency in the 3D space.\\nTo improve the training efﬁciency, more simple shape pri-\\nors could be utilized as auxiliary regularizations. Inspired\\nby [18], we impel 2D projections of the predicted volume to\\nmatch the ones from corresponding ground-truth in differ-\\nent views. Orthogonal projections, instead of perspective\\nprojections, are carried out to simplify the process as this\\nauxiliary loss focuses only on the general shape consistency,\\nnot the X-ray veracity. We choose three orthogonal projec-\\ntion planes (axial, coronal, and sagittal, as shown in Fig.\\n2, following the convention in the medical imaging com-\\nmunity). Finally, the proposed projection loss is deﬁned as\\nbelow:\\nLpl = 1\\n3[Ex,y∥Pax(y) − Pax(G(x))∥1+\\nEx,y∥Pco(y) − Pco(G(x))∥1+\\nEx,y∥psa(y) − Psa(G(x))∥1],\\n(4)\\nwhere the Pax, Pco and Psa represent the projection in the\\naxial, coronal, and sagittal plane, respectively. The L1 dis-\\ntance is used to enforce sharper image boundaries.\\n3.4. Total Objective\\nGiven the deﬁnitions of the adversarial loss, reconstruc-\\ntion loss, and projection loss, our ﬁnal objective function is\\nformulated as:\\nD∗ = arg min\\nD λ1LLSGAN(D),\\nG∗ = arg min\\nG [λ1LLSGAN(G) + λ2Lre + λ3Lpl],\\n(5)\\nwhere λ1, λ2 and λ3 control the relative importance of\\ndifferent loss terms.\\nIn our X-ray to CT reconstruction\\ntask, the adversarial loss plays an important role of encour-\\naging local realism of the synthesized output, but global\\n10614\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='shape consistency should be prioritized during the opti-\\nmization process. Taking this into consideration, we set\\nλ1 = 0.1, λ2 = λ3 = 10 in our experiments.\\n4. Network Architecture of X2CT-GAN\\nIn this section, we introduce our proposed network de-\\nsigns that are used in the 3D CT reconstruction task from\\n2D biplanar X-rays. Similar to other 3D GAN architectures,\\nour method involves a 3D generator and a 3D discriminator.\\nThese two models are alternatively trained with the super-\\nvision deﬁned in previous section.\\n4.1. Generator\\nThe proposed 3D generator, as illustrated in Fig. 3, con-\\nsists of three individual components: two encoder-decoder\\nnetworks with the same architecture working in parallel for\\nposterior-anterior (PA) and lateral X-rays respectively, and a\\nfusion network. The encoder-decoder network aims to learn\\nthe mapping from the input 2D X-ray to the target 3D CT in\\nthe feature space, and the fusion network is responsible for\\nreconstructing the 3D CT volume with the fused biplanar\\ninformation from the two encoder-decoder networks. Since\\nthe training process in our reconstruction task involves cir-\\nculating information between input and output from two\\ndifferent modalities and dimensionalities, several modiﬁca-\\ntions of the network architecture are made to adapt to the\\nchallenge.\\nDensely Connected Encoder Dense connectivity [15]\\nhas a compelling advantage in the feature extraction pro-\\ncess. To optimally utilize information from 2D X-ray im-\\nages, we embed dense modules to generator’s encoding\\npath.\\nAs shown in Fig.\\n3, each dense module consists\\nof a down-sampling block (2D convolution with stride=2),\\na densely connected convolution block and a compressing\\nblock (output channels halved). The cascaded dense mod-\\nules encode different level information of the input image\\nand pass it to the decoder along different shortcut paths.\\nBridging 2D Encoder and 3D Decoder Some existing\\nencoder-decoder networks [17, 25] link encoder and de-\\ncoder by means of convolution. There is no obstacle in a\\npure 2D or 3D encode-decode process, but our special 2D\\nto 3D mapping procedure requires a new design to bridge\\nthe information from two dimensionalities. Motivated by\\n[40], we extend fully connected layer to a new connec-\\ntion module, named Connection-A (Fig. 4a), to bridge the\\n2D encoder and 3D decoder in the middle of our genera-\\ntor. To better utilize skip connections in the 2D-3D gen-\\nerator, we design another novel connection module, named\\nConnection-B (Fig. 4b), to shuttle low-level features from\\nencoder to decoder.\\nTo be more speciﬁc, Connection-A achieves the 2D-3D\\nconversion through fully-connected layers, where the last\\nencoder layer’s output is ﬂattened and elongated to a 1D\\n�\\n�������\\n��\\n����\\n�������\\n�\\n�\\n�\\n�\\n�������\\n�������\\n�������\\n�\\n�������\\n������\\n����������������\\n����������������\\n����������������\\n�������\\n�������\\nFigure 4. Different types of connections.\\nConnection-A and\\nConnection-B aim to increase dimensionality of feature maps, and\\nConnection-C is for fusing information from two different views.\\nvector that is further reshaped to 3D. However, most of the\\n2D spatial information gets lost during such conversion so\\nthat we only use Connection-A to link the last encoder layer\\nand ﬁrst decoder layer. For the rest of skip connections, we\\nuse Connection-B and take following steps: 1) enforce the\\nchannel number of the encoder being equal to the one on\\nthe corresponding decoder side by a basic 2D convolution\\nblock; 2) expand the 2D feature map to a pseudo-3D one\\nby duplicating the 2D information along the third axis; 3)\\nuse a basic 3D convolution block to encode the pseudo-3D\\nfeature map. The abundant low-level information shuttled\\nacross two parts of the network imposes strong correlations\\non the shape and appearance between input and output.\\nFeature Fusion of Biplanar X-rays As a common\\nsense, a 2D photograph from frontal view could not retain\\nlateral information of the object and vice versa. In our task,\\nwe resort biplanar X-rays captured from two orthogonal di-\\nrections, where the complementary information could help\\nthe generative model achieve more accurate results. Two\\nencoder-decoder networks in parallel extract features from\\neach view while the third decoder network is set to fuse the\\nextracted information and output the reconstructed volume.\\nAs we assume the biplanar X-rays are captured within a\\nnegligible time interval, meaning no data shift caused by pa-\\ntient motions, we can directly average the extracted features\\nafter transforming them into the same coordinate space, as\\nshown in Fig. 4c. Any structural inconsistency between\\ntwo decoders’ outputs will be captured by the fusion net-\\nwork and back-propagated to two networks.\\n4.2. Discriminator\\nPatchGANs have been used frequently in recent works\\n[26, 17, 25, 41, 35] due to the good generalization prop-\\nerty. We adopt a similar architecture in our discriminator\\nnetwork from Phillip et al. [17], named as 3DPatchDis-\\n10615\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='criminator.\\nIt consists of three conv3d − norm − relu\\nmodules with stride = 2 and kernelsize = 4, another\\nconv3d − norm − relu module with stride = 1 and\\nkernelsize = 4, and a ﬁnal conv3d layer. Here, conv3d\\ndenotes a 3D convolution layer; norm stands for an in-\\nstance normalization layer [34]; and relu represents a rec-\\ntiﬁed linear unit [10]. The proposed discriminator architec-\\nture improves the discriminative capacity inherited from the\\nPatchGAN framework and can distinguish real or fake 3D\\nvolumes.\\n4.3. Training and Inference Details\\nThe generator and discriminator are trained alternatively\\nfollowing the standard process [11].\\nWe use the Adam\\nsolver [20] to train our networks. The initial learning rate\\nof Adam is 2e-4, momentum parameters β1 = 0.5 and\\nβ2 = 0.99. After training 50 epochs, we adopt a linear\\nlearning rate decay policy to decrease the learning rate to 0.\\nWe train our model for a total of 100 epochs.\\nAs instance normalization [34] has been demonstrated to\\nbe superior to batch normalization [16] in image generation\\ntasks, we use instance normalization to regularize interme-\\ndiate feature maps of our generator. At inference time, we\\nobserve that better generating results could be obtained if\\nwe use the statistics of the test batch itself instead of the\\nrunning average of training batches, as suggested in [17].\\nConstrained by GPU memory limit, the batch size is set to\\none in all our experiments.\\n5. Experiments\\nIn this section, we introduce an augmented dataset built\\non LIDC-IDRI [1]. We evaluate the proposed X2CT-GAN\\nmodel with several widely used metrics, e.g., peak signal-\\nto-noise ratio (PSNR) and structural similarity (SSIM) in-\\ndex. To demonstrate the effectiveness of our method, we\\nreproduce a baseline model named 2DCNN [13]. Fair com-\\nparisons and comprehensive analysis are given to demon-\\nstrate the improvement of our proposed method over the\\nbaseline and other mutants. Finally, we show the real-world\\nX-ray evaluation results of X2CT-GAN. Input images to\\nX2CT-GAN are resized to 128×128 pixels, while the input\\nof 2DCNN is 256 × 256 pixels as suggested by [13]. The\\noutput of all models is set to 128 × 128 × 128 voxels.\\n5.1. Datasets\\nCT and X-ray Paired Dataset Ideally, to train and val-\\nidate the proposed CT reconstruction approach, we need a\\nlarge dataset with paired X-rays and corresponding CT re-\\nconstructions. Furthermore, the X-ray machine needs to be\\ncalibrated to get an accurate projection matrix. However, no\\nsuch dataset is available and it is very costly to collect such\\nreal paired dataset. Therefore, we take a real CT volume\\n���\\n���\\n���\\n���\\nFigure 5. DRR [28] simulated X-rays. (a) and (c) are simulated\\nPA view X-rays of two subjects, (b) and (d) are the corresponding\\nlateral views.\\nand use the digitally reconstructed radiographs (DRR) tech-\\nnology [28] to synthesize corresponding X-rays, as shown\\nin Fig. 5. It is much cheaper to collect such synthesized\\ndatasets to train our networks. To be speciﬁc, we use the\\npublicly available LIDC-IDRI dataset [1], which contains\\n1,018 chest CT scans. The heterogeneous of imaging pro-\\ntocols result in different capture ranges and resolutions. For\\nexample, the number of slices varies a lot for different vol-\\numes. The resolution inside a slice is isotropic but also\\nvaries for different volumes. All these factors lead to a non-\\ntrivial reconstruction task from 2D X-rays. To simplify, we\\nﬁrst resample the CT scans to the 1 × 1 × 1 mm3 resolu-\\ntion. Then, a 320 × 320 × 320 mm3 cubic area is cropped\\nfrom each CT scan. We randomly select 916 CT scans for\\ntraining and the rest 102 CT scans are used for testing.\\nMapping from Real to Synthetic X-rays Although\\nDRR synthetic X-rays are quite photo-realistic, there is still\\na gap between the real and synthetic X-rays, especially\\nfor those subtle anatomical structures, e.g., blood vessels.\\nSince our networks are trained with synthesized X-rays, a\\nsub-optimal result will be obtained if we directly feed a real\\nX-ray into the network. We propose to perform style trans-\\nfer to map real X-rays to the synthesized style. Without\\npaired dataset of real and synthesized X-rays, we exploit\\nCycleGAN [41] to learn the mapping. We collected 200 real\\nX-rays and randomly selected 200 synthetic X-rays from\\nthe training set of the paired LIDC dataset.\\n5.2. Metrics\\nPSNR is often used to measure the quality of recon-\\nstructed digital signals [31]. Conventionally, CT value is\\nrecorded with 12 bits, representing a range of [0, 4095]\\n(the actual Hounsﬁeld unit equals the CT value minus 1024)\\n[4], which makes PSNR an ideal criterion for image quality\\nevaluation.\\nSSIM is a metric to measure the similarity of two\\nimages, including brightness, contrast and structure [36].\\nCompared to PSNR, SSIM can match human’s subjective\\nevaluation better.\\n5.3. Qualitative Results\\nWe ﬁrst qualitatively evaluate CT reconstruction results\\nshown in Fig. 6, where X2CT-CNN is the proposed network\\n10616\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='GT \\n2DCNN \\nX2CT-CNN+S \\nX2CT-CNN+B \\nX2CT-GAN+S \\nX2CT-GAN+B \\nFigure 6. Reconstructed CT scans from different approaches. 2DCNN is our reproduced baseline model [13]; X2CT-CNN is our generator\\nnetwork optimized by the MSE loss alone and X2CT-GAN is our GAN-based model optimized by total objective. ‘+S’ means single-view\\nX-ray input and ‘+B’ means biplanar X-rays input. The ﬁrst row demonstrates axial slices generated by different models. The last two\\nrows are 3D renderings of generated CT scans in the PA and lateral view, respectively.\\nsupervised only by the reconstruction loss while X2CT-\\nGAN is the one trained with full objectives; ‘+S’ means\\nsingle-view X-ray input and ‘+B’ means biplanar X-rays\\ninput. For comparison, we also reproduce the method pro-\\nposed in [13] (referred as 2DCNN in Fig. 6) as the base-\\nline, one of very few published works tackling the X-ray\\nto CT reconstruction problem using deep learning. Since\\n2DCNN is designed to deal with single X-ray input, no bi-\\nplanar results are shown. From the visual quality evalua-\\ntion, it is obvious to see the differences. First of all, 2DCNN\\nand X2CT-CNN generate very blurry volumes while X2CT-\\nGAN maintains small anatomical structures.\\nSecondly,\\nthough missing reconstruction details, X2CT-CNN+S gen-\\nerates sharper boundaries of large organs (e.g., heart, lungs\\nand chest wall) than 2DCNN. Last but not least, models\\ntrained with biplanar X-rays outperform the ones trained\\nwith single view X-ray. More reconstructed CT slices could\\nbe found in Fig. 8.\\n5.4. Quantitative Results\\nQuantitative results are summarized in Table 1. Biplanar\\ninputs signiﬁcantly improve the reconstruction accuracy,\\nabout 4 dB improvement for both X2CT-CNN and X2CT-\\nGAN, compared to single X-ray input. It is well known\\nthat the GAN models often sacriﬁce MSE-based metrics to\\nachieve visually better results. This phenomenon is also ob-\\nserved here. However, by tuning the relative weights of the\\nvoxel-level MSE loss and semantic-level adversarial loss\\nTable 1. Quantitative results. 2DCNN is our reproduced model\\nfrom [13]; X2CT-CNN is our generator network optimized by the\\nMSE loss alone; and X2CT-GAN is our GAN-based model opti-\\nmized by total objective. ‘+S’ means single-view X-ray input and\\n‘+B’ means biplanar X-rays input.\\nMethods\\nPSNR (dB)\\nSSIM\\n2DCNN\\n23.10(±0.21)\\n0.461(±0.005)\\nX2CT-CNN+S\\n23.12(±0.02)\\n0.587(±0.001)\\nX2CT-CNN+B\\n27.29(±0.04)\\n0.721(±0.001)\\nX2CT-GAN+S\\n22.30(±0.10)\\n0.525(±0.004)\\nX2CT-GAN+B\\n26.19(±0.13)\\n0.656(±0.008)\\nis our cost function, we can make a reasonable trade-off.\\nFor example, there is only 1.1 dB decrease in PSNR from\\nX2CT-CNN+B to X2CT-GAN+B, while the visual image\\nquality is dramatically improved as shown in Fig. 6. We ar-\\ngue that visual image quality is as important as (if not more\\nimportant than) PSNR in CT reconstruction since eventu-\\nally the images will be read visually by a physician.\\n5.5. Ablation Study\\nAnalysis of Proposed Connection Modules To validate\\nthe effectiveness of proposed connection modules, we also\\nperform an ablation study in the setting of X2CT-CNN.\\nAs shown in Table 2, single view input with Connection-\\nB achieves 0.7 dB improvement in PSNR. The biplanar\\n10617\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='Table 2. Evaluation of different connection modules. ‘XC’ de-\\nnotes X2CT-CNN model without the proposed Connection-B and\\nConnection-C module. ‘+S’ means the model’s input is a single-\\nview X-ray and ‘+B’ means biplanar X-rays. ‘CB’ and ‘CC’ de-\\nnote Connection-B and Connection-C respectively as shown in\\nFig. 4.\\nCombination\\nMetrics\\nXC+S\\nXC+B\\nCB\\nCC\\nPSNR(dB)\\nSSIM\\n✓\\n22.46(±0.02)\\n0.549(±0.002)\\n✓\\n✓\\n23.12(±0.02)\\n0.587(±0.001)\\n✓\\n24.84(±0.05)\\n0.620(±0.003)\\n✓\\n✓\\n✓\\n27.29(±0.04)\\n0.721(±0.001)\\nTable 3. Evaluation of different settings in the GAN framework.\\n‘RL’ and ‘PL’ denote the reconstruction and projection loss, re-\\nspectively. ‘CD’ means that input X-ray information is fed to the\\ndiscriminator to achieve a conditional GAN.\\nFormulation\\nMetrics\\nGAN\\nRL\\nPL\\nCD\\nPSNR(dB)\\nSSIM\\n✓\\n17.38(±0.36)\\n0.347(±0.022)\\n✓\\n✓\\n25.82(±0.08)\\n0.645(±0.001)\\n✓\\n✓\\n✓\\n26.05(±0.02)\\n0.645(±0.002)\\n✓\\n✓\\n✓\\n✓\\n26.19(±0.13)\\n0.656(±0.008)\\ninput, even without skip connections, surpasses the single\\nview due to the complementary information injected to the\\nnetwork. And in our biplanar model, Connection-B and\\nConnection-C are interdependent so that we regard them\\nas one module. As can be seen, the biplanar model with\\nthis module surpasses other combinations by a large margin\\nboth in PSNR and SSIM.\\nDifferent Settings in GAN Framework The effects of\\ndifferent settings in the GAN framework are summarized in\\nTable 3. As the ﬁrst row shows, adversarial loss alone per-\\nforms poorly on PSNR and SSIM due to the lack of strong\\nconstraints. The most signiﬁcant improvement comes from\\nthe reconstruction loss being added to the GAN framework.\\nProjection loss and the conditional information bring addi-\\ntional improvement slightly.\\n5.6. Real-World Data Evaluation\\nSince the ultimate goal is to reconstruct a CT scan from\\nreal X-rays, we ﬁnally evaluate our model on real-world\\ndata, despite the model is trained on synthetic data. As we\\nhave no corresponding 3D CT volumes for real X-rays, only\\nqualitative evaluation is conducted. Visual results are pre-\\nsented in Fig. 7, we could see that the reconstructed lung\\nand surface structures are quite plausible.\\nFigure 7. CT reconstruction from real-world X-rays. Two subjects\\nare shown here. The ﬁrst and second columns are real X-rays in\\ntwo views. The following two columns are transformed X-rays\\nby CycleGAN [41]. The last two columns show 3D renderings\\nof reconstructed internal structures and surfaces. Dotted ellipses\\nhighlight regions of high-quality anatomical reconstruction.\\n���\\n���\\nFigure 8. Examples of reconstructed CT slices (a) and correspond-\\ning groundtruth (b). As could be seen, our method reconstructs the\\nshape and appearance of major anatomical structures accurately.\\n6. Conclusions\\nIn this paper, we explored the possibility of reconstruct-\\ning a 3D CT scan from biplanar 2D X-rays in an end-to-\\nend fashion. To solve this challenging task, we combined\\nthe reconstruction loss, the projection loss and the adver-\\nsarial loss in the GAN framework. Moreover, a specially\\ndesigned generator network is exploited to increase the data\\ndimension from 2D to 3D. Our experiments qualitatively\\nand quantitatively demonstrate that biplanar X-rays are su-\\nperior to single view X-ray in the 3D reconstruction process.\\nFor future work, we will collaborate physicians to evaluate\\nthe clinical value of the reconstructed CT scans, including\\nmeasuring the size of major organs and dose planning in\\nradiation therapy, etc.\\n10618\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-\\nGray, C. R. Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I.\\nHenschke, E. A. Hoffman, et al. The lung image database\\nconsortium (LIDC) and image database resource initiative\\n(IDRI): a completed reference database of lung nodules on\\nCT scans. MP, 38(2):915–931, 2011.\\n[2] B. Aubert, C. Vergari, B. Ilharreborde, A. Courvoisier, and\\nW. Skalli. 3D reconstruction of rib cage geometry from bi-\\nplanar radiographs using a statistical parametric model ap-\\nproach. Computer Methods in Biomechanics and Biomedical\\nEngineering: Imaging & Visualization, 4(5):281–295, 2016.\\n[3] K. Bahrami, F. Shi, I. Rekik, and D. Shen. Convolutional\\nneural network for reconstruction of 7T-like images from\\n3T MRI using appearance and anatomical features. In Deep\\nLearning and Data Labeling for Medical Applications, pages\\n39–47. Springer, 2016.\\n[4] I. Bankman.\\nHandbook of medical image processing and\\nanalysis. Elsevier, 2008.\\n[5] N. Burgos, M. J. Cardoso, F. Guerreiro, C. Veiga, M. Modat,\\nJ. McClelland, A.-C. Knopf, S. Punwani, D. Atkinson, S. R.\\nArridge, et al. Robust CT synthesis for radiotherapy plan-\\nning: application to the head and neck region. In Proc. Int’l\\nConf. Medical Image Computing and Computer Assisted In-\\ntervention, pages 476–484, 2015.\\n[6] L. Caponetti and A. Fanelli. 3D bone reconstruction from\\ntwo X-ray views. In Proc. Annual Int. Conf. the IEEE En-\\ngineering in Medicine and Biology Society, pages 208–210,\\n1990.\\n[7] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-\\nR2N2: A uniﬁed approach for single and multi-view 3D ob-\\nject reconstruction. In Proc. European Conf. Computer Vi-\\nsion, pages 628–644, 2016.\\n[8] J. Dworzak, H. Lamecker, J. von Berg, T. Klinder, C. Lorenz,\\nD. Kainm¨uller, H. Seim, H.-C. Hege, and S. Zachow. 3D\\nreconstruction of the human rib cage from 2D projection im-\\nages using a statistical shape model. International Journal\\nof Computer Assisted Radiology and Surgery, 5(2):111–124,\\n2010.\\n[9] H. Fan, H. Su, and L. J. Guibas. A point set generation net-\\nwork for 3D object reconstruction from a single image. In\\nProc. IEEE Conf. Computer Vision and Pattern Recognition,\\npages 2463–2471, 2017.\\n[10] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer\\nneural networks. In Proceedings Int’l Conf. Artiﬁcial Intelli-\\ngence and Statistics, pages 315–323, 2011.\\n[11] I. Goodfellow,\\nJ. Pouget-Abadie,\\nM. Mirza,\\nB. Xu,\\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\\nerative adversarial nets. In Advances in Neural Information\\nProcessing Systems, pages 2672–2680, 2014.\\n[12] K. Hammernik, T. W¨urﬂ, T. Pock, and A. Maier. A deep\\nlearning architecture for limited-angle computed tomogra-\\nphy reconstruction.\\nIn Bildverarbeitung f¨ur die Medizin\\n2017, pages 92–97. Springer, 2017.\\n[13] P. Henzler, V. Rasche, T. Ropinski, and T. Ritschel. Single-\\nimage tomography: 3D volumes from 2D X-rays.\\narXiv\\npreprint arXiv:1710.04867, 2017.\\n[14] G. T. Herman. Fundamentals of computerized tomography:\\nImage reconstruction from projection. Springer-Verlag Lon-\\ndon, 2009.\\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger.\\nDensely connected convolutional networks. In Proc. IEEE\\nConf. Computer Vision and Pattern Recognition, volume 1,\\npage 3, 2017.\\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\narXiv preprint arXiv:1502.03167, 2015.\\n[17] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image\\ntranslation with conditional adversarial networks. In Proc.\\nIEEE Conf. Computer Vision and Pattern Recognition, pages\\n5967–5976, 2017.\\n[18] L. Jiang, S. Shi, X. Qi, and J. Jia. GAL: Geometric adver-\\nsarial loss for single-view 3D-object reconstruction. In Proc.\\nEuropean Conf. Computer Vision, pages 820–834, 2018.\\n[19] V. Karade and B. Ravi. 3D femur model reconstruction from\\nbiplane X-ray images: a novel method based on Laplacian\\nsurface deformation. International Journal of Computer As-\\nsisted Radiology and Surgery, 10(4):473–485, 2015.\\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic\\noptimization. arXiv preprint arXiv:1412.6980, 2014.\\n[21] C. Koehler and T. Wischgoll. 3-D reconstruction of the hu-\\nman ribcage based on chest X-ray images and template mod-\\nels. IEEE MultiMedia, 17(3):46–53, 2010.\\n[22] C. Koehler and T. Wischgoll.\\nKnowledge-assisted recon-\\nstruction of the human rib cage and lungs. IEEE Computer\\nGraphics and Applications, 30(1):17–29, 2010.\\n[23] C. Koehler and T. Wischgoll. 3D reconstruction of human\\nribcage and lungs and improved visualization of lung X-ray\\nimages through removal of the ribcage. Dagstuhl Follow-\\nUps, 2:176–187, 2011.\\n[24] H. Lamecker, T. H. Wenckebach, and H.-C. Hege. Atlas-\\nbased 3D-shape reconstruction from X-ray images. In Proc.\\nInt’l Conf. Pattern Recognition, volume 1, pages 371–374,\\n2006.\\n[25] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,\\nA. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.\\nPhoto-realistic single image super-resolution using a gener-\\native adversarial network. In Proc. IEEE Conf. Computer\\nVision and Pattern Recognition, pages 4681–4690, 2017.\\n[26] C. Li and M. Wand. Precomputed real-time texture synthesis\\nwith Markovian generative adversarial networks. In Proc.\\nEuropean Conf. Computer Vision, pages 702–716, 2016.\\n[27] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley.\\nLeast squares generative adversarial networks. In Proc. Int’l\\nConf. Computer Vision, pages 2813–2821. IEEE, 2017.\\n[28] N. Milickovic, D. Baltas, S. Giannouli, M. Lahanas, and\\nN. Zamboglou.\\nCT imaging based digitally reconstructed\\nradiographs and their application in brachytherapy. Physics\\nin Medicine & Biology, 45(10):2787–7800, 2000.\\n[29] M. Mirza and S. Osindero. Conditional generative adversar-\\nial nets. arXiv preprint arXiv:1411.1784, 2014.\\n[30] D. Nie, R. Trullo, J. Lian, C. Petitjean, S. Ruan, Q. Wang,\\nand D. Shen. Medical image synthesis with context-aware\\n10619\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='generative adversarial networks. In Proc. Int’l Conf. Med-\\nical Image Computing and Computer Assisted Intervention,\\npages 417–425, 2017.\\n[31] E. Oriani. qpsnr: A quick PSNR/SSIM analyzer for Linux.\\nhttp://qpsnr.youlink.org.\\nAccessed: 2018-11-\\n12.\\n[32] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.\\nEfros. Context encoders: Feature learning by inpainting. In\\nProc. IEEE Conf. Computer Vision and Pattern Recognition,\\npages 2536–2544, 2016.\\n[33] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,\\nand R. Webb. Learning from simulated and unsupervised im-\\nages through adversarial training. In Proc. IEEE Conf. Com-\\nputer Vision and Pattern Recognition, pages 2242–2251,\\n2017.\\n[34] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky. Instance nor-\\nmalization: The missing ingredient for fast stylization. arXiv\\npreprint arXiv:1607.08022, 2016.\\n[35] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and\\nB. Catanzaro. High-resolution image synthesis and seman-\\ntic manipulation with conditional GANs.\\narXiv preprint\\narXiv:1711.11585, 2017.\\n[36] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.\\nImage quality assessment: from error visibility to structural\\nsimilarity. IEEE Trans. Image Processing, 13(4):600–612,\\n2004.\\n[37] World Health Organization.\\nBaseline country survey on\\nmedical devices 2010, 2011.\\n[38] T. W¨urﬂ, F. C. Ghesu, V. Christlein, and A. Maier. Deep\\nlearning computed tomography. In Proc. Int’l Conf. Medi-\\ncal Image Computing and Computer Assisted Intervention,\\npages 432–440, 2016.\\n[39] Z. Zhang, L. Yang, and Y. Zheng.\\nTranslating and seg-\\nmenting multimodal medical volumes with cycle- and shape-\\nconsistency generative adversarial network. In Proc. IEEE\\nConf. Computer Vision and Pattern Recognition, pages\\n9242–9251, 2018.\\n[40] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen.\\nImage reconstruction by domain-transform manifold learn-\\ning. Nature, 555(7697):487–492, 2018.\\n[41] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-\\nto-image translation using cycle-consistent adversarial net-\\nworks. In Proc. IEEE Conf. Computer Vision and Pattern\\nRecognition, pages 2242–2251, 2017.\\n10620\\nAuthorized licensed use limited to: Korea University. Downloaded on August 14,2023 at 15:24:38 UTC from IEEE Xplore.  Restrictions apply. \\n', metadata={'source': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'file_path': 'data/Ying et al. - 2019 - X2CT-GAN Reconstructing CT From Biplanar X-Rays W.pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.6', 'title': 'X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks', 'author': '', 'subject': '2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR);2019; ; ;10.1109/CVPR.2019.01087', 'keywords': '', 'creator': '', 'producer': 'OpenPDF 1.0.0-SNAPSHOT; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creationDate': 'D:20190613171942Z', 'modDate': \"D:20200108125040-05'00'\", 'trapped': ''}),\n",
       " Document(page_content='pixelNeRF: Neural Radiance Fields from One or Few Images\\nAlex Yu\\nVickie Ye\\nMatthew Tancik\\nAngjoo Kanazawa\\nUC Berkeley\\nInput: 3 views of held-out scene\\nNeRF\\npixelNeRF\\nOutput: Rendered new views\\nInput\\nNovel views\\nInput\\nNovel views\\nInput\\nNovel views\\nFigure 1: NeRF from one or few images. We present pixelNeRF, a learning framework that predicts a Neural Radiance Field (NeRF)\\nrepresentation from a single (top) or few posed images (bottom). PixelNeRF can be trained on a set of multi-view images, allowing it to\\ngenerate plausible novel view synthesis from very few input images without test-time optimization (bottom left). In contrast, NeRF has no\\ngeneralization capabilities and performs poorly when only three input views are available (bottom right).\\nAbstract\\nWe propose pixelNeRF, a learning framework that pre-\\ndicts a continuous neural scene representation conditioned\\non one or few input images.\\nThe existing approach for\\nconstructing neural radiance ﬁelds [27] involves optimiz-\\ning the representation to every scene independently, requir-\\ning many calibrated views and signiﬁcant compute time.\\nWe take a step towards resolving these shortcomings by in-\\ntroducing an architecture that conditions a NeRF on im-\\nage inputs in a fully convolutional manner.\\nThis allows\\nthe network to be trained across multiple scenes to learn\\na scene prior, enabling it to perform novel view synthesis in\\na feed-forward manner from a sparse set of views (as few as\\none). Leveraging the volume rendering approach of NeRF,\\nour model can be trained directly from images with no ex-\\nplicit 3D supervision. We conduct extensive experiments\\non ShapeNet benchmarks for single image novel view syn-\\nthesis tasks with held-out objects as well as entire unseen\\ncategories. We further demonstrate the ﬂexibility of pixel-\\nNeRF by demonstrating it on multi-object ShapeNet scenes\\nand real scenes from the DTU dataset. In all cases, pix-\\nelNeRF outperforms current state-of-the-art baselines for\\nnovel view synthesis and single image 3D reconstruction.\\nFor the video and code, please visit the project website:\\nhttps://alexyu.net/pixelnerf.\\n1. Introduction\\nWe study the problem of synthesizing novel views of a\\nscene from a sparse set of input views. This long-standing\\nproblem has recently seen progress due to advances in dif-\\nferentiable neural rendering [27, 20, 24, 39]. Across these\\napproaches, a 3D scene is represented with a neural net-\\nwork, which can then be rendered into 2D views. Notably,\\nthe recent method neural radiance ﬁelds (NeRF) [27] has\\nshown impressive performance on novel view synthesis of\\na speciﬁc scene by implicitly encoding volumetric density\\nand color through a neural network. While NeRF can ren-\\nder photorealistic novel views, it is often impractical as it\\nrequires a large number of posed images and a lengthy per-\\nscene optimization.\\nIn this paper, we address these shortcomings by propos-\\ning pixelNeRF, a learning framework that enables predict-\\ning NeRFs from one or several images in a feed-forward\\nmanner. Unlike the original NeRF network, which does not\\nmake use of any image features, pixelNeRF takes spatial\\nimage features aligned to each pixel as an input. This im-\\nage conditioning allows the framework to be trained on a\\nset of multi-view images, where it can learn scene priors\\nto perform view synthesis from one or few input views. In\\ncontrast, NeRF is unable to generalize and performs poorly\\nwhen few input images are available, as shown in Fig. 1.\\n4578\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Speciﬁcally, we condition NeRF on input images by ﬁrst\\ncomputing a fully convolutional image feature grid from the\\ninput image. Then for each query spatial point x and view-\\ning direction d of interest in the view coordinate frame, we\\nsample the corresponding image feature via projection and\\nbilinear interpolation. The query speciﬁcation is sent along\\nwith the image features to the NeRF network that outputs\\ndensity and color, where the spatial image features are fed\\nto each layer as a residual. When more than one image is\\navailable, the inputs are ﬁrst encoded into a latent represen-\\ntation in each camera’s coordinate frame, which are then\\npooled in an intermediate layer prior to predicting the color\\nand density. The model is supervised with a reconstruction\\nloss between a ground truth image and a view rendered us-\\ning conventional volume rendering techniques. This frame-\\nwork is illustrated in Fig. 2.\\nPixelNeRF has many desirable properties for few-view\\nnovel-view synthesis. First, pixelNeRF can be trained on a\\ndataset of multi-view images without additional supervision\\nsuch as ground truth 3D shape or object masks. Second,\\npixelNeRF predicts a NeRF representation in the camera\\ncoordinate system of the input image instead of a canoni-\\ncal coordinate frame. This is not only integral for general-\\nization to unseen scenes and object categories [41, 37], but\\nalso for ﬂexibility, since no clear canonical coordinate sys-\\ntem exists on scenes with multiple objects or real scenes.\\nThird, it is fully convolutional, allowing it to preserve the\\nspatial alignment between the image and the output 3D rep-\\nresentation. Lastly, pixelNeRF can incorporate a variable\\nnumber of posed input views at test time without requiring\\nany test-time optimization.\\nWe conduct an extensive series of experiments on syn-\\nthetic and real image datasets to evaluate the efﬁcacy of our\\nframework, going beyond the usual set of ShapeNet experi-\\nments to demonstrate its ﬂexibility. Our experiments show\\nthat pixelNeRF can generate novel views from a single im-\\nage input for both category-speciﬁc and category-agnostic\\nsettings, even in the case of unseen object categories. Fur-\\nther, we test the ﬂexibility of our framework, both with a\\nnew multi-object benchmark for ShapeNet, where pixel-\\nNeRF outperforms prior approaches, and with simulation-\\nto-real transfer demonstration on real car images. Lastly,\\nwe test capabilities of pixelNeRF on real images using the\\nDTU dataset [14], where despite being trained on under 100\\nscenes, it can generate plausible novel views of a real scene\\nfrom three posed input views.\\n2. Related Work\\nNovel View Synthesis. The long-standing problem of novel\\nview synthesis entails constructing new views of a scene\\nfrom a set of input views. Early work achieved photore-\\nalistic results but required densely captured views of the\\nscene [19, 11]. Recent work has made rapid progress to-\\nNeRF DISN ONet\\nDVR SRN Ours\\nLearns scene prior?\\n✗\\n✓\\n✓\\n✓\\n✓\\n✓\\nSupervision\\n2D\\n3D\\n3D\\n2D\\n2D\\n2D\\nImage features\\n✗\\nLocal Global Global\\n✗\\nLocal\\nAllows multi-view?\\n✓\\n✓\\n✗\\n✗\\n✓\\n✓\\nView space?\\n-\\n✗\\n✗\\n✗\\n✗\\n✓\\nTable 1: A comparison with prior works reconstructing neu-\\nral scene representations. The proposed approach learns a scene\\nprior for one or few-view reconstruction using only multi-view 2D\\nimage supervision. Unlike previous methods in this regime, we\\ndo not require a consistent canonical space across the training cor-\\npus. Moreover, we incorporate local image features to preserve\\nlocal information which is in contrast to methods that compress\\nthe structure and appearance into a single latent vector such as Oc-\\ncupancy Networks (ONet) [25] and DVR [28].\\nward photorealism for both wider ranges of novel views\\nand sparser sets of input views, by using 3D representations\\nbased on neural networks [27, 23, 26, 38, 42, 7]. However,\\nbecause these approaches ﬁt a single model to each scene,\\nthey require many input views and substantial optimization\\ntime per scene.\\nThere are methods that can predict novel view from few\\ninput views or even single images by learning shared priors\\nacross scenes. Methods in the tradition of [35, 3] use depth-\\nguided image interpolation [54, 10, 32]. More recently, the\\nproblem of predicting novel views from a single image has\\nbeen explored [44, 47, 36, 5]. However, these methods em-\\nploy 2.5D representations, and are therefore limited in the\\nrange of camera motions they can synthesize. In this work\\nwe infer a 3D volumetric NeRF representation, which al-\\nlows novel view synthesis from larger baselines.\\nSitzmann et al. [39] introduces a representation based on\\na continuous 3D feature space to learn a prior across scene\\ninstances.\\nHowever, using the learned prior at test time\\nrequires further optimization with known absolute camera\\nposes. In contrast, our approach is completely feed-forward\\nand only requires relative camera poses. We offer exten-\\nsive comparisons with this approach to demonstrate the ad-\\nvantages our design affords. Lastly, note that concurrent\\nwork [43] adds image features to NeRF. A key difference is\\nthat we operate in view rather than canonical space, which\\nmakes our approach applicable in more general settings.\\nMoreover, we extensively demonstrate our method’s perfor-\\nmance in few-shot view synthesis, while GRF shows very\\nlimited quantitative results for this task.\\nLearning-based 3D reconstruction.\\nAdvances in deep\\nlearning have led to rapid progress in single-view or multi-\\nview 3D reconstruction. Many approaches [15, 12, 46, 53,\\n38, 33, 49, 25, 31] propose learning frameworks with vari-\\nous 3D representations that require ground-truth 3D models\\nfor supervision. Multi-view supervision [50, 45, 21, 22, 39,\\n4579\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='28, 8, 2] is less restrictive and more ecologically plausible.\\nHowever, many of these methods [50, 45, 21, 22, 28] re-\\nquire object masks; in contrast, pixelNeRF can be trained\\nfrom images alone, allowing it to be applied to scenes of\\ntwo objects without modiﬁcation.\\nMost single-view 3D reconstruction methods condition\\nneural 3D representations on input images. The majority\\nemploys global image features [29, 6, 28, 25, 8], which,\\nwhile memory efﬁcient, cannot preserve details that are\\npresent in the image and often lead to retrieval-like results.\\nSpatially-aligned local image features have been shown to\\nachieve detailed reconstructions from a single view [49, 33].\\nHowever, both of these methods require 3D supervision.\\nOur method is inspired by these approaches, but only re-\\nquires multi-view supervision.\\nWithin existing methods, the types of scenes that can\\nbe reconstructed are limited, particularly so for object-\\ncentric approaches (e.g. [46, 21, 12, 45, 38, 53, 25, 49, 28]).\\nCoReNet [31] reconstructs scenes with multiple objects via\\na voxel grid with offsets, but it requires 3D supervision in-\\ncluding the identity and placement of objects. In compari-\\nson, we formulate a scene-level learning framework that can\\nin principle be trained to scenes of arbitrary structure.\\nViewer-centric 3D reconstruction For the 3D learning\\ntask, prediction can be done either in a viewer-centered co-\\nordinate system, i.e. view space, or in an object-centered co-\\nordinate system, i.e. canonical space. Most existing meth-\\nods [49, 25, 28, 39] predict in canonical space, where all ob-\\njects of a semantic category are aligned to a consistent ori-\\nentation. While this makes learning spatial regularities eas-\\nier, using a canonical space inhibits prediction performance\\non unseen object categories and scenes with more than one\\nobject, where there is no pre-deﬁned or well-deﬁned canon-\\nical pose. PixelNeRF operates in view-space, which has\\nbeen shown to allow better reconstruction of unseen object\\ncategories in [37, 2], and discourages the memorization of\\nthe training set [41]. We summarize key aspects of our ap-\\nproach relative to prior work in Table 1.\\n3. Background: NeRF\\nWe ﬁrst brieﬂy review the NeRF representation [27]. A\\nNeRF encodes a scene as a continuous volumetric radiance\\nﬁeld f of color and density. Speciﬁcally, for a 3D point\\nx ∈ R3 and viewing direction unit vector d ∈ R3, f returns\\na differential density σ and RGB color c: f(x, d) = (σ, c).\\nThe volumetric radiance ﬁeld can then be rendered into\\na 2D image via\\nˆC(r) =\\n� tf\\ntn\\nT(t)σ(t)c(t)dt\\n(1)\\nwhere T(t) = exp\\n�\\n−\\n� t\\ntn σ(s) ds\\n�\\nhandles occlusion. For\\na target view with pose P, a camera ray can be parameter-\\nized as r(t) = o + td, with the ray origin (camera center)\\no ∈ R3 and ray unit direction vector d ∈ R3. The inte-\\ngral is computed along r between pre-deﬁned depth bounds\\n[tn, tf]. In practice, this integral is approximated with nu-\\nmerical quadrature by sampling points along each pixel ray.\\nThe rendered pixel value for camera ray r can then\\nbe compared against the corresponding ground truth pixel\\nvalue, C(r), for all the camera rays of the target view with\\npose P. The NeRF rendering loss is thus given by\\nL =\\n�\\nr∈R(P)\\n��� ˆC(r) − C(r)\\n���\\n2\\n2\\n(2)\\nwhere R(P) is the set of all camera rays of target pose P.\\nLimitations While NeRF achieves state of the art novel\\nview synthesis results, it is an optimization-based approach\\nusing geometric consistency as the sole signal, similar to\\nclassical multiview stereo methods [1, 34]. As such each\\nscene must be optimized individually, with no knowledge\\nshared between scenes. Not only is this time-consuming,\\nbut in the limit of single or extremely sparse views, it is un-\\nable to make use of any prior knowledge of the world to\\naccelerate reconstruction or for shape completion.\\n4. Image-conditioned NeRF\\nTo overcome the NeRF representation’s inability to share\\nknowledge between scenes, we propose an architecture to\\ncondition a NeRF on spatial image features. Our model\\nis comprised of two components: a fully-convolutional im-\\nage encoder E, which encodes the input image into a pixel-\\naligned feature grid, and a NeRF network f which outputs\\ncolor and density, given a spatial location and its corre-\\nsponding encoded feature. We choose to model the spa-\\ntial query in the input view’s camera space, rather than a\\ncanonical space, for the reasons discussed in § 2. We vali-\\ndate this design choice in our experiments on unseen object\\ncategories (§ 5.2) and complex unseen scenes (§ 5.3). The\\nmodel is trained with the volume rendering method and loss\\ndescribed in § 3.\\nIn the following, we ﬁrst present our model for the single\\nview case. We then show how this formulation can be easily\\nextended to incorporate multiple input images.\\n4.1. Single-Image pixelNeRF\\nWe now describe our approach to render novel views\\nfrom one input image. We ﬁx our coordinate system as\\nthe view space of the input image and specify positions and\\ncamera rays in this coordinate system.\\nGiven a input image I of a scene, we ﬁrst extract a feature\\nvolume W = E(I). Then, for a point on a camera ray x,\\nwe retrieve the corresponding image feature by projecting\\nx onto the image plane to the image coordinates π(x) using\\n4580\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='f\\n(RGBσ)\\nW\\nd\\nW(πx)\\n(x,d)\\nTarget View\\nσ\\ng.t.\\n2\\n2\\nRay Distance\\nVolume Rendering\\nRendering Loss\\nCNN Encoder\\nx\\ny\\nz\\nInput View\\nFigure 2: Proposed architecture in the single-view case. For a query point x along a target camera ray with view direction d, a\\ncorresponding image feature is extracted from the feature volume W via projection and interpolation. This feature is then passed into the\\nNeRF network f along with the spatial coordinates. The output RGB and density value is volume-rendered and compared with the target\\npixel value. The coordinates x and d are in the camera coordinate system of the input view.\\nknown intrinsics, then bilinearly interpolating between the\\npixelwise features to extract the feature vector W(π(x)).\\nThe image features are then passed into the NeRF network,\\nalong with the position and view direction (both in the input\\nview coordinate system), as\\nf(γ(x), d; W(π(x))) = (σ, c)\\n(3)\\nwhere γ(·) is a positional encoding on x with 6 expo-\\nnentially increasing frequencies introduced in the original\\nNeRF [27]. The image feature is incorporated as a residual\\nat each layer; see § 5 for more information. We show our\\npipeline schematically in Fig. 2.\\nIn the few-shot view synthesis task, the query view direc-\\ntion is a useful signal for determining the importance of a\\nparticular image feature in the NeRF network. If the query\\nview direction is similar to the input view orientation, the\\nmodel can rely more directly on the input; if it is dissimilar,\\nthe model must leverage the learned prior. Moreover, in the\\nmulti-view case, view directions could serve as a signal for\\nthe relevance and positioning of different views. For this\\nreason, we input the view directions at the beginning of the\\nNeRF network.\\n4.2. Incorporating Multiple Views\\nMultiple views provide additional information about the\\nscene and resolve 3D geometric ambiguities inherent to the\\nsingle-view case. We extend our model to allow for an arbi-\\ntrary number of views at test time, which distinguishes our\\nmethod from existing approaches that are designed to only\\nuse single input view at test time. [8, 53] Moreover, our for-\\nmulation is independent of the choice of world space and\\nthe order of input views.\\nIn the case that we have multiple input views of the\\nscene, we assume only that the relative camera poses are\\nknown.\\nFor purposes of explanation, an arbitrary world\\ncoordinate system can be ﬁxed for the scene.\\nWe de-\\nnote the ith input image as I(i) and its associated cam-\\nera transform from the world space to its view space as\\nP(i) =\\n�\\nR(i)\\nt(i)�\\n.\\nFor a new target camera ray, we transform a query point\\nx, with view direction d, into the coordinate system of each\\ninput view i with the world to camera transform as\\nx(i) = P(i)x,\\nd(i) = R(i)d\\n(4)\\nTo obtain the output density and color, we process the coor-\\ndinates and corresponding features in each view coordinate\\nframe independently and aggregate across the views within\\nthe NeRF network. For ease of explanation, we denote the\\ninitial layers of the NeRF network as f1, which process in-\\nputs in each input view space separately, and the ﬁnal layers\\nas f2, which process the aggregated views.\\nWe encode each input image into feature volume\\nW(i) = E(I(i)). For the view-space point x(i), we extract\\nthe corresponding image feature from the feature volume\\nW(i) at the projected image coordinate π(x(i)). We then\\npass these inputs into f1 to obtain intermediate vectors:\\nV(i) = f1\\n�\\nγ(x(i)), d(i); W(i)�\\nπ(x(i))\\n��\\n.\\n(5)\\nThe intermediate V(i) are then aggregated with the aver-\\nage pooling operator ψ and passed into a the ﬁnal layers,\\ndenoted as f2, to obtain the predicted density and color:\\n(σ, c) = f2\\n�\\nψ\\n�\\nV(1), . . . , V(n)��\\n.\\n(6)\\nIn the single-view special case, this simpliﬁes to Equation 3\\nwith f = f2◦f1, by considering the view space as the world\\nspace. An illustration is provided in the supplemental.\\n5. Experiments\\nWe extensively demonstrate our approach in three exper-\\nimental categories: 1) existing ShapeNet [4] benchmarks\\nfor category-speciﬁc and category-agnostic view synthesis,\\n2) ShapeNet scenes with unseen categories and multiple\\nobjects, both of which require geometric priors instead of\\n4581\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='recognition, as well as domain transfer to real car photos\\nand 3) real scenes from the DTU MVS dataset [14].\\nBaselines For ShapeNet benchmarks, we compare quan-\\ntitatively and qualitatively to SRN [39] and DVR [28],\\nthe current state-of-the-art in few-shot novel-view synthe-\\nsis and 2D-supervised single-view reconstruction respec-\\ntively. We use the 2D multiview-supervised variant of DVR.\\nIn the category-agnostic setting (§ 5.1.2), we also include\\ngrayscale rendering of SoftRas [21] results. 1 In the exper-\\niments with multiple ShapeNet objects, we compare with\\nSRN, which can also model entire scenes.\\nFor the experiment on the DTU dataset, we compare to\\nNeRF [27] trained on sparse views. Because NeRF is a\\ntest-time optimization method, we train a separate model\\nfor each scene in the test set.\\nMetrics We report the standard image quality metrics\\nPSNR and SSIM [55] for all evaluations.\\nWe also in-\\nclude LPIPS [52], which more accurately reﬂects human\\nperception, in all evaluations except in the category-speciﬁc\\nsetup (§ 5.1.1). In this setting, we exactly follow the pro-\\ntocol of SRN [39] to remain comparable to prior works\\n[40, 48, 9, 8, 43], for which source code is unavailable.\\nImplementation Details For the image encoder E, to cap-\\nture both local and global information effectively, we ex-\\ntract a feature pyramid from the image. We use a ResNet34\\nbackbone pretrained on ImageNet for our experiments. Fea-\\ntures are extracted prior to the ﬁrst 4 pooling layers, upsam-\\npled using bilinear interpolation, and concatenated to form\\nlatent vectors of size 512 aligned to each pixel.\\nTo incorporate a point’s corresponding image feature\\ninto the NeRF network f, we choose a ResNet architec-\\nture with a residual modulation rather than simply concate-\\nnating the feature vector with the point’s position and view\\ndirection. Speciﬁcally, we feed the encoded position and\\nview direction through the network and add the image fea-\\nture as a residual at the beginning of each ResNet block. We\\ntrain an independent linear layer for each block residual, in\\na similar manner as AdaIn and SPADE [13, 30], a method\\npreviously used with success in [25, 28]. Please refer to the\\nsupplemental for additional details.\\n5.1. ShapeNet Benchmarks\\nWe ﬁrst evaluate our approach on category-speciﬁc and\\ncategory-agnostic view synthesis tasks on ShapeNet.\\n5.1.1\\nCategory-speciﬁc View Synthesis Benchmark\\nWe perform one-shot and two-shot view synthesis on the\\n“chair” and “car” classes of ShapeNet, using the protocol\\nand dataset introduced in [39]. The dataset contains 6591\\n1Color inference is not supported by the public SoftRas code.\\nInput\\nSRN\\nOurs\\nGT\\nInput\\nSRN\\nOurs\\nGT\\nFigure 3: Category-speciﬁc single-view reconstruction bench-\\nmark. We train a separate model for cars and chairs and compare\\nto SRN. The corresponding numbers may be found in Table 2.\\n2 Input Views\\nSRN\\nOurs\\nGT\\nFigure 4: Category-speciﬁc 2-view reconstruction benchmark.\\nWe provide two views (left) to each model, and show two novel\\nview renderings in each case (right). Please also refer to Table 2.\\n1-view\\n2-view\\nPSNR\\nSSIM\\nPSNR\\nSSIM\\nChairs\\nGRF [43]\\n21.25\\n0.86\\n22.65\\n0.88\\nTCO [40] *\\n21.27\\n0.88\\n21.33\\n0.88\\ndGQN [9]\\n21.59\\n0.87\\n22.36\\n0.89\\nENR [8] *\\n22.83\\n-\\n-\\n-\\nSRN [39]\\n22.89\\n0.89\\n24.48\\n0.92\\nOurs *\\n23.72\\n0.91\\n26.20\\n0.94\\nCars\\nSRN [39]\\n22.25\\n0.89\\n24.84\\n0.92\\nENR [8] *\\n22.26\\n-\\n-\\n-\\nOurs *\\n23.17\\n0.90\\n25.66\\n0.94\\nTable 2: Category-speciﬁc 1- and 2-view reconstruction. Meth-\\nods marked * do not require canonical poses at test time. In all\\ncases, a single model is trained for each category and used for\\nboth 1- and 2-view evaluation. Note ENR is a 1-view only model.\\n1-view\\n2-view\\n↑ PSNR ↑ SSIM ↓ LPIPS ↑ PSNR ↑ SSIM ↓ LPIPS\\n− Local 20.39\\n0.848\\n0.196\\n21.17\\n0.865\\n0.175\\n− Dirs\\n21.93\\n0.885\\n0.139\\n23.50\\n0.909\\n0.121\\nFull\\n23.43\\n0.911\\n0.104\\n25.95\\n0.939\\n0.071\\nTable 3: Ablation studies for ShapeNet chair reconstruction.\\nWe show the beneﬁt of using local features over a global code to\\ncondition the NeRF network (−Local vs Full), and of providing\\nview directions to the network (−Dirs vs Full).\\n4582\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Input\\nSoftRas\\nDVR\\nSRN\\nOurs\\nGT\\nInput\\nSoftRas\\nDVR\\nSRN\\nOurs\\nGT\\nInput\\nSoftRas\\nDVR\\nSRN\\nOurs\\nGT\\nFigure 5: Category-agnostic single-view reconstruction. Going beyond the SRN benchmark, we train a single model to the 13 largest\\nShapeNet categories; we ﬁnd that our approach produces superior visual results compared to a series of strong baselines. In particular,\\nthe model recovers ﬁne detail and thin structure more effectively, even for outlier shapes. Quite visibly, images on monitors and tabletop\\ntextures are accurately reproduced; baselines representing the scene as a single latent vector cannot preserve such details of the input image.\\nSRN’s test-time latent inversion becomes less reliable as well in this setting. The corresponding quantitative evaluations are available in\\nTable 4. Due to space constraints, we show objects with interesting properties here. Please see the supplemental for sampled results.\\nplane\\nbench\\ncbnt.\\ncar\\nchair\\ndisp.\\nlamp\\nspkr.\\nriﬂe\\nsofa\\ntable\\nphone\\nboat\\nmean\\n↑ PSNR\\nDVR\\n25.29\\n22.64\\n24.47\\n23.95\\n19.91\\n20.86\\n23.27\\n20.78\\n23.44\\n23.35\\n21.53\\n24.18\\n25.09\\n22.70\\nSRN\\n26.62\\n22.20\\n23.42\\n24.40\\n21.85\\n19.07\\n22.17\\n21.04\\n24.95\\n23.65\\n22.45\\n20.87\\n25.86\\n23.28\\nOurs\\n29.76\\n26.35\\n27.72\\n27.58\\n23.84\\n24.22\\n28.58\\n24.44\\n30.60\\n26.94\\n25.59\\n27.13\\n29.18\\n26.80\\n↑ SSIM\\nDVR\\n0.905\\n0.866\\n0.877\\n0.909\\n0.787\\n0.814\\n0.849\\n0.798\\n0.916\\n0.868\\n0.840\\n0.892\\n0.902\\n0.860\\nSRN\\n0.901\\n0.837\\n0.831\\n0.897\\n0.814\\n0.744\\n0.801\\n0.779\\n0.913\\n0.851\\n0.828\\n0.811\\n0.898\\n0.849\\nOurs\\n0.947\\n0.911\\n0.910\\n0.942\\n0.858\\n0.867\\n0.913\\n0.855\\n0.968\\n0.908\\n0.898\\n0.922\\n0.939\\n0.910\\n↓ LPIPS\\nDVR\\n0.095\\n0.129\\n0.125\\n0.098\\n0.173\\n0.150\\n0.172\\n0.170\\n0.094\\n0.119\\n0.139\\n0.110\\n0.116\\n0.130\\nSRN\\n0.111\\n0.150\\n0.147\\n0.115\\n0.152\\n0.197\\n0.210\\n0.178\\n0.111\\n0.129\\n0.135\\n0.165\\n0.134\\n0.139\\nOurs\\n0.084\\n0.116\\n0.105\\n0.095\\n0.146\\n0.129\\n0.114\\n0.141\\n0.066\\n0.116\\n0.098\\n0.097\\n0.111\\n0.108\\nTable 4: Category-agnostic single-view reconstruction. Quantitative results for category-agnostic view-synthesis are presented, with a\\ndetailed breakdown by category. Our method outperforms the state-of-the-art by signiﬁcant margins in all categories.\\nchairs and 3514 cars with a predeﬁned split across object\\ninstances. All images have resolution 128 × 128.\\nA single model is trained for each object class with 50\\nrandom views per object instance, randomly sampling ei-\\nther one or two of the training views to encode. For testing,\\nWe use 251 novel views on an Archimedean spiral for each\\nobject in the test set of object instances, ﬁxing 1-2 infor-\\nmative views as input. We report our performance in com-\\nparison with state-of-the-art baselines in Table 2, and show\\nselected qualitative results in Fig. 4. We also include the\\nquantitative results of baselines TCO [40] and dGQN [9]\\nreported in [39] where applicable, and the values available\\nin the recent works ENR [8] and GRF [43] in this setting.\\nPixelNeRF achieves noticeably superior results despite\\nsolving a problem signiﬁcantly harder than SRN because\\nwe: 1) use feed-forward prediction, without test-time opti-\\nmization, 2) do not use ground-truth absolute camera poses\\nat test-time, 3) use view instead of canonical space.\\nAblations. In Table 3, we show the beneﬁt of using local\\nfeatures and view directions in our model for this category-\\nspeciﬁc setting. Conditioning the NeRF network on pixel-\\naligned local features instead of a global code (−Local vs\\nFull) improves performance signiﬁcantly, for both single\\nand two-view settings. Providing view directions (−Dirs vs\\nFull) also provides a signiﬁcant boost. For these ablations,\\nwe follow an abbreviated evaluation protocol on ShapeNet\\nchairs, using 25 novel views on the Archimedean spiral.\\n5.1.2\\nCategory-agnostic Object Prior\\nWhile we found appreciable improvements over baselines\\nin the simplest category-speciﬁc benchmark, our method is\\nby no means constrained to it. We show in Table 4 and\\nFig. 5 that our approach offers a much greater advantage in\\nthe category-agnostic setting of [21, 28], where we train\\na single model to the 13 largest categories of ShapeNet.\\nPlease see the supplemental for randomly sampled results.\\nWe follow community standards for 2D-supervised\\nmethods on multiple ShapeNet categories [28, 16, 21] and\\nuse the renderings and splits from Kato et al. [16], which\\nprovide 24 ﬁxed elevation views of 64 × 64 resolution for\\neach object instance. During both training and evaluation,\\na random view is selected as the input view for each object\\nand shared across all baselines. The remaining 23 views are\\nused as target views for computing metrics (see § 5).\\n5.2. Pushing the Boundaries of ShapeNet\\nTaking a step towards reconstruction in less controlled\\ncapture scenarios, we perform experiments on ShapeNet\\ndata in three more challenging setups: 1) unseen object cat-\\negories, 2) multiple-object scenes, and 3) simulation-to-real\\n4583\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Input\\nDVR\\nSRN\\nOurs\\nGT\\nInput\\nDVR\\nSRN\\nOurs\\nGT\\nFigure 6: Generalization to unseen categories. We evaluate a\\nmodel trained on planes, cars, and chairs on 10 unseen ShapeNet\\ncategories. We ﬁnd that the model is able to synthesize reasonable\\nviews even in this difﬁcult case.\\nInputs\\nSRN\\nOurs\\nGT\\nFigure 7: 360◦view prediction with multiple objects. We show\\nqualitative results of our method compared with SRN on scenes\\ncomposed of multiple ShapeNet chairs.\\nWe are easily able to\\nhandle this setting, because our prediction is done in view space;\\nin contrast, SRN predicts in canonical space, and struggles with\\nscenes that cannot be aligned in such a way.\\nUnseen category\\nMultiple chairs\\n↑ PSNR ↑ SSIM ↓ LPIPS ↑ PSNR ↑ SSIM ↓ LPIPS\\nDVR\\n17.72\\n0.716\\n0.240\\n-\\n-\\n-\\nSRN\\n18.71\\n0.684\\n0.280\\n14.67\\n0.664\\n0.431\\nOurs\\n22.71\\n0.825\\n0.182\\n23.40\\n0.832\\n0.207\\nTable 5: Image quality metrics for challenging ShapeNet tasks.\\n(Left) Average metrics on 10 unseen categories for models trained\\non only planes, cars, and chairs. See the supplemental for a break-\\ndown by category. (Right) Average metrics for two-view recon-\\nstruction for scenes with multiple ShapeNet chairs.\\nInput\\nNovel views\\nFigure 8: Results on real car photos. We apply the car model\\nfrom § 5.1.1 directly to images from the Stanford cars dataset [18].\\nThe background has been masked out using PointRend [17]. The\\nviews are rotations about the view-space vertical axis.\\ntransfer on car images. In these settings, successful recon-\\nstruction requires geometric priors; recognition or retrieval\\nalone is not sufﬁcient.\\nGeneralization to novel categories. We ﬁrst aim to recon-\\nstruct ShapeNet categories which were not seen in training.\\nUnlike the more standard category-agnostic task described\\nin the previous section, such generalization is impossible\\nwith semantic information alone. The results in Table 5 and\\nFig. 6 suggest our method learns intrinsic geometric and\\nappearance priors which are fairly effective even for objects\\nquite distinct from those seen during training.\\nWe loosely follow the protocol used for zero-shot cross-\\ncategory reconstruction from [53, ?]. Note that our base-\\nlines [39, 28] do not evaluate in this setting, and we adapt\\nthem for the sake of comparison. We train on the airplane,\\ncar, and chair categories and test on 10 categories unseen\\nduring training, continuing to use the Kato et al. renderings\\ndescribed in § 5.1.2.\\nMultiple-object scenes. We further perform few-shot 360◦\\nreconstruction for scenes with multiple randomly placed\\nand oriented ShapeNet chairs. In this setting, the network\\ncannot rely solely on semantic cues for correct object place-\\nment and completion. The priors learned by the network\\nmust be applicable in an arbitrary coordinate system. We\\nshow in Fig. 7 and Table 5 that our formulation allows us\\nto perform well on these simple scenes without additional\\ndesign modiﬁcations. In contrast, SRN models scenes in a\\ncanonical space and struggles on held-out scenes.\\nWe generate training images composed with 20 views\\nrandomly sampled on the hemisphere and render test im-\\nages composed of a held out test set of chair instances, with\\n50 views sampled on an Archimedean spiral. During train-\\ning, we randomly encode two input views; at test-time, we\\nﬁx two informative views across the compared methods.\\nIn the supplemental, we provide example images from our\\ndataset as well as additional quantitative results and quali-\\ntative comparisons with varying numbers of input views.\\nSim2Real on Cars.\\nWe also explore the performance\\nof pixelNeRF on real images from the Stanford cars\\ndataset [18]. We directly apply car model from § 5.1.1 with-\\nout any ﬁne-tuning. As seen in Fig. 8, the network trained\\non synthetic data effectively infers shape and texture of the\\nreal cars, suggesting our model can transfer beyond the syn-\\nthetic domain.\\nSynthesizing the 360◦ background from a single view\\nis nontrivial and out of the scope for this work. For this\\ndemonstration, the off-the-shelf PointRend [17] segmenta-\\ntion model is used to remove the background.\\n5.3. Scene Prior on Real Images\\nFinally, we demonstrate that our method is applicable for\\nfew-shot wide baseline novel-view synthesis on real scenes\\nin the DTU MVS dataset [14]. Learning a prior for view\\nsynthesis on this dataset poses signiﬁcant challenges: not\\nonly does it consist of more complex scenes, without clear\\nsemantic similarities across scenes, it also contains incon-\\nsistent backgrounds and lighting between scenes. More-\\n4584\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='Input: 3 views of held-out scene\\nNovel views\\nNeRF\\nFigure 9: Wide baseline novel-view synthesis on a real image dataset. We train our model to distinct scenes in the DTU MVS\\ndataset [14]. Perhaps surprisingly, even in this case, our model is able to infer novel views with reasonable quality for held-out scenes\\nwithout further test-time optimization, all from only three views. Note the train/test sets share no overlapping scenes.\\nFigure 10: PSNR of few-shot feed-forward DTU reconstruc-\\ntion. We show the quantiles of PSNR on DTU for our method and\\nNeRF, given 1, 3, 6, or 9 input views. Separate NeRFs are trained\\nper scene and number of input views, while our method requires\\nonly a single model trained with 3 encoded views.\\nover, under 100 scenes are available for training. We found\\nthat the standard data split introduced in MVSNet [51] con-\\ntains overlap between scenes of the training and test sets.\\nTherefore, for our purposes, we use a different split of 88\\ntraining scenes and 15 test scenes, in which there are no\\nshared or highly similar scenes between the two sets. Im-\\nages are down-sampled to a resolution of 400 × 300.\\nWe train one model across all training scenes by en-\\ncoding 3 random views of a scene. During test time, we\\nchoose a set of ﬁxed informative input views shared across\\nall instances. We show in Fig. 9 that our method can per-\\nform view synthesis on the held-out test scenes. We further\\nquantitatively compare the performance of our feed-forward\\nmodel with NeRF optimized to the same set of input views\\nin Fig. 10. Note that training each of 60 NeRFs took 14\\nhours; in contrast, pixelNeRF is applied to new scenes im-\\nmediately without any test-time optimization.\\n6. Discussion\\nWe have presented pixelNeRF, a framework to learn a\\nscene prior for reconstructing NeRFs from one or a few im-\\nages. Through extensive experiments, we have established\\nthat our approach can be successfully applied in a variety\\nof settings. We addressed some shortcomings of NeRF, but\\nthere are challenges yet to be explored: 1) Like NeRF, our\\nrendering time is slow, and in fact, our runtime increases lin-\\nearly when given more input views. Further, some methods\\n(e.g. [28, 21]) can recover a mesh from the image enabling\\nfast rendering and manipulation afterwards, while NeRF-\\nbased representations cannot be converted to meshes very\\nreliably. Improving NeRF’s efﬁciency is an important re-\\nsearch question that can enable real-time applications. 2) As\\nin the vanilla NeRF, we manually tune ray sampling bounds\\ntn, tf and a scale for the positional encoding.\\nMaking\\nNeRF-related methods scale-invariant is a crucial challenge.\\n3) While we have demonstrated our method on real data\\nfrom the DTU dataset, we acknowledge that this dataset was\\ncaptured under controlled settings and has matching camera\\nposes across all scenes with limited viewpoints. Ultimately,\\nour approach is bottlenecked by the availability of large-\\nscale wide baseline multi-view datasets, limiting the appli-\\ncability to datasets such as ShapeNet and DTU. Learning\\na general prior for 360◦ scenes in-the-wild is an exciting\\ndirection for future work.\\nAcknowledgements\\nWe thank Shubham Goel and Hang Gao for comments\\non the text. We also thank Emilien Dupont and Vincent\\nSitzmann for helpful discussions.\\n4585\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='References\\n[1] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and R.\\nSzeliski. Building rome in a day. In ICCV, pages 72–79,\\n2009.\\n[2] Miguel Angel Bautista, Walter Talbott, Shuangfei Zhai, Ni-\\ntish Srivastava, and Joshua M. Susskind. On the generaliza-\\ntion of learning-based 3d reconstruction. In WACV, pages\\n2180–2189, January 2021.\\n[3] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\\nGortler, and Michael Cohen. Unstructured lumigraph ren-\\ndering. In SIGGRAPH, pages 425–432, 2001.\\n[4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat\\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\\nlis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi,\\nand Fisher Yu. ShapeNet: An Information-Rich 3D Model\\nRepository.\\nTechnical Report arXiv:1512.03012 [cs.GR],\\n2015.\\n[5] Xu Chen, Jie Song, and Otmar Hilliges. Monocular neu-\\nral image based rendering with continuous view control. In\\nICCV, pages 4090–4100, 2019.\\n[6] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for\\ngenerative shape modeling.\\nIn CVPR, pages 5939–5948,\\n2019.\\n[7] Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, and\\nBing Zeng.\\nNeural point cloud rendering via multi-plane\\nprojection. In CVPR, pages 7830–7839, 2020.\\n[8] Emilien Dupont, Miguel Angel Bautista, Alex Colburn,\\nAditya Sankar, Carlos Guestrin, Joshua Susskind, and Qi\\nShan. Equivariant neural rendering. In ICML, 2020.\\n[9] S. Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio\\nViola, Ari Morcos, Marta Garnelo, Avraham Ruderman, An-\\ndrei Rusu, Ivo Danihelka, Karol Gregor, David Reichert,\\nLars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosen-\\nbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt\\nBotvinick, and Demis Hassabis. Neural scene representation\\nand rendering. Science, 360:1204–1210, 06 2018.\\n[10] J. Flynn, I. Neulander, J. Philbin, and N. Snavely.\\nDeep\\nstereo: Learning to predict new views from the world’s im-\\nagery. In CVPR, pages 5515–5524, 2016.\\n[11] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\\nMichael F Cohen. The lumigraph. In SIGGRAPH, pages\\n43–54, 1996.\\n[12] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan\\nRussell, and Mathieu Aubry. AtlasNet: A Papier-Mˆach´e Ap-\\nproach to Learning 3D Surface Generation. In CVPR, 2018.\\n[13] Xun Huang and Serge Belongie. Arbitrary style transfer in\\nreal-time with adaptive instance normalization.\\nIn ICCV,\\n2017.\\n[14] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola,\\nand Henrik Aanæs. Large scale multi-view stereopsis evalu-\\nation. In CVPR, pages 406–413, 2014.\\n[15] Abhishek Kar, Christian H¨ane, and Jitendra Malik. Learning\\na multi-view stereo machine. In NeurIPS, 2017.\\n[16] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-\\nral 3d mesh renderer. In CVPR, 2018.\\n[17] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\\nshick.\\nPointRend: Image segmentation as rendering.\\nIn\\nCVPR, 2020.\\n[18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\\n3d object representations for ﬁne-grained categorization. In\\n4th International IEEE Workshop on 3D Representation and\\nRecognition (3dRR-13), Sydney, Australia, 2013.\\n[19] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering. In\\nSIGGRAPH, pages 31–42, 1996.\\n[20] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\\nChristian Theobalt. Neural Sparse Voxel Fields. In NeurIPS,\\n2020.\\n[21] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-\\nterizer: A differentiable renderer for image-based 3d reason-\\ning. In ICCV, 2019.\\n[22] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li.\\nLearning to infer implicit surfaces without 3d supervision.\\nIn NeurIPS, 2019.\\n[23] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\\numes: Learning dynamic renderable volumes from images.\\nACM Trans. Graph., 38(4):65:1–65:14, July 2019.\\n[24] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\\nworth. NeRF in the wild: Neural radiance ﬁelds for uncon-\\nstrained photo collections. In CVPR, 2021.\\n[25] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\\nLearning 3d reconstruction in function space.\\nIn CVPR,\\n2019.\\n[26] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues\\nHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-\\nBrualla. Neural rerendering in the wild. In CVPR, 2019.\\n[27] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\\nRepresenting scenes as neural radiance ﬁelds for view syn-\\nthesis. In ECCV, 2020.\\n[28] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\\nAndreas Geiger. Differentiable volumetric rendering: Learn-\\ning implicit 3d representations without 3d supervision. In\\nCVPR, 2020.\\n[29] Jeong Joon Park, Peter Florence, Julian Straub, Richard\\nNewcombe, and Steven Lovegrove.\\nDeepSDF: Learning\\ncontinuous signed distance functions for shape representa-\\ntion. In CVPR, June 2019.\\n[30] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\\nZhu. Semantic image synthesis with spatially-adaptive nor-\\nmalization. In CVPR, 2019.\\n[31] Stefan Popov, Pablo Bauszat, and Vittorio Ferrari. CoReNet:\\nCoherent 3d scene reconstructionfrom a single rgb image. In\\nECCV, 2020.\\n[32] Gernot Riegler and Vladlen Koltun. Free view synthesis. In\\nECCV, pages 623–640, 2020.\\n[33] S. Saito, Z. Huang, R. Natsume, S. Morishima, H. Li, and A.\\nKanazawa. PIFu: Pixel-aligned implicit function for high-\\nresolution clothed human digitization. In ICCV, pages 2304–\\n2314, 2019.\\n4586\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='[34] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys,\\nand Jan-Michael Frahm. Pixelwise view selection for un-\\nstructured multi-view stereo. In ECCV, 2016.\\n[35] Jonathan Shade, Steven Gortler, Li-wei He, and Richard\\nSzeliski. Layered depth images. In SIGGRAPH, pages 231–\\n242, 1998.\\n[36] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\\nHuang. 3d photography using context-aware layered depth\\ninpainting. In CVPR, 2020.\\n[37] Daeyun Shin, Charless Fowlkes, and Derek Hoiem. Pixels,\\nvoxels, and views: A study of shape representations for sin-\\ngle view 3d object shape prediction. In CVPR, 2018.\\n[38] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\\nNießner, Gordon Wetzstein, and Michael Zollh¨ofer. Deep-\\nVoxels: Learning persistent 3d feature embeddings. In Proc.\\nComputer Vision and Pattern Recognition (CVPR), IEEE,\\n2019.\\n[39] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wet-\\nzstein.\\nScene representation networks:\\nContinuous 3d-\\nstructure-aware neural scene representations.\\nIn NeurIPS,\\n2019.\\n[40] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\\nSingle-view to multi-view: Reconstructing unseen views\\nwith a convolutional network. CoRR abs/1511.06702, 1(2):2,\\n2015.\\n[41] Maxim Tatarchenko, Stephan R Richter, Ren´e Ranftl,\\nZhuwen Li, Vladlen Koltun, and Thomas Brox. What do\\nsingle-view 3d reconstruction networks learn?\\nIn CVPR,\\npages 3405–3414, 2019.\\n[42] Justus Thies, Michael Zollh¨ofer, and Matthias Nießner. De-\\nferred neural rendering: Image synthesis using neural tex-\\ntures, 2019.\\n[43] Alex Trevithick and Bo Yang. GRF: Learning a general ra-\\ndiance ﬁeld for 3d scene representation and rendering. arXiv\\npreprint arXiv:2010.04595, 2020.\\n[44] Richard Tucker and Noah Snavely. Single-view view syn-\\nthesis with multiplane images. In CVPR, 2020.\\n[45] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Ji-\\ntendra Malik. Multi-view supervision for single-view recon-\\nstruction via differentiable ray consistency. In CVPR, 2017.\\n[46] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\\nmodels from single rgb images. In ECCV, 2018.\\n[47] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\\nJohnson. SynSin: End-to-end view synthesis from a single\\nimage. In CVPR, 2020.\\n[48] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukham-\\nbetov, and Gabriel J. Brostow. Interpretable transformations\\nwith encoder-decoder networks. In ICCV, pages 5737–5746,\\n2017.\\n[49] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radom´ır\\nMech, and Ulrich Neumann. DISN: deep implicit surface\\nnetwork for high-quality single-view 3d reconstruction. In\\nNeurIPS, pages 490–500, 2019.\\n[50] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and\\nHonglak Lee. Perspective transformer nets: Learning single-\\nview 3d object reconstruction without 3d supervision.\\nIn\\nNeurIPS. 2016.\\n[51] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\\nQuan.\\nMVSNet: Depth inference for unstructured multi-\\nview stereo. In ECCV, 2018.\\n[52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\\nand Oliver Wang. The unreasonable effectiveness of deep\\nfeatures as a perceptual metric. In CVPR, 2018.\\n[53] Xiuming\\nZhang,\\nZhoutong\\nZhang,\\nChengkai\\nZhang,\\nJoshua B Tenenbaum, William T Freeman, and Jiajun Wu.\\nLearning to Reconstruct Shapes from Unseen Classes. In\\nNeurIPS, 2018.\\n[54] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-\\nlik, and Alexei A Efros. View synthesis by appearance ﬂow.\\nIn ECCV, pages 286–301, 2016.\\n[55] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.\\nImage quality assessment: from error visibility to structural\\nsimilarity. IEEE TIP, 13(4):600–612, 2004.\\n4587\\n', metadata={'source': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'file_path': 'data/Yu et al. - pixelNeRF Neural Radiance Fields From One or Few .pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.3', 'title': 'pixelNeRF: Neural Radiance Fields From One or Few Images', 'author': 'Alex Yu,  Vickie Ye,  Matthew Tancik,  Angjoo Kanazawa', 'subject': 'IEEE Conference on Computer Vision and Pattern Recognition', 'keywords': '', 'creator': '', 'producer': 'PyPDF2', 'creationDate': '', 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loader\n",
    "\n",
    "loaders = [PyMuPDFLoader(filename) for filename in filenames]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.02563,\n",
       " 'namespaces': {'': {'vector_count': 2563}},\n",
       " 'total_vector_count': 2563}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_KEY, \n",
    "    environment=PINECONE_ENV, \n",
    ")\n",
    "\n",
    "index_name = \"document\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print(\"Index does not exist: \", index_name)\n",
    "    \n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "# index.delete(deleteAll='true', namespace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.vectorstores.pinecone.Pinecone"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch = Pinecone.from_texts([text.page_content for text in texts], \n",
    "                                embeddings, \n",
    "                                index_name=index_name)\n",
    "type(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the llm model for our qa session\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up the query \n",
    "query = \"What is NeRF?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "# Run the QA chain with your query to get the answer\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' NeRF is a neural scene representation that combines a single-scene '\n",
      " 'optimization setting with a neural scene representation capable of '\n",
      " 'representing complex scenes much more efficiently than a discrete 3D voxel '\n",
      " 'grid.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process the response from the QA chain \n",
    "# and isolate result and source docs and page numbers\n",
    "def parse_response(response):\n",
    "    print(response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source_name in response[\"source_documents\"]:\n",
    "        print(source_name.metadata['source'], \"page #:\", source_name.metadata['page'])\n",
    "\n",
    "# Set up the retriever on the pinecone vectorstore\n",
    "# Make sure to set include_metadata = True\n",
    "retriever = docsearch.as_retriever(include_metadata=True, metadata_key = 'source')\n",
    "\n",
    "# Set up the RetrievalQA chain with the retriever\n",
    "# Make sure to set return_source_documents = True\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain to get the response\n",
    "query = \"How are NeRF and iNeRF different?\"\n",
    "response = qa_chain(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'query': 'How are NeRF and iNeRF different?',\n",
      " 'result': ' NeRF is a 3D scene or object representation model, while iNeRF is '\n",
      "           'a framework for 6 DoF pose estimation by inverting a NeRF model. '\n",
      "           'iNeRF takes an observed image, an initial estimate of the pose, '\n",
      "           'and a NeRF model as inputs and uses an analysis-by-synthesis '\n",
      "           'approach to compute the appearance differences between the pixels '\n",
      "           'rendered from the NeRF model and the pixels from the observed '\n",
      "           'image. The gradients from these residuals are then backpropagated '\n",
      "           'through the NeRF model to produce the gradients for the estimated '\n",
      "           'pose.',\n",
      " 'source_documents': [Document(page_content='and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.', metadata={}),\n",
      "                      Document(page_content='and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.', metadata={}),\n",
      "                      Document(page_content='Here we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model. . iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image. We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image. The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose. As illustrated in Figure (\\\\ensuremath{<}\\\\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate.', metadata={}),\n",
      "                      Document(page_content='Here we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model. . iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image. We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image. The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose. As illustrated in Figure (\\\\ensuremath{<}\\\\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate.', metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "pprint(type(response))\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
