Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery 
Abstract. Reconstruction of the soft tissues in robotic surgery from endoscopic stereo videos is important for many applications such as intra-operative navigation and image-guided robotic surgery automation. Previous works on this task mainly rely on SLAM-based approaches, which struggle to handle complex surgical scenes. Inspired by recent progress in neural rendering, we present a novel framework for deformable tissue reconstruction from binocular captures in robotic surgery under the single-viewpoint setting. Our framework adopts dynamic neural radiance fields to represent deformable surgical scenes in MLPs and optimize shapes and deformations in a learning-based manner. In addition to non-rigid deformations, tool occlusion and poor 3D clues from a single viewpoint are also particular challenges in soft tissue reconstruction. To overcome these di\"yculties, we present a series of strategies of tool mask-guided ray casting, stereo depth-cueing ray marching and stereo depth-supervised optimization. With experiments on DaVinci robotic surgery videos, our method significantly outperforms the current state-of-the-art reconstruction method for handling various complex non-rigid deformations. To our best knowledge, this is the first work leveraging neural rendering for surgical scene 3D reconstruction with remarkable potential demonstrated. Code is available at: (\ensuremath{<}https://github.com/med-air/EndoNeRF\ensuremath{>})https://github.com/med-air/EndoNeRF. 
Keywords: 3D Reconstruction {\textperiodcentered} Neural Rendering {\textperiodcentered} Robotic Surgery. 
1 Introduction 
Surgical scene reconstruction from endoscope stereo video is an important but di\"ycult task in robotic minimally invasive surgery. It is a prerequisite for many downstream clinical applications, including intra-operative navigation and augmented reality, surgical environment simulation, immersive education, and robotic surgery automation [(\ensuremath{<}\ensuremath{>})2,(\ensuremath{<}\ensuremath{>})12,(\ensuremath{<}\ensuremath{>})20,(\ensuremath{<}\ensuremath{>})25]. Despite much recent progress [(\ensuremath{<}\ensuremath{>})10,(\ensuremath{<}\ensuremath{>})22,(\ensuremath{<}\ensuremath{>})28,(\ensuremath{<}\ensuremath{>})29,(\ensuremath{<}\ensuremath{>})30,(\ensuremath{<}\ensuremath{>})33], several key challenges still remain unsolved. First, surgical scenes are deformable with significant topology changes, requiring dynamic reconstruction to capture a high degree of non-rigidity. Second, endoscopic videos show sparse viewpoints due to constrained camera movement in confined space, resulting in limited 3D clues of soft tissues. Third, the surgical instruments always occlude part of the soft tissues, which a{\textperiodcentered}ects the completeness of surgical scene reconstruction. 
Previous works [(\ensuremath{<}\ensuremath{>})1,(\ensuremath{<}\ensuremath{>})13] explored the e{\textperiodcentered}ectiveness of surgical scene reconstruction via depth estimation. Since most of the endoscopes are equipped with stereo cameras, depth can be estimated from binocular vision. Follow-up SLAM-based methods [(\ensuremath{<}\ensuremath{>})23,(\ensuremath{<}\ensuremath{>})31,(\ensuremath{<}\ensuremath{>})32] fuse depth maps in 3D space to reconstruct surgical scenes under more complex settings. Nevertheless, these methods either hypothesize scenes as static or surgical tools not present, limiting their practical use in real scenarios. Recent work SuPer [(\ensuremath{<}\ensuremath{>})8] and E-DSSR [(\ensuremath{<}\ensuremath{>})11] present frameworks consisting of tool masking, stereo depth estimation and SurfelWarp [(\ensuremath{<}\ensuremath{>})4] to perform single-view 3D reconstruction of deformable tissues. However, all these methods track deformation based on a sparse warp field [(\ensuremath{<}\ensuremath{>})16], which degenerates when deformations are significantly beyond the scope of non-topological changes. 
As an emerging technology, neural rendering [(\ensuremath{<}\ensuremath{>})6,(\ensuremath{<}\ensuremath{>})27,(\ensuremath{<}\ensuremath{>})26] is recently developed to break through the limited performance of traditional 3D reconstruction by leveraging di{\textperiodcentered}erentiable rendering and neural networks. In particular, neural radiance fields (NeRF) [(\ensuremath{<}\ensuremath{>})15], a popular pioneering work of neural rendering, proposes to use neural implicit field for continuous scene representations and achieves great success in producing high-quality view synthesis and 3D reconstruction on diverse scenarios [(\ensuremath{<}\ensuremath{>})14,(\ensuremath{<}\ensuremath{>})15,(\ensuremath{<}\ensuremath{>})17]. Meanwhile, recent variants of NeRF [(\ensuremath{<}\ensuremath{>})18,(\ensuremath{<}\ensuremath{>})19,(\ensuremath{<}\ensuremath{>})21] targeting dynamic scenes have managed to track deformations through various neural representations on non-rigid objects. 
In this paper, we endeavor to reconstruct highly deformable surgical scenes captured from single-viewpoint stereo endoscopes. We embark on adapting the emerging neural rendering framework to the regime of deformable surgical scene reconstruction. We summarize our contributions as follows: 1) To accommodate a wide range of geometry and deformation representations on soft tissues, we leverage neural implicit fields to represent dynamic surgical scenes. 2) To address the particular tool occlusion problem in surgical scenes, we design a new mask-guided ray casting strategy for resolving tool occlusion. 3) We incorporate a depth-cueing ray marching and depth-supervised optimization scheme, using stereo prior to enable neural implicit field reconstruction for single-viewpoint input. To the best of our knowledge, this is the first work introducing cutting-edge neural rendering to surgical scene reconstruction. We evaluate our method on 6 typical in-vivo surgical scenes of robotic prostatectomy. Compared with previous methods, our results exhibit great performance gain, both quantitatively and qualitatively, on 3D reconstruction and deformation tracking of surgical scenes. 
2 Method 
Given a single-viewpoint stereo video of a dynamic surgical scene, we aim to reconstruct 3D structures and textures of surgical scenes without occlusion of surgical instruments. We denote as a sequence of input stereo video frames, where T is the total number of frames and (I , I ) is the pair of left and time of the i-th frame is i/T . We also extract binary tool masks for the ri li right images at the i-th frame. The video duration is normalized to [0, 1]. Thus, 
Fig. 1: Illustration of our proposed novel approach of neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery. 
left views to identify the region of surgical instruments. To utilize stereo clues, we estimate coarse depth maps \{Di\}iT=1 for the left views from the binocular captures. We follow the modeling in D-NeRF [(\ensuremath{<}\ensuremath{>})21] and represent deformable surgical scenes as a canonical neural radiance field along with a time-dependent neural displacement field (cf. Sec. (\ensuremath{<}\ensuremath{>})2.2). In our pipeline, each training iteration consists of the following six stages: i) randomly pick a frame for training, ii) run tool-guided ray casting (cf. Sec. (\ensuremath{<}\ensuremath{>})2.3) to shoot camera rays into the scene, iii) sample points along each camera ray via depth-cueing ray marching (cf. Sec. (\ensuremath{<}\ensuremath{>})2.4), iv) send sampled points to networks to obtain color and space occupancy of each point, v) evaluate volume rendering integral on sampled points to produce rendering results, vi) optimize the rendering loss plus depth loss to reconstruct shapes, colors and deformations of the surgical scene (cf. Sec. (\ensuremath{<}\ensuremath{>})2.5). The overview of key components in our approach is illustrated in Fig. (\ensuremath{<}\ensuremath{>})1. We will describe the detailed methods in the following subsections. 
We represent a surgical scene as a canonical radiance field and a time-dependent displacement field. Accordingly, each frame of the surgical scene can be regarded as a deformation of the canonical field. The canonical field, denoted as F\ensuremath{\Theta}(x, d), is an 8-layer MLP with network parameter \ensuremath{\Theta}, mapping coordinates x \ensuremath{\in} R3 and unit view-in directions d \ensuremath{\in} R3 to RGB colors c(x, d) \ensuremath{\in} R3 and space occupancy \ensuremath{\sigma}(x) \ensuremath{\in} R. The time-dependent displacement field G\ensuremath{\Phi}(x, t) is encoded in another 8-layer MLP with network parameters \ensuremath{\Phi} and maps input space-time coordinates (x, t) into displacement between the point x at time t and the corresponding point in the canonical field. For any time t, the color and occupancy at x can be retrieved as F\ensuremath{\Theta}(x + G\ensuremath{\Phi}(x, t), d). Compared with other dynamic modeling approaches [(\ensuremath{<}\ensuremath{>})18,(\ensuremath{<}\ensuremath{>})19], a displacement field is su\"ycient to explicitly and physically express all tissue deformations. To capture high-frequency details, we use positional encoding \ensuremath{\gamma}({\textperiodcentered}) to map the input coordinates and time into Fourier features [(\ensuremath{<}\ensuremath{>})24] before feeding them to the networks. 
With scene representations, we further leverage the di{\textperiodcentered}erentiable volume rendering used in NeRF to yield renderings for supervision. The di{\textperiodcentered}erentiable volume rendering begins with shooting a batch of camera rays into the surgical scene from a fixed viewpoint at an arbitrary time t. Every ray is formulated as r(s) = o + sd, where o is a fixed origin of the ray, d is the pointing direction of the ray and s is the ray parameter. In the original NeRF, rays are shot towards a batch of randomly selected pixels on the entire image plane. However, there are many pixels of surgical tools on the captured images, while our goal is to reconstruct underlying tissues. Thus, training on these tool pixels is unexpected. Our main idea for solving this issue is to bypass those rays traveling through tool pixels over the training stage. We utilize binary tool masks \{Mi\}Ti=1, where 0 stands for tissue pixels and 1 stands for tool pixels, to inform which rays should be neglected. In this regard, we create importance maps \{V\}iT=1 according to Mi and perform importance sampling to avoid shooting rays for those pixels of surgical tools. Eq. ((\ensuremath{<}\ensuremath{>})1) exhibits the construction of importance maps, where \ensuremath{\otimes} is element-wise multiplication, {\textperiodcentered}F is Frobenius norm and 1 is a matrix with the same shape as Mi while filled with ones: 
(1) 
The 1 \ensuremath{-} Mi term initializes the importance of tissue pixels to 1 and the importance of tool pixels to 0. To balance the sampling rate of occluded pixels across frames, the scaling term \ensuremath{\Lambda} specifies higher importance scaling for those tissue areas with higher occlusion frequencies. Normalizing each importance map as V i = Vi/ViF will yield a probability mass function over the image plane. During our ray casting stage for the i-th frame, we sample pixels from the distribution V i using inverse transform sampling and cast rays towards these sampled pixels. In this way, the probability of shooting rays for tool pixels is guaranteed to be zero as the importance of tool pixels is constantly zero. 
After shooting camera rays over tool occlusion, we proceed ray marching to sample points in the space. Specifically, we discretize each camera ray r(s) into batch of points \{xj |xj = r(sj)\}m j=1 by sampling a sequence of ray steps s1 \ensuremath{\leq} s2 \ensuremath{\leq} {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} \ensuremath{\leq} sm. The original NeRF proposes hierarchical stratified sampling to obtain \{sj\}m j=1. However, this sampling strategy hardly exploits accurate 3D structures when NeRF models are trained on single-view input. Drawing inspiration from early work in iso-surface rendering [(\ensuremath{<}\ensuremath{>})7], we create Gaussian transfer functions with stereo depth to guide point sampling near tissue surfaces. For the i-th frame, the transfer function for a ray r(s) shooting towards pixel (u, v) is formulated as: 
(2) 
The transfer function \ensuremath{\delta}(s; u, v, i) depicts an impulse distribution that continuously allocates sampling weights for every location on r(s). The impulse is centered at Di[u, v], i.e., the depth at the (u, v) pixel. The width of the impulse is controlled by the hyperparameter \ensuremath{\xi}, which is set to a small value to mimic Dirac delta impulse. In our ray marching, are drawn from the normalized impulse distribution By this means, sampled points are concentrated around tissue surfaces, imposing stereo prior in rendering. 
Once we obtain the sampled points in the space, the emitted color C and optical depth D of a camera ray r(s) can be evaluated by volume rendering [(\ensuremath{<}\ensuremath{>})5] as: 
(3) 
To reconstruct the canonical and displacement fields from single-view captures, we optimize the network parameters \ensuremath{\Theta} and \ensuremath{\Phi} by jointly supervising the rendered color and optical depth [(\ensuremath{<}\ensuremath{>})3]. Specifically, the loss function for training the networks is defined as: 
(4) 
where (u, v) is the location of the pixel that r(s) shoots towards, \ensuremath{\lambda} is a hyper-parameter weighting the depth loss. 
Last but not least, we conduct statistical depth refinement to handle corrupt stereo depth caused by fuzzy pixels and specular highlights on the images of surgical scenes. Direct supervision on the estimated depth will overfit corrupt depth in the end, leading to abrupt artifacts in reconstruction results (Fig. (\ensuremath{<}\ensuremath{>})3). Our preliminary findings reveal that our model at the early training stage would produce smoother results both in color and depth since the underfitting model tends to average learned colors and occupancy. Thus, minority corrupt depth is smoothed by majority normal depth. Based on this observation, we propose to patch the corrupt depth with the output from underfitting radiance fields. Denoting D iK as the underfitting output depth maps for the i-th frame after K iterations of training, we firstly find residual maps through i = |D iK \ensuremath{-} Di|, then we compute a probabilistic distribution over the residual maps. After that, we set a small number \ensuremath{\alpha} \ensuremath{\in} [0, 1] and locate those pixels with the last \ensuremath{\alpha}-quantile residuals. Since those located pixels statistically correspond to large residuals, we can identify them as occurrences of corrupt depth. Finally, we replace those identified corrupt depth pixels with smoother depth pixels in D iK . After this refinement procedure, the radiance fields are optimized on the patched depth maps in the subsequent training iterations, alleviating corrupt depth fitting. 
3 Experiments 
We evaluate our proposed method on typical robotic surgery stereo videos from 6 cases of our in-house DaVinci robotic prostatectomy data. We totally extracted 6 clips with a total of 807 frames. Each clip lasts for 4\ensuremath{\sim}8s with 15fps. Each case is captured from stereo cameras at a single viewpoint and encompasses challenging scenes with non-rigid deformation and tool occlusion. Among the selected 6 cases, 2 cases contain traction on thin structures such as fascia, 2 cases contain significant pushing and pulling of tissue, and 2 cases contain tissue cutting, which altogether present the typical soft tissue situations in robotic surgery. For comparison, we take the most recent state-of-the-art surgical scene reconstruction method of E-DSSR [(\ensuremath{<}\ensuremath{>})11] as a strong comparison. For qualitative evaluation, We exhibit our reconstructed point clouds and compare textural and geometric details obtained by di{\textperiodcentered}erent methods. We also conduct an ablation study on our depth-related modules through qualitative comparison. Due to clinical regulation in practice, it is impossible to collect ground truth depth for numerical evaluation on 3D structures. Following the evaluation method in [(\ensuremath{<}\ensuremath{>})11] and wide literature in neural rendering, we alternatively use photometric errors, including PSNR, SSIM and LPIPS, as evaluation metrics for quantitative comparisons. 
In our implementation, we empirically set the width of the transfer function \ensuremath{\xi} = 1, the weight of depth loss \ensuremath{\lambda} = 1, depth refinement iteration K = 4000 and \ensuremath{\alpha}=0.1. Other training hyper-parameters follow the settings in the state-of-the-art D-NeRF [(\ensuremath{<}\ensuremath{>})21]. We calibrate the endoscope in advance to acquire its intrinsics. In all of our experiments, tool masks are obtained by manually labeling and coarse stereo depth maps are generated by STTR-light [(\ensuremath{<}\ensuremath{>})9] pretrained on Scene Flow. We optimize each model over 100K iterations on a single case. To recover explicit geometry from implicit fields, we render optimized radiance fields to RGBD maps, smooth rendered depth maps via bilateral filtering, and back-project RGBD into point clouds based on the endoscope intrinsics. 
For qualitative evaluation, Fig. (\ensuremath{<}\ensuremath{>})2 illustrates the reconstruction results of our approach and the comparison method, along with a reference to the original video. In the test case of Fig. (\ensuremath{<}\ensuremath{>})2(a), the tissues are pulled by surgical instruments, yielding relatively large deformations. Benefitting from the underlying continuous scene representations, our method can reconstruct water-tight tissues without being a{\textperiodcentered}ected by the tool occlusion. More importantly, per-frame deformations are captured continuously, achieving stable results over the episode of consecutive pulling. In contrast, the current state-of-the-art method [(\ensuremath{<}\ensuremath{>})11] could not fully track these large deformations and its reconstruction results include holes 
Fig. 2: Qualitative comparisons of 2 cases, demonstrating reconstruction of soft tissues with large deformations and topology changes. 
and noisy points under such a challenging situation. We further demonstrate a more di\"ycult case in Fig. (\ensuremath{<}\ensuremath{>})2(b) which includes soft tissue cutting with topology changes. From the reconstruction results, it is observed that our method manages to track the detailed cutting procedures, owing to the powerful neural 
Fig. 3: Ablation study on our depth-related modules, i.e., depth-supervised loss, depth refinement and depth-cueing ray marching. 
Table 1: Quantitative evaluation on photometric errors of the dynamic reconstruction on metrics of PSNR, SSIM and LPIPS. 
representation of displacement fields. In addition, it can bypass the issue of tool occlusion and recover the hidden tissues, which is cooperatively achieved by our mask-guided ray casting and the interpolation property of neural implicit fields. On the other hand, the comparison method is not able to capture these small changes on soft tissues nor patch all the tool-occluded areas. Table (\ensuremath{<}\ensuremath{>})1 summarizes our quantitative experiments, showing overall performance on the dataset. Our method dramatically outperforms E-DSSR by {\textuparrow} 16.433 PSNR, {\textuparrow} 0.295 SSIM and {\textdownarrow} 0.342 LPIPS. To assess the contribution of the dynamics modeling, we also evaluate our model without neural displacement field (Ours w/o D). As expected, removing this component leads to a noticeable performance drop, which reflects the e{\textperiodcentered}ectiveness of the displacement modeling. 
We present a qualitative ablation study on our depth-related modules in Fig. (\ensuremath{<}\ensuremath{>})3. Without depth-supervision loss, we observe that the pipeline is not capable of learning correct geometry from single-viewpoint input. Moreover, when depth refinement is disabled, abrupt artifacts occur on the reconstruction results due to corruption in stereo depth estimation. Our depth-cueing ray marching can further diminish artifacts on 3D structures, especially for boundary points. 
4 Conclusion 
This paper presents a novel neural rendering-based framework for dynamic surgical scene reconstruction from single-viewpoint binocular images, as well as 
addressing complex tissue deformation and tool occlusion. We adopt the cutting-edge dynamic neural radiance field method to represent surgical scenes. In addition, we propose mask-guided ray casting to handle tool occlusion and impose stereo depth prior upon the single-viewpoint situation. Our approach has achieved superior performance on various scenarios in robotic surgery data such as large elastic deformations and tissue cutting. We hope the emerging NeRF-based 3D reconstruction techniques could inspire new pathways for robotic surgery scene understanding, and empower various down-stream clinical-oriented tasks. 
Acknowledgements. This work was supported in part by CUHK Shun Hing Institute of Advanced Engineering (project MMT-p5-20), in part by Shenzhen-HK Collaborative Development Zone, and in part by Multi-Scale Medical Robotics Centre InnoHK. 
References 