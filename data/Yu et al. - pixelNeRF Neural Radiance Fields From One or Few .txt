pixelNeRF: Neural Radiance Fields from One or Few Images 

We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixel-NeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pix-elNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf. 

1. Introduction 

We study the problem of synthesizing novel views of a scene from a sparse set of input views. This long-standing problem has recently seen progress due to advances in differentiable neural rendering [27, 20, 24, 39]. Across these approaches, a 3D scene is represented with a neural network, which can then be rendered into 2D views. Notably, the recent method neural radiance fields (NeRF) [27] has shown impressive performance on novel view synthesis of a specific scene by implicitly encoding volumetric density and color through a neural network. While NeRF can render photorealistic novel views, it is often impractical as it requires a large number of posed images and a lengthy per-scene optimization. 

In this paper, we address these shortcomings by proposing pixelNeRF, a learning framework that enables predicting NeRFs from one or several images in a feed-forward manner. Unlike the original NeRF network, which does not make use of any image features, pixelNeRF takes spatial image features aligned to each pixel as an input. This image conditioning allows the framework to be trained on a set of multi-view images, where it can learn scene priors to perform view synthesis from one or few input views. In contrast, NeRF is unable to generalize and performs poorly when few input images are available, as shown in Fig. 1. 

Specifically, we condition NeRF on input images by first computing a fully convolutional image feature grid from the input image. Then for each query spatial point x and viewing direction d of interest in the view coordinate frame, we sample the corresponding image feature via projection and bilinear interpolation. The query specification is sent along with the image features to the NeRF network that outputs density and color, where the spatial image features are fed to each layer as a residual. When more than one image is available, the inputs are first encoded into a latent representation in each camera{\textquoteright}s coordinate frame, which are then pooled in an intermediate layer prior to predicting the color and density. The model is supervised with a reconstruction loss between a ground truth image and a view rendered using conventional volume rendering techniques. This framework is illustrated in Fig. 2. 

PixelNeRF has many desirable properties for few-view novel-view synthesis. First, pixelNeRF can be trained on a dataset of multi-view images without additional supervision such as ground truth 3D shape or object masks. Second, pixelNeRF predicts a NeRF representation in the camera coordinate system of the input image instead of a canonical coordinate frame. This is not only integral for generalization to unseen scenes and object categories [41, 37], but also for flexibility, since no clear canonical coordinate system exists on scenes with multiple objects or real scenes. Third, it is fully convolutional, allowing it to preserve the spatial alignment between the image and the output 3D representation. Lastly, pixelNeRF can incorporate a variable number of posed input views at test time without requiring any test-time optimization. 

We conduct an extensive series of experiments on synthetic and real image datasets to evaluate the efficacy of our framework, going beyond the usual set of ShapeNet experiments to demonstrate its flexibility. Our experiments show that pixelNeRF can generate novel views from a single image input for both category-specific and category-agnostic settings, even in the case of unseen object categories. Further, we test the flexibility of our framework, both with a new multi-object benchmark for ShapeNet, where pixel-NeRF outperforms prior approaches, and with simulation-to-real transfer demonstration on real car images. Lastly, we test capabilities of pixelNeRF on real images using the DTU dataset [14], where despite being trained on under 100 scenes, it can generate plausible novel views of a real scene from three posed input views. 

2. Related Work 

Novel View Synthesis. The long-standing problem of novel view synthesis entails constructing new views of a scene from a set of input views. Early work achieved photore-alistic results but required densely captured views of the scene [19, 11]. Recent work has made rapid progress to-

Table 1: A comparison with prior works reconstructing neural scene representations. The proposed approach learns a scene prior for one or few-view reconstruction using only multi-view 2D image supervision. Unlike previous methods in this regime, we do not require a consistent canonical space across the training corpus. Moreover, we incorporate local image features to preserve local information which is in contrast to methods that compress the structure and appearance into a single latent vector such as Occupancy Networks (ONet) [25] and DVR [28]. 

ward photorealism for both wider ranges of novel views and sparser sets of input views, by using 3D representations based on neural networks [27, 23, 26, 38, 42, 7]. However, because these approaches fit a single model to each scene, they require many input views and substantial optimization time per scene. 

There are methods that can predict novel view from few input views or even single images by learning shared priors across scenes. Methods in the tradition of [35, 3] use depth-guided image interpolation [54, 10, 32]. More recently, the problem of predicting novel views from a single image has been explored [44, 47, 36, 5]. However, these methods employ 2.5D representations, and are therefore limited in the range of camera motions they can synthesize. In this work we infer a 3D volumetric NeRF representation, which allows novel view synthesis from larger baselines. 

Sitzmann et al. [39] introduces a representation based on a continuous 3D feature space to learn a prior across scene instances. However, using the learned prior at test time requires further optimization with known absolute camera poses. In contrast, our approach is completely feed-forward and only requires relative camera poses. We offer extensive comparisons with this approach to demonstrate the advantages our design affords. Lastly, note that concurrent work [43] adds image features to NeRF. A key difference is that we operate in view rather than canonical space, which makes our approach applicable in more general settings. Moreover, we extensively demonstrate our method{\textquoteright}s performance in few-shot view synthesis, while GRF shows very limited quantitative results for this task. 

Most single-view 3D reconstruction methods condition neural 3D representations on input images. The majority employs global image features [29, 6, 28, 25, 8], which, while memory efficient, cannot preserve details that are present in the image and often lead to retrieval-like results. Spatially-aligned local image features have been shown to achieve detailed reconstructions from a single view [49, 33]. However, both of these methods require 3D supervision. Our method is inspired by these approaches, but only requires multi-view supervision. 

Within existing methods, the types of scenes that can be reconstructed are limited, particularly so for object-centric approaches (e.g. [46, 21, 12, 45, 38, 53, 25, 49, 28]). CoReNet [31] reconstructs scenes with multiple objects via a voxel grid with offsets, but it requires 3D supervision including the identity and placement of objects. In comparison, we formulate a scene-level learning framework that can in principle be trained to scenes of arbitrary structure. 

Viewer-centric 3D reconstruction For the 3D learning task, prediction can be done either in a viewer-centered coordinate system, i.e. view space, or in an object-centered coordinate system, i.e. canonical space. Most existing methods [49, 25, 28, 39] predict in canonical space, where all objects of a semantic category are aligned to a consistent orientation. While this makes learning spatial regularities easier, using a canonical space inhibits prediction performance on unseen object categories and scenes with more than one object, where there is no pre-defined or well-defined canonical pose. PixelNeRF operates in view-space, which has been shown to allow better reconstruction of unseen object categories in [37, 2], and discourages the memorization of the training set [41]. We summarize key aspects of our approach relative to prior work in Table 1. 

3. Background: NeRF 

We first briefly review the NeRF representation [27]. A NeRF encodes a scene as a continuous volumetric radiance field f of color and density. Specifically, for a 3D point x \ensuremath{\in}R3 and viewing direction unit vector d \ensuremath{\in}R3 , f returns a differential density {\textperiodcentered} and RGB color c: f(x,d) = ({\textperiodcentered}, c). 

The volumetric radiance field can then be rendered into a 2D image via 

(1) 

where ds  handles occlusion. For a target view with pose P, a camera ray can be parameter-

ized as r(t) = o + td, with the ray origin (camera center) o \ensuremath{\in}R3 and ray unit direction vector d \ensuremath{\in}R3 . The integral is computed along r between pre-defined depth bounds [tn, tf ]. In practice, this integral is approximated with numerical quadrature by sampling points along each pixel ray. 

The rendered pixel value for camera ray r can then be compared against the corresponding ground truth pixel value, C(r), for all the camera rays of the target view with pose P. The NeRF rendering loss is thus given by 

(2) 

where R(P) is the set of all camera rays of target pose P. 

Limitations While NeRF achieves state of the art novel view synthesis results, it is an optimization-based approach using geometric consistency as the sole signal, similar to classical multiview stereo methods [1, 34]. As such each scene must be optimized individually, with no knowledge shared between scenes. Not only is this time-consuming, but in the limit of single or extremely sparse views, it is unable to make use of any prior knowledge of the world to accelerate reconstruction or for shape completion. 

4. Image-conditioned NeRF 

To overcome the NeRF representation{\textquoteright}s inability to share knowledge between scenes, we propose an architecture to condition a NeRF on spatial image features. Our model is comprised of two components: a fully-convolutional image encoder E, which encodes the input image into a pixel-aligned feature grid, and a NeRF network f which outputs color and density, given a spatial location and its corresponding encoded feature. We choose to model the spatial query in the input view{\textquoteright}s camera space, rather than a canonical space, for the reasons discussed in {\textsection}2. We validate this design choice in our experiments on unseen object categories ({\textsection}5.2) and complex unseen scenes ({\textsection}5.3). The model is trained with the volume rendering method and loss described in {\textsection}3. 

In the following, we first present our model for the single view case. We then show how this formulation can be easily extended to incorporate multiple input images. 

We now describe our approach to render novel views from one input image. We fix our coordinate system as the view space of the input image and specify positions and camera rays in this coordinate system. 

Given a input image I of a scene, we first extract a feature volume W = E(I). Then, for a point on a camera ray x, we retrieve the corresponding image feature by projecting x onto the image plane to the image coordinates {\textasciicaron}(x) using 

Figure 2: Proposed architecture in the single-view case. For a query point x along a target camera ray with view direction d, a corresponding image feature is extracted from the feature volume W via projection and interpolation. This feature is then passed into the NeRF network f along with the spatial coordinates. The output RGB and density value is volume-rendered and compared with the target pixel value. The coordinates x and d are in the camera coordinate system of the input view. 

known intrinsics, then bilinearly interpolating between the pixelwise features to extract the feature vector W({\textasciicaron}(x)). The image features are then passed into the NeRF network, along with the position and view direction (both in the input view coordinate system), as 

(3) 

where ({\textperiodcentered}) is a positional encoding on x with 6 exponentially increasing frequencies introduced in the original NeRF [27]. The image feature is incorporated as a residual at each layer; see {\textsection}5 for more information. We show our pipeline schematically in Fig. 2. 

In the few-shot view synthesis task, the query view direction is a useful signal for determining the importance of a particular image feature in the NeRF network. If the query view direction is similar to the input view orientation, the model can rely more directly on the input; if it is dissimilar, the model must leverage the learned prior. Moreover, in the multi-view case, view directions could serve as a signal for the relevance and positioning of different views. For this reason, we input the view directions at the beginning of the NeRF network. 

Multiple views provide additional information about the scene and resolve 3D geometric ambiguities inherent to the single-view case. We extend our model to allow for an arbitrary number of views at test time, which distinguishes our method from existing approaches that are designed to only use single input view at test time. [8, 53] Moreover, our formulation is independent of the choice of world space and the order of input views. 

In the case that we have multiple input views of the scene, we assume only that the relative camera poses are known. For purposes of explanation, an arbitrary world coordinate system can be fixed for the scene. We denote the ith input image as I(i) and its associated camera transform from the world space to its view space as 

For a new target camera ray, we transform a query point x, with view direction d, into the coordinate system of each input view i with the world to camera transform as 

(4) 

To obtain the output density and color, we process the coordinates and corresponding features in each view coordinate frame independently and aggregate across the views within the NeRF network. For ease of explanation, we denote the initial layers of the NeRF network as f1, which process inputs in each input view space separately, and the final layers as f2, which process the aggregated views. 

We encode each input image into feature volume For the view-space point x(i) , we extract the corresponding image feature from the feature volume W(i) at the projected image coordinate {\textasciicaron}(x(i)). We then pass these inputs into f1 to obtain intermediate vectors: 

(5) 

The intermediate V(i) are then aggregated with the average pooling operator and passed into a the final layers, denoted as f2, to obtain the predicted density and color: 

(6) 

In the single-view special case, this simplifies to Equation 3 with f = f2 {\textopenbullet}f1, by considering the view space as the world space. An illustration is provided in the supplemental. 

5. Experiments 

Baselines For ShapeNet benchmarks, we compare quantitatively and qualitatively to SRN [39] and DVR [28], the current state-of-the-art in few-shot novel-view synthesis and 2D-supervised single-view reconstruction respectively. We use the 2D multiview-supervised variant of DVR. In the category-agnostic setting ({\textsection}5.1.2), we also include grayscale rendering of SoftRas [21] results. 1 In the experiments with multiple ShapeNet objects, we compare with SRN, which can also model entire scenes. 

For the experiment on the DTU dataset, we compare to NeRF [27] trained on sparse views. Because NeRF is a test-time optimization method, we train a separate model for each scene in the test set. 

Metrics We report the standard image quality metrics PSNR and SSIM [55] for all evaluations. We also include LPIPS [52], which more accurately reflects human perception, in all evaluations except in the category-specific setup ({\textsection}5.1.1). In this setting, we exactly follow the protocol of SRN [39] to remain comparable to prior works [40, 48, 9, 8, 43], for which source code is unavailable. 

Implementation Details For the image encoder E, to capture both local and global information effectively, we extract a feature pyramid from the image. We use a ResNet34 backbone pretrained on ImageNet for our experiments. Features are extracted prior to the first 4 pooling layers, upsam-pled using bilinear interpolation, and concatenated to form latent vectors of size 512 aligned to each pixel. 

To incorporate a point{\textquoteright}s corresponding image feature into the NeRF network f, we choose a ResNet architecture with a residual modulation rather than simply concatenating the feature vector with the point{\textquoteright}s position and view direction. Specifically, we feed the encoded position and view direction through the network and add the image feature as a residual at the beginning of each ResNet block. We train an independent linear layer for each block residual, in a similar manner as AdaIn and SPADE [13, 30], a method previously used with success in [25, 28]. Please refer to the supplemental for additional details. 

We first evaluate our approach on category-specific and category-agnostic view synthesis tasks on ShapeNet. 

We perform one-shot and two-shot view synthesis on the {\textquotedblleft}chair{\textquotedblright} and {\textquotedblleft}car{\textquotedblright} classes of ShapeNet, using the protocol and dataset introduced in [39]. The dataset contains 6591 

Figure 3: Category-specific single-view reconstruction benchmark. We train a separate model for cars and chairs and compare to SRN. The corresponding numbers may be found in Table 2. 

Figure 4: Category-specific 2-view reconstruction benchmark. We provide two views (left) to each model, and show two novel view renderings in each case (right). Please also refer to Table 2. 

Table 2: Category-specific 1-and 2-view reconstruction. Methods marked * do not require canonical poses at test time. In all cases, a single model is trained for each category and used for both 1-and 2-view evaluation. Note ENR is a 1-view only model. 

Table 3: Ablation studies for ShapeNet chair reconstruction. We show the benefit of using local features over a global code to condition the NeRF network (\ensuremath{-}Local vs Full), and of providing view directions to the network (\ensuremath{-}Dirs vs Full). 

Figure 5: Category-agnostic single-view reconstruction. Going beyond the SRN benchmark, we train a single model to the 13 largest ShapeNet categories; we find that our approach produces superior visual results compared to a series of strong baselines. In particular, the model recovers fine detail and thin structure more effectively, even for outlier shapes. Quite visibly, images on monitors and tabletop textures are accurately reproduced; baselines representing the scene as a single latent vector cannot preserve such details of the input image. SRN{\textquoteright}s test-time latent inversion becomes less reliable as well in this setting. The corresponding quantitative evaluations are available in Table 4. Due to space constraints, we show objects with interesting properties here. Please see the supplemental for sampled results. 

chairs and 3514 cars with a predefined split across object instances. All images have resolution 128 {\texttimes}128. 

A single model is trained for each object class with 50 random views per object instance, randomly sampling either one or two of the training views to encode. For testing, We use 251 novel views on an Archimedean spiral for each object in the test set of object instances, fixing 1-2 informative views as input. We report our performance in comparison with state-of-the-art baselines in Table 2, and show selected qualitative results in Fig. 4. We also include the quantitative results of baselines TCO [40] and dGQN [9] reported in [39] where applicable, and the values available in the recent works ENR [8] and GRF [43] in this setting. 

PixelNeRF achieves noticeably superior results despite solving a problem significantly harder than SRN because we: 1) use feed-forward prediction, without test-time optimization, 2) do not use ground-truth absolute camera poses at test-time, 3) use view instead of canonical space. 

While we found appreciable improvements over baselines in the simplest category-specific benchmark, our method is by no means constrained to it. We show in Table 4 and Fig. 5 that our approach offers a much greater advantage in the category-agnostic setting of [21, 28], where we train a single model to the 13 largest categories of ShapeNet. Please see the supplemental for randomly sampled results. 

We follow community standards for 2D-supervised methods on multiple ShapeNet categories [28, 16, 21] and use the renderings and splits from Kato et al. [16], which provide 24 fixed elevation views of 64 {\texttimes}64 resolution for each object instance. During both training and evaluation, a random view is selected as the input view for each object and shared across all baselines. The remaining 23 views are used as target views for computing metrics (see {\textsection}5). 

Taking a step towards reconstruction in less controlled capture scenarios, we perform experiments on ShapeNet data in three more challenging setups: 1) unseen object categories, 2) multiple-object scenes, and 3) simulation-to-real 

Figure 6: Generalization to unseen categories. We evaluate a model trained on planes, cars, and chairs on 10 unseen ShapeNet categories. We find that the model is able to synthesize reasonable views even in this difficult case. 

Figure 7: 360{\textopenbullet}view prediction with multiple objects. We show qualitative results of our method compared with SRN on scenes composed of multiple ShapeNet chairs. We are easily able to handle this setting, because our prediction is done in view space; in contrast, SRN predicts in canonical space, and struggles with scenes that cannot be aligned in such a way. 

Table 5: Image quality metrics for challenging ShapeNet tasks. (Left) Average metrics on 10 unseen categories for models trained on only planes, cars, and chairs. See the supplemental for a breakdown by category. (Right) Average metrics for two-view reconstruction for scenes with multiple ShapeNet chairs. 

Figure 8: Results on real car photos. We apply the car model from {\textsection} 5.1.1 directly to images from the Stanford cars dataset [18]. The background has been masked out using PointRend [17]. The views are rotations about the view-space vertical axis. 

transfer on car images. In these settings, successful reconstruction requires geometric priors; recognition or retrieval alone is not sufficient. 

Generalization to novel categories. We first aim to reconstruct ShapeNet categories which were not seen in training. 

Unlike the more standard category-agnostic task described in the previous section, such generalization is impossible with semantic information alone. The results in Table 5 and Fig. 6 suggest our method learns intrinsic geometric and appearance priors which are fairly effective even for objects quite distinct from those seen during training. 

We loosely follow the protocol used for zero-shot cross-category reconstruction from [53, ?]. Note that our baselines [39, 28] do not evaluate in this setting, and we adapt them for the sake of comparison. We train on the airplane, car, and chair categories and test on 10 categories unseen during training, continuing to use the Kato et al. renderings described in {\textsection}5.1.2. 

Multiple-object scenes. We further perform few-shot 360{\textopenbullet}reconstruction for scenes with multiple randomly placed and oriented ShapeNet chairs. In this setting, the network cannot rely solely on semantic cues for correct object placement and completion. The priors learned by the network must be applicable in an arbitrary coordinate system. We show in Fig. 7 and Table 5 that our formulation allows us to perform well on these simple scenes without additional design modifications. In contrast, SRN models scenes in a canonical space and struggles on held-out scenes. 

We generate training images composed with 20 views randomly sampled on the hemisphere and render test images composed of a held out test set of chair instances, with 50 views sampled on an Archimedean spiral. During training, we randomly encode two input views; at test-time, we fix two informative views across the compared methods. In the supplemental, we provide example images from our dataset as well as additional quantitative results and qualitative comparisons with varying numbers of input views. 

Sim2Real on Cars. We also explore the performance of pixelNeRF on real images from the Stanford cars dataset [18]. We directly apply car model from {\textsection}5.1.1 without any fine-tuning. As seen in Fig. 8, the network trained on synthetic data effectively infers shape and texture of the real cars, suggesting our model can transfer beyond the synthetic domain. 

Synthesizing the 360{\textopenbullet}background from a single view is nontrivial and out of the scope for this work. For this demonstration, the off-the-shelf PointRend [17] segmentation model is used to remove the background. 

Finally, we demonstrate that our method is applicable for few-shot wide baseline novel-view synthesis on real scenes in the DTU MVS dataset [14]. Learning a prior for view synthesis on this dataset poses significant challenges: not only does it consist of more complex scenes, without clear semantic similarities across scenes, it also contains inconsistent backgrounds and lighting between scenes. More-

Figure 9: Wide baseline novel-view synthesis on a real image dataset. We train our model to distinct scenes in the DTU MVS dataset [14]. Perhaps surprisingly, even in this case, our model is able to infer novel views with reasonable quality for held-out scenes without further test-time optimization, all from only three views. Note the train/test sets share no overlapping scenes. 

Figure 10: PSNR of few-shot feed-forward DTU reconstruction. We show the quantiles of PSNR on DTU for our method and NeRF, given 1, 3, 6, or 9 input views. Separate NeRFs are trained per scene and number of input views, while our method requires only a single model trained with 3 encoded views. 

over, under 100 scenes are available for training. We found that the standard data split introduced in MVSNet [51] contains overlap between scenes of the training and test sets. Therefore, for our purposes, we use a different split of 88 training scenes and 15 test scenes, in which there are no shared or highly similar scenes between the two sets. Images are down-sampled to a resolution of 400 {\texttimes}300. 

We train one model across all training scenes by encoding 3 random views of a scene. During test time, we choose a set of fixed informative input views shared across all instances. We show in Fig. 9 that our method can perform view synthesis on the held-out test scenes. We further quantitatively compare the performance of our feed-forward model with NeRF optimized to the same set of input views in Fig. 10. Note that training each of 60 NeRFs took 14 hours; in contrast, pixelNeRF is applied to new scenes immediately without any test-time optimization. 

6. Discussion 

We have presented pixelNeRF, a framework to learn a scene prior for reconstructing NeRFs from one or a few images. Through extensive experiments, we have established that our approach can be successfully applied in a variety of settings. We addressed some shortcomings of NeRF, but there are challenges yet to be explored: 1) Like NeRF, our rendering time is slow, and in fact, our runtime increases linearly when given more input views. Further, some methods (e.g. [28, 21]) can recover a mesh from the image enabling fast rendering and manipulation afterwards, while NeRF-based representations cannot be converted to meshes very reliably. Improving NeRF{\textquoteright}s efficiency is an important research question that can enable real-time applications. 2) As in the vanilla NeRF, we manually tune ray sampling bounds tn, tf and a scale for the positional encoding. Making NeRF-related methods scale-invariant is a crucial challenge. 3) While we have demonstrated our method on real data from the DTU dataset, we acknowledge that this dataset was captured under controlled settings and has matching camera poses across all scenes with limited viewpoints. Ultimately, our approach is bottlenecked by the availability of large-scale wide baseline multi-view datasets, limiting the applicability to datasets such as ShapeNet and DTU. Learning a general prior for 360{\textopenbullet}scenes in-the-wild is an exciting direction for future work. 

Acknowledgements 

We thank Shubham Goel and Hang Gao for comments on the text. We also thank Emilien Dupont and Vincent Sitzmann for helpful discussions. 

References 
