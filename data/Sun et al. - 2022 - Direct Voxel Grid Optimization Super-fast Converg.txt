chengsun@gapp.nthu.edu.tw 

Min Sun1,3 

sunmin@ee.nthu.edu.tw 

Hwann-Tzong Chen1,4 

htchen@cs.nthu.edu.tw 

We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF{\textquoteright}s quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO. 

1. Introduction 

Achieving free-viewpoint navigation of 3D objects or scenes from only a set of calibrated images as input is a demanding task. For instance, it enables online product showcase to provide an immersive user experience comparing to static image demonstration. Recently, Neural Radiance Fields (NeRFs) [37] have emerged as powerful representations yielding state-of-the-art quality on this task. 

23.64 PSNR. 32.72 PSNR. 34.22 PSNR. Ours at 2.33 mins. Ours at 5.07 mins. Ours at 13.72 mins. (a) The synthesized novel view by our method at three training checkpoints. 

(b) The training curves of different methods on Lego scene. The training time of each method is measured on our machine with a single NVIDIA RTX 2080 Ti GPU. 

Figure 1. Super-fast convergence by our method. The key to our speedup is to optimize the volume density modeled in a dense voxel grid directly. Note that our method needs neither a conversion step from any trained implicit model (e.g., NeRF) nor a cross-scene pretraining, i.e., our voxel grid representation is directly and efficiently trained from scratch for each scene. 

Despite its effectiveness in representing scenes, NeRF is known to be hampered by the need of lengthy training time and the inefficiency in rendering new views. This makes NeRF infeasible for many application scenarios. Several follow-up methods [15, 18, 29, 30, 42, 43,66] have shown significant speedup of FPS in testing phase, some of which even achieve real-time rendering. However, only few methods show training times speedup, and the improvements are not comparable to ours [1,10,31] or lead to worse quality [6,59]. On a single GPU machine, several hours of per scene optimization or a day of pretraining is typically required. 

To reconstruct a volumetric scene representation from a set of images, NeRF uses multilayer perceptron (MLP) to implicitly learn the mapping from a queried 3D point (with a viewing direction) to its colors and densities. The queried properties along a camera ray can then be accumulated into a pixel color by volume rendering techniques. Our work takes inspiration from the recent success [15, 18, 66] that uses classic voxel grid to explicitly store the scene properties, 

which enables real-time rendering and shows good quality. However, their methods can not train from scratch and need a conversion step from the trained implicit model, which causes a bottleneck to the training time. 

The key to our speedup is to use a dense voxel grid to directly model the 3D geometry (volume density). Developing an elaborate strategy for view-dependent colors is not in the main scope of this paper, and we simply use a hybrid representation (feature grid with shallow MLP) for colors. 

Directly optimizing the density voxel grid leads to super-fast converges but is prone to suboptimal solutions, where our method allocates {\textquotedblleft}cloud{\textquotedblright} at free space and tries to fit the photometric loss with the cloud instead of searching a geometry with better multi-view consistency. Our solution to this problem is simple and effective. First, we initialize the density voxel grid to yield opacities very close to zero everywhere to avoid the geometry solutions being biased toward the cameras{\textquoteright} near planes. Second, we give a lower learning rate to voxels visible to fewer views, which can avoid redundant voxels that are allocated just for explaining the observations from a small number of views. We show that the proposed solutions can successfully avoid the suboptimal geometry and work well on the five datasets. 

Using the voxel grid to model volume density still faces a challenge in scalability. For parsimony, our approach automatically finds a BBox tightly encloses the volume of interest to allocate the voxel grids. Besides, we propose post-activation{\textemdash}applying all the activation functions after trilinearly interpolating the density voxel grid. Previous work either interpolates the voxel grid for the activated opacity or uses nearest-neighbor interpolation, which results in a smooth surface in each grid cell. Conversely, we prove mathematically and empirically that the proposed post-activation can model (beyond) a sharp linear surface within a single grid cell. As a result, we can use fewer voxels to achieve better qualities{\textemdash}our method with 1603 dense voxels already outperforms NeRF in most cases. 

In summary, we have two main technical contributions. First, we implement two priors to avoid suboptimal geometry in direct voxel density optimization. Second, we propose the post-activated voxel-grid interpolation, which enables sharp boundary modeling in lower grid resolution. The resulting key merits of this work are highlighted as follows: 

to 13003 to achieve NeRF-comparable quality. 

2. Related work 

Representations for novel view synthesis. Images synthesis from novel viewpoints given a set of images capturing the scene is a long-standing task with rich studies. Previous work has presented several scene representations reconstructed from the input images to synthesize the unobserved viewpoints. Lumigraph [4, 16] and light field representation [7, 23, 24, 46] directly synthesize novel views by interpolating the input images but require very dense scene capture. Layered depth images [11, 45, 47, 57] work for sparse input views but rely on depth maps or estimated depth with sacrificed quality. Mesh-based representations [8, 54, 58, 63] can run in real-time but have a hard time with gradient-based optimization without template meshes provided. Recent approaches employ 2D/3D Con-volutional Neural Network (CNNs) to estimate multiplane images (MPIs) [12, 26, 36, 51, 56, 71] for forward-facing captures; estimate voxel grid [17, 32, 48] for inward-facing captures. Our method uses gradient-descent to optimize voxel grids directly and does not rely on neural networks to predict the grid values, and we still outperform the previous works [17, 32, 48] with CNNs by a large margin. 

Neural radiance fields. Recently, NeRF [37] stands out to be a prevalent method for novel view synthesis with rapid progress, which takes a moderate number of input images with known camera poses. Unlike traditional explicit and discretized volumetric representations (e.g., voxel grids and MPIs), NeRF uses coordinate-based multilayer perceptrons (MLP) as an implicit and continuous volumetric representation. NeRF achieves appealing quality and has good flexibility with many follow-up extensions to various setups, e.g., relighting [2, 3, 50, 70], deformation [13, 38{\textendash}40, 55], self-calibration [19, 27, 28, 35, 61], meta-learning [52], dynamic scene modeling [14, 25, 33, 41, 64], and generative modeling [5, 22, 44]. Nevertheless, NeRF has unfavorable limitations of lengthy training progress and slow rendering speed. In this work, we mainly follow NeRF{\textquoteright}s original setup, while our method can optimize the volume density explicitly encoded in a voxel grid to speed up both training and testing by a large margin with comparable quality. 

Hybrid volumetric representations. To combine NeRF{\textquoteright}s implicit representation and traditional grid representations, the coordinate-based MLP is extended to also conditioning on the local feature in the grid. Recently, hybrid vox-els [18, 30] and MPIs [62] representations have shown success in fast rendering speed and result quality. We use hybrid representation to model view-dependent color as well. 

Fast NeRF rendering. NSVF [30] uses octree in its hybrid representation to avoid redundant MLP queries in free 

Figure 2. Approach overview. We first review NeRF in Sec. 3. In Sec. 4, we present a novel post-activated density voxel grid to support sharp surface modeling in lower grid resolutions. In Sec. 5, we show our approach to the reconstruction of radiance field with super-fast convergence, where we first find a coarse geometry in Sec. 5.1 and then reconstruct the fine details and view-dependent effects in Sec. 5.2. 

space. However, NSVF still needs many training hours due to the deep MLP in its representation. Recent methods further use thousands of tiny MLPs [43] or explicit volumetric representations [15,18,62,66] to achieve real-time rendering. Unfortunately, gradient-based optimization is not directly applicable to their methods due to their topological data structures or the lack of priors. As a result, these methods [15, 18, 43, 62, 66] still need a conversion step from a trained implicit model (e.g., NeRF) to their final representation that supports real-time rendering. Their training time is still burdened by the lengthy implicit model optimization. 

Fast NeRF convergence. Recent works that focus on fewer input views setup also bring faster convergence as a side benefit. These methods rely on generalizable pre-training [6, 59, 67] or external MVS depth information [10, 31], while ours does not. Further, they still require several per-scene fine-tuning hours [10] or fail to achieve NeRF quality in the full input-view setup [6,59,67]. Most recently, NeuRay [31] shows NeRF{\textquoteright}s quality with 40 minutes per-scene training time in the lower-resolution setup. Under the same GPU spec, our method achieves NeRF{\textquoteright}s quality in 15 minutes per scene on the high-resolution setup and does not require depth guidance and cross-scene pre-training. 

3. Preliminaries 

To represent a 3D scene for novel view synthesis, Neural Radiance Fields (NeRFs) [37] employ multilayer perceptron (MLP) networks to map a 3D position x and a viewing direction d to the corresponding density \ensuremath{\sigma} and view-dependent color emission c: 

To render the color of a pixel C{\textasciicircum}(r), we cast the ray r from the camera center through the pixel; K points are then sampled on r between the pre-defined near and far planes; the K ordered sampled points are then used to query for their densities and colors \{(\ensuremath{\sigma}i, ci)\}K i=1 (MLPs are queried in NeRF). Finally, the K queried results are accumulated into a single color with the volume rendering quadrature in accordance with the optical model given by Max [34]: 

(2a) 

(2b) 

(2c) 

where \ensuremath{\alpha}i is the probability of termination at the point i; Ti is the accumulated transmittance from the near plane to point i; \ensuremath{\delta}i is the distance to the adjacent sampled point, and cbg is a pre-defined background color. 

Given the training images with known poses, NeRF model is trained by minimizing the photometric MSE between the observed pixel color C(r) and the rendered color C{\textasciicircum}(r): 

(3) 

where R is the set of rays in a sampled mini-batch. 

Figure 3. A single grid cell with post-activation is capable of modeling sharp linear surfaces. Left: We depict the toy task for a 2D grid cell, where a grid cell is optimized for the linear surface (decision boundary) across it. Right: Each column shows an example task for three different methods. The results show that a single grid cell with post-activation (Eq. (6c)) is adequate to recover faithfully the linear surface. Conversely, pre-activation (Eq. (6a)) and in-activation (Eq. (6b)) fail to accomplish the tasks as they can only fit into smooth results, and thus would require more grid cells to recover the surface detail. See supplementary material for the mathematical proof. 

4. Post-activated density voxel grid 

Voxel-grid representation. A voxel-grid representation models the modalities of interest (e.g., density, color, or feature) explicitly in its grid cells. Such an explicit scene representation is efficient to query for any 3D positions via interpolation: 

where x is the queried 3D point, V is the voxel grid, C is the dimension of the modality, and Nx {\textperiodcentered} Ny {\textperiodcentered} Nz is the total number of voxels. Trilinear interpolation is applied if not specified otherwise. 

Density voxel grid for volume rendering. Density voxel grid, V (density) , is a special case with C = 1, which stores the density values for volume rendering (Eq. (2)). We use \ensuremath{\sigma}{\textasciidieresis} \ensuremath{\in} R to denote the raw voxel density before applying the density activation (i.e., a mapping of R {\textrightarrow} R\ensuremath{\geq}0). In this work, we use the shifted softplus mentioned in Mip-NeRF [1] as the density activation: 

(5) 

where the shift b is a hyperparameter. Using softplus instead of ReLU is crucial to optimize voxel density directly, as it is irreparable when a voxel is falsely set to a negative value with ReLU as the density activation. Conversely, softplus allows us to explore density very close to 0. 

Sharp decision boundary via post-activation. The interpolated voxel density is processed by softplus (Eq. (5)) and alpha (Eq. (2b)) functions sequentially for volume rendering. We consider three different orderings{\textemdash}pre-activation, in-activation, and post-activation{\textemdash}of plugging in the tri-linear interpolation and performing the activation, given a 

(a) Visual comparison of image fitting results under grid resolution (H/5){\texttimes}(W/5). The first row is the results of pre-, in-, and post-activation. The second row is their per-pixel absolute difference to the target image. 

(b) PSNRs achieved by pre-, in-and post-activation under different grid strides. A grid stride s means that the grid resolution is (H/s) {\texttimes} (W/s). The black dashed line highlights that post-activation with stride \ensuremath{\approx} 8.5 can achieve the same PSNR as pre-activation with stride 2 in this example. 

Figure 4. Toy example on image fitting. The target 2D image is binary to imitate the scenario that most of the 3D space is either occupied or free. The objective is to reconstruct the target image by a low-resolution 2D grid. In each optimization step, the tunable 2D grid is queried by interpolation with pre-activation (Eq. (6a)), in-activation (Eq. (6b)), or post-activation (Eq. (6c)) to minimize the mean squared error to the target image. The result reveals that the post-activation can produce sharp boundaries even with low grid resolution (Fig. 4a) and is much better than the other two under various grid resolutions (Fig. 4b). This motivates us to model the 3D geometry directly via voxel grids with post-activation. 

queried 3D point x: 

The input \ensuremath{\delta} to the function alpha (Eq. (2b)) is omitted for simplicity. We show that the post-activation, i.e., applying all the non-linear activation after the trilinear interpolation, is capable of producing sharp surfaces (decision boundaries) with much fewer grid cells. In Fig. 3, we use a 2D grid cell as an example to show that a grid cell with post-activation can produce a sharp linear boundary, while pre- and inactivation can only produce smooth results and thus require more cells for the surface detail. In Fig. 4, we further use binary image regression as a toy example to compare their capability, which also shows that post-activation can achieve a much better efficiency in grid cell usage. 

5. Fast and direct voxel grid optimization 

We depict an overview of our approach in Fig. 2. In Sec. 5.1, we first search the coarse geometry of a scene. In Sec. 5.2, we then reconstruct the fine detail including view-dependent effects. Hereinafter we use superscripts (c) and (f) to denote variables in the coarse and fine stages. 

Typically, a scene is dominated by free space (i.e., unoccupied space). Motivated by this fact, we aim to efficiently find the coarse 3D areas of interest before reconstructing the fine detail and view-dependent effect that require more computation resources. We can thus greatly reduce the number of queried points on each ray in the later fine stage. 

Coarse scene representation. We use a coarse denactivation (Eq. (6c)) to model scene geometry. We only model view-invariant color emissions by V (rgb)(c) \ensuremath{\in} in the coarse stage. A query of any 3D point x is efficient with interpolation: 

(7a) 

(7b) 

where c(c) \ensuremath{\in} R3 is the view-invariant color and \ensuremath{\sigma}{\textasciidieresis}(c) \ensuremath{\in} R is the raw volume density. 

Coarse voxels allocation. We first find a bounding box (BBox) tightly enclosing the camera frustums of training views (See the red BBox in Fig. 2c for an example). Our voxel grids are aligned with the BBox. Let Lx(c) y z, L(c) , L(c) be the lengths of the BBox and M(c) be the hyperparameter for the expected total number of voxels in the coarse stage. The voxel size is s(c) = 3 L(c) x {\textperiodcentered} L(c) y {\textperiodcentered} Lz (c) /M(c), so there are 

Coarse-stage points sampling. On a pixel-rendering ray, we sample query points as 

(8a) 

(8b) 

where o is the camera center, d is the ray-casting direction, t(near) is the camera near bound, and \ensuremath{\delta}(c) is a hyperparameter for the step size that can be adaptively chosen according to the voxel size s(c). The query index i ranges from 1 to t(far) {\textperiodcentered} d2/\ensuremath{\delta}(c), where t(far) is the camera far bound, so the last sampled point stops nearby the far plane. 

Prior 1: low-density initialization. At the start of training, the importance of points far from a camera is down-weighted due to the accumulated transmittance term in Eq. (2c). As a result, the coarse density voxel grid V (density)(c) could be accidentally trapped into a suboptimal {\textquotedblleft}cloudy{\textquotedblright} geometry with higher densities at camera near planes. We thus have to initialize V (density)(c) more carefully to ensure that all sampled points on rays are visible to the cameras at the beginning, i.e., the accumulated transmittance rates Tis in Eq. (2c) are close to 1. 

In practice, we initialize all grid values in V (density)(c) to 0 and set the bias term in Eq. (5) to 

(9) 

where \ensuremath{\alpha}(init)(c) is a hyperparameter. Thereby, the accumulated transmittance Ti is decayed by 1 \ensuremath{-} \ensuremath{\alpha}(init)(c) \ensuremath{\approx} 1 for a ray that traces forward a distance of a voxel size s(c). See supplementary material for the derivation and proof. 

Prior 2: view-count-based learning rate. There could be some voxels visible to too few training views in real-world capturing, while we prefer a surface with consistency in many views instead of a surface that can only explain few views. In practice, we set different learning rates for different grid points in V (density)(c). For each grid point indexed by j, we count the number of training views nj to which point j is visible, and then scale its base learning rate by nj/nmax, where nmax is the maximum view count over all grid points. 

Training objective for coarse representation. The scene representation is reconstructed by minimizing the mean square error between the rendered and observed colors. To regularize the reconstruction, we mainly use background entropy loss to encourage the accumulated alpha values to concentrate on background or foreground. Please refer to supplementary material for more detail. 

Given the optimized coarse geometry V (density)(c) in Sec. 5.1, we now can focus on a smaller subspace to reconstruct the surface details and view-dependent effects. The optimized V (density)(c) is frozen in this stage. 

Fine scene representation. In the fine stage, we use V (density)(f)a higher-resolution density voxel grid \ensuremath{\in} with post-activated interpolation (Eq. (6c)). , where(f)N{\texttimes} z (f)N{\texttimes} y (f)N{\texttimes} z (f)N{\texttimes} y (f)1 N{\texttimes}R x Note that, alternatively, it is also possible to use a more advanced data structure [18, 30, 66] to refine the voxel grid based on the current V (density)(c) but we leave that for future work. To model view-dependent color emission, we opt to use an explicit-implicit hybrid representation as we find in our prior experiments that an explicit representation tends to produce worse results, and an implicit representation entails a slower training speed. Our hybrid representation comprises (f)i) D is a hyperparameter for feature-space dimension, and ii) a shallow MLP parameteriszed by \ensuremath{\Theta}. Finally, queries of 3D points x and viewing-direction d are performed by 

where c(f) \ensuremath{\in} R3 is the view-dependent color emission and \ensuremath{\sigma}{\textasciidieresis}(f) \ensuremath{\in} R is the raw volume density in the fine stage. Positional embedding [37] is applied on x, d for the MLP\ensuremath{\Theta} (rgb) . 

Known free space and unknown space. A query point is in the known free space if the post-activated alpha value from the optimized V (density)(c) is less than the threshold \ensuremath{\tau} (c). Otherwise, we say the query point is in the unknown space. 

Fine voxels allocation. We densely query V (density)(c) to find a BBox tightly enclosing the unknown space, where are the lengths of the BBox. The only hyper-parameter is the expected total number of voxels M (f). The (f) canN,z (f)N,y (f)L, z (f)L, y (f)L x (f) (f)voxel size and the grid dimensions Ns x then be derived automatically from M (f) as per Sec. 5.1. 

Progressive scaling. Inspired by NSVF [30], we progressively scale our voxel grid V (density)(f) and V (feat)(f). Let pg ckpt be the set of checkpoint steps. The initial number of voxels is set to M (f)/2|pg ckpt|. When reaching the training step in pg ckpt, we double the number of voxels such that the number of voxels after the last checkpoint is M (f); (f)N, z (f)N, y (f) (f)the voxel size and the grid dimensions Ns x are updated accordingly. Scaling our scene representation is much simpler. At each checkpoint, we resize our voxel grids, V (density)(f) and V (feat)(f) , by trilinear interpolation. 

Fine-stage points sampling. The points sampling strategy is similar to Eq. (8) with some modifications. We first filter out rays that do not intersect with the known free space. For each ray, we adjust the near-and far-bound, t(near) and t(far) , to the two endpoints of the ray-box intersection. We do not adjust t(near) if x0 is already inside the BBox. 

Free space skipping. Querying V (density)(c) (Eq. (7a)) is faster than querying V (density)(f) (Eq. (10a)); querying for view-dependent colors (Eq. (10b)) is the slowest. We improve fine-stage efficiency by free space skipping in both training and testing. First, we skip sampled points that are in the known free space by checking the optimized V (density)(c) (Eq. (7a)). Second, we further skip sampled points in unknown space with low activated alpha value (threshold at \ensuremath{\tau} (f)) by querying V (density)(f) (Eq. (10a)). 

Training objective for fine representation. We use the same training losses as the coarse stage, but we use a smaller weight for the regularization losses as we find it empirically leads to slightly better quality. 

6. Experiments 

We choose the same hyperparameters generally for all scenes. The expected numbers of voxels are set to M (c) = 1003 and M (f) = 1603 in coarse and fine stages if not stated otherwise. The activated alpha values are initialized to be \ensuremath{\alpha}(init)(c) = 10\ensuremath{-}6 in the coarse stage. We use a higher \ensuremath{\alpha}(init)(f) = 10\ensuremath{-}2 as the query points are concentrated on the optimized coarse geometry in the fine stage. The points sampling step sizes are set to half of the voxel sizes, i.e., \ensuremath{\delta}(c) = 0.5 {\textperiodcentered} s(c) and \ensuremath{\delta}(f) = 0.5 {\textperiodcentered} s(f). The shallow MLP layer comprises two hidden layers with 128 channels. We use the Adam optimizer [20] with a batch size of 8,192 rays to optimize the coarse and fine scene representations for 10k and 20k iterations. The base learning rates are 0.1 for all voxel grids and 10\ensuremath{-}3 for the shallow MLP. The exponential learning rate decay is applied. See supplementary material for detailed hyperparameter setups. 

Quantitative evaluation on the synthesized novel view. We first quantitatively compare the novel view synthesis results in Tab. 1. PSNR, SSIM [60], and LPIPS [69] are employed as evaluation metrics. Our model with M (f) = 1603 voxels already outperforms the original NeRF [37] and the improved JaxNeRF [9] re-implementation. Besides, our results are also comparable to most of the recent methods, except JaxNeRF+ [9] and Mip-NeRF [1]. Moreover, our per-scene optimization only takes about 15 minutes, while all the methods after NeRF in Tab. 1 need quite a few hours per scene. We also show our model with M (f) = 2563 voxels, which significantly improves our results under all metrics and achieves more comparable results to JaxNeRF+ and Mip-NeRF. We defer detail comparisons on the much simpler DeepVoxels [48] dataset to supplementary material, where we achieve 45.83 averaged PSNR and outperform NeRF{\textquoteright}s 40.15 and IBRNet{\textquoteright}s 42.93. 

Training time comparisons. The key merit of our work is the significant improvement in convergence speed with NeRF-comparable quality. In Tab. 2, we show a training 

Table 1. Quantitative comparisons for novel view synthesis. Our method excels in convergence speed, i.e., 15 minutes per scene compared to many hours or days per scene using other methods. Besides, our rendering quality is better than the original NeRF [37] and the improved JaxNeRF [9] on the four datasets under all metrics. We also show comparable results to most of the recent methods. 

time comparison. We also show GPU specifications after each reported time as it is the main factor affecting run-time. 

NeRF [37] with a more powerful GPU needs 1{\textendash}2 days per scene to achieve 31.01 PSNR, while our method achieves a superior 31.95 and 32.80 PSNR in about 15 an 22 minutes per scene respectively. MVSNeRF [6], IBRNet [59], and NeuRay [31] also show less per-scene training time than NeRF but with the additional cost to run a generalizable cross-scene pre-training. MVSNeRF [6], after pre-training, optimizes a scene in 15 minutes as well, but the PSNR is degraded to 28.14. IBRNet [59] shows worse PSNR and longer training time than ours. NeuRay [31] originally reports time in lower-resolution (NeuRay-Lo) setup, and we receive the training time of the high-resolution (NeuRay-Hi) setup from the authors. NeuRay-Hi achieves 32.42 PSNR and requires 23 hours to train, while our method with M (f) = 2563 voxels achieves superior 32.80 in about 22 minutes. For the early-stopped NeuRay-Hi, unfortunately, only its training time is retained (early-stopped NeuRay-Lo achieves NeRF-similar PSNR). NeuRay-Hi still needs 70 minutes to train with early stopping, while we only need 15 minutes to achieve NeRF-comparable quality and do not rely on generalizable pre-training or external depth information. Mip-NeRF [1] has similar run-time to NeRF but with much better PSNRs, which also signifies using less training time to achieve NeRF{\textquoteright}s PSNR. We train early-stopped Mip-NeRFs on our machine and show the averaged PSNR and training 

Table 2. Training time comparisons. We take the training time and GPU specifications reported in previous works directly. A V100 GPU can run faster and has more storage than a 2080Ti GPU. Our method achieves good PSNR in a significantly less per-scene optimization time. 

time. The early-stopped Mip-NeRF achieves 30.85 PSNR after 6 hours of training, while we can achieve 31.95 PSNR in just 15 minutes. 

Rendering speed comparisons. Improving test-time rendering speed is not the main focus of this work, but we still achieve \ensuremath{\sim} 45{\texttimes} speedups from NeRF{\textemdash}0.64 seconds versus 29 seconds per 800 {\texttimes} 800 image on our machine. 

Qualitative comparison. Fig. 5 shows our rendering results on the challenging parts and compare them with the results (better than NeRF{\textquoteright}s) provided by PlenOctrees [66]. 

Figure 5. Qualitative comparisons on the challenging parts. Top: On ficus scene, we do not show blocking artifacts as PlenOc-tree and recover the pot better. Middle: We produce blurrier results on ship{\textquoteright}s body and rigging, but we do not have the background artifacts. Bottom: On real-world captured Ignatius, we show better quality without blocking artifacts (left) and recover the color tone better (right). See supplementary material for more visualizations. 

We mainly validate the effectiveness of the two proposed techniques{\textemdash}post-activation and the imposed priors{\textemdash}that enable voxel grids to model scene geometry with NeRF-comparable quality. We subsample two scenes for each dataset. See supplementary material for more detail and additional ablation studies on the number of voxels, point-sampling step size, progressive scaling, free space skipping, view-dependent colors modeling, and the losses. 

Effectiveness of the post-activation. We show in Sec. 4 that the proposed post-activated trilinear interpolation enables the discretized grid to model sharper surfaces. In Tab. 3, we compare the effectiveness of post-activation in scene reconstruction for novel view synthesis. Our grid in the fine stage consists of only 1603 voxels, where nearest-neighbor interpolation results in worse quality than trilinear interpolation. The proposed post-activation can improve the results further compared to pre-and in-activation. We find that we gain less in the real-world captured BlendedMVS and Tanks and Temples datasets. The intuitive reason is that real-world data introduces more uncertainty (e.g., inconsistent lightning, SfM error), which results in multi-view inconsistent and blurrier surfaces. Thus, the advantage is lessened for scene representations that can model sharper surfaces. We speculate that resolving the uncertainty in future work can increase the gain of the proposed post-activation. 

Effectiveness of the imposed priors. As discussed in Sec. 5.1, it is crucial to initialize the voxel grid with low density to avoid suboptimal geometry. The hyperparameter \ensuremath{\alpha}(init)(c) controls the initial activated alpha values via Eq. (9). In Tab. 4, we compare the quality with different \ensuremath{\alpha}(init)(c) and the view-count-based learning rate. Without the low-density 

Table 3. Effectiveness of the post-activation. Geometry modeling with density voxel grid can achieve better PSNRs by using the proposed post-activated trilinear interpolation. 

Table 4. Effectiveness of the imposed priors. We compare our different settings in the coarse geometry search. Top: We show their impacts on the final PSNRs after the fine stage reconstruction. Bottom: We visualize the allocated voxels by coarse geometry search on the Truck scene. Overall, low-density initialization is essential; using \ensuremath{\alpha}(init)(c) = 10\ensuremath{-}6 and view-count-based learning rate generally achieves cleaner voxels allocation in the coarse stage and better PSNR after the fine stage. 

initialization, the quality drops severely for all the scenes. When \ensuremath{\alpha}(init)(c) = 10\ensuremath{-}7 , we have to train the coarse stage of some scenes for more iterations. The effective range of \ensuremath{\alpha}(init)(c) is scene-dependent. We find \ensuremath{\alpha}(init)(c) = 10\ensuremath{-}6 generally works well on all the scenes in this work. Finally, using a view-count-based learning rate can further improve the results and allocate noiseless voxels in the coarse stage. 

7. Conclusion 

Our method directly optimizes the voxel grid and achieves super-fast convergence in per-scene optimization with NeRF-comparable quality{\textemdash}reducing training time from many hours to 15 minutes. However, we do not deal with the unbounded or forward-facing scenes, while we believe our method can be a stepping stone toward fast convergence in such scenarios. We hope our method can boost the progress of NeRF-based scene reconstruction and its applications. Acknowledgements: This work was supported in part by the MOST grants 110-2634-F-001-009 and 110-2622-8-007-010-TE2 of Taiwan. We are grateful to National Center for High-performance Computing for providing computational resources and facilities. 

References 
