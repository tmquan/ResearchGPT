XctNet: Reconstruction network of volumetric images from a single X-ray image 
X-ray Volumetric images 
ABSTRACT  
Conventional Computed Tomography (CT) produces volumetric images by computing inverse Radon transformation using X-ray projections from different angles, which results in high dose radiation, long reconstruction time and artifacts. Biologically, prior knowledge or experience can be utilized to identify volumetric information from 2D images to certain extents. a deep learning network, XctNet, is proposed to gain this prior knowledge from 2D pixels and produce volumetric data. In the proposed framework, self-attention mechanism is used for feature adaptive optimization; multiscale feature fusion is used to further improve the reconstruction accuracy; a 3D branch generation module is proposed to generate the details of different generation fields. Comparisons are made with the state-of-arts methods using public dataset and XctNet shows significantly higher image quality as well as better accuracy (SSIM and PSNR values of XctNet are 0.8681 and 29.2823 respectively).   
1. Introduction 
Image based 3D reconstruction, which is to infer 3D shapes from single or multiple 2D images, has been explored in the field of computer vision for decades ((\ensuremath{<}\ensuremath{>})Han et al., 2021) and has become the basis of many fields, such as robot navigation, 3D modeling and animation, object recognition, scene understanding, medical diagnosis, etc. However, it is not straightforward to extract volumetric information from digital images without disparity knowledge from stereo correspondence, due to the lack of approaches to derive depth information from pixels. Pro-jectional radiography, a conventional way to observe the inside of objects or bodies, is no different than normal photography, except that the pixels carry rich observations of transparent volumetric structure other than opaque surface. Specifically, in X-ray radiographs, each pixel is the line integral of attenuation data following Radon transformation in 2D space, so that the inversion of radiographs is not 2D{\textendash}3D point conversion but the conversion from 2D pixel to spatial line distribution. Therefore, 3D reconstruction from radiographs is an even more chal-
Biologically, although our eye-brain vision system does not make 3D reconstruction from plain pixels, we can still partially obtain the hidden spatial information from subtle evidence like: shadow, occlusion, light/ 
Available online 15 April 2022 Received 18 December 2021; Received in revised form 15 March 2022; Accepted 8 April 2022   0895-6111/{\textcopyright} 2022 Elsevier Ltd. All rights reserved. 
shade, relative size, etc. The knowledge we use in this evidence-based stereo reconstruction process can be defined as prior knowledge, which plays an essential part in human vision-based judgment and identification system. When we look at photographs or images by bare eyes, the relative spatial relation of the objects or bodies could always be deduced by combining pixel information with prior knowledge. Similarly, radiologists are able to tell the spatial information of human bodies from radiographs by applying prior knowledge from anatomy and everyday practice. Therefore, from the aspect of biomimetic, prior knowledge has potential to reconstruct 3D information at least partially from radiographs theoretically. 
Deep learning, which shows great advantages over traditional methods in fitting complex nonlinear mathematical relations, has brought an evolution to numerous medical fields such as medical image segmentation, lesion area recognition, medical image registration, etc. ((\ensuremath{<}\ensuremath{>})Feng et al., 2020; (\ensuremath{<}\ensuremath{>})Schwartz et al., 2019; (\ensuremath{<}\ensuremath{>})Singh et al., 2020). The possibilities of 3D reconstruction from 2D radiographs have not been observed for long, but only in recent years does the attempts of the inverse mapping emerge. (\ensuremath{<}\ensuremath{>})Henzler et al. (2018) first, to our knowledge, applied a deep Convolutional Neural Network (CNN) to single-radiograph tomography and reconstructed 3D cranial volumes from 2D X-rays. (\ensuremath{<}\ensuremath{>})Kasten et al. (2020) used an end-to-end CNN for 3D reconstruction of knee bones from bi-planar X-ray images. (\ensuremath{<}\ensuremath{>})Shen et al. (\ensuremath{<}\ensuremath{>})(2019) developed a deep network system with representation, transformation, generation modules to generate volumetric tomography images from single or multiple 2D X-rays. Through the current literature research, it can be found that the current CNN-based reconstruction methods use the end-to-end network structure, which will cause a certain loss of image resolution due to the network sampling process; Secondly, the task of CT volumetric images reconstruction based on X-ray image is quite computationally expensive. Thus, this paper constructs a lightweight CNN-based reconstruction network, XctNet, which can not only improve the information loss caused by the sampling process, but also greatly reduce the required computing resources. To summarize, the contribution can be seen as follow:  
2. Related work 
CT reconstruction is an inverse mapping mathematical process, which generates tomographic images from X-ray projection data acquired at many different angles around the patient ((\ensuremath{<}\ensuremath{>})Stierstorfer et al., (\ensuremath{<}\ensuremath{>})2004). The quality of reconstruction has a fundamental impact on the radiation dose used and the researchers are trying to find better reconstruction algorithm to ensure both the accuracy and resolution of the reconstructed image while minimizing radiation dose ((\ensuremath{<}\ensuremath{>})Kak and Slaney, (\ensuremath{<}\ensuremath{>})1987; Hsieh, 2003). 
A multi-detector spiral CT reconstruction method is proposed based on cone beam geometry ((\ensuremath{<}\ensuremath{>})Taguchi and Aradate, 1998). (\ensuremath{<}\ensuremath{>})Hu (1999) studied the scanning and reconstruction principles of multi-slice spiral CT, especially the scanning and reconstruction principles of 4-slice spiral CT, and concluded that the volume coverage speed of 4-slice spiral CT is 2{\textendash}3 times that of single-slice spiral CT, which can provide the same image quality. (\ensuremath{<}\ensuremath{>})Schaller et al. (2001) introduced a high-quality image reconstruction approach for helical CBCT and (\ensuremath{<}\ensuremath{>})Flohr et al. (2003) proved its effectiveness in a 16-slice CT scanner. 
The EOS system, originated from the Nobel prize-winning invention MWPC (the Multiwire Proportional Chamber) particle detector by Dr. Georges Charpak, is able to produce full-body stereo images of patients using biplanar low-dose X-ray scan and is regarded as the most advanced image acquisition equipment in orthopedics at present ((\ensuremath{<}\ensuremath{>})Melhem et al., (\ensuremath{<}\ensuremath{>})2016; Song et al., 2020). (\ensuremath{<}\ensuremath{>})Rehm et al. (2017) compared EOS imaging equipment with CT imaging equipment and showed that the EOS system can obtain high-quality images with less doses. (\ensuremath{<}\ensuremath{>})Post et al. (2018) proposed a three-dimensional spine classification method based on the EOS system. However, the 3D reconstruction of EOS depends on parametric models and statistical inferences from collected biplanar X-ray scans, so that the generated skeleton model is only a parametric virtual substitute and is limited in circumstances of severe skeletal malformations or abnormalities like congenital scoliosis (CS) and ankylosing spondylitis (AS). 
In recent years, deep learning has been widely adopted in the field of medical imaging. (\ensuremath{<}\ensuremath{>})Meng et al. (2020) used semi-supervised learning to reconstruct high-dose volumetric images from low-dose volumetric images. (\ensuremath{<}\ensuremath{>})Henzler et al. (2018) proposed a deep convolution to generate 3D images from a single X-ray animal skull image. Its network architecture adopts an end-to-end structure, and compared with some previously proposed network structures, it proved that their network can achieve better reconstruction results. (\ensuremath{<}\ensuremath{>})Shen et al. (2019) proposed a 2D to 3D network model architecture and brought the idea of converting the 2D feature information into a spatial tensor in order to perform a 3D deconvolution. However, Shen{\textquoteright}s reconstruction network includes enormous amount of parameters to update, which leads to computational inefficiency. Multiple studies on machine-learning-based 3D reconstruction have also been carried out in the field of dentistry, spine, chest, etc ((\ensuremath{<}\ensuremath{>})Ying et al., 2019; Bayat et al., 2020; {\textasciicaron} a et al., 2020). Cavojsk{\textasciiacute} The mentioned works also have limitations in generalization and tend to underperform on different datasets in practice. In this paper, a more lightweight CNN-based network is constructed to improve the reconstruction accuracy and reduce the computational cost. 
3. Methodology 
The architecture of XctNet reconstruction network, shown in (\ensuremath{<}\ensuremath{>})Fig. 1, includes the two major parts:The X-ray feature extraction module, Multi-scale feature fusion module and the volumetric image generation 
Fig. 1. Architecture of XctNet. The model contains X-ray feature extraction module, multi-scale feature fusion module and the volumetric images generation module. The input of the model is a single 2D projection image. The X-ray feature extraction module extract feature information from the input X-ray image. The multi-scale feature extraction module converts 2D features into 3D features and performs feature fusion with the corresponding 3D generation module. The volumetric images generation module, which consists of a series of New Inception module, uses the extracted feature data to generate the corresponding volumetric image. 
module. The details of each module and the loss function will be explained in the following sections. 
X-ray feature extraction module: The introduction of deep residual network is to solve the problem of gradient disappearance caused by too many network layers in the training process. The most representative structure of the residual network is ResNet ((\ensuremath{<}\ensuremath{>})He et al., 2016), which directly connects the input terminal to the following layer through the shortcut structure, thereby protecting the integrity of the transmitted data. The feature extraction module is built based on ResNet34. The input is a single X-ray image which size is 128 {\texttimes} 128, the first layer of the model is composed of a convolutional layer with a kernel size of 7 {\texttimes} 7 and stride 2, and the second to fifth layers are composed of four residual blocks which contains two 3 {\texttimes} 3 convolutional layers. The number of channels of the convolutional layer in each residual block is kept the same to ensure that the shortcut path and the residual path can maintain the same size during the element-wise addition operation. The size of the feature representation output is 4 {\texttimes} 4. In addition, through experiments, we find that when using the encoder/decoder structure for pixel level vision tasks, the convolution layer can only use local information to calculate the target pixel value. Therefore, the lack of global information will undoubtedly lead to deviation. The error caused by the convolution layer can be described by the covariance between the pixel values shown in (\ensuremath{<}\ensuremath{>})Eq. (1). Each pixel value xi in the feature map obtained by the convolution layer can be used as a random variable, and x is the mean value of the feature map. The similarity between the two variables can be evaluated by calculating the covariance of the two random variables. The attention mechanism is to use the similarity between pixels to improve the performance of convolution layer. 
(1) 
In order to reduce the error caused by the convolution process, this paper introduces two attention mechanisms, CBAM (Convolutional Block Attention module) ((\ensuremath{<}\ensuremath{>})Woo et al., 2018) and ECA (Efficient channel Attention module) module ((\ensuremath{<}\ensuremath{>})Wang et al., 2020), to adaptively improve the feature extraction ability and reduce the error. Specifically, CBAM derives the attention graph from the 2D information of space and channel, then, the attention graph with the input features will be multiplied to adaptively optimize the eigenvalues. The module structure is shown in (\ensuremath{<}\ensuremath{>})Fig. 1(b). In the 2D feature extraction module, CBAM is mainly used for convolution feature extraction of the first layer and the last layer, so as to improve the ability of feature adaptive extraction on the premise of ensuring that the overall network structure is not affected. For the intermediate convolution layer, ECA module is used to improve its feature extraction performance. As a local cross-channel interaction module that does not reduce the feature dimension, ECA module obtains local cross-channel interaction information by combining each channel and its adjacent k channels. ECA module can be realized by one-dimensional convolution layer with the size of k. It is worth noting that ECA, as a lightweight module, does not add a large number of additional parameters. The network structure diagram is shown in (\ensuremath{<}\ensuremath{>})Fig. 1(c). In this paper, the feature extraction capability of the middle layer is improved by combining the residual module and ECA module. The feature map obtained by the residual module will be input into the ECA module for adaptive optimization. In addition, the feature map obtained by the residual module will also be combined with the adaptively optimized feature map through element-wise product, so as 
The process of multi-scale convolution mainly includes two factors: feature propagation and cross-scale communication ((\ensuremath{<}\ensuremath{>})Feng et al., 2020). In the multi-scale feature extraction structure proposed in this paper, the input feature will be divided into high-scale feature Xhigh and low-scale feature Xlow to obtain the corresponding high-scale feature output Yhigh and low-scale feature output Ylow respectively. The multi-scale feature transform process can be seen as follow: 
(2) 
The overall network structure is shown in (\ensuremath{<}\ensuremath{>})Fig. 1(a). Feature maps of different layers are extracted from the original network structure and converted into corresponding 3D feature maps through transform module and then the multi-scale 3D feature maps are combined with corresponding 3D feature generation layers, so as to improve the fine granularity of generation results and reduce the loss of information in the reconstruction process. For details, to convert 2D projection data to volumetric images data requires data conversion. A transform module is added to bridge the 2D feature extraction module and the 3D generation module. the multi-scale 2D features, which size is(C, H, W), are converted to (C, 1, H, W) by the dimension conversion function, after that, the converted 3D feature map, which size is (C, D, H, W) can be obtained through a deconvolution operation with a kernel size of D {\texttimes} 1 {\texttimes} 1. In addition, the ReLU activation function and the batch normalization function are also included to better learn the transformation relationship in the transform process. 
The original data needs to be preprocessed before fed into the network model for training. First of all, all input data need to be resized to the same size. The 2D images and the corresponding 3D CT images used for training are resized to 128 {\texttimes} 128 and 128 {\texttimes} 128 {\texttimes} 128 separately. In practice, 2D{\textendash}3D data pairs should be composed of X-ray and CT images, owing to the lack of corresponding paired images, this paper uses digitally reconstructed radio algorithm (DRR) to generate an approximate single 2D projection image to obtain the corresponding 2D{\textendash}3D data pairs. As shown in the (\ensuremath{<}\ensuremath{>})Fig. 2, this article uses point source vision based DRR projection algorithm to generated 2D projection ((\ensuremath{<}\ensuremath{>})Moturu and Chang, 2018). The advantage of this method is that the point source can be randomly selected to obtain X-rays, which makes the data change slightly. Specifically, after the light source point is selected and the projection distance is fixed (centered on the front of the CT volumetric image), 2D projections are generated according to Beer{\textquoteright}s law ((\ensuremath{<}\ensuremath{>})Feeman, 2010), in which the intensity loss measurement of X-rays passing through the body is modeled by Beer{\textquoteright}s law. The information (spacing, size, direction) of the CT image is obtained and the image is used as the input of the DRR algorithm, and then the image is resampled by coordinate transformation. Moreover, we set the distance from the light source to the projection plane to 400 mm, and the default pixel spacing of the projection pixel plane is 0.8 {\texttimes} 0.8 and set the threshold to {\L} 80, and the bilinear interpolation is used to integrate each voxel plane traversed, so as to obtain the anterior-posterior positions of the 2D projected image. In order to enrich the sample size of training data, data augmentation, which includes scale change, rotation change, mirror image and translation change, brightness change, chroma change, contrast change and sharpness change, is performed before training. Moreover, the pixel-wise input data is normalized to the interval [0,1]. 
In order to evaluate the performance of the model, we tested the trained model on the test set and used different evaluation metrics to evaluate the predicted reconstruction results. Four evaluation functions is used in this paper for model evaluation, namely: MSE (mean squared error), MAE (mean absolute error), SSIM (structural similarity) and PSNR (peak signal noise ratio). MSE and MAE are used to evaluate the deviation between the predicted reconstruction result and the target value. The smaller MSE/MAE value, the closer the reconstruction result is to the real situation. The image evaluation metric SSIM, incorporating the information of luminance, contrast and structures, is used to evaluate the degree of similarity between images. The commonly used PSNR is applied to evaluate the quality of our reconstructed volumetric images. Generally, the resultant images with better structural and higher resolution will have higher SSIM and PSNR values. Each metric value is averaged for all test samples and different methods are compared as 
Fig. 2. DRR projection algorithm based on point source vision.  
shown in (\ensuremath{<}\ensuremath{>})Table 2. 
4. Reconstruction experiments results 
The input sample consists of a single 2D projection image X and volumetric CT images Y. In the training process, with the single X-ray image as input X \ensuremath{\in} RH{\texttimes}W , the model output is volumetric images while the ground truth which is the reference standards for model training. 
In order to verify the effectiveness of the model, we used the public dataset, the Lung Image Database Consortium image collection (LIDCIDRI) ((\ensuremath{<}\ensuremath{>})Armato et al., 2011), which contains 1081 CT volumetric image cases. The original data will be divided into training set, test set and verification set separately, on this basis, the original data is expanded to 59,708 cases through data augmentation. Among them, the training set is 35,825 cases, the verification set is 11,941 cases, and the test set is 11, 942 cases. At the same time, the corresponding input X of each case is generated by DRR projection. 
The size of the input X is 128 {\texttimes} 128 and the size of the ground truth YGT is 128 {\texttimes} 128 {\texttimes} 128. The network is trained on a device with three NVIDIA Tesla V100 graphic processing units, and the training platform is Pytorch. Training epoch is 64. As an important parameter in deep learning training, it is particularly important to select the appropriate learning rate. This paper constructs an adaptive learning rate adjustment strategy, which can modify the learning rate according to the specific situation in the training process, so as to ensure the best effect of training. As shown in (\ensuremath{<}\ensuremath{>})Eq. (3), the loss function used in all three trained networks was MSE. The training results were shown in (\ensuremath{<}\ensuremath{>})Fig. 3. 
(3) 
We show the reconstruction results of ResXct, CBAM/ECAXct, XctNet as well as ReconNet on the LIDC-IDRI data set. These abnormalities can further prove the effectiveness of the XctNet model. 
As shown in (\ensuremath{<}\ensuremath{>})Fig. 3, a test sample is randomly selected to show the generation results of the different model and the numbers of the selected slices are 3, 15, 35, 65 and 85. The results shown in (\ensuremath{<}\ensuremath{>})Fig. 3(a){\textendash}(d) are slice images which are randomly selected from the test sample; (\ensuremath{<}\ensuremath{>})Fig. 3(e) is the corresponding ground truth. From the overall result of reconstruction, the result generated by our XctNet is closer to the ground truth. In terms of details, the content of the slice image generated by the original version model is relatively vague; compared with the ReconNet model, the overall contour of the intermediate version model is clearer, and some internal details, such as rib areas, can be reconstructed. On the other hand, from the chest slice data at different positions, the best reconstruction detail is the bony area, while the reconstruction accuracy of internal organs in the chest is blurred to varying degrees. Besides, by randomly selecting multiple test data for analysis, it can be found that in the reconstructed volumetric data, the reconstruction performance of the middle of the volume is generally better than that of the front of the volume and the end of the volume. The main reasons for this phenomenon are as follows: First of all, owing to this paper is to reconstruct the 
Fig. 3. Volumetric image examples from the test set. (a){\textendash}(d) represent the results generated by the ReconNet, ResXct, CBAM/ECAXct and XctNet respectively; (e) is the ground truth. The results shown in the figure comes from different slice graphs in a volumetric image randomly selected. 
whole thoracic cavity, we do not preprocess according to the HU values of different tissues and organs, but process the CT data of the whole thoracic cavity. Therefore, there will be some deviation for the details such as internal organs in the reconstruction process. Secondly, due to the different sources of the data sets, the quality of the thoracic cavity area can not be guaranteed to be completely consistent, resulting in the quality of the front and end volume data of the reconstructed CT volumetric image will be relatively worse than that of the middle volume. It can be seen that the main reason for this phenomenon is due to the complexity of data, but this does not mean that our model has limitations. As can be seen, compared with the above different models, XctNet has the best reconstruction performance in terms of overall contour and internal details. 
To show the details of the differences in results between different models, as can be seen in (\ensuremath{<}\ensuremath{>})Fig. 4, gray values indicate areas with insignificant differences, while white and black values represent areas with large differences. It can be seen from the figure that XctNet has the smallest difference with ground truth compared with other models. 
In order to make a quantitative analysis of XctNet and the proposed contrast network, four evaluation metric functions are used to analyze the difference between the predicted reconstructed image and the ground truth. In addition, by comparing the evaluation metric differences between the models, the effectiveness of attention mechanism and multi-scale fusion module can be further illustrated. 
It is worth noting that the volumetric data used in this paper is composed of 128 slices of data. It is worth noting that the volumetric data used in this paper are composed of 128 slices of data. The 128 slices of volumetric data is evaluated separately by using different evaluation metric functions, and then obtain the final evaluation result by taking the average of all slices.As shown in (\ensuremath{<}\ensuremath{>})Table 2, our XctNet can achieve the best evaluation results, and its PSNR and SSIM can reach 29.2823 and 0.8681 respectively. Incidentally, all the evaluation metric values in (\ensuremath{<}\ensuremath{>})Table 2 are the mean values of the test samples. On the other hand, the evaluation results obtained by ReconNet perform better than our baseline model, RexXct, which shows that increasing the network depth is effective for the reconstruction results. In addition, the evaluation results of CBAM/ECAXct are similar to ReconNet, that is, adding lightweight attention mechanism is also an effective method to enhance the performance of the model without increasing the network depth. 
From the overall distribution of the evaluation results of the test set, as shown in (\ensuremath{<}\ensuremath{>})Fig. 5, the four violin graphs represent the results of different evaluation functions. Overall, XctNet achieves the better results in all evaluation metrics. On the other hand, as shown in (\ensuremath{<}\ensuremath{>})Fig. 5(c), the distribution of PSNR values of all models are mostly concentrated near the inferior quartile, that is mainly because PSNR evaluates the gray difference between images and due to the data set used in this paper is complicated, the prediction results are usually different. From the 
Fig. 5. Distribution of evaluation results of different models. (a){\textendash}(d) represents the distribution of evaluation results of MAE, MSE, PSNR and SSIM on LIDC-IDRI data set respectively. It can be seen that the result distribution in (a){\textendash} (c) approach to the inferior quartile and (d) approach to the superior quartile. 
comparison results, the interquartile range (IQR) of XctNet is smaller than the other three models, which shows that XctNet model is more stable. Through the above data analysis, we can get the conclusions that self-attention mechanism and multi-scale feature fusion module can 
Fig. 4. Comparison of deviation with respect to ground truth. The first column correspond to the ground truth. The second column shows the difference between ReconNet and ground truth. Other columns represent the difference between the corresponding model and the ground truth. 
greatly improve the output accuracy of the reconstructed model. By summarizing and analyzing the training results of the four groups of models, the evaluation metrics obtained by our XctNet exceeds the ReconNet. This result confirms that without additional network depth, attention mechanism and multi-scale feature fusion module can also improve the accuracy of the model. 
5. Discussion 
The advantages of the XctNet model is further illustrated by analyzing the semantic representation of the model. For the 2D projection image based reconstruction task, only when the feature extraction module extracts the key and useful feature information can the volumetric images be correctly reconstructed. The three models constructed in this paper contain 512 feature maps with a size of 4 {\texttimes} 4. For better visualization, 16 feature maps are randomly selected to illustrate the feature representation between different models. As shown in (\ensuremath{<}\ensuremath{>})Fig. 6(a){\textendash} (b), it can be seen that the feature map generated by the feature extraction module with self-attention mechanism is more concise than the feature map without self-attention mechanism. Comparing (b){\textendash}(c), it can be seen that CBAM/ECA attention mechanism can further remove redundant features. On the other hand, from the perspective of the generated volumetric image, the image quality generated by the model with self-attention mechanism is significantly better. In other words, the feature extraction module without self-attention mechanism learns a lot of redundant information, which leads to the phenomenon of fuzzy generation results. Therefore, the conclusion can be got that the self- attention mechanism can help the model to better learn the feature information. 
In order to further verify the performance of XctNet model, the current state of art CNN models are compared. As shown in (\ensuremath{<}\ensuremath{>})Table 3, ReconNet model has the highest model complexity, and its FLOPs (floating point operations) reaches 1.304 {\texttimes} 1012 . ResXct as our baseline model, which complexity is lower than ReconNet, and the error rate is almost the same. Our XctNet has greatly improved its error rate with only a little increase in complexity. This phenomenon shows the following two aspects. Firstly, the lightweight attention mechanism can improve the performance of the model without increasing the complexity of the model. Secondly, the New Inception module proposed in this paper can greatly reduce the amount of model calculation and generate volumetric images with richer content. 
According to the definition of information entropy, it represents the overall characteristics of an information source in an average sense. For image information, we can describe the amount of information contained in the image according to image entropy. As shown in (\ensuremath{<}\ensuremath{>})Eq. (4), x represents each pixel in the image, the image entropy reflects the average information of an image and the image entropy obtained for specific image information is unique. Therefore, the generation quality of volumetric images can be evaluated from the perspective of image 
entropy. 
(4) 
As shown in (\ensuremath{<}\ensuremath{>})Fig. 7, two test samples are randomly selected to illustrate the distribution of image entropy comes from different model, which shows the ground truth and the entropy map of the different models. It can be seen that, the area in which the entropy map tends to be cold indicates that it contains less information and the area in which the entropy map tends to be warm indicates that it contains more information. By comparing the results of the three models with the ground truth, we can find that XctNet can get a distribution map more inclined to the ground truth by adding self-attention mechanism and multi-scale fusion module. By comparing the ReconNet and ResXct with the other two models, it further shows that the traditional end-to-end network will bring deviation in the convolution process, and also verifies the effectiveness of the improved method proposed in this paper. 
To verify the performance of the model on clinical X-ray images, we obtained 10 original X-ray chest images through the spine surgery of the General Hospital of Shenzhen University. As shown in (\ensuremath{<}\ensuremath{>})Fig. 8, the reconstruction results of two groups of clinical X-ray images are shown. It can be seen that the accuracy of the reconstruction result are worse than that of the 2D projection used in this paper. The main reason for this phenomenon is that there are some differences between clinical X- ray images and 2D projections. Therefore, in the future work, more in- depth research from clinical X-rays need to conducted. However, from the generation results of ReconNet and XctNet proposed in this paper, XctNet performs better in clinical X-rays data, which also confirms that the network we constructed has considerable superiority. 
6. Conclusion 
In this paper, we focus on the reconstruction quality of volumetric image. In order to obtain more accurate reconstruction results, a lightweight reconstruction network, XctNet, is constructed. The network structure has the following three innovations: 
Firstly, self-attention mechanism is be added to the original residual feature extraction module to remove redundant features; Secondly, a multi-scale feature fusion module is proposed in this paper to improve 
Fig. 6. Feature extraction and network structure analysis. a) Feature map learned from 2D feature extraction module without attention mechanism; b) Feature map learned from 2D feature extraction module only with CBAM attention mechanism; c) Feature map learned from 2D feature extraction module with CBAM/ECA attention mechanism. 
Fig. 7. The entropy map generated by different network structures. a{\textendash}b) represent different test samples and their corresponding entry information. The variation of image entropy is closely related to the content contained in the image. As can be seen, the less content the image contains, the lower the image information entropy, that is, the color of the entropy map tends to be cold. 
Fig. 8. Clinical X-ray reconstruction results of difference cases. a{\textendash}b) represent volumetric images reconstructed from different X-ray images. The number of slices shown in the figure are 30, 60, 70, and 90. 
the quality of reconstructed image and other details. Finally, a feature generation module called New Inception module is constructed to obtain richer feature information and more accurate reconstruction results. At the same time, there are still some problems to be solved in this paper. In the actual application scenario, the corresponding 2D projection image should be an X-ray image, but the 2D projection used in this article is projected by the DRR algorithm. To solve this problem, using style transfer algorithm may considered to solve the difference between clinical X-rays and DRR projection. In conclusion, XctNet as a lightweight framework can further improve the results of volumetric image reconstruction. 
CRediT authorship contribution statement 
Zhiqiang Tan: Conceptualization, Methodology, Software, Investigation. Jun Li: Data curation, Software Huiren Tao: Resources, Validation. Shibo Li: Visualization, Writing {\textendash} review \& editing. Ying Hu: Supervision, Project administration. 
Declaration of Competing Interest 
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 
Acknowledgements 
This work was supported in part by Key-Area Research and Development Program of Guangdong Province (No.2020B0909020002), National Natural Science Foundation of China (Grant No. 62003330), Shenzhen Fundamental Research Funds (Grant No. JCYJ20200109114233670No.JCYJ20190807170407391, JCYJ20180507182415428), Natural Science Foundation of Guangdong Province (Grant No. 2019A1515011699) and Guangdong-Hong Kong- Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institute of Advanced Technology. 
References 
Armato, Samuel G., McLennan, Geoffrey, Bidaut, Luc M., McNitt-Gray, Michael F., Meyer, C.R., Reeves, Anthony P., Zhao, Binsheng, Aberle, Denise R., Henschke, Claudia I., Hoffman, Eric A., Kazerooni, Ella A., MacMahon, Heber, Van Beeke, Edwin J.R., Yankelevitz, David F., Biancardi, Alberto M., Bland, Peyton H., Brown, Matthew S., Engelmann, Roger M., Laderach, G.E., Max, Daniel, Pais, Richard C., Qing, D.P., Roberts, Rachael Y., Smith, Amanda R., Starkey, Adam, Batrah, Poonam, Caligiuri, Philip, Farooqi, Ali O., Gladish, Gregory W., Jude, Cecilia Matilda, Munden, Reginald F., Petkovska, Iva, Quint, Leslie E., Schwartz, Lawrence H., Sundaram, Baskaran, Dodd, Lori E., Fenimore, Charles, Gur, David, Petrick, Nicholas A., Freymann, John B., Kirby, Justin S., Hughes, Brian, Casteele, Alessi Vande, Gupte, Sangeeta, Sallamm, Maha, Heath,Michael, Kuhn, M., Dharaiya, Ekta, Burns, Richard, Fryd, David, Salganicoff, Marcos, Anand, V., Shreter, Uri, Vastagh, Stephen, Croft, Barbara Y., 2011. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules 
on CT scans. Med. Phys. vol. 38, issue 2, pp. 915{\textendash}31. (\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\ensuremath{>})Bayat, Amirhossein, Kumar Sekuboyina, Anjany, Paetzold, Johannes C., Payer, Christian, (\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\ensuremath{>}){\textasciicaron}Stern, Darko, Urschler, Martin, Kirschke, Jan S., Menze, Bjoern H., 2020. Inferring (\ensuremath{<}http://refhub.elsevier.com/S0895-6111(22)00040-4/sbref1\ensuremath{>})the 3D standing spine posture from 2D radiographs. MICCAI. 
Feeman, Timothy G., 2010. The Mathematics of Medical Imaging. 
Feng, Ruicheng, Guan, Weipeng, Qiao, Yu, Dong, Chao, 2020. Exploring Multi-Scale Feature Propagation and Communication for Image Super Resolution. ArXiv abs/ 2008.00239, n. pag. 
He, Kaiming, Zhang, X., Ren, Shaoqing, Sun, Jian, 2016. Deep residual learning for image recognition. In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770{\textendash}8. 
Hsieh, Jiang, 2003. Computed Tomography: Principles, Design, Artifacts, and Recent Advances. 
Kasten, Yoni, Doktofsky, Daniel, Kovler I., 2020. End-To-End Convolutional Neural Network for 3D Reconstruction of Knee Bones from Bi-Planar X-Ray Images. ArXiv abs/2004.00871, n. pag. 
Li, Jun, Xu, Kai, Chaudhuri, Siddhartha, Yumer, Ersin, Zhang, Hao, Guibas, Leonidas J., 2017. GRASS: Generative Recursive Autoencoders for Shape Structures. ArXiv abs/ 1705.02090, n. pag. 
Moturu, Abhishek, Chang, Alex, 2018. Creation of Synthetic X-Rays to Train a Neural Network to Detect Lung Cancer. 
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan, D., Vanhoucke, Vincent, Rabinovich, Andrew, 2015. Going deeper with convolutions. In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1{\textendash}9. 
Wang, Qilong, Wu, Banggu, Zhu, Pengfei, Li, P., Zuo, Wangmeng, Hu, Qinghua, 2020. ECA-net: efficient channel attention for deep convolutional neural networks. In: Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11531{\textendash}9. 
Wang, Weiyue, Huang, Qiangui, You, Suya, Yang, Chao, Neumann, Ulrich, 2017. Shape Inpainting using 3D generative adversarial network and recurrent convolutional networks. In: Proceedings of the 2017 IEEE International Conference on Computer 
Vision (ICCV), pp. 2317{\textendash}25. Woo, Sanghyun, Park, Jongchan, Lee, Joon-Young, Kweon, In-So, 2018. CBAM: convolutional block attention module. ECCV. 
Wu, Jiajun, Zhang, Chengkai, Xue, Tianfan, Freeman, Bill, Tenenbaum, Joshua B., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. NIPS. 
Wu, Zhirong, Song, Shuran, Khosla, Aditya, Yu, Fisher, Zhang, Linguang, Tang, Xiaoou, Xiao, Jianxiong, 2015. 3D shapenets: a deep representation for volumetric shapes. In: Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1912{\textendash}20. 
Ying, Xingde, Guo, Heng, Ma, Kai, Wu, Jian, Weng, Zhengxin, Zheng, Yefeng, 2019. X2CT-GAN: reconstructing CT from biplanar X-rays with generative adversarial networks. In: Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10611{\textendash}20. 