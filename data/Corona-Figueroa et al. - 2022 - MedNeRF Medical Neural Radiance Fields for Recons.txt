2022 44th Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC) Scottish Event Campus, Glasgow, UK, July 11-15, 2022 
MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray 
Abstract{\textemdash} Computed tomography (CT) is an effective medical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multiplanar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qualitative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets are available at https: //github.com/abrilcf/mednerf 
Clinical relevance{\textemdash} Our model is able to infer the anatomical 3D structure from a few or a single-view X-ray, showing future potential for reduced ionising radiation exposure during the imaging process. 
I. INTRODUCTION 
3D medical imaging often involves joining multiple 2D slices from CT or Magnetic Resonance Imaging (MRI), and part of their workflow consists of specifying values for the position of the patient, the imaging source, and the detector. The quality and accuracy of a CT 3D representation require hundreds of X-ray projections with a thin slice thickness [1]. Moreover, this process exposes patients to more ionising radiation than typical X-rays and requires the patient to remain immobile for up to more than 1 hour, depending on the type of test [2]. Continuous 3D representations would give radiologists optics of every point in the internal anatomy captured. While such representations are useful, there are practical challenges in CT due to the increased radiation exposure, angle-dependent structures, and time consumption [3]. 
The Neural Radiance Fields (NeRF) [12] model is a recent reformulation for estimating a 3D volumetric representation from images. Such representations encode the radiance field and density of the scene in the parameters of a neural network. The neural network learns to synthesize new views via volume rendering from point samples along cast rays. However, these representations are often captured in controlled settings [13]. First, the scene is taken by a set of fixed cameras within a short time frame. Second, all content in the scene is static and real images often need masking. These constraints prohibit the direct application of NeRF to the medical domain, where the imaging system greatly differs from conventional cameras, and the images are captured over a long time frame hampering the patient{\textquoteright}s stillness. Moreover, the overlapping of anatomical structures in medical images hinders the definition of edges which cannot be easily solved with masking. These aspects explain why the NeRF approach especially shows successes for {\textquotedblleft}natural images{\textquotedblright}. 
To address these challenges, we propose MedNeRF, a model that adapts Generative Radiance Fields (GRAF) [14] in the medical domain to render CT projections given a few or even a single-view X-ray. Our approach not only synthesizes realistic images, but also capture the data manifold and provides a continuous representation of how the attenuation and volumetric depth of anatomical structures vary with the viewpoint without 3D supervision. This is achieved via a new discriminator architecture that provides a stronger and more comprehensive signal to GRAF when dealing with CT scans. 
We render CT projections of our two datasets of digitally reconstructed radiographs (DRR) from chest and knee. We qualitative and quantitative demonstrate high-fidelity renderings and compare our approach to other recent radiance field-based methods. Furthermore, we render CT projections of a medical instance given a single-view X-ray and show the effectiveness of our model to cover surface and internal structures. 
II. METHODS 
To train our models, we generate DRRs instead of collecting paired X-rays and corresponding CT reconstructions, which would expose patients to more radiation. Furthermore, DRR generation removes patient data and enables control in capture ranges and resolutions. We generated DRRs by using 20 CT chest scans from [15], [16] and five CT knee scans from [17], [18]. These scans cover a diverse group of patients at different contrast types showing both normal and abnormal anatomy. The radiation source and imaging panel are assumed to rotate around the vertical-axis, generating a DRR of 128 {\texttimes} 128 resolution at every five degrees, resulting in 72 DRRs for each object. During training we use the whole set of 72 DRRs (a fifth of all views within a full 360-degree vertical rotation) per patient and let the model render the rest. Our work did not involve experimental procedures on human subjects or animals and thus did not require Institutional Review Board approval. 
GRAF [14] is a model that builds from NeRF and defines it within an Generative Adversarial Network (GAN). It consists of a generator G\ensuremath{\theta} that predicts an image patch P pred and a discriminator D\ensuremath{\phi} that compares the predicted patch to a patch P real extracted from a real image. GRAF has shown an effective capacity to disentangle 3D shape and viewpoint of objects from 2D images alone, in contrast to the original NeRF [12] and similar approaches such as [19]. Therefore, we aim to translate GRAF{\textquoteright}s methods to our task, and in subsection II-C we describe our new discriminator architecture, which allows us to disentangle 3D properties from DRRs. 
We consider the experimental setting to obtain the radiation attenuation response instead of the color used in natural images. To obtain the attenuation response at a pixel location for an arbitrary projection K with pose \ensuremath{\xi}, first, we consider a pattern \ensuremath{\nu} = (u, s) to sample R X-ray beams within a K {\texttimes} K image-patch P . Then, we sample N 3D points x i r along the X-ray beam r originating from the pixel location and ordered between the near and far planes of the projection (Fig. 1a). 
The object representation is encoded in a multi-layer perceptron (MLP) that takes as input a 3D position x = (x, y, z) and a viewing direction d = (\ensuremath{\theta}, \ensuremath{\phi}), and produces 
Fig. 1. An overview of GRAF{\textquoteright}s generator. 
as output a density scalar \ensuremath{\sigma} and a pixel value c. To learn high-frequency features, the input is mapped into a 2Ldimensional representation (Fig. 1b): 
(1) 
where p represents the 3D position or viewing direction, for j = 0, ..., m \ensuremath{-} 1. 
For modeling the shape and appearance of anatomical structures, let zs \ensuremath{\sim} ps and za \ensuremath{\sim} pa be the latent codes sampled from a standard Gaussian distribution, respectively (Fig. 1c). To obtain the density prediction \ensuremath{\sigma}, the shape encoding q is transformed to volume density through a density head \ensuremath{\sigma}\ensuremath{\theta}. Then, the network g\ensuremath{\theta}({\textperiodcentered}) operates on a shape encoding q = (\ensuremath{\gamma}(x), zs) that is later concatenated with the positional encoding of d and appearance code za (Fig. 1c): 
(2) 
(3) 
(4) 
The final pixel response cr is computed by the compositing operation (Fig. 1c): 
(5) 
where \ensuremath{\alpha}ri = 1 \ensuremath{-} exp (\ensuremath{-}\ensuremath{\sigma}ri \ensuremath{\delta}ri ) is the alpha compositing value of sampled point i and \ensuremath{\delta}ri =\ensuremath{\parallel} xri+1 \ensuremath{-} xri \ensuremath{\parallel}2 is the distance between the adjacent sampled points. 
In this way, both the density and pixel values are computed at each sampled point along the beam r with network g\ensuremath{\theta}. Finally, combining the results of all R beams, the generator G\ensuremath{\theta} predicts an image patch P pred, as illustrated in Fig. 1d. 
We investigate how we can adapt GRAF to the medical domain and apply it to render a volumetric representation from DRRs. Leveraging a large dataset, GRAF{\textquoteright}s discriminator D\ensuremath{\phi} is able to continuously provide useful signals to train the generator G\ensuremath{\theta}. However, medical datasets like those considered in our problem are generally small, which causes two sequential issues: 
Brittle adversarial training: With a limited training dataset, the generator or discriminator may fall into ill-posed settings such as mode collapse, which would lead to generating a limited number of instances and consequently, a suboptimal data distribution estimation. While some works have applied data augmentation techniques to leverage more data in the medical domain, some transformations could mislead the generator to learn the infrequent or even non-existent augmented data distribution [20]. We find that naively applying classic data augmentation works less favorably than our adopted framework. 
To assess global structure in decoded patches from D\ensuremath{\phi}, we use the Learned Perceptual Image Patch Similarity (LPIPS) metric [22]. We compute the weighted pairwise image distance between two VGG16 feature spaces, where the pretrained weights are fit to better match human perceptual judgments. The additional discriminator loss is therefore: 
where \ensuremath{\phi}i({\textperiodcentered}) denotes the ith layer output of a pretrained VGG16 network, and w, h, and d stand for the width, height and depth of a feature space, respectively. Let G be the processing on the intermediate feature-maps f from D\ensuremath{\phi}, and T the processing on real image patches. When coupled with this additional reconstruction loss, the network learns representations that transfer across tasks. 
We improve learning of G\ensuremath{\theta} and D\ensuremath{\phi} by adopting the Data Augmentation Optimized for GAN (DAG) framework [20] in which a data augmentation transformation Tk (Fig. 2b) is applied using multiple discriminator heads \{Dk\}. To further reduce memory usage, we share all layers of D\ensuremath{\phi} except the last layers corresponding to each head (Fig. 
Fig. 2. An overview of our discriminator with self-supervised learning and DAG. 
2c). Because applying differentiable and invertible data augmentation transformations Tk has the Jenssen-Shannon (JS) preserving property [20]: 
(7) 
where p Td k is the transformed training data distribution and the transformed distribution captured by G\ensuremath{\theta}. By using a total of four transformations combining flipping and rotation, we encourage optimization to the original data distribution, which also brings the most performance boost. These choices allow our model to benefit from not only JS(pd \ensuremath{\parallel} pg) but also thereby improving the learning of G\ensuremath{\theta} and generalization of D\ensuremath{\phi}. Furthermore, using multiple discriminators with weight-sharing provides learning regularization of D\ensuremath{\phi}. 
Replacing GRAF{\textquoteright}s logistic objective with a hinge loss, we then define our overall loss as below: 
(8) 
L(\ensuremath{\theta}, \ensuremath{\phi}k) = 
where f(u) = max(0, 1+u). We optimize this loss with n = 4, where k = 0 corresponds to the identity transformation and \ensuremath{\lambda} = 0.2 (as in [20]). 
After training a model, we reconstruct the complete X-ray projections within a full vertical rotation of a medical instance given a single view X-ray. We follow the relaxed reconstruction formulation in [23], which fits the generator to a single image. Then, we allow the parameters of the generator G\ensuremath{\theta} to be slightly fine-tuned along with the shape and appearance latent vectors zs and za. The distortion and perception tradeoff is well known in GAN methods [24] and therefore we modify our generation objective by adding the distortion Mean Square Error (MSE) loss, which incentivises 
Fig. 3. Knee renderings from continuous viewpoint rotations showing tissue and bone. Given a single-view X-ray from a CT, we can generate the complete set of CT-projections within a full vertical rotation by slightly fine-tuning a pretrained model along with the shape and appearance latent codes. 
TABLE I. Quantitative results based on PSNR and SSIM of rendered X-ray projections with single-view X-ray input. 
a balance between blurriness and accuracy: 
where NLLL corresponds to the negative log-likelihood loss and the tuned hyperparameters lr = 0.0005, \ensuremath{\beta}1 = 0, \ensuremath{\beta}2 = 0.999, \ensuremath{\lambda}1 = 0.3, \ensuremath{\lambda}2 = 0.1 and \ensuremath{\lambda}3 = 0.3. 
Once the model locates an optimal combination of zs and za, we replicate them and use them to render the rest of the X-ray projections by continuously controlling the angle viewpoint. 
III. RESULTS 
Here we provide an evaluation of MedNeRF on our datasets. We compare our model{\textquoteright}s results to the ground truth, two baselines, perform an ablation study, and show qualitative and quantitative evaluations. We train all models for 100,000 iterations with a batch size of 8. Projection parameters (u, v) are chosen to evenly sample points on the surface of a sphere, specifically a slight horizontal elevation of 70-85 degrees and umin = 0, umax = 1 for a full 360-degree vertical rotation. However, we only provide a fifth of the views (72-views each at five degrees) during training and let the model render the rest. 
We evaluate our model{\textquoteright}s representation for 3D-aware DRR synthesis given a single-view X-ray as input. We find that despite the implicit linear network{\textquoteright}s limited capacity, our model can disentangle 3D anatomy identity and attenuation response of different medical instances, which are retrieved through the described reconstruction reformulation in II-C.3. 
Our model can also facilitate distinguishing bone from tissue via a contrast transformation, as it renders a brighter pixel value for denser structures (e.g. bone) (Fig. 3). 
Table I summarises our results based on the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), which measure the quality of reconstructed signals and human subjective similarity, respectively. We find that our generative loss can achieve a reasonable perception-distortion curve in renderings and show consistency with the location and volumetric depth of anatomical structures at continuous viewpoints compared to the ground truth. 
We evaluate our model on the task of 2D rendering and compare it to pixelNeRF [19], and GRAF [14] baseline, wherein the original architecture is used. Our model can more accurately estimate volumetric depth compared to 
TABLE II. FID and KID analysis comparing other methods. 
TABLE III. FID and KID analysis of ablations of our model. 
GRAF and pixelNeRF (Fig. 4). For each category, we find an unseen target instance with a similar view direction and shape. Volumetric depth estimation is given by bright colors (far) and dark colors (near). Lacking a perceptual loss, GRAF is not incentivized to produce high-frequency textures. In contrast, we find our model renders a more detailed internal structure with varied attenuation. GRAF produces a consistent attenuation response, but seems to be unable to distinguish the anatomical shape from the background. Our self-supervised discriminator enables the generator to disentangle shape and background by rendering a brighter color for the background and a darker color for the shape, while GRAF renders a bright or dark color for both. 
We find pixelNeRF produces blurred attenuation renderings for all datasets, and volumetric maps tend to exhibit strong color shifts (Fig. 4). We believe these artifacts are due to the see-through nature of the dataset, compared to solid-like natural objects on which NeRFs are trained. This data characteristic impairs not only volumetric maps but also fine anatomical structures. In contrast, our model is better able to render both volumetric depth and attenuation response. We also find pixelNeRF is sensitive to slight changes in projection parameters, hampering optimization for the knee category. Our model produces a consistent 3D geometry and does not rely on explicit projection matrices. 
Table II compares image quality based on Frechet Inception Distance (FID) and Kernel Inception Distance (KID) metrics, in which lower values mean better. Optimizing pixelNeRF on our datasets leads to particularly poor results that are unable to compete with the GRAF baseline and our model. In contrast, our model outperforms the baselines on FID and KID metrics for all datasets. 
IV. CONCLUSION 
We have presented a novel Deep Learning architecture based on Neural Radiance Fields for learning a continuous representation of CT scans. We learn a medical category encoding of the attenuation response of a set of 2D DRRs in the weights of a generator. Furthermore, we have found that a stronger and more comprehensive signal from our discriminator allows generative radiance fields to model 3Daware CT-projections. Experimental evaluation demonstrates significant qualitative and quantitative reconstructions and improvements over other Neural Radiance Field approaches. Whilst the proposed model may not replace CT entirely, the functionality of generating 3D-aware CT-projections from X-rays has great potential for clinical use in osseous trauma, skeletal evaluation in dysplasia and for orthopaedic pre-surgical planning. This could cut down on the radiation dose given to patients, with significant economic implications such as bringing down the cost of investigations. 
ACKNOWLEDGMENT 
This work is partially supported by the Mexican Council of Science and Technology (CONACyT). 
REFERENCES 