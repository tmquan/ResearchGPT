https://doi.org/10.1038/s41551-019-0466-4 
Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning 
Tomographic imaging using penetrating waves generates cross-sectional views of the internal anatomy of a living subject. For artefact-free volumetric imaging, projection views from a large number of angular positions are required. Here we show that a deep-learning model trained to map projection radiographs of a patient to the corresponding 3D anatomy can subsequently generate volumetric tomographic X-ray images of the patient from a single projection view. We demonstrate the feasibility of the approach with upper-abdomen, lung, and head-and-neck computed tomography scans from three patients. Volumetric reconstruction via deep learning could be useful in image-guided interventional procedures such as radiation therapy and needle biopsy, and might help simplify the hardware of tomographic imaging systems. 
The ability of computed tomography (CT) to take a deep and quantitative image of a patient or an object with high spatial resolution is highly valuable in scientific research and medical practice. Traditionally, a tomographic image is obtained via the mathematical inversion of the encoding function of the imaging wave for a given set of measured data from different angular posi-tions (Fig. (\ensuremath{<}\ensuremath{>})1a,b). A prerequisite for artefact-free inversion is the satisfaction of the classical Shannon{\textendash}Nyquist theorem in angular-data sampling, which imposes a practically achievable limit in imaging time and object irradiation. To mitigate the problem, image reconstruction with sparse sampling has been investigated extensively using techniques such as compressed sensing1(\ensuremath{<}\ensuremath{>}){\textendash} and maximum a posteriori,. These types of approaches introduce a regularization term to the inversion to encourage some ad hoc or presumed char-acteristics in the resultant image{\textendash}. If imaging quality cannot be compromised, the resultant sparsity is generally limited and does not address the unmet demand for real-time imaging with substan-tially reduced subject irradiation (Fig. (\ensuremath{<}\ensuremath{>})1c). Indeed, while continuous efforts have been made to reduce the number of angular measure-ments in medical imaging, tomographic imaging with ultra-sparse sampling has not yet been realized. 
In this study, we push sparse sampling to the limit of a single projection view and demonstrate single-view tomographic imaging with a patient-specific prior by leveraging deep learning and the seamless integration of prior knowledge in the data-driven image-reconstruction process. The harnessing of prior knowledge by machine-learning techniques in different data domains for improved imaging is an emerging topic of research. Some recent studies{\textendash}have also investigated machine-learning-based image reconstruction. Whereas the data-driven approach represents a potentially general strategy for image reconstruction, here single-view CT imaging is achieved via a patient-specific prior. Practically, it is actually advantageous to work with the patient-specific prior: for many image-guided interventional applications, the approach would enable scenarios most relevant to the specific patient under treatment. 
Deep neural networks have attracted much attention for their ability to learn complex relationships and to incorporate existing knowledge into the inference model through feature extraction and representation learning{\textendash}. The method has found widespread applications across disciplines, such as computer vision{\textendash}, autonomous driving, natural language processing and biomedicine,{\textendash}. Here we design a hierarchical neural network for X-ray CT imaging with ultra-sparse projection views, and develop a structured training process for deep learning to generate three-dimensional (3D) CT images from two-dimensional (2D) X-ray projections. Our approach introduces a feature-space transformation between a 2D projection and a 3D volumetric CT image within a representation{\textendash}generation (encoder{\textendash}decoder) framework. By using the transformation module, we transfer the representations learned from the 2D projection to a representative tensor for 3D volume reconstruction in the subsequent generation network. Through the model-training process, the transformation module learns the underlying relationship between feature representations across dimensionality, making it possible to generate a volumetric CT image from a 2D projection. It should be emphasized that an X-ray projection is not a purely 2D cross-sectional image, as higher dimensional information is already encoded during the projection process (see schematic in Fig. (\ensuremath{<}\ensuremath{>})1a), with the encoding function determined by the physics of interactions between the X-ray and media. Generally, a single projection alone is not sufficient for capturing the anatomical information in the projection direction for the subsequent volumetric image reconstruction. What enables our deep-learning model for patient-specific volumetric image reconstruction is that anatomical relations (including the information in the direction of the projection view) are encoded during the model-training process via the use of augmented datasets containing different 2D{\textendash}3D data pairs of body positions and anatomical distributions. The deep-learning transformation deciphers the hidden information in the projection data and predicts a volumetric image with the help of prior knowledge gained during model training (Fig. (\ensuremath{<}\ensuremath{>})1d). 
Fig. 1 | 3d image reconstruction with ultra-sparse projection-view data. a, A geometric view of an X-ray source, a patient and a detector in a CT system. b, X-ray projection views of a patient from three different angles. c, Different image-reconstruction schemes in the context of prior knowledge and projection sampling. d, Volumetric image reconstruction using deep learning with one or multiple 2D projection images. 
Fig. 2 | architecture of the deep-learning network. a, The input of the model is a single projection view or multiple 2D projection views. b, The representation network learns the feature representation of the imaged object from the input. c, The extracted 2D features are reshaped and transferred by the transformation module to a 3D representation, for subsequent reconstruction. d, The generation network uses representation features extracted in the former stages to generate the corresponding volumetric images. e, The output of the model is the correponding volumetric image. Conv, convolution layer; deconv, deconvolution layer (the numbers indicate the specific kernel size used); BN, batch normalization; ReLU, rectified linear unit; +, feature-map addition with residual path; the numbers underneath each layer denote the number of feature maps for each layer. 
results 
Fig. 3 | training-loss and validation-loss curves for the abdominal ct and lung ct cases. a,b, Graph of mean-squared error (MSE) loss for training data (in blue) and validation data (in orange) against the number of training iterations for abdominal CT (a) and lung CT (b), during the image reconstruction with a single projection view. Details are shown in the zoom-in figures on the right. 
embedding features and learns a semantic representation of the actual 3D scene from the input 2D projection(s). The transformation module bridges the representation and generation networks through convolution and deconvolution operations and relates the 2D and 3D feature representations. The role of the generation network is to provide volumetric images with subtle structures on the basis of the learned features from the representation network. In constructing the model, we assume that one or more 2D projections and the corresponding 3D image possess the same semantic representation, as they represent the same object or scene. In other words, the representation in feature space remains invariant in the transformation of a 2D projection into a 3D image. To a large extent, the task of 3D image reconstruction here is to train the encoder (that is, the representation network) and decoder (that is, the generation network) to reliably learn the relationship between the feature space and the image space. Details about the network architecture are included in Methods. 
We evaluate the approach by using different disease sites: an upper-abdominal case, a lung case and a head-and-neck case. We use the anterior{\textendash}posterior 2D projection as input (Fig. (\ensuremath{<}\ensuremath{>})2). In all experiments, the same network architecture and training strategy are used. The loss curves (Fig. (\ensuremath{<}\ensuremath{>})3) indicate that the model is trained to fit the training data well and can also be generalized to work on the data not included in the training datasets. The details of dataset generation and the training process are described in Methods. 
To evaluate the feasibility of the approach, we deploy the trained network on an independent testing dataset. Figure (\ensuremath{<}\ensuremath{>})4a shows our reconstruction results with a single anterior{\textendash}posterior-view input for the abdominal CT and lung CT cases, together with the ground-truth CT images and the difference images between the obtained images and the ground truth. The deep-learning-derived images resemble the target images, indicating the potential of the model for volumetric imaging. We also reconstruct volumetric images 
Fig. 4 | examples from the abdominal ct and lung ct cases. a{\textendash}d, Images reconstructed by using 1 (a), 2 (b), 5 (c) and 10 (d) projection views. Predicted and difference images between predicted and ground truth are shown. The corresponding coronal and sagittal views of the images for both experiments are presented in Supplementary Figs. 3{\textendash}6. For the abdominal CT case, 720, 180 and 600 images are used for training, validation and testing, respectively. For the lung CT case, 2,400, 600 and 200 images are used for training, validation and testing, respectively. 
with a single lateral view as input for the abdominal case, with similar results (see the Experiments section of the Supplementary Information). Furthermore, we use multiple quantitative evalua-tion metrics to measure the results. Table (\ensuremath{<}\ensuremath{>})1 summarizes the average values of the evaluation metrics. The qualitative and quantitative results demonstrate that our model is capable of achieving 3D image reconstruction even with only a single 2D projection. The results (Fig. (\ensuremath{<}\ensuremath{>})5) also confirm the validity of our approach. 
discussion 
To better understand the deep-learning model, we analyse the semantic representations learned from the model. Generally 
Fig. 5 | examples from the head-and-neck ct case. a, 3D CT images of a head-and-neck case used for the training of the deep-learning model. b, Left: testing samples and the corresponding difference images (with respect to the training samples) in the transverse, sagittal and coronal planes. Right: predicted images and the corresponding difference images (with respect to the ground truth) in the transverse, sagittal and coronal planes. For this case, 2,000, 500 and 200 images are used for training, validation and testing, respectively. 
Fig. 6 | analysis of feature maps. a, Visualization of the feature maps, learned from different 2D projections, for two testing samples. The different colours in the figure indicate different intensity values in the feature maps (a lighter colour indicates a higher intensity value). b, t-SNE visualization of the feature representations of 15 testing examples with the input of different 2D views. A total of 15 clusters (4 points of the same colour) are shown. The four points in a cluster represent the learned features from 1-, 2-, 5- and 10-view reconstruction models. Each cluster denotes the embedded representations for each of the 15 randomly chosen testing samples. c, Correlation matrix of representation vectors in 1-view and 2-view reconstructions from 50 randomly chosen testing samples out of 600. 
from the transformation module for two testing samples. For visualization purposes, only 5 randomly chosen channels among the 4,096 feature maps are shown, each with a size of 4 {\texttimes}4 pixels. The feature maps learned from different numbers of 2D projections are displayed separately in different columns. The results show that, when different 2D views are given, the model extracts similar semantic representations of the underlying 3D scene. Furthermore, Fig. (\ensuremath{<}\ensuremath{>})6b shows the visualization of t-distributed stochastic neighbour embedding (t-SNE) for the feature maps of 15 testing samples. The t-SNE technique is commonly used to visualize high-dimensional data by embedding each sample as a point in a 2D space. The four points in a cluster of the same colour represent the learned features from one-, two-, five- and ten-view reconstructions. The figure shows clustering behaviour for feature maps from the same sample, indicating that the model learns a similar representation from different 2D projections. 
Outlook. We have described a deep-learning approach for volumetric imaging with ultra-sparse data sampling and a patient-specific prior. The data-driven strategy is capable of holistically extracting the feature characteristics embedded in a single projection or in a few 2D projections, and of transforming them into the corresponding 3D image through model learning. The image-feature space transformation plays an essential role in the ultra-sparse image reconstruction. At the training stage, the method incorporates diverse forms of a priori knowledge into the reconstruction. The manifold-mapping function is learned from the training datasets, rather than relying on any ad hoc form of motion trajectory. Although we have used X-ray imaging and patient-specific data, the concept and implementation of the approach could be extended to other imaging modalities or to other data domains with ultra-sparse sampling. Practically, single-view imaging represents a potential solution for many image-guided interventional procedures and may help to simplify the hardware of tomographic imaging systems. 
methods 
Problem formulation. We formulate the problem of 3D image reconstruction from 2D projection(s) into a deep-learning framework. Given a sequence of 2D projections denoted as  where  for alland N is the number of given 2D projections, the goal is to generate a volumetric 3D image Y describing the corresponding 3D physical scene. With the sequence of 2D projections as input, the deep-learning model outputs the predicted 3D volume denoted as  while  is the ground-truth 3D image as the reconstruction target. Note that network prediction Ypred is of the same size as ground-truth image Ytruth, where each entry is a voxel-wise intensity value. Thus, the problem is formulated as finding a mapping function F transforming 2D projections to volumetric images. To tackle this problem, a deep-learning model is trained to find the mapping function F, which uses 2D projections fX1; X2; ; XN g as input and predicts the corresponding 3D image Ypred, as expressed in Equation ((\ensuremath{<}\ensuremath{>})1). 
{\dh}1{\TH} 
In order to use a sequence of 2D projections as model input, we stack all the 2D  as a 3D tensor. In other words, a set of 2D projections  are stacked as a 3D volume where N is the number of 2D projections. In what follows, we introduce the model architecture of the deep neural network in detail. 
Representation network. Superb performance has been achieved by deep residual networks (such as ResNet) in many tasks. A key step in residual learning is the identity mapping that facilitates the training process and avoids gradient vanish in back-propagation, which encourages residual learning of the hierarchical representation at each stage and eases the training of the deep network. Motivated by this feature, we introduce a residual-learning scheme in the representation network (Fig. (\ensuremath{<}\ensuremath{>})2), in which the 2D convolution residual block is used to assist the deep model to learn semantic representations from 2D projections. More details about the residual-learning scheme are presented in Supplementary Information, Ablative study and discussion, and the results are summarized in Supplementary Table 2. Specifically, each 2D convolution residual block consists of a pattern of {\textquoteleft}2D convolution layer (with kernel size 4 and stride 2) {\textrightarrow} 2D batch normalization layer {\textrightarrow} ReLU layer {\textrightarrow} 2D convolution layer (with kernel size 3 and stride 1) {\textrightarrow} 2D batch normalization layer {\textrightarrow} ReLU layer{\textquoteright}. The first layer performs 2D convolution operations using a 4 {\texttimes} 4 kernel with sliding stride 2 {\texttimes} 2, which down-samples the spatial size of the feature map by a factor 2. In addition, to keep the sparsity of high-dimensional feature representation, we correspondingly double the channel number of the feature maps by increasing the number of convolutional filters. A distribution normalization layer among the training mini-batch (batch normalization) then follows before feeding the feature maps through the ReLU layer. Next, the second 2D convolution layer and 2D batch normalization layer are done by a kernel size of 3 {\texttimes} 3 and sliding stride 1 {\texttimes} 1, which keeps the spatial shape of the feature maps. Moreover, before applying the second ReLU layer, an extra shortcut path is established to add up the output of the first convolution layer to obtain the final output. By setting up the shortcut path of identity mapping, the second convolution layer is encouraged to learn the residual feature representations. To extract hierarchical semantic features from 2D projections, we constructed the representation network by concatenating five 2D convolution residual blocks with different number of convolutional filters. A detailed discussion of the network depth is available in Supplementary Information, Ablative study and discussion, with some results illustrated in Supplementary Fig. 8. To be concise, we use the notation k {\texttimes} m {\texttimes} n to denote k channels of feature maps in a spatial size of m {\texttimes} n. In the generation network, the size of input images is denoted as N {\texttimes} 128 {\texttimes} 128, where N is the number of 2D projections. The change of feature-map size through the network is N {\texttimes} 128 {\texttimes} 128 {\textrightarrow} 256 {\texttimes} 64 {\texttimes} 64 {\textrightarrow} 512 {\texttimes} 32 {\texttimes} 32 {\textrightarrow} 1024 {\texttimes} 16 {\texttimes} 16 {\textrightarrow} 2048 {\texttimes} 8 {\texttimes} 8 {\textrightarrow} 4096 {\texttimes} 4 {\texttimes} 4, where each {\textquoteleft}{\textrightarrow} {\textquoteright} represents going through a 2D convolution residual block as described above, except that batch normalization and ReLU activation are removed in the first convolution layer. Thus, the output of the representation network is a feature representation  extracted from 2D projections with a size of 4096 {\texttimes} 4 {\texttimes} 4. 
Transformation module. To bridge the representation and generation networks, a transformation module is deployed after learning the representations. As shown in Fig. (\ensuremath{<}\ensuremath{>})2, by taking the convolution operations with a kernel size of 1 {\texttimes} 1 and ReLU activation, the 2D convolution layer learns a transformation across all 2D feature maps. Then, we reshape embedded representations from 4096 {\texttimes} 4 {\texttimes} 4 to 2048 {\texttimes} 2 {\texttimes} 4 {\texttimes} 4. In this way, we transform the feature representation across dimensions for subsequent 3D volume generation. Next, a 3D deconvolution layer with a kernel size of 1 {\texttimes} 1 {\texttimes} 1 and sliding stride of 1 {\texttimes} 1 {\texttimes} 1 learns a transformation among all 3D feature cubes while keeping the feature size unchanged. This transformation module bridges the 2D and 3D feature spaces. Moreover, as described in previous work, we also remove the batch normalization in the transformation module to help the knowledge transfer through this module. 
Materials. The approach is evaluated by using three cases of different disease sites. In the first study, a ten-phase upper-abdominal 4D CT scan of a patient for radiation therapy treatment planning is selected. To proceed, the first six phases are used to generate the CT-DRR pairs for model training and validation with the procedure described above. We use the anterior{\textendash}posterior 2D projection as input (Fig. (\ensuremath{<}\ensuremath{>})2). With translation, rotation and deformation introduced to the CT volume, we obtain a total of 720 DRRs representing different scenarios of the patient anatomy for model training and 180 DRRs for validation. To ensure that the testing data are not seen in the model-training process, we generate 600 testing DRR samples independently from the remaining 4 phases of the 4D CT. The 4D CT images are acquired with 120 kV, 80 mA on a positron emission tomography{\textendash} CT simulator (Biograph mCT 128, Siemens Medical Solutions) together with a Varian Real-time Position Management system (Varian Medical Systems). The 2D projection data are obtained by projecting each of the 3D CT data points in the geometry of the on-board imager of the TrueBeam system (Varian Medical System). In the second experiment, a lung cancer patient is chosen with two independent treatment-planning 4D CT scans acquired at two different times with the same imaging parameter settings as above. Using the data-augmentation strategy as described above, the first 4D CT is used to generate training (2,400 samples) and validation (600 samples) datasets, whereas the second 4D CT was used to generate a testing dataset (200 samples). For each of the images in the training and testing datasets, the corresponding 2D projections are produced by projecting the 3D CT volume in the geometry of the on-board imager of the TrueBeam system. To build a reliable model, the training and testing datasets might come from the same data distribution, but the datasets are independently sampled. The data acquisition and processing for the head-and-neck case are described in Supplementary Information. 
Image pre-processing. Data are pre-processed before feeding them into the network. First, we resize all data samples to the same size. For example, all the 2D projection images are resized to 128 {\texttimes} 128. The volumetric images of the abdominal CT and the lung CT are resized to 46 {\texttimes} 128 {\texttimes} 128 and 168 {\texttimes} 128 {\texttimes} 128 respectively, due to their different depths in the z axis. Each data sample is a pair of 2D projected view(s) and the corresponding 3D CT. Similar to other deep-learning-based imaging studies, down-sampling is introduced purely because of the memory limitation and for the purpose of computational efficiency. The formulation and algorithm are scalable to full-size images (512 {\texttimes} 512), because the component layers used in our model are also scalable to images of different sizes. At the current resolution of 128 {\texttimes} 128 (which is the same as that used in the deep-learning-based MRI reconstruction), small motions of less than 3 mm may not be described accurately. However, we should emphasize that this resolution does not represent a fundamental limit of the deep-learning-based approach and can be improved as computational technology advances. In practice, methods such as deep-learning-based super-resolution are being actively pursued, which may be employed to improve the spatial resolution of the approach. Additionally, following the standard protocol of data pre-processing, we conduct scaling normalization for both the 2D projections and the 3D volumetric images, where pixel-wise or voxel-wise intensities are normalized to the interval [0,1]. Moreover, we normalize the statistical distribution of the pixel-wise intensity values in the input 2D projections to be closer to a standard Gaussian distribution N {\dh}0; 1{\TH}. Specifically, we calculate the statistical mean and standard derivation among all the training data. When a new sample is inputted, we subtract the mean value from the input image(s) and divide the image(s) by the standard derivation to get the input 2D image(s). 
Evaluation. To evaluate the performance of the approach, we deploy the trained model on a testing dataset, and analyse the reconstruction results using both qualitative and quantitative evaluation metrics. We use four different metrics to measure the quality of predicted 3D images: MAE, RMSE, SSIM and PSNR.  We compute the average values across all testing samples, and they are shown in Table (\ensuremath{<}\ensuremath{>})1. MAE/MSE is the L1-norm/L2-norm error between Ypred and Ytruth. As usual, we take the square root of MSE to get RMSE. In practice, MAE and RMSE are commonly used to estimate the difference between the prediction and ground-truth images. SSIM score is calculated with a windowing approach in an image, and is used for measuring the overall similarity between two images. In general, a lower value of MAE and RMSE or a higher SSIM score indicates a better prediction closer to the ground-truth images. PSNR is defined as the ratio between the maximum signal power and the noise power that affects the image quality.  PSNR is widely used to measure the quality of image reconstruction. 
Comparison study. To better benchmark the proposed method against the existing techniques, we conduct a comparative study with the published PCA-based method{\textendash} and elaborate the difference and advantages of our proposed approach. The comparison is done for a special situation of 4D CT reconstruction (abdominal CT), where the anatomical motion may be characterized by principal components. We find that the PCA and deep-learning-based methods produce similar results in an ideal case when there is no inter-scan variation in patient positioning (since the results are very similar, the resultant images are not shown). However, the deep-learning model outperforms the PCA method in more realistic scenarios when the patient position deviates slightly from that of the reference  scan (see Supplementary Information for details). 
Reporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article. 
data availability 
The authors declare that the main data supporting the results in this study are available within the paper and its Supplementary Information. The raw datasets from Stanford Hospital are protected because of patient privacy yet can be made available upon request provided that approval is obtained after an Institutional Review Board procedure at Stanford. 
code availability 
The source code of the deep-learning algorithm is available for research uses at (\ensuremath{<}https://github.com/liyues/PatRecon\ensuremath{>})https://github.com/liyues/PatRecon. 
Received: 29 November 2018; Accepted: 19 September 2019; Published online: 28 October 2019 
references 
acknowledgements 
This research is partially supported by the National Institutes of Health (R01CA176553 and R01EB016777). The contents of this article are solely the responsibility of the authors and do not necessarily represent the official NIH views. 
author contributions 
L.X. proposed the original notion of single-view reconstruction for tomographic imaging and supervised the research, L.S. designed and implemented the algorithm. W.Z. designed the experiments and implemented the data generation process. L.S. and W.Z. carried out experimental work. L.X., L.S. and W.Z. wrote the manuscript. All the authors reviewed the manuscript. 
competing interests 
The authors declare no competing interests. 
additional information 
Supplementary information is available for this paper at (\ensuremath{<}https://doi.org/10.1038/s41551-019-0466-4\ensuremath{>})https://doi.org/10.1038/ (\ensuremath{<}https://doi.org/10.1038/s41551-019-0466-4\ensuremath{>})s41551-019-0466-4. 
Correspondence and requests for materials should be addressed to L.X. 
Reprints and permissions information is available at (\ensuremath{<}http://www.nature.com/reprints\ensuremath{>})www.nature.com/reprints. 
Publisher{\textquoteright}s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 
Reporting Summary 
Corresponding author(s): Lei Xing 
Last updated by author(s): Sep 19, 2019 
Nature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Research policies, see Authors \& Referees and the Editorial Policy Checklist. 
For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section. 
Our web collection on statistics for biologists contains articles on many of the points above. 
Policy information about availability of computer code 
For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code \& software for further information. 
Policy information about availability of data 
All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: 
The authors declare that the main data supporting the results in this study are available within the paper and its Supplementary Information. The raw datasets from Stanford Hospital are protected because of patient privacy yet can be made available upon request provided that approval is obtained after an Institutional Review Board procedure at Stanford. 
Field-specific reporting 
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection. 
Life sciences 
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf 
Life sciences study design 
All studies must disclose on these points even when the disclosure is negative. 
Reporting for specific materials, systems and methods 
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. 
Policy information about clinical studies 
All manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions. 
nature research  |  reporting summary 
October 2018 