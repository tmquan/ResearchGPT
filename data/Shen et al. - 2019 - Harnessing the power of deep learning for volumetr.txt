PROCEEDINGS OF SPIE 

Harnessing the power of deep learning for volumetric CT imaging with single or limited number of projections 

Liyue Shen, Wei Zhao, Lei Xing 

Liyue Shen, Wei Zhao, Lei Xing, ''Harnessing the power of deep learning for volumetric CT imaging with single or limited number of projections,'' Proc. SPIE 10948, Medical Imaging 2019: Physics of Medical Imaging, 1094826 (1 March 2019); doi: 10.1117/12.2513032 

Event: SPIE Medical Imaging, 2019, San Diego, California, United States 

Liyue Shena , Wei Zhaob , and Lei Xinga,b 

ABSTRACT 

Tomographic imaging using a penetrating wave, such as X-ray, light and microwave, is a fundamental approach to generate cross-sectional views of internal anatomy in a living subject or interrogate material composition of an object and plays an important role in modern science. To obtain an image free of aliasing artifacts, a sufficiently dense angular sampling that satisfies the Shannon-Nyquist criterion is required. In the past two decades, image reconstruction strategy with sparse sampling has been investigated extensively using approaches such as compressed-sensing. This type of approach is, however, ad hoc in nature as it encourages certain form of images. Recent advancement in deep learning provides an enabling tool to transform the way that an image is constructed. Along this line, Zhu et al1 presented a data-driven supervised learning framework to relate the sensor and image domain data and applied the method to magnetic resonance imaging (MRI). Here we investigate a deep learning strategy of tomographic X-ray imaging in the limit of a single-view projection data input. For the first time, we introduce the concept of dimension transformation in image feature domain to facilitate volumetric imaging by using a single or multiple 2D projections. The mechanism here is fundamentally different from the traditional approaches in that the image formation is driven by prior knowledge casted in the deep learning model. This work pushes the boundary of tomographic imaging to the single-view limit and opens new opportunities for numerous practical applications, such as image guided interventions and security inspections. It may also revolutionize the hardware design of future tomographic imaging systems. 

Keywords: image reconstruction, deep learning, convolutional neural network, sparse data sampling 

1. INTRODUCTION 

The ability of tomographic imaging to take a deep and quantitative look into a patient or an object with high spatial resolution holds significant value in scientific explorations and medical practice. Traditionally, a tomographic image is obtained by mathematical inversion of the encoding function of the imaging wave for a given set of measured data from different angular positions. A prerequisite of artifacts-free inversion is the satisfaction of classical Shannon-Nyquist theorem in angular data sampling, which imposes a practically achievable limit in imaging time and object irradiation. To mitigate the problem, image reconstruction strategy with sparse or limited sampling has been investigated extensively using techniques such as compressed-sensing,2{\textendash}5 and maximum a posteriori.6, 7 This type of approach introduces a regularization term to the fidelity function to encourage some ad hoc or presumed characteristics in the resultant image.8{\textendash}17 The resultant sparsity without compromising the image quality is generally limited and does not meet the unmet demand for real-time imaging with substantially reduced subject irradiation. Indeed, while continuous effort has been made in imaging with reduced angular measurements over the years, tomographic imaging with ultra-sparse sampling has yet to be realized. In this work, we push the sparsity to the limit of a single projection and demonstrate what seemingly unlikely scenario of a single-view tomographic imaging is readily achievable by leveraging from the state-of-the-art deep learning technique and seamless integration of prior knowledge in the deep learning-based image reconstruction process. 

Deep neural network has recently attracted much attention for its unprecedented ability to learn complex relationships and incorporate existing knowledge into the inference model through feature extraction and representation learning.18{\textendash}20 The method has found widespread applications across disciplines, such as computer 

Medical Imaging 2019: Physics of Medical Imaging, edited by Taly Gilat Schmidt, Guang-Hong Chen, Hilde Bosmans, Proc. of SPIE Vol. 10948, 1094826 {\textperiodcentered} {\textcopyright} 2019 SPIE {\textperiodcentered} CCC code: 1605-7422/19/\$18 {\textperiodcentered} doi: 10.1117/12.2513032 

Table 1: Table 1. Quantitative reconstruction results. Different numbers of 2D projections are utilized for prediction. We evaluate on testing dataset and display average values of four metrics including mean absolute error (MAE), root mean squared error (RMSE), structural similarity (SSIM), and peak signal noise ratio (PSNR). 

using corresponding mean and variance, which is usually used to make the data distribution closer to normal distribution in statistics. 

For the training objective, we define the cost function based on the mean squared error (L2 norm loss) between the predicted results and the ground truth. For comparison purpose, we use the same training strategy and hyper-parameters for all experiments. We implement the network with PyTorch36 deep learning framework. In training process, we use a mean squared error (MSE) loss function to compute voxel-wise mean-squared-error between the ground truth and predicted 3D images as defined below. By using a random initialization for network parameters, the Adam optimizer37 is utilized to minimize loss objective and update network parameters through back-propagation with iterative epochs. Specifically, we use the learning rate of 0.00002 and the mini-batch size of 1 because of the memory limitation. As shown in 4, the training loss objective is minimized iteratively. And at the end of each epoch, the trained model is evaluated on an independent validation data set. This strategy is commonly used to monitor the model performance and avoid overfitting the training samples. In addition, learning rate is scheduled decaying according to the validation loss. Specifically, if the validation loss remains unchanged for 10 epochs, learning rate will be reduced by a factor 2. Finally, the best checkpoint model with the smallest validation loss is selected as final model in the experiments. We train the network using one NVIDIA TITAN V100 graphics processing unit (GPU) for 100 epochs. 

Reconstruction experiments were conducted using the proposed method and 4D-CT data as described above. To evaluate the performance of our network, we deployed the trained model on a separate testing dataset and analyzed reconstruction results using both qualitative and quantitative evaluation metrics. Specially, in order to investigate reconstruction performance with different number of 2D projections, we have conducted experiments with 1, 2, 5, and 10 projections as input for comparison purpose. The multiple view angles are distributed evenly around a 180-degree semicircle. For instance, for 2-views, the two orthogonal directions are 0 degree (AP) and 90 degrees (lateral). Here we stack the 2D projections from different view angles as different channels of the input data and modify the first convolution layer to fit the input data size. 

With the same model training procedure and hyper-parameter, we obtain the qualitative reconstructed CT images for 1, 2, 5, and 10 views in Figs. 5-7. In the figures, we display our reconstruction results for one example chosen from testing set. Specially, we demonstrate the 3D CT image from three view points: axial (Fig. 5), coronal (Fig. 6), and sagittal (Fig. 7). For visualization purpose, in each figure, each column shows one slice image selected from the predicted 3D images together with the ground truth. The difference image between the predicted image and ground truth image is also shown. It is seen that the prediction images are very similar to the target images, which shows that our model performs well for 3D CT reconstruction even with only a single projection. 

For quantitative evaluation, the metrics of mean absolute error (MAE), root mean squared error (RMSE), structural similarity (SSIM) are calculated to measure the prediction error between estimated images and ground truth images. In addition, we also compute the peak signal noise ratio (PSNR) to show the reconstructed image 

quality. By computing the average values of various evaluation metrics for all the 100 examples in testing set, the quantitative evaluation of the results are presented in Table 1. Interestingly, we observed that a single 2D projection provides sufficient data to produce a high-quality reconstruction similar to the reconstructions performed with multiple projection images, when comparing the quantitative evaluation metrics. 

From above results, we conclude that the proposed deep learning reconstruction framework is capable of providing reasonable 3D images by using only a single or a few view projections. 

4. CONCLUSION 

We have presented a novel deep learning framework for volumetric imaging with ultra-sparse data sampling. It is both intriguing and important that the proposed strategy is capable of holistically extracting the feature characteristics embedded in a single or a few 2D projection data and transform them into the corresponding 3D image with high fidelity. Although this work is focused on the most commonly used X-ray imaging, the concept and implementation should be easily extendable to other imaging modalities with ultra-sparse sampling. Practically, the single-view imaging may present a revolutionary solution to various practical applications, ranging from image guidance in interventions, cellular imaging, objection inspection, to greatly simplified imaging system design. A few medical applications along these directions are underway. 

REFERENCES 
