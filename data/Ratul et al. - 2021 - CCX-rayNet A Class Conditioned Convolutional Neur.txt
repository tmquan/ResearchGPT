CCX-RAYNET: A CLASS CONDITIONED CONVOLUTIONAL NEURAL NETWORK FOR BIPLANAR X-RAYS TO CT VOLUME 

2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) | 978-1-6654-1246-9/20/\$31.00 {\textcopyright}2021 IEEE | DOI: 10.1109/ISBI48211.2021.9433870

School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada 

ABSTRACT 

Despite the advancement of the deep neural network, the 3D CT reconstruction from its correspondence 2D X-ray is still a challenging task in computer vision. To tackle this issue here, we proposed a new class-conditioned network, namely CCX-rayNet, which is proficient in recapturing the shapes and textures with prior semantic information in the resulting CT volume. Firstly, we propose a Deep Feature Transform (DFT) module to modulate the 2D feature maps of semantic segmentation spatially by generating the affine transformation parameters. Secondly, by bridging 2D and 3D features (Depth-Aware Connection), we heighten the feature representation of the X-ray image. Particularly, we approximate a 3D attention mask to be employed on the enlarged 3D feature map, where the contextual association is emphasized. Furthermore, in the biplanar view model, we incorporate the Adaptive Feature Fusion (AFF) module to relieve the registration problem that occurs with unrestrained input data by using the similarity matrix. As far as we are aware, this is the first study to utilize prior semantic knowledge in the 3D CT reconstruction. Both qualitative and quantitative analyses manifest that our proposed CCX-rayNet outperforms the baseline method. 

Index Terms{\textemdash} CT Reconstruction, GAN, X-ray, Medical Imaging, CNN, LS-GAN, 3D Patch Discriminator 

1. INTRODUCTION 

Unlike several previous methods where hundreds of X-rays are required, for CCX-rayNet, we need at most two X-rays to provide a corresponding CT. Here, the contributions of our work are three folds: (1) to the best of our knowledge, we are the first to apply the semantic prior constraint during 3D CT reconstruction to provide the network distinct anatomical structures and textures information about different organs as different organs with different semantics should treat differently. Inspired by [3], we proposed DFT modules to modulate the 2D feature maps of semantic segmentation spatially. This procedure provides the structure and textures of different organs in our system to tackle the shape distortion problem. (2) We proposed Depth Aware Connection (DAC) to lessen the depth information loss and to remove insignificant features when bridging 2D and 3D features. (3) The proposed AFF module used weighted sum instead of the average sum to fuse the multiple views of features. The AFF module vastly relies on the attention mechanism that helps the network amplify the most productive features and restrain the unregistered ones. 

2. METHOD AND MATERIALS 

We have two parallel encoder-decoder networks for frontal and lateral X-rays, and a fusion network (in the middle) includes 3D basic blocks to fuse the information. We follow the same structure as X2CT [2] for the reconstruction and utilize UNet [4] to achieve the frontal prior segmentation information and feed this in the frontal encoder-decoder network. We precisely plug-in the DFT and DAC in encoder-decoder networks and attached AFF in the fusion network. We apply a 3D patch discriminator [5] with CCX-rayNet to produce CCX-rayGAN to improve the visualization of CT volumes. 

As reflected in Fig. 1, to conserve the shape information and topology, we fed the segmentation map of the frontal view X-

Fig. 1. The generator part of biplanar CCX-rayNet contains two encoder-decoder networks. It includes the proposed DFT, DAC, and AFF modules. The input of the system is posterior-anterior and lateral views of X-ray, and the segmentation map. 

ray into the condition network to originate the conditions. To transform the 2D feature, the DFT module uses the mapping function M to provide modulation parameter pair (\ensuremath{\alpha}, \ensuremath{\beta}). 

(1) 

Where c indicates the input conditions. The main feature map will be shifted and scaled after obtaining (\ensuremath{\alpha}, \ensuremath{\beta}). 

(2) 

Fin has the similar dimension of \ensuremath{\alpha} and \ensuremath{\beta}. With specific semantic prior information, the element-wise addition (+) and element-wise product  use to perform an element-wise transformation in DFT module. As illustrated in Fig. 1, conditions have been set as shared intermediate values and transmitted to every 2D encoder so that the DFT can acquire few parameters. End-to-end training use to optimize M. 

At first, the intermediate 3D feature map has been expanded from a 2D feature map by duplicating this 2D one along the third axis. Then, employ the 3D feature map into a basic CNN block impose it with a shape of (B, C, D, H, W). Next, from the 2D feature map, originate an attention matrix with the shape of (B, D, H, W). Lastly, we enhance the attention matrix C times and perform the element-wise multiplication with the 3D feature map. The action sums up as: 

I, G, and G{\textasciicircum} denote the input 2D feature map, intermediate 3D feature map, and 3D feature map before applying the attention map, respectively. Moreover, expanding operation is signified as E, and the attention matrix is produced from I. 

(7) 

Here, we compute a similarity matrix SIM for the distance of two 3D feature maps in the fusing part. I1 and I2 are orthogonal feature maps. The similarity matrix is a typical blueprint of the non-local module and is utilized when corresponding feature voxels are consistent. We apply the dot distance for distance measurement due to computational issues. Besides, two weighting matrices flexibly enhance one of the feature point and compress the other one{\textquoteright}s contribution. 

(8) 

(9) 

The conv 3D does not share the weights, and the weighted sum is implemented to fuse the features by multiplying the 3D feature maps with the corresponding weighting matrices. 

(10) 

Fig. 2. Posterior-anterior chest X-rays from the dataset (first row); predicted segmentation map from UNets (second row). 

For training, we require an X-ray and CT paired dataset. Ying et al. [2] provide synthetic X-rays from real CT scans by employing digitally reconstructed radiograph (DRR) technology followed by CycleGAN [6]. The 1018 chest CT scans are gathered from the LIDC-IDRI dataset [7]. For training and testing, we use 916 and 102 CT scans, respectively. 

3. EXPERIMENT 

To show the efficacy of our proposed CCX-rayNet in this section, we provide the model{\textquoteright}s qualitative and quantitive analyses. We also display the network settings and ablation study. 

We train this network for 100 epochs with Adam optimizer, where the initial learning rate is 2e\ensuremath{-} 3 and decay the learning rate after 50 epochs with 30 percent. For training, we apply instance normalization instead of batch normalization. 

We follow two different approaches to train our model: (1) the projection pixel-wise L1 loss and voxel-wise loss are incorporated, signified as CCX-rayNet. (2) the GAN-based training approach is called CCX-rayGAN, where the backpropagation steps of the generator and discriminator imitate the same procedure as in LS-GAN [11]. We resized the frontal and lateral view X-rays, and frontal view segmentation map to 128{\texttimes}128 for training. In the frontal view segmentation map, we acquire two categories, such as the heart and lungs, but the {\textquoteleft}background{\textquoteright} category is utilized to encompass regions that do not include in the categories above. Finally, the output dimension of these models is 128 {\texttimes} 128 {\texttimes} 128. 

Fig. 3. Lateral view (first row) and axial view (second row) from a CT volume yielded by CCX-rayGAN+B and X2CTGAN+B. CCX-rayGAN+B generates precise anatomical reconstructions than X2CT-GAN+B. 

CT volume reconstruction from X-rays is comparatively a new proposed approach, and our CCX-rayGAN+B can vastly enhance the perceptual quality of the reconstruction result. In 

Fig. 4. 3D CT volumes are reconstructed from CCX-rayGAN and X2CT-GAN. {\textquoteright}+B{\textquoteright} denotes biplanar X-ray inputs. The first row indicates the posterior-anterior (PA) view of the CT, and the second row signifies the bone structure reconstruction. 

Fig. 3 and Fig. 4, we compare the visual quality of our GAN-based biplanar network and baseline X2CT-GAN+B method. 

Through the lateral and axial view of Fig.3, we can exhibit that CCX-rayGAN+B generates high-quality reconstruction outcomes with intricate anatomical parts compared to X2CTGAN+B. Specifically, our method yields sharper boundaries and internal textures of the organs compared to the state-of-the-art-method. Next, in Fig. 4, we can inspect that the CCX-rayGAN+B can reconstruct the outline shape of the organs (e.g., lungs) and small complex anatomies such as small vessels in the lungs posterior-anterior (PA) view (first row). Besides, our network can reconstruct the chest (ribs) bone structure and backbone (second row) close to the ground truth. We can produce superior results in both internal anatomies and bone structure than the baseline method X2CT-GAN+B. 

Table 2. Several combinations of proposed biplanar CCX-rayNet and the best outcome marked in bold 

achieve 3.4 dB more PSNR value in our biplanar method (CCX-rayNet+B) than the single view one (CCX-rayNet+S). 

For the CCX-rayNet+B, we assess the impacts of our proposed modules, and the result displayed in Table 2. Without three modules, the network{\textquoteright}s PSNR value is 27.29 dB; wheres the combination of DFT, DAC, and AFF modules improve PSNR value to 28.18 dB. Additionally, the DFT and DAC modules also enhance the PSNR value than the basic model. 

4. CONCLUSION 

This paper represents a class-conditioned network called CCX-rayNet, to reconstruct 3D CT volume from synthetic chest X-rays. We proposed three modules (DFT, DAC, AFF) to recapturing textures and shapes faithful to semantic classes in the practical-world scenario. In order to make the dataset, we use two binary UNet architectures to generate the segmentation maps. Our proposed biplanar method with prior semantic information restore density information, anatomical structure, and shape in the reconstructed 3D volumes with high visual quality than baseline models. Our ablation study proclaims the competence of our proposed modules. 

5. COMPLIANCE WITH ETHICAL STANDARDS 

This is a numerical simulation study for which no ethical approval was required. 

6. CONFLICTS OF INTEREST 

No funding was received for conducting this study. The authors have no relevant financial or non-financial interests to disclose. 

7. REFERENCES 
