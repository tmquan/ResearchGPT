X-CTRSNet: 3D cervical vertebra CT reconstruction and segmentation directly from 2D X-ray images 

abstract 

Orthogonal 2D cervical vertebra (C-vertebra) X-ray images have the advantages of high imaging efficiency, low radiation risk, easy operation and low cost for rapid primary clinical diagnoses. Especially in emergency departments, this technique is known to be significantly useful in triage for trauma patients. However, the technique can only provide overlapping anatomic information from limited projection views and is unable to visually exhibit full-view anatomy and precise stereo structures without further CT examination. To promote {\textquoteleft}{\textquoteleft}once is enough'' for visualizing 3D anatomy \& structures and reducing repetitive radiation as much as possible, we proposed X-CTRSNet for 2D X-ray images. This is the first powerful work that simultaneously and accurately enables 3D C-vertebra CT reconstruction and segmentation directly from orthogonally anteroposterior-and lateral-view 2D X-ray images. X-CTRSNet combines the reciprocally coupled SpaDRNet for reconstruction \& MulSISNet for segmentation, and a RSC Learning for tasks consistency. The experiment shows that X-CTRSNet successfully reconstructs and segments the 3D C-vertebra CT from the 2D X-ray images with a PSNR of 24.58 dB, an SSIM of 0.749, and an average Dice of 80.44\%. All these findings reveal the great potential of X-CTRSNet in clinical imaging and diagnosis to facilitate emergency triage by enabling precise 3D reconstruction and segmentation on 2D X-ray images. 

{\textcopyright} 2021 Elsevier B.V. All rights reserved. 

Accurate 3D cervical vertebra (C-vertebra) CT reconstruction and segmentation directly from orthogonal 2D C-vertebra X-ray images is clinically significant to distinctly enable a detailed 3D imaging diagnosis basis for clinicians, and effectively reduce repetitive radiation for patients, especially in assessing 

Fig. 1. 3D C-vertebra computed tomography (CT) scan can provide full-view anatomy and enable precise stereo structure, compared to the 2D X-ray images. 

and the no-overlapping distinct anatomic structure from different views [(\ensuremath{<}\ensuremath{>})11{\textendash}(\ensuremath{<}\ensuremath{>})13] such as the axial, coronal and sagittal planes in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})1 (b1), (b2) \& (b3). However, additional pricey CT scans may cause repetitive radiation and unnecessary medical resources occupation of over-treatment, and itself also has a high-dose radiation risk due to multi-slice dense projection [(\ensuremath{<}\ensuremath{>})4]. Besides, for the rapid triage of trauma patients in emergency department, it also needs too much time in the race against time, and may overwhelm medical resources in a short time [(\ensuremath{<}\ensuremath{>})14]. (3) For efficient image interpretation, surgical planning and objective assessment, 3D C-vertebra segmentation directly enables the precise stereo biological structures, as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})1(c). It effectively reflects the morphological shapes, relative locations, and physiological curves for the C-vertebras that has highly flexible anatomy vulnerable to injuries and degeneration. Therefore, it is of great clinical contribution to improve clinical diagnosis efficiency and speed up emergency triage, that with only rapid 2D X-ray image inputs and achieving 3D C-vertebra anatomy and structures as far as possible. It does not aim to replace CT examination completely, but can provide more 3D diagnostic basis on primary 2D X-ray imaging without additional time costs. 

As shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})2, we proposed the first powerful work, X-CTRSNet, which simultaneously and accurately enables 3D C-vertebra CT reconstruction and segmentation directly from widely accessible AP and LA view 2D X-ray images. X-CTRSNet is composed of three elements, including Spatial Decomposition-Reconstruction Net (SpaDRNet), Multi-scale Space Interoperability Segmentation Net (MulSISNet), and Reconstruction{\textendash} segmentation Consistency (RSC) Learning. The effects of these three specially-designed elements can be summarized as follows: (1) SpaDRNet (Sect. 2.1) is used to achieve the 3D C-vertebra CT reconstruction directly from AP \& LA projected 2D planes, as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(b). It consists of progressive 2D-to-3D conversion to multi-scale decompose the compressed space to the corresponding stereo location from the overlapped domain, hierarchical 3D fusion to interpretively sort out the 3D spatial information in consistency, and multi-view vgg loss to expressively guide the reconstructed scene content learning. And (2) MulSISNet (Sect. 2.2) further enables 3D C-vertebra semantic segmentation on reconstructed CT, and promotes shape constraints to SpaDRNet, as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(c). It comprehensively extracts rich stereo structure covering from the small-scale details to the large-scale distribution with the information densely interoperated among multi-scale 3D feature. Interactively, (3) RSC Learning (Sect. 2.3) enhances reconstruction{\textendash}segmentation consistent with the ground truth (GT) for the multi-task learning, as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(d). It promotes segmenting on the CT ground truth (GT), and optimize the divergence of the reconstructed CT{\textquoteright}s 

segmentation with it. The contributions of this work are summarized as following: 

Fig. 2. Workflow diagram of X-CTRSNet. 

Fig. 3. X-CTRSNet is achieved via reciprocally coupled SpaDRNet of reconstruction \& MulSISNet of Segmentation, and a RSC Learning of tasks consistency, to simultaneously enable 3D C-vertebra CT reconstruction and segmentation directly from the 2D X-ray images. 

unstable reconstruction; and deeply feeds back the divergence between two segmentations for the reconstructed biological CT anatomy. 

2. Methodology 

As shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(a), the proposed X-CTRSNet is conducted on the AP and LA views 2D X-ray images to directly make the 3D C-vertebra CT reconstruction and segmentation. So that it achieves the full-view anatomy and precise stereo structure, making up the shortage in 2D imaging. It is built by three collaborate elements: (1) SpaDRNet (R, Section (\ensuremath{<}\ensuremath{>})2.1) combines progressive 2Dto-3D multi-paths, hierarchical 3D fusion and multi-view vgg loss to decompose the overlapped 2D X-ray images into reconstructing the detailed 3D CT. (2) MulSISNet (S, Section (\ensuremath{<}\ensuremath{>})2.2) extracts the 

robust multi-scale stereo features for the reciprocal 3D semantic segmentation on reconstructed CT and shape constraint feedback. (3) RSC Learning (LRSC , Section (\ensuremath{<}\ensuremath{>})2.3) promotes segmenting on the CT GT and optimizes the divergence between two segmentations to drive the reconstruction{\textendash}segmentation consistency. Given the AP \& LA views 2D X-ray images xAP \&xLA, the GT of 3D CT yCT and segmentation ySeg , the target of X-CTRSNet is formulated as: 

(1) 

Our SpaDRNet in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(b) innovatively uses progressive 2Dto-3D conversion, hierarchical 3D fusion, and multi-view vgg loss, to decompose the latent dimension space in the overlapped 2D projection, and spatially correspondingly reconstruct into 3D CT images, directly from the AP and LA views X-ray images. 

As shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(b), the progressive 2D-to-3D conversion exploits the 2D collateral multi-paths and the view alignment, to extract the 3D spatial information existing in 2D projection. 

In detail, the 3D features with projected information extracted from AP and LA views are firstly permuted to be assigned to each other and 3D CT orientation. For LA-view 3D feature FeatLA3D(XLA, YLA, ZLA), the first dimension XLA represents the length dimension of LA-view X-ray image, and corresponds to the width dimension YCT of 3D CT data. Such real-word geometric correspondence of all dimensions between 3D features of X-ray images and CT data can be described as: 

XLA \ensuremath{\leftrightarrow} YCT 

ZLA \ensuremath{\leftrightarrow} XCT XAP \ensuremath{\leftrightarrow} XCT 

(5) 

where XLA, YLA, ZLA are dimensions in LA-view 3D feature FeatLA3D (XLA, YLA, ZLA), XAP , YAP , ZAP mean dimensions in AP-view 3D feature Feat3D YCT , ZCT denote dimensionsAP (XAP , YAP , ZAP ), and XCT , in CT feature FeatCT3D(XCT , YCT , ZCT ). Therefore, to promote view alignment, the permutations of FeatLA3D and FeatAP 3D are formulated as: 

(6) 

where P is the permutation operation, according to the order [{\textperiodcentered}]. Then, the permuted 3D features Feat '3LAD and Feat '3APD are concatenated along channel as: 

(7) 

And a following multi-scale 3D fusion is conducted to further merge the multi-view decomposed stereo information and integrate multi-scale structure. Given the permuted 3D features depicted as 

where SeqConv3D({\textperiodcentered}) represents sequentially conducting the 3D 3 {\texttimes} 3 {\texttimes} 3 convolution with Leaky ReLU activation function. D({\textperiodcentered}) is the down-sampling operation composed of consecutive strided 3 {\texttimes} 3 {\texttimes} 3 convolutions with stride 2, U({\textperiodcentered}) represents the simple nearest neighbor sampling following a 1 {\texttimes} 1 {\texttimes} 1 convolution. 

where is UConv({\textperiodcentered}) is the up-sampling operation by using the 3 {\texttimes} 3 {\texttimes} 3 transpose convolution with stride 2. 

Hierarchical 3D fusion thus explicitly interprets the consistent multi-scale structure in 3D C-vertebra CT, and reconstructs the stereo anatomy according to the strong spatial relation inter adjacent scale. 

For the assembled expressiveness of the voxels in 3D reconstructed CT, multi-view vgg loss is creatively developed on all planes along the axial, the coronal and the sagittal views. It guides content consistency among the voxels in the 3D scene to express the anatomy. It is directly transferred from the widely accepted pre-trained VGGNet. With the pre-trained VGGNet [(\ensuremath{<}\ensuremath{>})29], the loss extracts the high-level feature representations of expressing [(\ensuremath{<}\ensuremath{>})30] for the scene interpretation, and further constraints in multi-views for the stereo context. Given the reconstructed 3D CT y{\textasciicircum}CT and its GT yCT , multi-view vgg loss is defined as: 

where and are planes exported from along the axial, the coronal and the sagittal views. L, W and H are the length, width and height of y{\textasciicircum}CT . 

Furthermore, the loss function of SpaDRNet consists of lossMV \ensuremath{-}vgg for anatomy expression and lossMAE for voxel details, as follows: 

(11) 

Our MulSISNet in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(c) robustly utilizes multi-scale 3D extraction and interoperation to interactively make the 3D C-vertebra semantic segmentation from the reconstructed CT, and transfers the shape constraint to SpaDRNet. 

and 

(12) 

(13) 

Finally, the multi-scale stereo features are transmitted into the decoder to summarize the semantics for segmentation, formu-laically described as: 

2.2.2. Dice loss 

Reciprocally, on the basis of the reconstructed CT, there is a strong relation inter the tasks, so that MulSISNet further transfers the segmentation learning to make shape constraint on the reconstruction in SpaDRNet. The segmentation learning loss is calculated with dice as Eq. ((\ensuremath{<}\ensuremath{>})15), where and are voxel segmentation and its GT at (m, n, p). 

(15) 

Our RSC Learning in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})3(d) creatively introduces CT GT-to-segmentation learning, to drive the reconstruction{\textendash}segmentation consistency of the interactive tasks. 

Fig. 4. X-CTRSNet anatomically enables a 3D C-vertebra physiological structure illustration. 

(2) RSC Learning on reconstruction: It enhances the reconstruction as penalizing the reconstruction training with the divergence between the two segmentations of reconstructed CT and CT GT, so that deeply reinforces the reconstructed biological CT anatomical texture. By penalizing the inter-segmentations divergence, it can converge the reconstructed CT to the CT with the real CT consistent segmentation. Through beneficial interaction, RSC Learning enables X-CTRSNet the consistently and precisely coupled tasks reconstruction{\textendash}segmentation. 

Given the segmentations y{\textasciicircum}Seg , y{\textasciicircum}GTCT \ensuremath{-}Seg of the reconstructed CT and CT GT, the loss function is defined as: 

(16) 

3. Experiments and analysis 

Clinical data from 69 patients were used for the evaluation. The segmentation GT of C-vertebras (ordered as C1,C2,C3,C4,C5, C6 and C7) is labeled by two radiologists with cross-check. Specifically, Instance Normalization and Group Normalization are used in SpaDRNet of reconstruction and MulSISNet of segmentation, respectively. The network is implemented using Tensorflow with the Adam optimizer. The initial learning rate is set as 10\ensuremath{-}3 . Ten-folder cross validation is adopted in the performance evaluation and comparison. The dataset is divided into 10 groups. In the first nine groups, there were 7 patients in each group. And the last group contains 6 patients. In each validation, nine groups are used to train the network, and the last group is used for test. The procedure was repeated 10 times, until all the subjects have been processed. 

Structural similarity index (SSIM) [(\ensuremath{<}\ensuremath{>})32] and peak signal to noise ratio (PSNR) are employed to evaluate the reconstruction performance, as well as Dice coefficient (Dice) [(\ensuremath{<}\ensuremath{>})33] is used for segmentation assessment. SSIM is defined as 

(17) 

where {\textmu} means average, \ensuremath{\sigma} is standard deviation, cov({\textperiodcentered}) denotes covariance, c represents variables to stabilize the division with weak denominator. 

PSNR is calculated as 

(18) 

(19) 

As the last row in (\ensuremath{<}\ensuremath{>})Table (\ensuremath{<}\ensuremath{>})1 shows, the proposed X-CTRSNet successfully achieves high-performance 3D C-vertebra CT reconstruction and segmentation directly from the 2D X-ray images. It gains a high SSIM of 0.749 and a high PSNR of 24.58 dB for the reconstructed CT, as well as an average Dice up to 80.44\% for the seven segmented C-vertebras (C1,C2,C3,C4,C5,C6 and C7). So that it anatomically enables a 3D cervical vertebra physiological structure illustration, as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})4, with {\textquoteleft}{\textquoteleft}once is enough'' fast 2D imaging to speed up the diagnostic procedure and reduce the repetitive radiation. 

As shown in (\ensuremath{<}\ensuremath{>})Table (\ensuremath{<}\ensuremath{>})1, the innovative components designed for X-CTRSNet, including SpaDRNet, lossMV \ensuremath{-}vgg , MulSISNet and RSC Learning, enable robust improvements. By using SpaDRNet, the anatomical structure from the overlapped 2D X-ray images are effectively decomposed layer-by-layer as shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})5, thanks to its progressive 2D{\textendash}3D conversion with spatial correspondence. By using lossMV \ensuremath{-}vgg , the performance of reconstruction gains 2.11\% improvement in SSIM and 0.31 dB improvement in PSNR. It is beneficial from the guidance of lossMV \ensuremath{-}vgg for content consistency among voxels in 3D scenes to express biological anatomy. By using MulSISNet, an accurate segmentation is further enabled for 3D morphology extraction, and meanwhile enhance 3D CT reconstruction. It is contributed by the multi-scale 3D extraction and interoperation in MulSISNet, as well as utilizing the reciprocal relation inter tasks. By using RSC Learning, the best performance in both reconstruction and segmentation is further achieved, as RSC Learning promotes the reconstruction{\textendash}segmentation consistency of the interactive tasks with CT GT-to-segmentation learning. 

As shown in (\ensuremath{<}\ensuremath{>})Figs. (\ensuremath{<}\ensuremath{>})2 \& (\ensuremath{<}\ensuremath{>})3, X-CTRSNet achieves superior accuracy in both tasks compared to the state-of-the-art methods: (1) SIT [(\ensuremath{<}\ensuremath{>})22], PSR [(\ensuremath{<}\ensuremath{>})23] and X2CT-GAN [(\ensuremath{<}\ensuremath{>})24] for reconstruction, as well as (2) 3D UNet [(\ensuremath{<}\ensuremath{>})31], DSN [(\ensuremath{<}\ensuremath{>})34], DenseBiasNet [(\ensuremath{<}\ensuremath{>})35], CS2-Net [(\ensuremath{<}\ensuremath{>})36] and ConResNet [(\ensuremath{<}\ensuremath{>})37] for after-reconstruction 3D segmentation. 

In the reconstruction comparison ((\ensuremath{<}\ensuremath{>})Table (\ensuremath{<}\ensuremath{>})2), X-CTRSNet improved the SSIM by 13.78\% on average for accurate anatomical structures, and increased the PSNR by 2.27 dB for clearly readable imaging. As shown in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})6, it visually enables clear and robust full-view biologic structures without overlapping that have legible distribution, shape and explicitly readable anatomical texture, and further promotes the precise 3D segmentation of the C-vertebras morphology on the reconstructed stereo CT data. So that X-CTRSNet distinctly provides the detailed 3D imaging diagnosis basis from the 2D X-ray imaging characterized by low radiation risk. But the compared ones behave poorly, which causes rough shape of C-vertebras groups, and fails on each detailed C-vertebra and the interlock relation among C-vertebras. 

In the segmentation comparison, as shown in (\ensuremath{<}\ensuremath{>})Table (\ensuremath{<}\ensuremath{>})3, our proposed method still effectively improves the average Dice with 3.25\%, and comprehensive promotes the more precise segmentation for all C-vertebras C1 to C7. Visually in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})7, it is robust to segment the multi-scale structure components distributed among C-vertebras. As the multi-scale structure components (circled by dotted line in (\ensuremath{<}\ensuremath{>})Fig. (\ensuremath{<}\ensuremath{>})7) including foramen transversarium, spinous process, and vertebral body which cause difficulties to the compared methods. Our X-CTRSNet still precisely segments them for the morphology extraction, thanks to the multi-scale path and interoperation in its sub module MulSISNet. 

Furthermore, besides the above accuracy comparison in both tasks, the comparison of model complexities is also made. As 

Fig. 5. SpaDRNet effectively decomposes the anatomical structure from the overlapped 2D X-ray images, by progressive 2D{\textendash}3D conversion with spatial correspondence. 

Fig. 6. X-CTRSNet achieves the reconstruction of the detailed CT anatomy of legible distribution, shape and explicitly readable anatomical texture, and thus further promotes the precise 3D structure segmentation. 

Table 1 

X-CTRSNet successfully achieves the accurate reconstruction and segmentation, contributed to its innovative components. 

Table 2 

X-CTRSNet gains superior accuracy in reconstruction compared to the state-of-the-art method, with the acceptable model complexity. 

Fig. 7. X-CTRSNet shows superiority to precisely segment C-vertebra from the interactively reconstructed 3D CT. For the cervical vertebra that has multi-scale structure components, it still makes robust segmentation. As the foramen transversarium, spinous process, and vertebral body of the multi-scale structure components (circled by dotted line) cause difficulties to the compared ones, X-CTRSNet precisely segment them for the morphology extraction. 

for one patient on a laptop with one Nvidia RTX 3080 GPU and an Intel i9 CPU. As can be seen, our method just takes less than a second for processing, so that remarkably saves time in clinical 3D CT imaging and analysis, as well as reduces unwanted repetitive radiation of excessive examination, especially for the triage of emergency department. (2) Besides, in clinical applications, the accuracy of the method is a more important priority [(\ensuremath{<}\ensuremath{>})38]. The results of the accuracy comparison show that the performance of our method is significantly better than those of other known methods, gaining 13.78\% improvements in SSIM for accurate anatomical structure, and increasing PSNR by 2.27dB 

for clearly readable imaging, as well as improving the average Dice with 3.25\% for precise segmentation. Especially compared with SIT and DSN which have the lowest model complexities for the reconstruction and the 3D segmentation, respectively, our method achieves 18.7\% higher SSIM, 2.58dB higher PSNR and 3.10\% higher Dice to achieve the best model accuracy. Combining both the accuracy and complexities of our method, our method has great potential to effectively and quickly make 3D CT reconstruction and segmentation directly from 2D X-ray images in clinical. 

4. Conclusion 

In this paper, we propose X-CTRSNet, the first powerful work to simultaneously and accurately enable 3D C-vertebra CT reconstruction and segmentation directly from 2D X-ray images. The method is innovatively achieved by the following components: (1) SpaDRNet for the overlapped anatomy decomposition and reconstructing into the pathological information detailed 3D CT; (2) MulSISNet for the multi-scale stereo structure extraction and the further segmentation on the reconstructed CT, where the shape constrains are interpretively fed back; and (3) RSC Learning for the reconstruction{\textendash}segmentation consistency in the interactive multi-tasks. Extensive experiments on reconstruction and segmentation reveal {\textquoteleft}{\textquoteleft}once is enough'' with X-CTRSNet to improve the diagnosis efficiency of 2D X-ray imaging and avoid the repetitive radiation of overtreatment in clinical. 

CRediT authorship contribution statement 

Rongjun Ge: Conceptualization, Methodology, Software, Validation, Writing {\textendash} original draft, Writing {\textendash} review \& editing. Yuting He: Formal analysis, Investigation. Cong Xia: Resources, Data curation. Chenchu Xu: Methodology. Weiya Sun: Validation. Guanyu Yang: Formal analysis. Junru Li: Validation. Zhihua Wang: Validation. Hailing Yu: Validation. Daoqiang Zhang: Supervision, Project administration. Yang Chen: Project administration, Funding acquisition. Limin Luo: Supervision. Shuo Li: Supervision, Conceptualization. Yinsu Zhu: Resources, Data curation, Writing {\textendash} review \& editing. 

Declaration of competing interest 

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 

Acknowledgments 

This study was funded by the Fundamental Research Funds for the Central University, China (No. NS2021067); the National Natural Science Foundation, China (No. 62101249, 61871117, 62171123 and 81871444); the China Postdoctoral Science Foundation (No. 2021TQ0149); the Natural Science Foundation of Jiangsu Province (No. BK20210291); the State{\textquoteright}s Key Project of Research and Development Plan (No. 2017YFC0109202, 2018YFA0704102). 

References 
