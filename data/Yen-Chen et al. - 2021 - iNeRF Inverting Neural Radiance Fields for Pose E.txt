iNeRF: Inverting Neural Radiance Fields for Pose Estimation 

Fig. 1: We present iNeRF which performs mesh-free pose estimation by inverting a neural radiance field of an object or scene. The middle figure shows the trajectory of estimated poses (gray) and the ground truth pose (green) in iNeRF{\textquoteright}s iterative pose estimation procedure. By comparing the observed and rendered images, we perform gradient-based optimization to estimate the camera{\textquoteright}s pose without accessing the object{\textquoteright}s mesh model. Click the image to play the video in a browser. 

Abstract{\textemdash} We present iNeRF, a framework that performs mesh-free pose estimation by {\textquotedblleft}inverting{\textquotedblright} a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis {\textemdash} synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation {\textendash} given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view. 

The recent advances of Neural Radiance Fields (NeRF [22]) provide a mechanism for capturing complex 3D and optical structures from only one or a few RGB images, which opens up the opportunity to apply analysis-by-synthesis to broader real-world scenarios without mesh models during training or test times. NeRF representations parameterize the density and color of the scene as a function of 3D scene coordinates. The function can either be learned from multi-view images with given camera poses [18], [22] or directly predicted by a generative model given one or few input images [45], [47]. 

Here we present iNeRF, a new framework for 6 DoF pose estimation by inverting a NeRF model. . iNeRF takes three inputs: an observed image, an initial estimate of the pose, and a NeRF model representing a 3D scene or an object in the image. We adopt an analysis-by-synthesis approach to compute the appearance differences between the pixels rendered from the NeRF model and the pixels from the observed image. The gradients from these residuals are then backpropagated through the NeRF model to produce the gradients for the estimated pose. As illustrated in Figure (\ensuremath{<}\ensuremath{>})1, this procedure is repeated iteratively until the rendered and observed images are aligned, thereby yielding an accurate pose estimate. 

Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB inputs by inverting a NeRF model inferred by pixelNeRF [47] given a single view of the object. The only prior work we are aware of that similarly provides RGB-only category-level pose estimation is the recent work of Chen et al. [3]. In Sec. (\ensuremath{<}\ensuremath{>})II we compare differences between [3] and our work, which mostly arise from the opportunities and challenges presented by a continuous, implicit NeRF parameterization. 

To summarize, our primary contributions are as follows. (i) We show that iNeRF can use a NeRF model to estimate 6 DoF pose for scenes and objects with complex geometry, without the use of 3D mesh models or depth sensing {\textemdash} only RGB images are used as input. (ii) We perform a thorough investigation of ray sampling and the batch sizes for gradient optimization to characterize the robustness and limitations of iNeRF. (iii) We show that iNeRF can improve NeRF by predicting the camera poses of additional images, that can then be added into NeRF{\textquoteright}s training set. (iv) We show category-level pose estimation results, for unseen objects, including a real-world demonstration. 

II. RELATED WORKS 

Pose Estimation from RGB Images. Classical methods for object pose estimation address the task by detecting and matching keypoints with known 3D models [1], [4], [5], [29]. Recent approaches based on deep learning have proposed to 1) directly estimate objects pose using CNN-based architectures [32], [40], [46] or 2) estimate 2D key-points [27], [35], [37], [38] and solve for pose using the PnP-RANSAC algorithm. Differentiable mesh renderers [2], [24] have also been explored for pose estimation. Although their results are impressive, all the aforementioned works require access to objects{\textquoteright} 3D models during both training and testing, which significantly limits the applicability of these approaches. Recently, Chen et al. [3] address category-level object pose estimation [44], in particular they impressively estimate object shape and pose across a category from a single image. They use a single-image reconstruction with a 3D voxel-based feature volume and then estimating pose using iterative image alignment. In contrast, in our work we use continuous implicit 3D representations in the form of NeRF models, which have been empirically shown to produce more photorealistic novel-image rendering [22], [18] and scale to large, building-scale volumes [18], which we hypothesize will enable higher-fidelity pose estimation. This also presents challenges, however, due to the expensive computational cost of NeRF rendering, for which we introduce a novel importance-sampling approach in Sec. (\ensuremath{<}\ensuremath{>})IV-B. Another practical difference in our approach to category-level pose estimation {\textendash} while [3] optimizes for shape with gradient descent, we show we can instead allow pixelNeRF to predict a NeRF model with just a forward pass of a network. Additionally, since NeRF models scale well to large 

Fig. 2: An overview of our pose estimation pipeline which inverts an optimized neural radiance field (NeRF). Given an initially estimated pose, we first decide which rays to emit. Sampled points along the ray and the corresponding viewing direction are fed into NeRF{\textquoteright}s volume rendering procedure to output rendered pixels. Since the whole pipeline is differentiable, we can refine our estimated pose by minimizing the residual between the rendered and observed pixels. 

scenes, we can use the same iNeRF formulation to perform localization, for example in challenging real-world LLFF scenes {\textendash} this capability was not demonstrated in [3], and may be challenging due to the memory limitations of voxel representations for sufficient fidelity in large scenes. While object pose estimation methods are often separate from methods used for visual localization of a camera in a scene as in the SfM literature (i.e. [33], [41], [31]), because NeRF and iNeRF only require posed RGB images as training, iNeRF can be applied to localization as well. 

III. BACKGROUND 

Given a collection of N RGB images \{Ii\}N i=1, Ii \ensuremath{\in} [0, 1]H{\texttimes}W {\texttimes}3 with known camera poses \{Ti\}iN=1, NeRF learns to synthesize novel views associated with unseen camera poses. NeRF does this by representing a scene as a {\textquotedblleft}radiance field{\textquotedblright}: a volumetric density that models the shape of the scene, and a view-dependent color that models the appearance of occupied regions of the scene, both of which lie within a bounded 3D volume. The density \ensuremath{\sigma} and RGB color c of each point are parameterized by the weights \ensuremath{\Theta} of a multilayer perceptron (MLP) F that takes as input the 3D position of that point x = (x, y, z) and the unit-norm viewing direction of that point d = (dx, dy, dz), where (\ensuremath{\sigma}, c) {\textleftarrow} F\ensuremath{\Theta}(x, d). To render a pixel, NeRF emits a camera ray from the center of the projection of a camera through that pixel on the image plane. Along the ray, a set of points are sampled for use as input to the MLP which outputs a set of densities and colors. These values are then used to approximate the image formation behind volume rendering [7] using numerical quadrature [19], producing using some sampled set of rays r \ensuremath{\in} R where C(r) is the observed RGB value of the pixel corresponding to ray r in some image, and C{\textasciicircum}(r) is the prediction produced from neural volume rendering. To improve rendering efficiency one may train two MLPs: one {\textquotedblleft}coarse{\textquotedblright} and one {\textquotedblleft}fine{\textquotedblright}, where the coarse model serves to bias the samples that are used for the fine model. For more details, we refer readers to Mildenhall et al. [22]. 

Although NeRF originally needs to optimize the representation for every scene independently, several extensions [28], [39], [45], [47] have been proposed to directly predict a continuous neural scene representation conditioned on one or few input images. In our experiments, we show that iNeRF can be used to perform 6D pose estimation with either an optimized or predicted NeRF model. 

IV. INERF FORMULATION 

We now present iNeRF, a framework that performs 6 DoF pose estimation by {\textquotedblleft}inverting{\textquotedblright} a trained NeRF. Let us assume that the NeRF of a scene or object parameterized by \ensuremath{\Theta} has already been recovered and that the camera intrinsics are known, but the camera pose T of an image observation I are as-yet undetermined. Unlike NeRF, which optimizes \ensuremath{\Theta} using a set of given camera poses and image observations, we instead solve the inverse problem of recovering the camera pose T given the weights \ensuremath{\Theta} and the image I as input: 

(1) 

To solve this optimization, we use the ability from NeRF to take some estimated camera pose T \ensuremath{\in} SE(3) in the coordinate frame of the NeRF model and render a corresponding image observation. We can then use the same photometric loss function L as was used in NeRF (Sec. (\ensuremath{<}\ensuremath{>})III), but rather than backpropagate to update the weights \ensuremath{\Theta} of the MLP, we instead update the pose T to minimize L. The overall procedure is shown in Figure (\ensuremath{<}\ensuremath{>})2. While the concept of inverting a NeRF to perform pose estimation can be concisely stated, it is not obvious that such a problem can be practically solved to a useful degree. The loss function L is non-convex over the 6DoF space of SE(3), and full-image NeRF renderings are computationally expensive, particularly if used in the loop of an optimization procedure. Our formulation and experimentation (Sec. (\ensuremath{<}\ensuremath{>})V) aim to address these challenges. In the next sections, we discuss (i) the gradient-based SE(3) optimization procedure, (ii) ray sampling strategies, and (iii) how to use iNeRF{\textquoteright}s predicted poses to improve NeRF. 

Let \ensuremath{\Theta} be the parameters of a trained and fixed T{\textasciicircum}i the estimated camera pose at current optimization i, I the observed image, and L(T{\textasciicircum}i | I,\ensuremath{\Theta}) be the to train the fine model in NeRF. We employ gradient-based optimization to solve for T{\textasciicircum} as defined in Equation ensure that the estimated pose T{\textasciicircum}i continues to lie SE(3) manifold during gradient-based optimization, rameterize T{\textasciicircum}i with exponential coordinates. Given pose estimate T{\textasciicircum}0 \ensuremath{\in} SE(3) from the camera frame model frame, we represent T{\textasciicircum}i as: 

where S = [\ensuremath{\omega},\ensuremath{\nu}]T represents the screw axis, \ensuremath{\theta} the magnitude, [w] represents the skew-symmetric 3 {\texttimes} 3 matrix of w, and K(S,\ensuremath{\theta}) = (I\ensuremath{\theta}+ (1 \ensuremath{-} cos \ensuremath{\theta})[\ensuremath{\omega}] + (\ensuremath{\theta}\ensuremath{-} sin \ensuremath{\theta})[\ensuremath{\omega}]2)\ensuremath{\nu} [14]. With this parameterization, our goal is to solve the optimal relative transformation from an initial estimated pose T0: 

(2) 

We iteratively differentiate the loss function through the MLP to obtain the gradient \ensuremath{\nabla}S\ensuremath{\theta}L(e[S]\ensuremath{\theta}T0 | I,\ensuremath{\Theta}) that is used to update the estimated relative transformation. We use Adam optimizer [9] with an exponentially decaying learning rate (See Supplementary for parameters). For each observed image, we initialize S\ensuremath{\theta} near 0, where each element is drawn at random from a zero-mean normal distribution N (0,\ensuremath{\sigma} = 10\ensuremath{-}6). In practice, parameterizing with e[S]\ensuremath{\theta} T0 rather than T0 e[S]\ensuremath{\theta} results in a center-of-rotation at the initial estimate{\textquoteright}s center, rather than at the camera frame{\textquoteright}s center. This alleviates coupling between rotations and translations during optimization. 

In a typical differentiable render-and-compare pipeline, one would want to leverage the gradients contributed by all of the output pixels in the rendered image [43]. However, with NeRF, each output pixel{\textquoteright}s value is computed by weighing the values of n sampled points along each ray r \ensuremath{\in} R during ray marching, so given the amount of sampled rays in a batch b = |R|, then O(bn) forward/backward passes of the underlying NeRF MLP will be queried. Computing and backpropagating the loss of all pixels in an image (i.e., , b = HW, where H and W represent the height and width of a high-resolution image) therefore require significantly more memory than is present on any commercial GPU. While we may perform multiple forward and backward passes to accumulate these gradients, this becomes prohibitively slow to perform each step of our already-iterative optimization procedure. In the following, we explore strategies for selecting a sampled set of rays R for use in evaluating the loss function L at each optimization step. In our experiments we find that we are able to recover accurate poses while sampling only 

Fig. 3: An illustration of 3 sampling strategies. The input image and the rendering corresponding to the estimated pose of the scene are averaged. We use x to represent sampled pixels on the background; + to represent sampled pixels that are covered by both rendered and observed images; o to represent sampled pixels that are only covered by either the rendered or the input image. When performing random sampling (left) many sampled pixels are x, which provide no gradients for updating the pose. For {\textquotedblleft}interest point{\textquotedblright} sampling (middle) some of the sampled pixels are already aligned and therefore provide little information. For {\textquotedblleft}interest region{\textquotedblright} sampling, many sampled pixels are o, which helps pose estimation achieve higher accuracy and faster convergence. 

b = 2048 rays per gradient step, which corresponds to a single forward/backward pass that fits within GPU memory and provides 150{\texttimes} faster gradient steps on a 640 {\texttimes} 480 image. 

morphological dilation for I iterations to enlarge the sampled region. In practice, we find this to speed up the optimization when the batch size of rays is small. Note that if I is set to a large number, Interest Region Sampling falls back to Random Sampling. 

In addition to using iNeRF to perform pose estimation given a trained NeRF, we also explore using the estimated poses to feed back into training the NeRF representation. Specifically, we first (1) train a NeRF given a set of training RGB images with known camera poses \{(Ii,Ti)\}i=1 , yield-Ntrain ing NeRF parameters \ensuremath{\Theta}train. We then (2) use iNeRF to take in additional unknown-pose observed images and solve for estimated poses Given these estimated poses, we can then (3) use the self-supervised pose labels to add into the training set. This procedure allows NeRF to be trained in a semi-supervised setting. 

V. RESULTS 

We first conduct extensive experiments on the synthetic dataset from NeRF [22] and the real-world complex scenes from LLFF [21] to evaluate iNeRF for 6DoF pose estimation. Specifically, we study how the batch size of rays and sampling strategy affect iNeRF. We then show that iNeRF can improve NeRF by estimating the camera poses of images with unknown poses and using these images as additional training data for NeRF. Finally, we show that iNeRF works well in tandem with pixelNeRF [47] which predicts a NeRF model conditioned on a single RGB image. We test our method for category-level object pose estimation in both simulation and the real world. We found that iNeRF achieving competitive results against feature-based methods without accessing object mesh models during either training or test time. 

TABLE I: Benchmark on Fern scene. NeRFs trained with pose labels generated by iNeRF can achieve higher PSNR. 

Fig. 5: iNeRF can be used to improve NeRF by augmenting training data with images whose camera poses are unknown. We present an ablation study using 25\% and 50\% of training images to train NeRF models. These models are compared with models trained using 100\% of the training images but where a fraction of that data use estimated poses from iNeRF rather than ground-truth poses from the dataset. 

test set, I0 is selected randomly from one of the 251 views and the other image I1 is selected from views whose rotation and translation are within 30-degree from I0. At test time, our method uses a pre-trained pixelNeRF to predict a NeRF model conditioned on image I0. Then, we apply iNeRF to align against I1 for estimating the relative pose T01 . 

TABLE II: Quantitative results for the ShapeNet Cars dataset. We report performance using the mean and median of the translation and rotation error. A prediction is defined as an outlier when either the translation error or the rotation error is larger than 20{\textopenbullet} . 

requires a segmented image as input, we use PointRend to remove the background for frames that pixelNeRF takes inputs. In this iterative tracking setting, iNeRF only requires less than 10 iterations of optimization to converge which enables tracking at approximately 1Hz. 

VI. LIMITATIONS AND FUTURE WORK 

While iNeRF has shown promising results on pose estimation, it is not without limitations. Both lighting and occlusion can severely affect the performance of iNeRF and are not modeled by our current formulation. One potential solution is to model appearance variation using transient latent codes as was done in NeRF-W [18] when training NeRFs, and jointly optimize these appearance codes alongside camera pose within iNeRF. Also, currently iNeRF takes around 20 seconds to run 100 optimization steps, which prevents it from being practical for real-time use. We expect that this issue may be mitigated with recent improvements in NeRF{\textquoteright}s rendering speed [13]. 

(b) Results on real-world scenes from the LLFF dataset. 

Fig. 6: (a) Quantitative results on the synthetic dataset. {\textquotedblleft}s{\textquotedblright} stands for sampling strategy and {\textquotedblleft}b{\textquotedblright} stands for the batch size. Applying Interest Region Sampling (s=region) improves the accuracy by 15\% across various batch sizes. (b) Quantitative results on LLFF. Interest Region Sampling is always applied and we show the effect of various batch sizes on performance. Larger batch sizes can improve accuracy while reducing the number of gradient steps needed for convergence. 

Fig. 8: Qualitative results of pose tracking in real-world images without the need for mesh/CAD model. In the left column, we show input video frames at different time steps. At each time t, iNeRF leverages a NeRF model inferred by pixelNeRF based on input frame at time t \ensuremath{-} 1 to estimate the object{\textquoteright}s pose. In the right column, we show the resulting reconstructed frames and the estimated poses at each time step. The background has been masked out using PointRend [10] before feeding the frame into pixelNeRF. The views are rotations about the view-space vertical axis.Click the image to play the video in a browser. 

VII. CONCLUSION 

We have presented iNeRF, a framework for mesh-free, RGB-only pose estimation that works by inverting a NeRF model. We have demonstrated that iNeRF is able to perform accurate pose estimation using gradient-based optimization. We have thoroughly investigated how to best construct mini-batches of sampled rays for iNeRF and have demonstrated its performance on both synthetic and real datasets. Lastly, we have shown how iNeRF can perform category-level object pose estimation and track pose for novel object instances 

Fig. 9: Histogram of pose errors on real-world scenes from the LLFF dataset. 

with an image conditioned generative NeRF model. 

VIII. IMPLEMENTATION DETAILS 

The Y channel is not considered in the computation of loss. 

IX. HISTOGRAM OF POSE ERRORS 

We visualize the histogram of pose errors, before and after iNeRF optimization, on the LLFF dataset in Figure (\ensuremath{<}\ensuremath{>})9 using the data from Section 5.2. The data is generated by applying random perturbations within [\ensuremath{-}40, 40] degrees for rotation and [\ensuremath{-}0.1, 0.1] meters along each axis for translation. Note that when the batch size is 2048, more than 70\% of the data has \ensuremath{<} 5{\textopenbullet} and \ensuremath{<} 5 cm error after iNeRF is applied. 

X. MORE ANALYSIS IN SELF-SUPERVISED NERF 

For the Fern scene, we found that when only 10\% of labeled camera poses are used, it worsens the PSNR from 18.5 to 15.64. The results show that having enough labels for a good initalization is important. 

REFERENCES 
