Patient-specific three-dimensional image reconstruction from a single X-ray projection using a convolutional neural network for on-line radiotherapy applications 

Institute of Information and Communication Technologies, Electronics and Applied Mathematics (ICTEAM), UCLouvain, Place de l{\textquoteright}Universit{\textasciiacute}e 1, 1348 Louvain-la-Neuve, Belgium   

ARTICLE INFO  

Keywords: 3D-CT reconstruction 

ABSTRACT  

Background and purpose: Radiotherapy is commonly chosen to treat thoracic and abdominal cancers. However, irradiating mobile tumors accurately is extremely complex due to the organs{\textquoteright} breathing-related movements. Different methods have been studied and developed to treat mobile tumors properly. The combination of X-ray projection acquisition and implanted markers is used to locate the tumor in two dimensions (2D) but does not provide three-dimensional (3D) information. The aim of this work is to reconstruct a high-quality 3D computed tomography (3D-CT) image based on a single X-ray projection to locate the tumor in 3D without the need for implanted markers. 

Materials and Methods: Nine patients treated for a lung or liver cancer in radiotherapy were studied. For each patient, a data augmentation tool was used to create 500 new 3D-CT images from the planning four-dimensional computed tomography (4D-CT). For each 3D-CT, the corresponding digitally reconstructed radiograph was generated, and the 500 2D images were input into a convolutional neural network that then learned to reconstruct the 3D-CT. The dice score coefficient, normalized root mean squared error and difference between the ground-truth and the predicted 3D-CT images were computed and used as metrics. 

Results: Metrics{\textquoteright} averages across all patients were 85.5\% and 96.2\% for the gross target volume, 0.04 and 0.45 Hounsfield unit (HU), respectively. 

Conclusions: The proposed method allows reconstruction of a 3D-CT image from a single digitally reconstructed radiograph that could be used in real-time for better tumor localization and improved treatment of mobile tumors without the need for implanted markers.   

1. Introduction 

Radiotherapy is one of the most widely used treatments in oncology and is prescribed for more than half of all cancer patients, either alone or in combination with surgery and chemotherapy (\ensuremath{<}\ensuremath{>})[1]. In radiotherapy, ionizing radiation is used to kill cancer cells. A trade-off must be made between delivering the prescribed dose to the target and not delivering large doses to healthy tissues, which could lead to undesirable effects and induce secondary cancer (\ensuremath{<}\ensuremath{>})[2]. Applying radiotherapy to lung and liver cancers is even more challenging as the treatment must consider the respiratory motion. This requires specific strategies in the radiotherapy workflow to ensure adequate target coverage through successive treatment fractions. These strategies are generally classified in two categories. 

The first category consists in acquiring a four-dimensional computed tomography (4D-CT) scan prior to the treatment and defining security margins. Safety margins ensure target coverage regardless of the breathing phase, but this method irradiates more the surrounding healthy organs (\ensuremath{<}\ensuremath{>})[3]. The breathing motion in the treatment room may also differ significantly from the motion captured in the 4D-CT from time to time (\ensuremath{<}\ensuremath{>})[4]. 

The second category encompasses breathing-synchronized methods that aim to minimize the contribution of the tumor{\textquoteright}s motion in the computation of the safety margins by monitoring the tumor{\textquoteright}s position or reducing/regularizing its motion amplitude during breathing. These methods gather abdominal compression (\ensuremath{<}\ensuremath{>})[5], audio coaching (\ensuremath{<}\ensuremath{>})[6], mechanically assisted ventilation (\ensuremath{<}\ensuremath{>})[7] and respiratory gating (\ensuremath{<}\ensuremath{>})[8]. Tumor 

monitoring in these techniques is based on external surrogates of the internal motion to avoid the use of invasive procedures (the placement of markers pinpoints the tumor position with greater accuracy but involves surgery before the treatment (\ensuremath{<}\ensuremath{>})[9]). This approach requires a stable correlation between the internal tumor motion and its external surrogate, which is usually not the case when changes occur in the patient{\textquoteright}s breathing movement. 

Image-guided radiation therapy (IGRT) incorporates imaging techniques during each treatment session. By adding detailed images, it ensures that the radiation is narrowly focused on the target. A broad range of IGRT is now available (\ensuremath{<}\ensuremath{>})[10]. X-ray projections are commonly acquired to estimate the tumor{\textquoteright}s position, but their use often requires implanted markers to identify the tumor volume correctly and make it visible on the X-ray projection (\ensuremath{<}\ensuremath{>})[11]. Another disadvantage of this method is that it does not provide 3D information. 

All these methods result in a small reduction in the safety margins, while adapting the treatment in 3D and in real-time will lead to a big reduction in the motion margins thanks to precise tracking of the 3D anatomical structures. To achieve this, the real-time positions of the target and surrounding organs must be known throughout treatment delivery. Most of the radiotherapy treatment rooms are equipped with 2D fluoroscopy to validate the patient positioning before treatment, we propose to rely on this equipment to estimate the related 3D information. 

Many studies that reconstruct a 3D volume from a 2D X-ray projection have already been performed. Different fields of application in the biomedical sector have been explored: Henzler et al. investigated how to reconstruct 3D volumes from 2D cranial x-rays by applying deep learning (\ensuremath{<}\ensuremath{>})[12], while Liang et al. developed a new model architecture to reconstruct a tooth in 3D from a single panoramic radiograph (\ensuremath{<}\ensuremath{>})[13]. Montaya et al. in (\ensuremath{<}\ensuremath{>})[14], as well as Ying et al. in (\ensuremath{<}\ensuremath{>})[15], demonstrated that it was possible to reconstruct a 3D-CT image from biplanar X-ray projections using a neural network, and Shen et al. used a neural network to reconstruct a 3D image from a single projection view (\ensuremath{<}\ensuremath{>})[16]. 

2. Materials and methods 

The data used in this work come from nine patients who were treated for lung or liver cancer at Cliniques universitaires Saint-Luc in Brussels between 2010 and 2015. This retrospective study was approved by the Hospital Research Ethics Committee (B403201628906). (\ensuremath{<}\ensuremath{>})Table 1 shows patients information (tumor size and location, and its motion in the different sets). A planning 4D-CT composed of 10 breathing phases evenly spread over the respiratory cycle was acquired for each patient prior to treatment delivery. The dimensions of each 3D-CT image were 512 {\texttimes} 512 {\texttimes} 173, and the voxel size was 1 mm2 in plane with a slice thickness of 2 mm. The Mid-Position (MidP)-CT image, defined as the local mean position in the respiratory cycle, was computed using the average of all velocity fields obtained by non-rigid registration between the 4D-CT phases (\ensuremath{<}\ensuremath{>})[18]. On the MidP-CT image, the gross target volume (GTV) and surrounding organs at risk were delineated manually by an experienced radiation oncologist. 

As training a neural network requires a lot of data, it was necessary to generate new 3D-CT images. To do so, we consider a polar coordinate system (r, n) related to a breathing cycle, whose origin is the MidP-CT image and where n are the periodic phases. In this system, we know the deformation fields associated to the 10 breathing phases of the 4DCT which are F(1, N), with N \ensuremath{\in} \{0, 0.1{\textellipsis}, 0.9\}. Then, to generate the breathing phase n at a normalized distance r of the MidP-CT, we compute the deformation field F(r, n) using a linear interpolation between the two closest discrete breathing phases plus a scaling: 

Fig. 1. Overview of the proposed method{\textquoteright}s workflow.  

Patient characteristics. MR4D{\L} CT, MRTrainSet and MRTestSet stand for the motion range in 3D of the GTV{\textquoteright}s centroid in the 4D-CT, training set and test set, respectively. The motion range is defined as the Euclidean distance between the two most distant positions.  

(1)  

where N\ensuremath{\leqslant}n\ensuremath{\leqslant}N + 0.1. Using this method, based on a previous work of our team (\ensuremath{<}\ensuremath{>})[19] and developed in (\ensuremath{<}\ensuremath{>})[20], we can generate slightly different 3D-CT images, spread around the ten original phases of the 4D-CT, for every patient. The training set was composed of 500 images where n was a uniform random draw between 0 and 1, and r a random sample from a normal distribution N (1, 0.25) truncated between 0.4 and 1.1. A digitally reconstructed radiograph (DRR) was generated from each of these images using the Beer{\textendash}Lambert absorption-only model (implemented in the TomoPy Python library (\ensuremath{<}\ensuremath{>})[21]) and a projection angle of 0{\textopenbullet} along the anterior-posterior axis. The projection geometry was a 1440 {\texttimes} 1440 image with a pixel size of 0.296 {\texttimes} 0.296 mm2. The source-to-origin and source-to-detector distances were 1000 mm and 1500 mm. Each patient{\textquoteright}s training dataset was made up of 500 pairs containing the created 3D-CT image and the associated DRR. An independent test set composed of 100 3D-CT/DRR pairs was also created for each patient. For each image of the test set, the masks of the GTV, lungs and heart were also generated by deforming the MidP-CT image{\textquoteright}s 3D binary masks. The difference between the test and training sets comes from the normalized distance r used to generate the 3D-CT image. In the case of the training set, r was a random sample from a normal distribution N (1, 0.25) truncated between 0.4 and 1.1, while r was a random sample from a normal distribution N (1, 0.5) truncated between 0.8 and 1.5 for the test set. This means that deeper breathing situations were present in the test set than in the training set. All breathing phases were used in both cases. 

In order to evaluate the performance of the proposed method, 100 3D-CT images independent of the training set were created for each patient. These 3D-CT images are called the ground truth (GT) 3D-CT images in the rest of the paper. 100 DRRs were generated from these images to form the test set. The trained network was used on these ra-diographs to predict the corresponding 3D-CT images, called the predicted (P) 3D-CT images. The predicted 3D-CT images were compared with the ground truth 3D-CT images to evaluate the performance of the model using several metrics. 

Dice similarity coefficient (DSC) is a common overlap-based metric used to measure the performance of a segmentation algorithm, and is defined by: 

(2)  

where A and B are the sets containing the matrix indices of both binary masks A and B. In this work, the DSC was computed between a 3D binary mask in the ground-truth 3D-CT image and the corresponding mask in the predicted 3D-CT image to evaluate the quality of the predicted 3DCT image in terms of anatomical structure positions. The 3D binary masks of a predicted 3D-CT image were obtained by computing the Morphons non-rigid registration (\ensuremath{<}\ensuremath{>})[23], then applying the resulting deformation fields to deform the masks on the predicted image. This was done between this predicted image and either the ground-truth 3D-CT image (GT-based), or the MidP-CT image (MidP-based). Using the ground-truth 3D-CT image for this part serves as a post-training quality evaluation, to evaluate if a state-of-the-art registration algorithm sees a difference between the ground-truth and the predicted images. Using the MidP-CT image simulates how it could be used to evaluate the quality of the predicted images after each treatment fraction as the ground-truth 3D-CT images are not available during a treatment. For both versions, the DSC was computed for the same 50 images of the 100 items constituting the test set, for each organ and each patient. In either case, this metric was an evaluation tool and not part of the real-time process as the computation time of the Morphons is about 150 s. As a complement to this analysis, the Euclidean distance was computed 

(further details in Appendix A. (\ensuremath{<}\ensuremath{>})Supplementary data). Normalized root mean squared error (NRMSE) was computed between two images A and B, and is defined by: 

(3)  

where Xa is the voxel a in the image X. Amax and Amin stand for the maximum and minimum in image A, the ground-truth 3D-CT image. The NRMSE was computed between the latter and the corresponding predicted 3D-CT image. This was repeated for all images in the test set. 

Difference was computed between a ground-truth 3D-CT image and the corresponding predicted 3D-CT image, and the mean and median of the difference were studied, as well as quantifying the percentage of the absolute value of the difference below a certain threshold to evaluate the proportion of the image that was correctly reconstructed. 

3. Results 

The results of the DSC analysis for both GT-based and MidP-based versions are summarized in (\ensuremath{<}\ensuremath{>})Table 2. For the GT-based version, the mean, the median and the 95th percentile of the DSC vary respectively from 93.2\% to 99.8\%, from 93.2\% to 99.9\%, and from 95.1\% to 99.9\% for the GTV; from 96.3\% to 99.8\%, from 96.5\% to 99.9\%, and from 96.8\% to 99.9\% for both lungs; from 93.5\% to 99.8\%, from 94.3\% to 99.8\%, and from 95.1\% to 99.9\% for the heart. While, for the MidP-based version, the mean, the median and the 95th percentile of this metric vary respectively from 76.7\% to 90.6\%, from 77.6\% to 90.8\%, and from 82.7\% to 93.4\% for the GTV; from 90.9\% to 97.3\%, from 93.4\% to 97.1\%, and from 96.1\% to 98.3\% for both lungs; from 78.1\% to 90.1\%, from 79.2\% to 89.9\%, and from 81.5\% to 91.7\% for the heart. 

The DSC results of the MidP-based version are lower than those of GT- based, but still over 75\%. As the same 50 images were used for both, the difference might be due to the approximations in the deformations and re-binarization of the masks, that probably have a higher impact with deformations over multiple voxels, but this was not quantified. 

The results of the NRMSE analysis are displayed in (\ensuremath{<}\ensuremath{>})Fig. 2. The mean of this metric is lower for Patients 5, 2, 6 and 1 who have smaller motions in the test set (from 0.032 to 0.039) than the mean obtained for Patients 7, 8, 3 and 9 (from 0.047 to 0.051) who have larger motions. This is also observed for the median and the 95th percentile, which range respectively from 0.032 to 0.038, and from 0.039 to 0.045 for the first batch of patients, while they are respectively between 0.045 and 0.052, and between 0.051 and 0.059 for the second group of patients. This analysis also shows that the breathing phases have no impact on the reconstruction process as there are uniformly distributed along the NRMSE values range. 

The results of the difference analysis are summarized in (\ensuremath{<}\ensuremath{>})Table 3. The mean of the difference between a ground-truth 3D-CT image and the corresponding predicted 3D-CT image ranges from {\L} 1.32 Hounsfield unit (HU) to 2.24 HU, with an average over all patients of 0.45 HU. The median of this metric is between {\L} 0.26 HU and 1.93 HU, with an average over all patients of 0.24 HU. Depending on the patient, 25.1\% to 39.8\% of the image volume has an absolute value of the difference lower than 5 HU, 69.9\% to 81.9\% below 25 HU, and 88.6\% to 94.6\% less than 50 HU. In summary, the difference between the ground-truth and the predicted images is very small, with about 91\% of the image volume having an absolute value of the difference smaller than 50 HU, which represents 1.25\% of the range of possible values, since the scale of a 3D- 

Fig. 2. Results of the NRMSE analysis. The NRMSE was computed between the ground-truth 3D-CT image and the corresponding predicted 3D-CT image for each test set data. The color of a dot represents the breathing phase at which the ground-truth 3D-CT image was created. Patients are sorted by increasing motion range in the test set. 

Results of the difference analysis. V\ensuremath{<}5HU, V\ensuremath{<}25HU and V\ensuremath{<}50HU stand for the percentage of the 3D-CT image{\textquoteright}s volume having an absolute value of the difference below 5 HU, 25 HU and 50 HU.  

CT image typically runs from {\L} 1000 HU for air to 3000 HU for dense bone (\ensuremath{<}\ensuremath{>})[24]. 

A representative example (whose results are: DSCGT (GTV) = 98.5\% , DSCMidP (GTV) = 88.6\%, NRMSE = 0.053, mean of the difference = {\L} 1.73 HU and V\ensuremath{<}25HU = 80.3\%) of the results obtained using the proposed method can be seen in (\ensuremath{<}\ensuremath{>})Fig. 3. For a human eye, the predicted 3DCT image looks pretty close in terms of anatomical structures. The zoom shows that a red pixel (difference \ensuremath{\approx} 200 HU) is commonly adjacent to a blue pixel (difference \ensuremath{\approx}  200 HU) or surrounded by two turquoise pixel (difference \ensuremath{\approx}  100 HU). This phenomenon is usually observed at tissue borders. Looking at the histogram, one sees that there are few voxels with a significant difference and over 30\% of the voxels have a difference between  5 HU and 5 HU. 

4. Discussion 

In this paper, it has been showed that the proposed CNN-based methodology (which requires a patient-specific training) allows to reconstruct a high-quality 3D-CT image from a single digitally reconstructed radiograph. 

Table 2 

Results of the DSC analysis for both GT-based and MidP-based versions. DSCGT and DSCMidP stand for the mean of the DSC over the 50 images taken from the test set for the GT-based version and MidP-based version, respectively. Patient 5{\textquoteright}s lungs and heart were not delineated.  

Fig. 3. Visualization of three slices of the ground-truth 3D-CT image of one patient compared with the corresponding slices of the predicted 3D-CT image, as well as the results of the difference analysis and a zoom of the boxed area. On the right of the color bar is the histogram of the difference concatenated for all patients and the 100 images of the nine test sets. 

The dice values computed between the masks of the predicted 3D-CT image and the corresponding ground-truth 3D-CT are all greater than 75\%, which is reliable. If we compare our results of the MidP-based version ((\ensuremath{<}\ensuremath{>})Table 2) for lungs and heart (94.6\% and 83.9\%) to previous works (\ensuremath{<}\ensuremath{>})[25{\textendash}27], whose goal was to segment organs at risk in lung cancer utilizing deep learning algorithms, (best in (\ensuremath{<}\ensuremath{>})[27]: 97.5\% and 92.5\%), lungs have similar results to the literature and the heart has a higher difference. However, our results should be taken in hindsight, given that the masks in the predicted image are defined as the manually segmented masks on the MidP-CT image deformed using the deformation fields obtained by the Morphons registration between both images. 

The mean of the difference between the ground truth image and the predicted image is small for each patient, with an average value of 0.45 HU over all patients. Comparing these results ((\ensuremath{<}\ensuremath{>})Fig. 3) with those obtained by (\ensuremath{<}\ensuremath{>})[16] when they use only 1 view, the quality of our reconstructed image is similar to their own. Their method also performs less at tissue borders. However, there is no scale or numerical value in their difference analysis, so it is not clear that the difference values are similar. 

One limitation of this study is that the CNN was trained using training sets composed of 3D-CT images created from deformations of a planning 4D-CT acquired prior to the treatment and paired DRRs generated using the Beer{\textendash}Lambert absorption-only model. This method supposes that inter-fraction variations such as tumor shrinking, tumor baseline shift and stomach and bladder fillings are not included in the training set. A next step of this work is to evaluate whether the network must be retrained for each fraction or whether these variations are negligible in the reconstruction process. Another possibility to counteract this limitation is to improve the data augmentation tool and incorporate inter-fraction changes in the training set. 

In conclusion, this study presents a method that allows reconstruction of a 3D-CT image from a single DRR. This method relies on a data augmentation algorithm and on a patient-specific training of a CNN. However, the study still needs to integrate inter-fractions changes and adjust the image resolution to confirm the potential clinical use of the method. 

Declaration of Competing Interest 

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. 

Acknowledgments 

Estelle Lo\"yen is a Televie grantee of the Fonds de la Recherche Sci-entifique - F.N.R.S. Damien Dasnoy-Sumell is supported by the Walloon Region, SPWEER Win2Wal program project 2010149. 

Appendix A. Supplementary data 

Supplementary data associated with this article can be found, in the online version, at (\ensuremath{<}https://doi.org/10.1016/j.phro.2023.100444\ensuremath{>})https://doi.org/10.1016/j.phro.2023.100444. 

References 
