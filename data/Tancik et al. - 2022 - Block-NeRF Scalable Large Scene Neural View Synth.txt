Block-NeRF: Scalable Large Scene Neural View Synthesis 
Ben P. Mildenhall3 
Vincent Casser2 
Pratul Srinivasan3 
Xinchen Yan2 Jonathan T. Barron3 
Sabeek Pradhan2 Henrik Kretzschmar2 
Figure 1. Block-NeRF is a method that enables large-scale scene reconstruction by representing the environment using multiple compact NeRFs that each fit into memory. At inference time, Block-NeRF seamlessly combines renderings of the relevant NeRFs for the given area. In this example, we reconstruct the Alamo Square neighborhood in San Francisco using data collected over 3 months. Block-NeRF can update individual blocks of the environment without retraining on the entire scene, as demonstrated by the construction on the right. Video results can be found on the project website waymo.com/research/block-nerf. 
We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco. 
1. Introduction 
Reconstructing large-scale environments enables several important use-cases in domains such as autonomous driving [30,43,69] and aerial surveying [14,33]. For example, a high-fidelity map of the operating domain can serve as a prior for robot navigation. Large-scale scene reconstructions can be used for closed-loop robotic simulations [13]. Autonomous driving systems are commonly evaluated by re-simulating previously encountered scenarios. Any deviation from the recorded encounter, however, may change the vehicle{\textquoteright}s trajectory, requiring high-fidelity novel view renderings along the altered path. Scene conditioned NeRFs can further augment simulation scenarios by changing environmental lighting conditions, such as camera exposure, weather, or time of day. 
Reconstructing such large-scale environments introduces additional challenges, including the presence of transient objects (cars and pedestrians), limitations in model capacity, 
along with memory and compute constraints. Furthermore, training data for such large environments is highly unlikely to be collected in a single capture under consistent conditions. Rather, data for different parts of the environment may need to be sourced from different data collection efforts, introducing variance in both scene geometry (e.g., construction work and parked cars), as well as appearance (e.g., weather conditions and time of day). 
We extend NeRF with appearance embeddings and learned pose refinement to address the environmental changes and pose errors in the collected data. We additionally add exposure conditioning to provide the ability to modify the exposure during inference. We refer to this modified model as a Block-NeRF. Scaling up the network capacity of Block-NeRF enables the ability to represent increasingly large scenes. However this approach comes with a number of limitations; rendering time scales with the size of the network, networks can no longer fit on a single compute device, and updating or expanding the environment requires retraining the entire network. 
To address these challenges, we propose dividing up large environments into individually trained Block-NeRFs, which are then rendered and combined dynamically at inference time. Modeling these Block-NeRFs independently allows for maximum flexibility, scales up to arbitrarily large environments and provides the ability to update or introduce new regions in a piecewise manner without retraining the entire environment as demonstrated in Figure 1. To compute a target view, only a subset of the Block-NeRFs are rendered and then composited based on their geographic location compared to the camera. To allow for more seamless compositing, we propose an appearance matching technique which brings different Block-NeRFs into visual alignment by optimizing their appearance embeddings. 
2. Related Work 
Researchers have been developing and refining techniques for 3D reconstruction from large image collections for decades [1,16,31,46,56,78], and much current work relies on mature and robust software implementations such as COLMAP to perform this task [54]. Nearly all of these reconstruction methods share a common pipeline: extract 2D image features (such as SIFT [37]), match these features across different images, and jointly optimize a set of 3D points and camera poses to be consistent with these matches (the well-explored problem of bundle adjustment [23,64]). Extending this pipeline to city-scale data is largely a matter of implementing highly robust and parallelized versions of these algorithms, as explored in work such as Photo Tourism [56] and Building Rome in a Day [1]. Core graphics research has also explored breaking up scenes for fast high quality rendering [36]. 
Figure 2. The scene is split into multiple Block-NeRFs that are each trained on data within some radius (dotted orange line) of a specific Block-NeRF origin coordinate (orange dot). To render a target view in the scene, the visibility maps are computed for all of the NeRFs within a given radius. Block-NeRFs with low visibility are discarded (bottom Block-NeRF) and the color output is rendered for the remaining blocks. The renderings are then merged based on each block origin{\textquoteright}s distance to the target view. 
These approaches typically output a camera pose for each input image and a sparse 3D point cloud. To get a complete 3D scene model, these outputs must be further processed by a dense multi-view stereo algorithm (e.g., PMVS [18]) to produce a dense point cloud or triangle mesh. This process presents its own scaling difficulties [17]. The resulting 3D models often contain artifacts or holes in areas with limited texture or specular reflections as they are challenging to triangulate across images. As such, they frequently require further postprocessing to create models that can be used to render convincing imagery [55]. However, this task is mainly the domain of novel view synthesis, and 3D reconstruction techniques primarily focus on geometric accuracy. 
In contrast, our approach does not rely on large-scale SfM to produce camera poses, instead performing odome-try using various sensors on the vehicle as the images are collected [63]. 
Given a set of input images of a given scene and their camera poses, novel view synthesis seeks to render observed scene content from previously unobserved viewpoints, allowing a user to navigate through a recreated environment with high visual fidelity. 
Volumetric Scene Representations. Recent view synthesis work has focused on unifying reconstruction and rendering and learning this pipeline end-to-end, typically using a volumetric scene representation. Methods for rendering small baseline view interpolation often use feed-forward networks to learn a mapping directly from input images to an output volume [15, 77], while methods such as Neural Volumes [35] that target larger-baseline view synthesis run a global optimization over all input images to reconstruct every new scene, similar to traditional bundle adjustment. 
Neural Radiance Fields (NeRF) [40] combines this single-scene optimization setting with a neural scene representation capable of representing complex scenes much more efficiently than a discrete 3D voxel grid; however, its rendering model scales very poorly to large-scale scenes in terms of compute. Followup work has proposed making NeRF more efficient by partitioning space into smaller regions, each containing its own lightweight NeRF network [42, 47, 48]. Unlike our method, these network ensembles must be trained jointly, limiting their flexibility. Another approach is to provide extra capacity in the form of a coarse 3D grid of latent codes [34]. This approach has also been applied to compress detailed 3D shapes into neural signed distance functions [61] and to represent large scenes using occupancy networks [45]. 
Concurrent works Mega-NeRF [65] and CityNeRF [67] utilize NeRFs to represent large scenes. Mega-NeRF splits data captured from drones into multiple partitions to train specialized NeRFs. CityNeRF learns a multi-scale representation from satellite imagery. 
We build our Block-NeRF implementation on top of mip-NeRF [3], which improves aliasing issues that hurt NeRF{\textquoteright}s performance in scenes where the input images observe the scene from many different distances. We incorporate techniques from NeRF in the Wild (NeRF-W) [38], which adds a latent code per training image to handle inconsistent scene appearance when applying NeRF to landmarks from the Photo Tourism dataset. NeRF-W creates a separate NeRF for each landmark from thousands of images, whereas our approach combines many NeRFs to reconstruct a coherent large environment from millions of images. Our model also incorporates a learned camera pose refinement which has been explored in previous works [32,58,66,70,71]. 
Some NeRF-based methods use segmentation data to 
Figure 3. Our model is an extension of the model presented in mip-NeRF [3]. The first MLP f predicts the density  for a position x in space. The network also outputs a feature vector that is concatenated with viewing direction d, the exposure level, and an appearance embedding. These are fed into a second MLP fc that outputs the color for the point. We additionally train a visibility network fv to predict whether a point in space was visible in the training views, which is used for culling Block-NeRFs during inference. 
isolate and reconstruct static [68] or moving objects (such as people or cars) [43,74] across video sequences. As we focus primarily on reconstructing the environment itself, we choose to simply mask out dynamic objects during training. 
Camera simulation has become a popular data source for training and validating autonomous driving systems on interactive platforms [2,27]. Early works [13,19,50,53] synthesized data from scripted scenarios and manually created 3D assets. These methods suffered from domain mismatch and limited scene-level diversity. Several recent works tackle the simulation-to-reality gaps by minimizing the distribution shifts in the simulation and rendering pipeline. Kar et al.[26] and Devaranjan et al.[12] proposed to minimize the scene-level distribution shift from rendered outputs to real camera sensor data through a learned scenario generation framework. Richter et al.[49] leveraged intermediate rendering buffers in the graphics pipeline to improve photorealism of synthetically generated camera images. 
Towards the goal of building photo-realistic and scalable camera simulation, prior methods [9, 30, 69] leverage rich multi-sensor driving data collected during a single drive to reconstruct 3D scenes for object injection [9] and novel view synthesis [69] using modern machine learning techniques, including image GANs for 2D neural rendering. Relying on a sophisticated surfel reconstruction pipeline, SurfelGAN [69] is still susceptible to errors in graphical reconstruction and can suffer from the limited range and vertical field-of-view of LiDAR scans. In contrast to existing efforts, our work tackles the 3D rendering problem and is capable of modeling the real camera data captured from multiple drives under varying environmental conditions, such as weather and time of day, which is a prerequisite for reconstructing large-scale areas. 
3. Background 
We build upon NeRF [40] and its extension mip-NeRF [3]. Here, we summarize relevant parts of these methods. For details, please refer to the original papers. 
Neural Radiance Fields (NeRF) [40] is a coordinate-based neural scene representation that is optimized through a differentiable rendering loss to reproduce the appearance of a set of input images from known camera poses. After optimization, the NeRF model can be used to render previously unseen viewpoints. 
The NeRF scene representation is a pair of multilayer perceptrons (MLPs). The first MLP f takes in a 3D position x and outputs volume density  and a feature vector. This feature vector is concatenated with a 2D viewing direction d and fed into the second MLP fc, which outputs an RGB color c. This architecture ensures that the output color can vary when observed from different angles, allowing NeRF to represent reflections and glossy materials, but that the underlying geometry represented by  is only a function of position. 
Each pixel in an image corresponds to a ray r(t)= o + td through 3D space. To calculate the color of r, NeRF randomly samples distances \{ti\}N i=0 along the ray and passes the points r(ti) and direction d through its MLPs to calculate i and ci. The resulting output color is 
(1) 
(2) 
The full implementation of NeRF iteratively resamples the points ti (by treating the weights wi as a probability distribution) in order to better concentrate samples in areas of high density. 
To enable the NeRF MLPs to represent higher frequency detail [62], the inputs x and d are each preprocessed by a componentwise sinusoidal positional encoding PE: 
where Lis the number of levels of positional encoding. 
(4) 
referred to as an integrated positional encoding. 
4. Method 
Training a single NeRF does not scale when trying to represent scenes as large as cities. We instead propose splitting the environment into a set of Block-NeRFs that can be independently trained in parallel and composited during inference. This independence enables the ability to expand the environment with additional Block-NeRFs or update blocks without retraining the entire environment (see Figure 1). We dynamically select relevant Block-NeRFs for rendering, which are then composited in a smooth manner when traversing the scene. To aid with this compositing, we optimize the appearances codes to match lighting conditions and use interpolation weights computed based on each Block-NeRF{\textquoteright}s distance to the novel view. 
The individual Block-NeRFs should be arranged such that they collectively achieve full coverage of the target environment. We typically place one Block-NeRF at each intersection, covering the intersection itself and any connected street 75\% of the way until it converges into the next intersection (see Figure 1). This results in a 50\% overlap between any two adjacent blocks on the connecting street segment, making appearance alignment easier between them. We make sure to train each Block-NeRF on data that is confined to a geographic area. This can be automated and only relies on basic map data, such as OpenStreetMap [22]. 
Other placement heuristics are conceivable. For example, for some of our experiments, we place Block-NeRFs along a street segment at uniform distances and define the block size to be a sphere around the origin of the blocks (see Figure 2). 
Given that different parts of our data may be captured under different environmental conditions, we follow NeRF-W [38] and use Generative Latent Optimization [5] to optimize per-image appearance embedding vectors, as shown in Figure 3. This allows the NeRF to explain away several appearance-changing conditions, such as varying weather and lighting. We can additionally manipulate these appearance embeddings to interpolate between different conditions observed in the training data (such as cloudy versus clear skies, or 
Figure 4. The appearance codes allow the model to represent different lighting and weather conditions. 
day and night). Examples of rendering with different appearances can be seen in Figure 4. In {\textsection} 4.3.3, we use test-time optimization over these embeddings to match the appearance of adjacent Block-NeRFs, which is important when combining multiple renderings. 
Although we assume that camera poses are provided, we find it advantageous to learn regularized pose offsets for further alignment. Pose refinement has been explored in previous NeRF based models [32,58,66,71]. These offsets are learned per driving segment and include both a translation and a rotation component. We optimize these offsets jointly with the NeRF itself, significantly regularizing the offsets in the early phase of training to allow the network to first learn a rough structure prior to modifying the poses. 
Training images may be captured across a wide range of exposure levels, which can impact NeRF training if left unaccounted for. We find that feeding the camera exposure information to the appearance prediction part of the model allows the NeRF to compensate for the visual differences (see Figure 3). Specifically, the exposure information is processed as  PE(shutter speed  analog gain/t) where  PE is a sinusoidal positional encoding with 4 levels, and t is a scaling factor (we use 1,000 in practice). An example of different learned exposures can be found in Figure 5. 
While the appearance embeddings account for variation in appearance, we assume that the scene geometry is consistent across the training data. Movable objects (e.g. cars, pedestrians) typically violate this assumption. We therefore use a semantic segmentation model [10] to ignore masks of common movable objects during training. Note that this does not account for changes in otherwise static parts of the environment, e.g. construction, it accommodates most common types of geometric inconsistency. 
When merging multiple Block-NeRFs, it can be useful to know whether a specific region of space was visible to a given NeRF during training. We extend our model with an additional small MLP fv that is trained to learn an approximation of the visibility of a sampled point (see Figure 3). For each sample along a training ray, fv takes in the location and view direction and regresses the corresponding transmittance of the point (Ti in Equation 2). The model is trained alongside f, which provides supervision. Transmittance represents how visible a point is from a particular input camera: points in free space or on the surface of the first intersected object will have transmittance near 1, and points inside or behind the first visible object will have transmittance near 0. If a point is seen from some viewpoints but not others, the regressed transmittance value will be the average over all training cameras and lie between zero and one, indicating that the point is partially observed. Our visibility prediction is similar to the visibility fields proposed by Srinivasan et al.[57]. However, they used an MLP to predict visibility to environment lighting to recover a relightable NeRF model, while we predict visibility to training rays. 
The visibility network is small and can be run independently from the color and density networks. This proves useful when merging multiple NeRFs, since it can help to determine whether a specific NeRF is likely to produce meaningful outputs for a given location, as explained in {\textsection} 4.3.1. The visibility predictions can also be used to determine locations to perform appearance matching between two NeRFs, as detailed in {\textsection} 4.3.3. 
The environment can be composed of an arbitrary number of Block-NeRFs. For efficiency, we utilize two filtering mechanisms to only render relevant blocks for the given target viewpoint. We only consider Block-NeRFs that are within a set radius of the target viewpoint. Additionally, for each of these candidates, we compute the associated visibility. If the mean visibility is below a threshold, we discard the Block-NeRF. An example of visibility filtering is provided in Figure 2. Visibility can be computed quickly because its network is independent of the color network, and 
Figure 5. Our model is conditioned on exposure, which helps account for exposure changes present in the training data. This allows users to alter the appearance of the output images in a human-interpretable manner during inference. 
it does not need to be rendered at the target image resolution. After filtering, there are typically one to three Block-NeRFs left to merge. 
We render color images from each of the filtered Block-NeRFs and interpolate between them using inverse distance weighting between the camera origin c and the centers xi of each Block-NeRF. Specifically, we calculate the respective weights as wi / distance(c, xi)p , where p influences the rate of blending between Block-NeRF renders. The interpolation is done in 2D image space and produces smooth transitions between Block-NeRFs. We also explore other interpolation methods in {\textsection} 5.4. 
We can control the appearance of our learned models by an appearance latent code after the Block-NeRF has been trained. These codes are randomly initialized during training and the same code therefore typically leads to different appearances when fed into different Block-NeRFs. This is undesirable when compositing as it may lead to inconsistencies between views. Given a target appearance in one of the Block-NeRFs, we match its appearance in the remaining blocks. To this end, we first select a 3D matching location between pairs of adjacent Block-NeRFs. The visibility prediction at this location should be high for both Block-NeRFs. Given the matching location, we freeze the Block-NeRF network weights and only optimize the appearance code of the target in order to reduce the ` 2 loss between the respective area renders. This optimization is quick, converging within 100 iterations. While not necessarily yielding perfect alignment, this procedure aligns most global and low-frequency attributes of the scene, such as time of day, color balance, and weather, which is a prerequisite for successful compositing. Figure 6 shows an example optimization, where appearance matching turns a daytime scene into nighttime to match the adjacent Block-NeRF. Starting from a root Block-NeRF, we propagate the optimized appearance through the scene by 
iteratively optimizing the appearance of its neighbors. If multiple blocks surrounding a target Block-NeRF have already been optimized, we consider each of them when computing the loss. 
5. Results and Experiments 
In this section we will discuss our datasets and experiments. We provide the architectural and optimization specifics in the supplement. The supplement also provides comparisons to reconstructions from COLMAP [54], a traditional Structure from Motion approach. This reconstruction is sparse and fails to represent reflective surfaces and the sky. 
We perform experiments on datasets that we collected for novel view synthesis of large-scale scenes using data collection vehicles driving on public roads. Existing public large-scale driving datasets are not designed for the task of view synthesis. For example, some datasets lack sufficient camera coverage (e.g., KITTI [21], Cityscapes [11]) or prioritize visual diversity over repeated observations of a target area (e.g., NuScenes [7], Waymo Open Dataset [60], Argov-erse [8]). Instead, these datasets are typically designed for tasks such as object detection and tracking. 
Our dataset includes both long-term sequence data (100 s or more) and distinct sequences captured repeatedly in a particular target area over a period of several months. We use image data captured by 12 cameras, where 8 cameras mounted on the roof of the car provide a 360{\textdegree} surround view, and 4 cameras located at the front of the vehicle point forward and sideways. Each camera captures images at 10 Hz and stores a scalar exposure value. The vehicle pose is known and all cameras are calibrated. We calculate the corresponding camera ray origins and directions in a common coordinate system, accounting for the rolling shutter of the cameras. As described in {\textsection} 4.2.4, we use a semantic segmentation model [10] to detect movable objects. 
San Francisco Alamo Square Dataset. We select San Francisco{\textquoteright}s Alamo Square neighborhood as the target area for our scalability experiments. The dataset spans an area of approximately 960 m  570 m, and was recorded in June, July, and August of 2021. We divide this dataset into 35 Block-NeRFs. Example renderings and Block-NeRF placements can be seen in Figure 1. To best appreciate the scale of the reconstruction, please refer to supplementary videos. Each Block-NeRF was trained on data from 38 to 48 different data collection runs, adding up to a total driving time of 18 to 28 minutes each. After filtering out some redundant image captures (e.g. stationary captures), each Block-NeRF is trained on between 64,575 to 108,216 images. The overall dataset is composed of 13.4h of driving time sourced from 1,330 different data collection runs, with a total of 2,818,745 training images. 
Figure 6. When rendering scenes based on multiple Block-NeRFs, we use appearance matching to obtain a consistent appearance across the scene. Given a fixed target appearance for one of the Block-NeRFs (left image), we optimize the appearances of the adjacent Block-NeRFs to match. In this example, appearance matching produces a consistent night appearance across Block-NeRFs. 
San Francisco Mission Bay Dataset. We choose San Francisco{\textquoteright}s Mission Bay District as the target area for our baseline, block size, and placement experiments. Mission Bay is an urban environment with challenging geometry and reflective facades. We identified a long stretch on Third Street with far-range visibility, making it an interesting test case. Notably, this dataset was recorded in a single capture in November 2020, with consistent environmental conditions allowing for simple evaluation. This dataset was recorded over 100 s, in which the data collection vehicle traveled 1.08 km and captured 12,000 total images from 12 cameras. We will release this single-capture dataset to aid reproducibility. 
Table 1. Ablations of different Block-NeRF components on a single intersection in the Alamo Square dataset. We show the performance of mip-NeRF as a baseline, as well as the effect of removing individual components from our method. 
We ablate our model modifications on a single intersection from the Alamo Square dataset. We report PSNR, SSIM, and LPIPS [76] metrics for the test image reconstructions in Table 1. The test images are split in half vertically, with the appearance embeddings being optimized on one half and tested on the other. We also provide qualitative examples in Figure 7. Mip-NeRF alone fails to properly reconstruct the scene and is prone to adding non-existent geometry and cloudy artifacts to explain the differences in appearance. When our method is not trained with appearance embeddings, these artifacts are still present. If our method is not trained with pose optimization, the resulting scene is blurrier and can contain duplicated objects due to pose misalignment. Finally, the exposure input marginally improves the reconstruction, but more importantly provides us with the ability 
to change the exposure during inference. 
Table 2. Comparison of different numbers of Block-NeRFs for reconstructing the Mission Bay dataset. Splitting the scene into multiple Block-NeRFs improves the reconstruction accuracy, even when holding the total number of weights constant (bottom section). The number of blocks determines the size of the area each block is trained on and the relative compute expense at inference time. 
We compare performance on our Mission Bay dataset versus the number of Block-NeRFs used. We show details in Table 2, where depending on granularity, the Block-NeRF sizes range from as small as 54 m to as large as 544 m.We ensure that each pair of adjacent blocks overlaps by 50\% and compare other overlap percentages in the supplement. All were evaluated on the same set of held-out test images spanning the entire trajectory. We consider two regimes, one where each Block-NeRF contains the same number of weights (top section) and one where the total number of weights across all Block-NeRFs is fixed (bottom section). In both cases, we observe that increasing the number of models improves the reconstruction metrics. In terms of computational expense, parallelization during training is trivial as each model can be optimized independently across devices. At inference, our method only requires rendering Block-NeRFs near the target view. Depending on the scene and NeRF layout, we typically render between one to three NeRFs. We report the relative compute expense in each setting without assuming any parallelization, which would also be possible and lead to an additional speed-up. We find that splitting the scene into multiple lower capacity models 
Figure 7. Model ablation results on multi segment data. Appearance embeddings help the network avoid adding cloudy geometry to explain away changes in the environment like weather and lighting. Removing exposure slightly decreases the accuracy. The pose optimization helps sharpen the results and removes ghosting from repeated objects, as observed with the telephone pole in the first row. 
can reduce the overall computational cost as not all of the models need to be evaluated (see bottom section of Table 2). 
Table 3. Comparison of interpolation methods. For our flythrough video results, we opt for 2D inverse distance weighting (IDW) as it produces temporally consistent results. 
We explore interpolation techniques in Table 3. The simple method of only rendering the nearest Block-NeRF to the camera requires the least amount of compute but results in harsh jumps when transitioning between blocks. These transitions can be smoothed by using inverse distance weighting (IDW) between the camera and Block-NeRF centers, as described in {\textsection} 4.3.2. We also explored a variant of IDW where the interpolation was performed over projected 3D points predicted by the expected Block-NeRF depth. This method suffers when the depth prediction is incorrect, leading to artifacts and temporal incoherence. 
Finally, we experiment with weighing the Block-NeRFs based on per-pixel and per-image predicted visibility. This produces sharper reconstructions of further-away areas but is prone to temporal inconsistency. Therefore, these methods are best used only when rendering still images. We provide further details in the supplement. 
6. Limitations and Future Work 
7. Conclusion 
We propose Block-NeRF, a method that reconstructs arbitrarily large environments using NeRFs. We demonstrate the efficacy of the method by building an entire neighborhood in San Francisco from 2.8M images, forming the largest neural scene representation to date. We accomplish this scale by splitting our representation into multiple blocks that can be optimized independently. At such a scale, the data collected will necessarily have transient objects and variations in appearance, which we account for by modifying the underlying NeRF architecture. We hope that this can inspire future work in large-scale scene reconstruction using modern neural rendering methods. 
References 